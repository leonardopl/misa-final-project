{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.26.0\n",
      "Pytorch version: 2.1.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: c:\\Users\\<username>\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.11.3\n",
      "Pillow version: 10.0.1\n",
      "Tensorboard version: 2.15.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.16.1\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.1.3\n",
      "einops version: 0.7.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from monai.apps.nnunet import nnUNetV2Runner, analyze_data\n",
    "from monai.config import print_config\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup working dir and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"D:\\\\Files\\\\Projects\\\\MAIA\\\\UdG\\\\MISA\\\\Project\\\\nnunetv2\"\n",
    "dataroot_dir = os.path.join(work_dir, \"dataset\")\n",
    "datalist_file = os.path.join(work_dir, \"misa_datalist.json\")\n",
    "datalist = json.load(open(datalist_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-11 15:41:48,170 - INFO - num_input_channels: 1\n",
      "2024-01-11 15:41:49,188 - INFO - num_foreground_classes: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_data(datalist, dataroot_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run nnUNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = nnUNetV2Runner(\n",
    "    work_dir=work_dir,\n",
    "    input_config={\n",
    "        \"modality\": \"MRI\",\n",
    "        \"datalist\": datalist_file,\n",
    "        \"dataroot\": dataroot_dir,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 16:52:06,351 - INFO - num_input_channels: 1\n",
      "2023-12-31 16:52:07,205 - INFO - num_foreground_classes: 3\n",
      "2023-12-31 16:52:07,210 - INFO - converting data section: training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 16:52:12,404 - INFO - converting data section: testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 16:52:15,094 - INFO - Fingerprint extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset001_dataset\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 16:52:20,300 - INFO - Experiment planning...\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': array([160, 160]), 'median_image_size_in_voxels': array([139. , 144.5]), 'spacing': array([0.9375, 0.9375]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': array([112, 128, 160]), 'median_image_size_in_voxels': array([115. , 139. , 144.5]), 'spacing': array([1.5   , 0.9375, 0.9375]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\nnUNetPlans.json\n",
      "2023-12-31 16:52:20,878 - INFO - Preprocessing...\n",
      "Preprocessing dataset Dataset001_dataset\n",
      "Configuration: 2d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 3d_fullres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset001_dataset. Skipping.\n",
      "2023-12-31 16:52:37,224 - INFO - number of GPUs is 1, device ids are (0,)\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2023-12-31 16:52:38.249817: unpacking dataset...\n",
      "2023-12-31 16:52:38.633070: unpacking done...\n",
      "2023-12-31 16:52:38.639071: do_dummy_2d_data_aug: False\n",
      "2023-12-31 16:52:38.644070: Creating new 5-fold cross-validation split...\n",
      "2023-12-31 16:52:38.649078: Desired fold for training: 0\n",
      "2023-12-31 16:52:38.653078: This split has 8 training and 2 validation cases.\n",
      "2023-12-31 16:52:38.702520: Unable to plot network architecture:\n",
      "2023-12-31 16:52:38.707527: No module named 'hiddenlayer'\n",
      "2023-12-31 16:52:38.731520: \n",
      "2023-12-31 16:52:38.736527: Epoch 0\n",
      "2023-12-31 16:52:38.743519: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2023-12-31 16:55:01.919475: train_loss -0.355\n",
      "2023-12-31 16:55:01.927325: val_loss -0.648\n",
      "2023-12-31 16:55:01.936149: Pseudo dice [0.8508, 0.8823, 0.8736]\n",
      "2023-12-31 16:55:01.943152: Epoch time: 143.19 s\n",
      "2023-12-31 16:55:01.952147: Yayy! New best EMA pseudo Dice: 0.8689\n",
      "2023-12-31 16:55:03.303614: \n",
      "2023-12-31 16:55:03.313848: Epoch 1\n",
      "2023-12-31 16:55:03.317931: Current learning rate: 0.00999\n",
      "2023-12-31 16:57:09.019357: train_loss -0.6812\n",
      "2023-12-31 16:57:09.029359: val_loss -0.7332\n",
      "2023-12-31 16:57:09.037360: Pseudo dice [0.8859, 0.9122, 0.9034]\n",
      "2023-12-31 16:57:09.045081: Epoch time: 125.72 s\n",
      "2023-12-31 16:57:09.052226: Yayy! New best EMA pseudo Dice: 0.8721\n",
      "2023-12-31 16:57:10.775054: \n",
      "2023-12-31 16:57:10.781056: Epoch 2\n",
      "2023-12-31 16:57:10.787053: Current learning rate: 0.00998\n",
      "2023-12-31 16:59:19.151940: train_loss -0.7471\n",
      "2023-12-31 16:59:19.164940: val_loss -0.7736\n",
      "2023-12-31 16:59:19.175940: Pseudo dice [0.8967, 0.9231, 0.9158]\n",
      "2023-12-31 16:59:19.184941: Epoch time: 128.38 s\n",
      "2023-12-31 16:59:19.194940: Yayy! New best EMA pseudo Dice: 0.876\n",
      "2023-12-31 16:59:20.942018: \n",
      "2023-12-31 16:59:20.948025: Epoch 3\n",
      "2023-12-31 16:59:20.953018: Current learning rate: 0.00997\n",
      "2023-12-31 17:01:28.394339: train_loss -0.7728\n",
      "2023-12-31 17:01:28.403345: val_loss -0.7863\n",
      "2023-12-31 17:01:28.412393: Pseudo dice [0.8998, 0.9238, 0.9162]\n",
      "2023-12-31 17:01:28.420914: Epoch time: 127.45 s\n",
      "2023-12-31 17:01:28.428915: Yayy! New best EMA pseudo Dice: 0.8798\n",
      "2023-12-31 17:01:29.812671: \n",
      "2023-12-31 17:01:29.817714: Epoch 4\n",
      "2023-12-31 17:01:29.822729: Current learning rate: 0.00996\n",
      "2023-12-31 17:03:36.975670: train_loss -0.7615\n",
      "2023-12-31 17:03:36.984177: val_loss -0.7859\n",
      "2023-12-31 17:03:36.997823: Pseudo dice [0.8996, 0.9247, 0.9184]\n",
      "2023-12-31 17:03:37.007822: Epoch time: 127.16 s\n",
      "2023-12-31 17:03:37.017820: Yayy! New best EMA pseudo Dice: 0.8832\n",
      "2023-12-31 17:03:38.411307: \n",
      "2023-12-31 17:03:38.417544: Epoch 5\n",
      "2023-12-31 17:03:38.422062: Current learning rate: 0.00995\n",
      "2023-12-31 17:05:47.314372: train_loss -0.7906\n",
      "2023-12-31 17:05:47.325582: val_loss -0.7919\n",
      "2023-12-31 17:05:47.335791: Pseudo dice [0.8988, 0.9291, 0.9195]\n",
      "2023-12-31 17:05:47.347897: Epoch time: 128.9 s\n",
      "2023-12-31 17:05:47.357898: Yayy! New best EMA pseudo Dice: 0.8865\n",
      "2023-12-31 17:05:49.133532: \n",
      "2023-12-31 17:05:49.141910: Epoch 6\n",
      "2023-12-31 17:05:49.151091: Current learning rate: 0.00995\n",
      "2023-12-31 17:07:56.976609: train_loss -0.8022\n",
      "2023-12-31 17:07:56.985611: val_loss -0.8129\n",
      "2023-12-31 17:07:56.994618: Pseudo dice [0.9157, 0.9352, 0.928]\n",
      "2023-12-31 17:07:57.001619: Epoch time: 127.84 s\n",
      "2023-12-31 17:07:57.009418: Yayy! New best EMA pseudo Dice: 0.8904\n",
      "2023-12-31 17:07:58.402139: \n",
      "2023-12-31 17:07:58.407141: Epoch 7\n",
      "2023-12-31 17:07:58.411674: Current learning rate: 0.00994\n",
      "2023-12-31 17:10:06.868116: train_loss -0.8074\n",
      "2023-12-31 17:10:06.879339: val_loss -0.8179\n",
      "2023-12-31 17:10:06.889581: Pseudo dice [0.9159, 0.9361, 0.9299]\n",
      "2023-12-31 17:10:06.899583: Epoch time: 128.47 s\n",
      "2023-12-31 17:10:06.909539: Yayy! New best EMA pseudo Dice: 0.8941\n",
      "2023-12-31 17:10:08.864835: \n",
      "2023-12-31 17:10:08.870829: Epoch 8\n",
      "2023-12-31 17:10:08.875826: Current learning rate: 0.00993\n",
      "2023-12-31 17:12:18.232368: train_loss -0.8129\n",
      "2023-12-31 17:12:18.243367: val_loss -0.8097\n",
      "2023-12-31 17:12:18.254474: Pseudo dice [0.9106, 0.9328, 0.9255]\n",
      "2023-12-31 17:12:18.266501: Epoch time: 129.37 s\n",
      "2023-12-31 17:12:18.276670: Yayy! New best EMA pseudo Dice: 0.897\n",
      "2023-12-31 17:12:20.501623: \n",
      "2023-12-31 17:12:20.509627: Epoch 9\n",
      "2023-12-31 17:12:20.519636: Current learning rate: 0.00992\n",
      "2023-12-31 17:14:27.715779: train_loss -0.8157\n",
      "2023-12-31 17:14:27.723797: val_loss -0.8245\n",
      "2023-12-31 17:14:27.732297: Pseudo dice [0.9178, 0.9385, 0.9295]\n",
      "2023-12-31 17:14:27.739706: Epoch time: 127.22 s\n",
      "2023-12-31 17:14:27.746724: Yayy! New best EMA pseudo Dice: 0.9002\n",
      "2023-12-31 17:14:29.213408: \n",
      "2023-12-31 17:14:29.218424: Epoch 10\n",
      "2023-12-31 17:14:29.223913: Current learning rate: 0.00991\n",
      "2023-12-31 17:16:34.981893: train_loss -0.8219\n",
      "2023-12-31 17:16:34.991925: val_loss -0.8246\n",
      "2023-12-31 17:16:35.001379: Pseudo dice [0.9193, 0.9377, 0.9309]\n",
      "2023-12-31 17:16:35.010384: Epoch time: 125.77 s\n",
      "2023-12-31 17:16:35.018393: Yayy! New best EMA pseudo Dice: 0.9031\n",
      "2023-12-31 17:16:36.324676: \n",
      "2023-12-31 17:16:36.330668: Epoch 11\n",
      "2023-12-31 17:16:36.336205: Current learning rate: 0.0099\n",
      "2023-12-31 17:18:41.930868: train_loss -0.8274\n",
      "2023-12-31 17:18:41.940869: val_loss -0.8289\n",
      "2023-12-31 17:18:41.948868: Pseudo dice [0.9197, 0.9399, 0.9348]\n",
      "2023-12-31 17:18:41.956868: Epoch time: 125.61 s\n",
      "2023-12-31 17:18:41.962868: Yayy! New best EMA pseudo Dice: 0.9059\n",
      "2023-12-31 17:18:43.160781: \n",
      "2023-12-31 17:18:43.165787: Epoch 12\n",
      "2023-12-31 17:18:43.170864: Current learning rate: 0.00989\n",
      "2023-12-31 17:20:47.700635: train_loss -0.8306\n",
      "2023-12-31 17:20:47.709633: val_loss -0.8309\n",
      "2023-12-31 17:20:47.719634: Pseudo dice [0.9179, 0.9408, 0.935]\n",
      "2023-12-31 17:20:47.728633: Epoch time: 124.54 s\n",
      "2023-12-31 17:20:47.734633: Yayy! New best EMA pseudo Dice: 0.9085\n",
      "2023-12-31 17:20:48.964998: \n",
      "2023-12-31 17:20:48.971065: Epoch 13\n",
      "2023-12-31 17:20:48.976064: Current learning rate: 0.00988\n",
      "2023-12-31 17:22:53.600737: train_loss -0.8322\n",
      "2023-12-31 17:22:53.608738: val_loss -0.8198\n",
      "2023-12-31 17:22:53.617611: Pseudo dice [0.918, 0.9348, 0.9288]\n",
      "2023-12-31 17:22:53.623619: Epoch time: 124.64 s\n",
      "2023-12-31 17:22:53.630619: Yayy! New best EMA pseudo Dice: 0.9103\n",
      "2023-12-31 17:22:54.887617: \n",
      "2023-12-31 17:22:54.893629: Epoch 14\n",
      "2023-12-31 17:22:54.898628: Current learning rate: 0.00987\n",
      "2023-12-31 17:24:59.705333: train_loss -0.8336\n",
      "2023-12-31 17:24:59.717338: val_loss -0.8335\n",
      "2023-12-31 17:24:59.724338: Pseudo dice [0.9187, 0.941, 0.9357]\n",
      "2023-12-31 17:24:59.733845: Epoch time: 124.82 s\n",
      "2023-12-31 17:24:59.741846: Yayy! New best EMA pseudo Dice: 0.9125\n",
      "2023-12-31 17:25:01.178098: \n",
      "2023-12-31 17:25:01.184153: Epoch 15\n",
      "2023-12-31 17:25:01.189095: Current learning rate: 0.00986\n",
      "2023-12-31 17:27:05.885947: train_loss -0.8358\n",
      "2023-12-31 17:27:05.894945: val_loss -0.827\n",
      "2023-12-31 17:27:05.904461: Pseudo dice [0.9124, 0.9391, 0.9334]\n",
      "2023-12-31 17:27:05.914454: Epoch time: 124.71 s\n",
      "2023-12-31 17:27:05.923461: Yayy! New best EMA pseudo Dice: 0.9141\n",
      "2023-12-31 17:27:07.221958: \n",
      "2023-12-31 17:27:07.227955: Epoch 16\n",
      "2023-12-31 17:27:07.231975: Current learning rate: 0.00986\n",
      "2023-12-31 17:29:11.851350: train_loss -0.8405\n",
      "2023-12-31 17:29:11.859350: val_loss -0.8271\n",
      "2023-12-31 17:29:11.867350: Pseudo dice [0.9186, 0.9379, 0.9328]\n",
      "2023-12-31 17:29:11.874350: Epoch time: 124.63 s\n",
      "2023-12-31 17:29:11.880350: Yayy! New best EMA pseudo Dice: 0.9156\n",
      "2023-12-31 17:29:13.148134: \n",
      "2023-12-31 17:29:13.154133: Epoch 17\n",
      "2023-12-31 17:29:13.159131: Current learning rate: 0.00985\n",
      "2023-12-31 17:31:19.555419: train_loss -0.8412\n",
      "2023-12-31 17:31:19.565418: val_loss -0.8227\n",
      "2023-12-31 17:31:19.575419: Pseudo dice [0.9139, 0.9369, 0.9318]\n",
      "2023-12-31 17:31:19.583425: Epoch time: 126.41 s\n",
      "2023-12-31 17:31:19.593427: Yayy! New best EMA pseudo Dice: 0.9168\n",
      "2023-12-31 17:31:21.170389: \n",
      "2023-12-31 17:31:21.176389: Epoch 18\n",
      "2023-12-31 17:31:21.182380: Current learning rate: 0.00984\n",
      "2023-12-31 17:33:28.430037: train_loss -0.8426\n",
      "2023-12-31 17:33:28.442187: val_loss -0.8277\n",
      "2023-12-31 17:33:28.453197: Pseudo dice [0.9162, 0.9377, 0.9327]\n",
      "2023-12-31 17:33:28.464354: Epoch time: 127.26 s\n",
      "2023-12-31 17:33:28.477353: Yayy! New best EMA pseudo Dice: 0.918\n",
      "2023-12-31 17:33:29.977537: \n",
      "2023-12-31 17:33:29.984545: Epoch 19\n",
      "2023-12-31 17:33:29.990547: Current learning rate: 0.00983\n",
      "2023-12-31 17:35:37.712513: train_loss -0.8422\n",
      "2023-12-31 17:35:37.724514: val_loss -0.8294\n",
      "2023-12-31 17:35:37.736805: Pseudo dice [0.9152, 0.9379, 0.934]\n",
      "2023-12-31 17:35:37.745823: Epoch time: 127.74 s\n",
      "2023-12-31 17:35:37.757045: Yayy! New best EMA pseudo Dice: 0.9191\n",
      "2023-12-31 17:35:39.369861: \n",
      "2023-12-31 17:35:39.376848: Epoch 20\n",
      "2023-12-31 17:35:39.383847: Current learning rate: 0.00982\n",
      "2023-12-31 17:37:48.787446: train_loss -0.8443\n",
      "2023-12-31 17:37:48.802447: val_loss -0.8351\n",
      "2023-12-31 17:37:48.815379: Pseudo dice [0.9217, 0.9413, 0.9367]\n",
      "2023-12-31 17:37:48.826893: Epoch time: 129.42 s\n",
      "2023-12-31 17:37:48.837404: Yayy! New best EMA pseudo Dice: 0.9205\n",
      "2023-12-31 17:37:50.855279: \n",
      "2023-12-31 17:37:50.862292: Epoch 21\n",
      "2023-12-31 17:37:50.872278: Current learning rate: 0.00981\n",
      "2023-12-31 17:40:00.414761: train_loss -0.8479\n",
      "2023-12-31 17:40:00.426760: val_loss -0.8374\n",
      "2023-12-31 17:40:00.438253: Pseudo dice [0.924, 0.9406, 0.9379]\n",
      "2023-12-31 17:40:00.449254: Epoch time: 129.56 s\n",
      "2023-12-31 17:40:00.464717: Yayy! New best EMA pseudo Dice: 0.9219\n",
      "2023-12-31 17:40:02.309262: \n",
      "2023-12-31 17:40:02.316814: Epoch 22\n",
      "2023-12-31 17:40:02.322818: Current learning rate: 0.0098\n",
      "2023-12-31 17:42:09.772402: train_loss -0.8497\n",
      "2023-12-31 17:42:09.785910: val_loss -0.8318\n",
      "2023-12-31 17:42:09.795919: Pseudo dice [0.9146, 0.9404, 0.9365]\n",
      "2023-12-31 17:42:09.805918: Epoch time: 127.46 s\n",
      "2023-12-31 17:42:09.812909: Yayy! New best EMA pseudo Dice: 0.9228\n",
      "2023-12-31 17:42:11.153804: \n",
      "2023-12-31 17:42:11.159801: Epoch 23\n",
      "2023-12-31 17:42:11.164796: Current learning rate: 0.00979\n",
      "2023-12-31 17:44:22.645278: train_loss -0.8496\n",
      "2023-12-31 17:44:22.683516: val_loss -0.834\n",
      "2023-12-31 17:44:22.708742: Pseudo dice [0.9168, 0.9416, 0.9372]\n",
      "2023-12-31 17:44:22.736052: Epoch time: 131.49 s\n",
      "2023-12-31 17:44:22.750388: Yayy! New best EMA pseudo Dice: 0.9237\n",
      "2023-12-31 17:44:25.494997: \n",
      "2023-12-31 17:44:25.511331: Epoch 24\n",
      "2023-12-31 17:44:25.526091: Current learning rate: 0.00978\n",
      "2023-12-31 17:46:36.004929: train_loss -0.8522\n",
      "2023-12-31 17:46:36.017927: val_loss -0.831\n",
      "2023-12-31 17:46:36.030255: Pseudo dice [0.9179, 0.9394, 0.9365]\n",
      "2023-12-31 17:46:36.041343: Epoch time: 130.52 s\n",
      "2023-12-31 17:46:36.052483: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2023-12-31 17:46:37.994841: \n",
      "2023-12-31 17:46:38.005359: Epoch 25\n",
      "2023-12-31 17:46:38.012354: Current learning rate: 0.00977\n",
      "2023-12-31 17:48:46.131025: train_loss -0.8524\n",
      "2023-12-31 17:48:46.149023: val_loss -0.8351\n",
      "2023-12-31 17:48:46.159528: Pseudo dice [0.9189, 0.9416, 0.9376]\n",
      "2023-12-31 17:48:46.171190: Epoch time: 128.14 s\n",
      "2023-12-31 17:48:46.183197: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2023-12-31 17:48:47.856908: \n",
      "2023-12-31 17:48:47.864897: Epoch 26\n",
      "2023-12-31 17:48:47.874899: Current learning rate: 0.00977\n",
      "2023-12-31 17:50:58.269339: train_loss -0.8543\n",
      "2023-12-31 17:50:58.353916: val_loss -0.8362\n",
      "2023-12-31 17:50:58.408848: Pseudo dice [0.9191, 0.9415, 0.9381]\n",
      "2023-12-31 17:50:58.493604: Epoch time: 130.41 s\n",
      "2023-12-31 17:50:58.560813: Yayy! New best EMA pseudo Dice: 0.926\n",
      "2023-12-31 17:51:01.505633: \n",
      "2023-12-31 17:51:01.516767: Epoch 27\n",
      "2023-12-31 17:51:01.527915: Current learning rate: 0.00976\n",
      "2023-12-31 17:53:09.466222: train_loss -0.8574\n",
      "2023-12-31 17:53:09.478785: val_loss -0.839\n",
      "2023-12-31 17:53:09.489948: Pseudo dice [0.9193, 0.9434, 0.9393]\n",
      "2023-12-31 17:53:09.500021: Epoch time: 127.96 s\n",
      "2023-12-31 17:53:09.508024: Yayy! New best EMA pseudo Dice: 0.9268\n",
      "2023-12-31 17:53:10.780299: \n",
      "2023-12-31 17:53:10.791948: Epoch 28\n",
      "2023-12-31 17:53:10.796948: Current learning rate: 0.00975\n",
      "2023-12-31 17:55:18.551664: train_loss -0.8544\n",
      "2023-12-31 17:55:18.561662: val_loss -0.8327\n",
      "2023-12-31 17:55:18.572669: Pseudo dice [0.9184, 0.9395, 0.9379]\n",
      "2023-12-31 17:55:18.583177: Epoch time: 127.77 s\n",
      "2023-12-31 17:55:18.592182: Yayy! New best EMA pseudo Dice: 0.9273\n",
      "2023-12-31 17:55:20.142539: \n",
      "2023-12-31 17:55:20.154538: Epoch 29\n",
      "2023-12-31 17:55:20.160258: Current learning rate: 0.00974\n",
      "2023-12-31 17:57:27.696059: train_loss -0.856\n",
      "2023-12-31 17:57:27.729348: val_loss -0.8387\n",
      "2023-12-31 17:57:27.743753: Pseudo dice [0.9207, 0.9428, 0.9403]\n",
      "2023-12-31 17:57:27.755846: Epoch time: 127.55 s\n",
      "2023-12-31 17:57:27.767855: Yayy! New best EMA pseudo Dice: 0.9281\n",
      "2023-12-31 17:57:29.495952: \n",
      "2023-12-31 17:57:29.503067: Epoch 30\n",
      "2023-12-31 17:57:29.509820: Current learning rate: 0.00973\n",
      "2023-12-31 17:59:42.662524: train_loss -0.8577\n",
      "2023-12-31 17:59:42.729039: val_loss -0.8316\n",
      "2023-12-31 17:59:42.788657: Pseudo dice [0.919, 0.9385, 0.9377]\n",
      "2023-12-31 17:59:42.828612: Epoch time: 133.17 s\n",
      "2023-12-31 17:59:42.877298: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2023-12-31 17:59:45.678211: \n",
      "2023-12-31 17:59:45.685212: Epoch 31\n",
      "2023-12-31 17:59:45.692211: Current learning rate: 0.00972\n",
      "2023-12-31 18:01:55.776566: train_loss -0.8589\n",
      "2023-12-31 18:01:55.789369: val_loss -0.8391\n",
      "2023-12-31 18:01:55.799864: Pseudo dice [0.9224, 0.9419, 0.9401]\n",
      "2023-12-31 18:01:55.810334: Epoch time: 130.1 s\n",
      "2023-12-31 18:01:55.819844: Yayy! New best EMA pseudo Dice: 0.9291\n",
      "2023-12-31 18:01:58.126203: \n",
      "2023-12-31 18:01:58.133193: Epoch 32\n",
      "2023-12-31 18:01:58.138205: Current learning rate: 0.00971\n",
      "2023-12-31 18:04:08.947825: train_loss -0.8621\n",
      "2023-12-31 18:04:08.958113: val_loss -0.8412\n",
      "2023-12-31 18:04:08.970201: Pseudo dice [0.9226, 0.9432, 0.9405]\n",
      "2023-12-31 18:04:08.982200: Epoch time: 130.82 s\n",
      "2023-12-31 18:04:08.992656: Yayy! New best EMA pseudo Dice: 0.9297\n",
      "2023-12-31 18:04:11.279154: \n",
      "2023-12-31 18:04:11.286659: Epoch 33\n",
      "2023-12-31 18:04:11.291869: Current learning rate: 0.0097\n",
      "2023-12-31 18:06:22.351148: train_loss -0.862\n",
      "2023-12-31 18:06:22.368327: val_loss -0.837\n",
      "2023-12-31 18:06:22.385995: Pseudo dice [0.919, 0.9422, 0.9393]\n",
      "2023-12-31 18:06:22.399516: Epoch time: 131.07 s\n",
      "2023-12-31 18:06:22.413521: Yayy! New best EMA pseudo Dice: 0.9301\n",
      "2023-12-31 18:06:24.833965: \n",
      "2023-12-31 18:06:24.840964: Epoch 34\n",
      "2023-12-31 18:06:24.845963: Current learning rate: 0.00969\n",
      "2023-12-31 18:08:37.801916: train_loss -0.8584\n",
      "2023-12-31 18:08:37.814916: val_loss -0.8396\n",
      "2023-12-31 18:08:37.826300: Pseudo dice [0.9224, 0.9417, 0.9395]\n",
      "2023-12-31 18:08:37.841771: Epoch time: 132.97 s\n",
      "2023-12-31 18:08:37.857114: Yayy! New best EMA pseudo Dice: 0.9305\n",
      "2023-12-31 18:08:39.816818: \n",
      "2023-12-31 18:08:39.825820: Epoch 35\n",
      "2023-12-31 18:08:39.832819: Current learning rate: 0.00968\n",
      "2023-12-31 18:10:48.392786: train_loss -0.862\n",
      "2023-12-31 18:10:48.401453: val_loss -0.8414\n",
      "2023-12-31 18:10:48.411443: Pseudo dice [0.9221, 0.9433, 0.9419]\n",
      "2023-12-31 18:10:48.418444: Epoch time: 128.58 s\n",
      "2023-12-31 18:10:48.427443: Yayy! New best EMA pseudo Dice: 0.931\n",
      "2023-12-31 18:10:49.833049: \n",
      "2023-12-31 18:10:49.839585: Epoch 36\n",
      "2023-12-31 18:10:49.844592: Current learning rate: 0.00968\n",
      "2023-12-31 18:12:59.493140: train_loss -0.8649\n",
      "2023-12-31 18:12:59.506225: val_loss -0.8338\n",
      "2023-12-31 18:12:59.517007: Pseudo dice [0.9197, 0.941, 0.9374]\n",
      "2023-12-31 18:12:59.528506: Epoch time: 129.66 s\n",
      "2023-12-31 18:12:59.542728: Yayy! New best EMA pseudo Dice: 0.9312\n",
      "2023-12-31 18:13:01.579003: \n",
      "2023-12-31 18:13:01.586006: Epoch 37\n",
      "2023-12-31 18:13:01.593125: Current learning rate: 0.00967\n",
      "2023-12-31 18:15:16.602538: train_loss -0.8634\n",
      "2023-12-31 18:15:16.614197: val_loss -0.8377\n",
      "2023-12-31 18:15:16.628204: Pseudo dice [0.923, 0.9406, 0.9397]\n",
      "2023-12-31 18:15:16.637201: Epoch time: 135.03 s\n",
      "2023-12-31 18:15:16.647204: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2023-12-31 18:15:18.838427: \n",
      "2023-12-31 18:15:18.844431: Epoch 38\n",
      "2023-12-31 18:15:18.852423: Current learning rate: 0.00966\n",
      "2023-12-31 18:17:28.529524: train_loss -0.8665\n",
      "2023-12-31 18:17:28.540581: val_loss -0.8435\n",
      "2023-12-31 18:17:28.548675: Pseudo dice [0.9203, 0.9441, 0.943]\n",
      "2023-12-31 18:17:28.557807: Epoch time: 129.69 s\n",
      "2023-12-31 18:17:28.567192: Yayy! New best EMA pseudo Dice: 0.932\n",
      "2023-12-31 18:17:30.414520: \n",
      "2023-12-31 18:17:30.420536: Epoch 39\n",
      "2023-12-31 18:17:30.425538: Current learning rate: 0.00965\n",
      "2023-12-31 18:19:40.141405: train_loss -0.8673\n",
      "2023-12-31 18:19:40.152096: val_loss -0.8395\n",
      "2023-12-31 18:19:40.162186: Pseudo dice [0.9213, 0.9427, 0.94]\n",
      "2023-12-31 18:19:40.171309: Epoch time: 129.73 s\n",
      "2023-12-31 18:19:40.180640: Yayy! New best EMA pseudo Dice: 0.9322\n",
      "2023-12-31 18:19:41.980295: \n",
      "2023-12-31 18:19:41.987288: Epoch 40\n",
      "2023-12-31 18:19:41.992281: Current learning rate: 0.00964\n",
      "2023-12-31 18:21:54.357948: train_loss -0.8674\n",
      "2023-12-31 18:21:54.495004: val_loss -0.8465\n",
      "2023-12-31 18:21:54.547674: Pseudo dice [0.925, 0.9452, 0.9418]\n",
      "2023-12-31 18:21:54.663829: Epoch time: 132.38 s\n",
      "2023-12-31 18:21:54.735445: Yayy! New best EMA pseudo Dice: 0.9327\n",
      "2023-12-31 18:21:57.721274: \n",
      "2023-12-31 18:21:57.729269: Epoch 41\n",
      "2023-12-31 18:21:57.739269: Current learning rate: 0.00963\n",
      "2023-12-31 18:24:09.172391: train_loss -0.8657\n",
      "2023-12-31 18:24:09.184432: val_loss -0.8444\n",
      "2023-12-31 18:24:09.195435: Pseudo dice [0.9232, 0.9443, 0.9422]\n",
      "2023-12-31 18:24:09.207433: Epoch time: 131.45 s\n",
      "2023-12-31 18:24:09.218432: Yayy! New best EMA pseudo Dice: 0.9331\n",
      "2023-12-31 18:24:10.923722: \n",
      "2023-12-31 18:24:10.929722: Epoch 42\n",
      "2023-12-31 18:24:10.935721: Current learning rate: 0.00962\n",
      "2023-12-31 18:26:20.593157: train_loss -0.8682\n",
      "2023-12-31 18:26:20.605155: val_loss -0.8438\n",
      "2023-12-31 18:26:20.612155: Pseudo dice [0.9209, 0.9433, 0.9419]\n",
      "2023-12-31 18:26:20.621161: Epoch time: 129.67 s\n",
      "2023-12-31 18:26:20.629157: Yayy! New best EMA pseudo Dice: 0.9334\n",
      "2023-12-31 18:26:22.068053: \n",
      "2023-12-31 18:26:22.074056: Epoch 43\n",
      "2023-12-31 18:26:22.079065: Current learning rate: 0.00961\n",
      "2023-12-31 18:28:31.258373: train_loss -0.8674\n",
      "2023-12-31 18:28:31.269373: val_loss -0.8419\n",
      "2023-12-31 18:28:31.280379: Pseudo dice [0.9221, 0.9425, 0.9403]\n",
      "2023-12-31 18:28:31.289380: Epoch time: 129.19 s\n",
      "2023-12-31 18:28:31.297896: Yayy! New best EMA pseudo Dice: 0.9335\n",
      "2023-12-31 18:28:32.883999: \n",
      "2023-12-31 18:28:32.889999: Epoch 44\n",
      "2023-12-31 18:28:32.895540: Current learning rate: 0.0096\n",
      "2023-12-31 18:30:42.350709: train_loss -0.8686\n",
      "2023-12-31 18:30:42.364710: val_loss -0.8474\n",
      "2023-12-31 18:30:42.375711: Pseudo dice [0.9248, 0.9457, 0.9429]\n",
      "2023-12-31 18:30:42.387948: Epoch time: 129.47 s\n",
      "2023-12-31 18:30:42.400452: Yayy! New best EMA pseudo Dice: 0.9339\n",
      "2023-12-31 18:30:44.661425: \n",
      "2023-12-31 18:30:44.671426: Epoch 45\n",
      "2023-12-31 18:30:44.682490: Current learning rate: 0.00959\n",
      "2023-12-31 18:32:52.555257: train_loss -0.8701\n",
      "2023-12-31 18:32:52.567604: val_loss -0.8494\n",
      "2023-12-31 18:32:52.580950: Pseudo dice [0.9288, 0.9459, 0.9432]\n",
      "2023-12-31 18:32:52.592165: Epoch time: 127.9 s\n",
      "2023-12-31 18:32:52.603518: Yayy! New best EMA pseudo Dice: 0.9345\n",
      "2023-12-31 18:32:54.531672: \n",
      "2023-12-31 18:32:54.537664: Epoch 46\n",
      "2023-12-31 18:32:54.543667: Current learning rate: 0.00959\n",
      "2023-12-31 18:35:03.732544: train_loss -0.8717\n",
      "2023-12-31 18:35:03.743967: val_loss -0.844\n",
      "2023-12-31 18:35:03.752966: Pseudo dice [0.9251, 0.9446, 0.9438]\n",
      "2023-12-31 18:35:03.764217: Epoch time: 129.2 s\n",
      "2023-12-31 18:35:03.774293: Yayy! New best EMA pseudo Dice: 0.9348\n",
      "2023-12-31 18:35:05.874294: \n",
      "2023-12-31 18:35:05.880294: Epoch 47\n",
      "2023-12-31 18:35:05.887286: Current learning rate: 0.00958\n",
      "2023-12-31 18:37:18.137840: train_loss -0.8708\n",
      "2023-12-31 18:37:18.149048: val_loss -0.8442\n",
      "2023-12-31 18:37:18.158626: Pseudo dice [0.9245, 0.9436, 0.9433]\n",
      "2023-12-31 18:37:18.166933: Epoch time: 132.26 s\n",
      "2023-12-31 18:37:18.176934: Yayy! New best EMA pseudo Dice: 0.935\n",
      "2023-12-31 18:37:19.648714: \n",
      "2023-12-31 18:37:19.654715: Epoch 48\n",
      "2023-12-31 18:37:19.659720: Current learning rate: 0.00957\n",
      "2023-12-31 18:39:29.849244: train_loss -0.871\n",
      "2023-12-31 18:39:29.861650: val_loss -0.8466\n",
      "2023-12-31 18:39:29.871649: Pseudo dice [0.9234, 0.9457, 0.9436]\n",
      "2023-12-31 18:39:29.885939: Epoch time: 130.2 s\n",
      "2023-12-31 18:39:29.901441: Yayy! New best EMA pseudo Dice: 0.9353\n",
      "2023-12-31 18:39:31.753052: \n",
      "2023-12-31 18:39:31.760044: Epoch 49\n",
      "2023-12-31 18:39:31.767043: Current learning rate: 0.00956\n",
      "2023-12-31 18:41:40.678132: train_loss -0.8743\n",
      "2023-12-31 18:41:40.688169: val_loss -0.8397\n",
      "2023-12-31 18:41:40.697191: Pseudo dice [0.9235, 0.9425, 0.9416]\n",
      "2023-12-31 18:41:40.706374: Epoch time: 128.93 s\n",
      "2023-12-31 18:41:41.123091: Yayy! New best EMA pseudo Dice: 0.9354\n",
      "2023-12-31 18:41:42.360465: \n",
      "2023-12-31 18:41:42.366466: Epoch 50\n",
      "2023-12-31 18:41:42.371471: Current learning rate: 0.00955\n",
      "2023-12-31 18:43:51.239259: train_loss -0.8748\n",
      "2023-12-31 18:43:51.252289: val_loss -0.8389\n",
      "2023-12-31 18:43:51.263802: Pseudo dice [0.9208, 0.9433, 0.9426]\n",
      "2023-12-31 18:43:51.275803: Epoch time: 128.88 s\n",
      "2023-12-31 18:43:51.286804: Yayy! New best EMA pseudo Dice: 0.9354\n",
      "2023-12-31 18:43:52.672596: \n",
      "2023-12-31 18:43:52.677598: Epoch 51\n",
      "2023-12-31 18:43:52.682596: Current learning rate: 0.00954\n",
      "2023-12-31 18:46:00.933530: train_loss -0.8761\n",
      "2023-12-31 18:46:00.943753: val_loss -0.8398\n",
      "2023-12-31 18:46:00.955428: Pseudo dice [0.9213, 0.944, 0.9424]\n",
      "2023-12-31 18:46:00.966424: Epoch time: 128.26 s\n",
      "2023-12-31 18:46:00.978425: Yayy! New best EMA pseudo Dice: 0.9354\n",
      "2023-12-31 18:46:02.575829: \n",
      "2023-12-31 18:46:02.581838: Epoch 52\n",
      "2023-12-31 18:46:02.586837: Current learning rate: 0.00953\n",
      "2023-12-31 18:48:11.197794: train_loss -0.8761\n",
      "2023-12-31 18:48:11.210795: val_loss -0.8381\n",
      "2023-12-31 18:48:11.221279: Pseudo dice [0.9196, 0.943, 0.9411]\n",
      "2023-12-31 18:48:11.232278: Epoch time: 128.62 s\n",
      "2023-12-31 18:48:12.520836: \n",
      "2023-12-31 18:48:12.530831: Epoch 53\n",
      "2023-12-31 18:48:12.536484: Current learning rate: 0.00952\n",
      "2023-12-31 18:50:19.986559: train_loss -0.8741\n",
      "2023-12-31 18:50:19.997746: val_loss -0.8485\n",
      "2023-12-31 18:50:20.006745: Pseudo dice [0.927, 0.9455, 0.9445]\n",
      "2023-12-31 18:50:20.016744: Epoch time: 127.47 s\n",
      "2023-12-31 18:50:20.024755: Yayy! New best EMA pseudo Dice: 0.9357\n",
      "2023-12-31 18:50:21.672525: \n",
      "2023-12-31 18:50:21.678523: Epoch 54\n",
      "2023-12-31 18:50:21.683523: Current learning rate: 0.00951\n",
      "2023-12-31 18:52:28.729185: train_loss -0.8779\n",
      "2023-12-31 18:52:28.738184: val_loss -0.8481\n",
      "2023-12-31 18:52:28.746499: Pseudo dice [0.9288, 0.9448, 0.9435]\n",
      "2023-12-31 18:52:28.755884: Epoch time: 127.06 s\n",
      "2023-12-31 18:52:28.763580: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2023-12-31 18:52:30.225863: \n",
      "2023-12-31 18:52:30.230855: Epoch 55\n",
      "2023-12-31 18:52:30.235855: Current learning rate: 0.0095\n",
      "2023-12-31 18:54:40.608464: train_loss -0.8764\n",
      "2023-12-31 18:54:40.623394: val_loss -0.8449\n",
      "2023-12-31 18:54:40.638907: Pseudo dice [0.9245, 0.9441, 0.9425]\n",
      "2023-12-31 18:54:40.656158: Epoch time: 130.38 s\n",
      "2023-12-31 18:54:40.674162: Yayy! New best EMA pseudo Dice: 0.9361\n",
      "2023-12-31 18:54:43.998297: \n",
      "2023-12-31 18:54:44.013491: Epoch 56\n",
      "2023-12-31 18:54:44.024697: Current learning rate: 0.00949\n",
      "2023-12-31 18:56:55.874902: train_loss -0.878\n",
      "2023-12-31 18:56:55.890316: val_loss -0.8372\n",
      "2023-12-31 18:56:55.914225: Pseudo dice [0.9198, 0.9416, 0.9395]\n",
      "2023-12-31 18:56:55.927225: Epoch time: 131.88 s\n",
      "2023-12-31 18:56:57.581534: \n",
      "2023-12-31 18:56:57.587535: Epoch 57\n",
      "2023-12-31 18:56:57.592544: Current learning rate: 0.00949\n",
      "2023-12-31 18:59:08.404455: train_loss -0.8789\n",
      "2023-12-31 18:59:08.417839: val_loss -0.8455\n",
      "2023-12-31 18:59:08.429839: Pseudo dice [0.9255, 0.9445, 0.9437]\n",
      "2023-12-31 18:59:08.441946: Epoch time: 130.82 s\n",
      "2023-12-31 18:59:10.137728: \n",
      "2023-12-31 18:59:10.144726: Epoch 58\n",
      "2023-12-31 18:59:10.150742: Current learning rate: 0.00948\n",
      "2023-12-31 19:01:20.409023: train_loss -0.8807\n",
      "2023-12-31 19:01:20.422024: val_loss -0.8443\n",
      "2023-12-31 19:01:20.434197: Pseudo dice [0.9241, 0.9436, 0.9432]\n",
      "2023-12-31 19:01:20.444702: Epoch time: 130.27 s\n",
      "2023-12-31 19:01:20.454707: Yayy! New best EMA pseudo Dice: 0.9362\n",
      "2023-12-31 19:01:22.343313: \n",
      "2023-12-31 19:01:22.351303: Epoch 59\n",
      "2023-12-31 19:01:22.358373: Current learning rate: 0.00947\n",
      "2023-12-31 19:03:34.149203: train_loss -0.8798\n",
      "2023-12-31 19:03:34.161467: val_loss -0.8476\n",
      "2023-12-31 19:03:34.172573: Pseudo dice [0.9259, 0.945, 0.9437]\n",
      "2023-12-31 19:03:34.183086: Epoch time: 131.81 s\n",
      "2023-12-31 19:03:34.192086: Yayy! New best EMA pseudo Dice: 0.9364\n",
      "2023-12-31 19:03:36.275735: \n",
      "2023-12-31 19:03:36.285252: Epoch 60\n",
      "2023-12-31 19:03:36.293258: Current learning rate: 0.00946\n",
      "2023-12-31 19:05:44.794316: train_loss -0.8795\n",
      "2023-12-31 19:05:44.803830: val_loss -0.8336\n",
      "2023-12-31 19:05:44.810310: Pseudo dice [0.9209, 0.941, 0.9395]\n",
      "2023-12-31 19:05:44.819310: Epoch time: 128.52 s\n",
      "2023-12-31 19:05:46.313346: \n",
      "2023-12-31 19:05:46.320420: Epoch 61\n",
      "2023-12-31 19:05:46.325355: Current learning rate: 0.00945\n",
      "2023-12-31 19:07:55.998250: train_loss -0.8817\n",
      "2023-12-31 19:07:56.008537: val_loss -0.8382\n",
      "2023-12-31 19:07:56.016539: Pseudo dice [0.9214, 0.9422, 0.9416]\n",
      "2023-12-31 19:07:56.022892: Epoch time: 129.69 s\n",
      "2023-12-31 19:07:57.415307: \n",
      "2023-12-31 19:07:57.423308: Epoch 62\n",
      "2023-12-31 19:07:57.430304: Current learning rate: 0.00944\n",
      "2023-12-31 19:10:06.627333: train_loss -0.8813\n",
      "2023-12-31 19:10:06.637339: val_loss -0.8414\n",
      "2023-12-31 19:10:06.646331: Pseudo dice [0.9236, 0.9433, 0.9431]\n",
      "2023-12-31 19:10:06.653739: Epoch time: 129.21 s\n",
      "2023-12-31 19:10:07.810469: \n",
      "2023-12-31 19:10:07.817468: Epoch 63\n",
      "2023-12-31 19:10:07.822472: Current learning rate: 0.00943\n",
      "2023-12-31 19:12:27.336433: train_loss -0.8777\n",
      "2023-12-31 19:12:27.349434: val_loss -0.8372\n",
      "2023-12-31 19:12:27.360517: Pseudo dice [0.9208, 0.9415, 0.9407]\n",
      "2023-12-31 19:12:27.372863: Epoch time: 139.53 s\n",
      "2023-12-31 19:12:28.996805: \n",
      "2023-12-31 19:12:29.005209: Epoch 64\n",
      "2023-12-31 19:12:29.012225: Current learning rate: 0.00942\n",
      "2023-12-31 19:14:43.609779: train_loss -0.8798\n",
      "2023-12-31 19:14:43.628503: val_loss -0.8402\n",
      "2023-12-31 19:14:43.646151: Pseudo dice [0.923, 0.9433, 0.9406]\n",
      "2023-12-31 19:14:43.657711: Epoch time: 134.61 s\n",
      "2023-12-31 19:14:45.412635: \n",
      "2023-12-31 19:14:45.419641: Epoch 65\n",
      "2023-12-31 19:14:45.425647: Current learning rate: 0.00941\n",
      "2023-12-31 19:16:54.500444: train_loss -0.8805\n",
      "2023-12-31 19:16:54.510449: val_loss -0.8446\n",
      "2023-12-31 19:16:54.518782: Pseudo dice [0.927, 0.9447, 0.9437]\n",
      "2023-12-31 19:16:54.536433: Epoch time: 129.09 s\n",
      "2023-12-31 19:16:55.667424: \n",
      "2023-12-31 19:16:55.673428: Epoch 66\n",
      "2023-12-31 19:16:55.678416: Current learning rate: 0.0094\n",
      "2023-12-31 19:19:04.243803: train_loss -0.8831\n",
      "2023-12-31 19:19:04.254664: val_loss -0.8394\n",
      "2023-12-31 19:19:04.265186: Pseudo dice [0.9221, 0.9433, 0.9429]\n",
      "2023-12-31 19:19:04.271718: Epoch time: 128.58 s\n",
      "2023-12-31 19:19:05.666154: \n",
      "2023-12-31 19:19:05.676765: Epoch 67\n",
      "2023-12-31 19:19:05.686536: Current learning rate: 0.00939\n",
      "2023-12-31 19:21:13.438213: train_loss -0.8847\n",
      "2023-12-31 19:21:13.447234: val_loss -0.8333\n",
      "2023-12-31 19:21:13.455412: Pseudo dice [0.9218, 0.9406, 0.9406]\n",
      "2023-12-31 19:21:13.464417: Epoch time: 127.77 s\n",
      "2023-12-31 19:21:14.635350: \n",
      "2023-12-31 19:21:14.642348: Epoch 68\n",
      "2023-12-31 19:21:14.647350: Current learning rate: 0.00939\n",
      "2023-12-31 19:23:22.749285: train_loss -0.8847\n",
      "2023-12-31 19:23:22.761806: val_loss -0.8459\n",
      "2023-12-31 19:23:22.770318: Pseudo dice [0.9269, 0.9451, 0.9436]\n",
      "2023-12-31 19:23:22.780344: Epoch time: 128.12 s\n",
      "2023-12-31 19:23:24.710374: \n",
      "2023-12-31 19:23:24.719888: Epoch 69\n",
      "2023-12-31 19:23:24.728433: Current learning rate: 0.00938\n",
      "2023-12-31 19:25:32.804944: train_loss -0.8853\n",
      "2023-12-31 19:25:32.813004: val_loss -0.8478\n",
      "2023-12-31 19:25:32.821741: Pseudo dice [0.9278, 0.9465, 0.9446]\n",
      "2023-12-31 19:25:32.829746: Epoch time: 128.1 s\n",
      "2023-12-31 19:25:32.840624: Yayy! New best EMA pseudo Dice: 0.9366\n",
      "2023-12-31 19:25:34.297077: \n",
      "2023-12-31 19:25:34.303142: Epoch 70\n",
      "2023-12-31 19:25:34.308078: Current learning rate: 0.00937\n",
      "2023-12-31 19:27:43.048162: train_loss -0.883\n",
      "2023-12-31 19:27:43.058163: val_loss -0.8392\n",
      "2023-12-31 19:27:43.068161: Pseudo dice [0.924, 0.943, 0.9422]\n",
      "2023-12-31 19:27:43.077162: Epoch time: 128.75 s\n",
      "2023-12-31 19:27:44.479979: \n",
      "2023-12-31 19:27:44.485980: Epoch 71\n",
      "2023-12-31 19:27:44.490988: Current learning rate: 0.00936\n",
      "2023-12-31 19:29:52.644405: train_loss -0.883\n",
      "2023-12-31 19:29:52.654454: val_loss -0.8292\n",
      "2023-12-31 19:29:52.662452: Pseudo dice [0.9172, 0.939, 0.9366]\n",
      "2023-12-31 19:29:52.669921: Epoch time: 128.17 s\n",
      "2023-12-31 19:29:53.779405: \n",
      "2023-12-31 19:29:53.785400: Epoch 72\n",
      "2023-12-31 19:29:53.791386: Current learning rate: 0.00935\n",
      "2023-12-31 19:32:01.451302: train_loss -0.8847\n",
      "2023-12-31 19:32:01.463806: val_loss -0.8244\n",
      "2023-12-31 19:32:01.478279: Pseudo dice [0.9161, 0.9369, 0.9362]\n",
      "2023-12-31 19:32:01.489645: Epoch time: 127.67 s\n",
      "2023-12-31 19:32:02.867190: \n",
      "2023-12-31 19:32:02.873191: Epoch 73\n",
      "2023-12-31 19:32:02.879189: Current learning rate: 0.00934\n",
      "2023-12-31 19:34:12.741019: train_loss -0.8826\n",
      "2023-12-31 19:34:12.753949: val_loss -0.8514\n",
      "2023-12-31 19:34:12.765688: Pseudo dice [0.9301, 0.9462, 0.9453]\n",
      "2023-12-31 19:34:12.774930: Epoch time: 129.87 s\n",
      "2023-12-31 19:34:14.013309: \n",
      "2023-12-31 19:34:14.022310: Epoch 74\n",
      "2023-12-31 19:34:14.029435: Current learning rate: 0.00933\n",
      "2023-12-31 19:36:22.317135: train_loss -0.885\n",
      "2023-12-31 19:36:22.326135: val_loss -0.8424\n",
      "2023-12-31 19:36:22.335462: Pseudo dice [0.9244, 0.9445, 0.9433]\n",
      "2023-12-31 19:36:22.343893: Epoch time: 128.31 s\n",
      "2023-12-31 19:36:23.437030: \n",
      "2023-12-31 19:36:23.443018: Epoch 75\n",
      "2023-12-31 19:36:23.448019: Current learning rate: 0.00932\n",
      "2023-12-31 19:38:32.244503: train_loss -0.884\n",
      "2023-12-31 19:38:32.257510: val_loss -0.8379\n",
      "2023-12-31 19:38:32.270262: Pseudo dice [0.9238, 0.9426, 0.9428]\n",
      "2023-12-31 19:38:32.281664: Epoch time: 128.81 s\n",
      "2023-12-31 19:38:33.684996: \n",
      "2023-12-31 19:38:33.692003: Epoch 76\n",
      "2023-12-31 19:38:33.696997: Current learning rate: 0.00931\n",
      "2023-12-31 19:40:41.869264: train_loss -0.8814\n",
      "2023-12-31 19:40:41.881775: val_loss -0.8195\n",
      "2023-12-31 19:40:41.890773: Pseudo dice [0.9155, 0.9364, 0.9318]\n",
      "2023-12-31 19:40:41.901188: Epoch time: 128.19 s\n",
      "2023-12-31 19:40:43.069816: \n",
      "2023-12-31 19:40:43.075815: Epoch 77\n",
      "2023-12-31 19:40:43.080342: Current learning rate: 0.0093\n",
      "2023-12-31 19:42:49.241539: train_loss -0.8854\n",
      "2023-12-31 19:42:49.250047: val_loss -0.8357\n",
      "2023-12-31 19:42:49.258045: Pseudo dice [0.9234, 0.9421, 0.9397]\n",
      "2023-12-31 19:42:49.267125: Epoch time: 126.17 s\n",
      "2023-12-31 19:42:50.389209: \n",
      "2023-12-31 19:42:50.395209: Epoch 78\n",
      "2023-12-31 19:42:50.400204: Current learning rate: 0.0093\n",
      "2023-12-31 19:44:56.876180: train_loss -0.8843\n",
      "2023-12-31 19:44:56.885697: val_loss -0.8459\n",
      "2023-12-31 19:44:56.892697: Pseudo dice [0.928, 0.944, 0.9445]\n",
      "2023-12-31 19:44:56.898697: Epoch time: 126.49 s\n",
      "2023-12-31 19:44:57.980964: \n",
      "2023-12-31 19:44:57.987489: Epoch 79\n",
      "2023-12-31 19:44:57.991489: Current learning rate: 0.00929\n",
      "2023-12-31 19:47:03.349344: train_loss -0.8827\n",
      "2023-12-31 19:47:03.358346: val_loss -0.8415\n",
      "2023-12-31 19:47:03.367733: Pseudo dice [0.9258, 0.9434, 0.9422]\n",
      "2023-12-31 19:47:03.374736: Epoch time: 125.37 s\n",
      "2023-12-31 19:47:04.499306: \n",
      "2023-12-31 19:47:04.504820: Epoch 80\n",
      "2023-12-31 19:47:04.512842: Current learning rate: 0.00928\n",
      "2023-12-31 19:49:09.740213: train_loss -0.8866\n",
      "2023-12-31 19:49:09.748215: val_loss -0.8451\n",
      "2023-12-31 19:49:09.756213: Pseudo dice [0.9289, 0.9451, 0.9443]\n",
      "2023-12-31 19:49:09.763214: Epoch time: 125.24 s\n",
      "2023-12-31 19:49:10.788265: \n",
      "2023-12-31 19:49:10.802627: Epoch 81\n",
      "2023-12-31 19:49:10.807667: Current learning rate: 0.00927\n",
      "2023-12-31 19:51:16.199481: train_loss -0.8832\n",
      "2023-12-31 19:51:16.211496: val_loss -0.8311\n",
      "2023-12-31 19:51:16.219999: Pseudo dice [0.9197, 0.939, 0.9383]\n",
      "2023-12-31 19:51:16.226998: Epoch time: 125.41 s\n",
      "2023-12-31 19:51:17.278194: \n",
      "2023-12-31 19:51:17.283194: Epoch 82\n",
      "2023-12-31 19:51:17.288186: Current learning rate: 0.00926\n",
      "2023-12-31 19:53:22.938874: train_loss -0.8836\n",
      "2023-12-31 19:53:22.946889: val_loss -0.8374\n",
      "2023-12-31 19:53:22.953873: Pseudo dice [0.9212, 0.9421, 0.9407]\n",
      "2023-12-31 19:53:22.960872: Epoch time: 125.66 s\n",
      "2023-12-31 19:53:24.150998: \n",
      "2023-12-31 19:53:24.156990: Epoch 83\n",
      "2023-12-31 19:53:24.161990: Current learning rate: 0.00925\n",
      "2023-12-31 19:55:29.459125: train_loss -0.8853\n",
      "2023-12-31 19:55:29.468113: val_loss -0.8439\n",
      "2023-12-31 19:55:29.475112: Pseudo dice [0.9265, 0.9446, 0.9429]\n",
      "2023-12-31 19:55:29.481123: Epoch time: 125.31 s\n",
      "2023-12-31 19:55:30.452336: \n",
      "2023-12-31 19:55:30.457335: Epoch 84\n",
      "2023-12-31 19:55:30.462335: Current learning rate: 0.00924\n",
      "2023-12-31 19:57:36.272190: train_loss -0.8855\n",
      "2023-12-31 19:57:36.282697: val_loss -0.8376\n",
      "2023-12-31 19:57:36.290697: Pseudo dice [0.9228, 0.9426, 0.9409]\n",
      "2023-12-31 19:57:36.298208: Epoch time: 125.82 s\n",
      "2023-12-31 19:57:37.339899: \n",
      "2023-12-31 19:57:37.344973: Epoch 85\n",
      "2023-12-31 19:57:37.349905: Current learning rate: 0.00923\n",
      "2023-12-31 19:59:42.605210: train_loss -0.8853\n",
      "2023-12-31 19:59:42.616208: val_loss -0.8503\n",
      "2023-12-31 19:59:42.624211: Pseudo dice [0.931, 0.9463, 0.9444]\n",
      "2023-12-31 19:59:42.630212: Epoch time: 125.27 s\n",
      "2023-12-31 19:59:43.642403: \n",
      "2023-12-31 19:59:43.648667: Epoch 86\n",
      "2023-12-31 19:59:43.652686: Current learning rate: 0.00922\n",
      "2023-12-31 20:01:49.112250: train_loss -0.8848\n",
      "2023-12-31 20:01:49.121249: val_loss -0.8462\n",
      "2023-12-31 20:01:49.127254: Pseudo dice [0.928, 0.9449, 0.9446]\n",
      "2023-12-31 20:01:49.134250: Epoch time: 125.47 s\n",
      "2023-12-31 20:01:49.141254: Yayy! New best EMA pseudo Dice: 0.9366\n",
      "2023-12-31 20:01:50.419799: \n",
      "2023-12-31 20:01:50.425805: Epoch 87\n",
      "2023-12-31 20:01:50.430785: Current learning rate: 0.00921\n",
      "2023-12-31 20:03:55.613823: train_loss -0.8878\n",
      "2023-12-31 20:03:55.622829: val_loss -0.85\n",
      "2023-12-31 20:03:55.630827: Pseudo dice [0.929, 0.9462, 0.9451]\n",
      "2023-12-31 20:03:55.639335: Epoch time: 125.2 s\n",
      "2023-12-31 20:03:55.645337: Yayy! New best EMA pseudo Dice: 0.9369\n",
      "2023-12-31 20:03:56.870950: \n",
      "2023-12-31 20:03:56.875947: Epoch 88\n",
      "2023-12-31 20:03:56.880938: Current learning rate: 0.0092\n",
      "2023-12-31 20:06:02.394221: train_loss -0.8855\n",
      "2023-12-31 20:06:02.402219: val_loss -0.8535\n",
      "2023-12-31 20:06:02.410223: Pseudo dice [0.9307, 0.9476, 0.9461]\n",
      "2023-12-31 20:06:02.418731: Epoch time: 125.52 s\n",
      "2023-12-31 20:06:02.427736: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2023-12-31 20:06:03.750274: \n",
      "2023-12-31 20:06:03.755282: Epoch 89\n",
      "2023-12-31 20:06:03.759283: Current learning rate: 0.0092\n",
      "2023-12-31 20:08:08.763369: train_loss -0.8891\n",
      "2023-12-31 20:08:08.773368: val_loss -0.8442\n",
      "2023-12-31 20:08:08.781546: Pseudo dice [0.9256, 0.944, 0.9441]\n",
      "2023-12-31 20:08:08.789546: Epoch time: 125.01 s\n",
      "2023-12-31 20:08:08.797552: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2023-12-31 20:08:10.149921: \n",
      "2023-12-31 20:08:10.155910: Epoch 90\n",
      "2023-12-31 20:08:10.159919: Current learning rate: 0.00919\n",
      "2023-12-31 20:10:15.687737: train_loss -0.8852\n",
      "2023-12-31 20:10:15.697246: val_loss -0.8458\n",
      "2023-12-31 20:10:15.705244: Pseudo dice [0.926, 0.9442, 0.9441]\n",
      "2023-12-31 20:10:15.712244: Epoch time: 125.54 s\n",
      "2023-12-31 20:10:15.718245: Yayy! New best EMA pseudo Dice: 0.9375\n",
      "2023-12-31 20:10:17.248441: \n",
      "2023-12-31 20:10:17.253449: Epoch 91\n",
      "2023-12-31 20:10:17.257449: Current learning rate: 0.00918\n",
      "2023-12-31 20:12:22.269289: train_loss -0.8887\n",
      "2023-12-31 20:12:22.278883: val_loss -0.8483\n",
      "2023-12-31 20:12:22.285881: Pseudo dice [0.9297, 0.946, 0.9447]\n",
      "2023-12-31 20:12:22.293878: Epoch time: 125.02 s\n",
      "2023-12-31 20:12:22.302877: Yayy! New best EMA pseudo Dice: 0.9378\n",
      "2023-12-31 20:12:23.524931: \n",
      "2023-12-31 20:12:23.529932: Epoch 92\n",
      "2023-12-31 20:12:23.534932: Current learning rate: 0.00917\n",
      "2023-12-31 20:14:28.690472: train_loss -0.887\n",
      "2023-12-31 20:14:28.700472: val_loss -0.8458\n",
      "2023-12-31 20:14:28.709479: Pseudo dice [0.9277, 0.9452, 0.9446]\n",
      "2023-12-31 20:14:28.717477: Epoch time: 125.17 s\n",
      "2023-12-31 20:14:28.724985: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2023-12-31 20:14:30.040545: \n",
      "2023-12-31 20:14:30.045547: Epoch 93\n",
      "2023-12-31 20:14:30.050557: Current learning rate: 0.00916\n",
      "2023-12-31 20:16:35.233618: train_loss -0.8869\n",
      "2023-12-31 20:16:35.242622: val_loss -0.8453\n",
      "2023-12-31 20:16:35.250617: Pseudo dice [0.9274, 0.9451, 0.9446]\n",
      "2023-12-31 20:16:35.258618: Epoch time: 125.19 s\n",
      "2023-12-31 20:16:35.265621: Yayy! New best EMA pseudo Dice: 0.938\n",
      "2023-12-31 20:16:36.583970: \n",
      "2023-12-31 20:16:36.589975: Epoch 94\n",
      "2023-12-31 20:16:36.594975: Current learning rate: 0.00915\n",
      "2023-12-31 20:18:41.813110: train_loss -0.8863\n",
      "2023-12-31 20:18:41.820113: val_loss -0.84\n",
      "2023-12-31 20:18:41.827113: Pseudo dice [0.9263, 0.9433, 0.9421]\n",
      "2023-12-31 20:18:41.834621: Epoch time: 125.23 s\n",
      "2023-12-31 20:18:42.808119: \n",
      "2023-12-31 20:18:42.814124: Epoch 95\n",
      "2023-12-31 20:18:42.819132: Current learning rate: 0.00914\n",
      "2023-12-31 20:20:47.802220: train_loss -0.8891\n",
      "2023-12-31 20:20:47.809216: val_loss -0.8435\n",
      "2023-12-31 20:20:47.817215: Pseudo dice [0.928, 0.9439, 0.9425]\n",
      "2023-12-31 20:20:47.824216: Epoch time: 125.0 s\n",
      "2023-12-31 20:20:48.778886: \n",
      "2023-12-31 20:20:48.784828: Epoch 96\n",
      "2023-12-31 20:20:48.788825: Current learning rate: 0.00913\n",
      "2023-12-31 20:22:54.100939: train_loss -0.8875\n",
      "2023-12-31 20:22:54.111944: val_loss -0.8505\n",
      "2023-12-31 20:22:54.120468: Pseudo dice [0.9295, 0.9469, 0.9439]\n",
      "2023-12-31 20:22:54.129466: Epoch time: 125.32 s\n",
      "2023-12-31 20:22:54.137467: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2023-12-31 20:22:55.456729: \n",
      "2023-12-31 20:22:55.461730: Epoch 97\n",
      "2023-12-31 20:22:55.466799: Current learning rate: 0.00912\n",
      "2023-12-31 20:25:00.628684: train_loss -0.8888\n",
      "2023-12-31 20:25:00.636684: val_loss -0.8431\n",
      "2023-12-31 20:25:00.643683: Pseudo dice [0.9293, 0.9437, 0.9426]\n",
      "2023-12-31 20:25:00.649684: Epoch time: 125.17 s\n",
      "2023-12-31 20:25:00.658688: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2023-12-31 20:25:02.015602: \n",
      "2023-12-31 20:25:02.020610: Epoch 98\n",
      "2023-12-31 20:25:02.025618: Current learning rate: 0.00911\n",
      "2023-12-31 20:27:07.014545: train_loss -0.8908\n",
      "2023-12-31 20:27:07.026058: val_loss -0.8405\n",
      "2023-12-31 20:27:07.032054: Pseudo dice [0.9245, 0.9424, 0.9416]\n",
      "2023-12-31 20:27:07.039053: Epoch time: 125.0 s\n",
      "2023-12-31 20:27:08.268112: \n",
      "2023-12-31 20:27:08.273663: Epoch 99\n",
      "2023-12-31 20:27:08.277658: Current learning rate: 0.0091\n",
      "2023-12-31 20:29:13.509103: train_loss -0.8874\n",
      "2023-12-31 20:29:13.516109: val_loss -0.834\n",
      "2023-12-31 20:29:13.524102: Pseudo dice [0.9223, 0.9405, 0.9404]\n",
      "2023-12-31 20:29:13.533101: Epoch time: 125.24 s\n",
      "2023-12-31 20:29:14.756310: \n",
      "2023-12-31 20:29:14.766386: Epoch 100\n",
      "2023-12-31 20:29:14.771829: Current learning rate: 0.0091\n",
      "2023-12-31 20:31:19.885398: train_loss -0.8879\n",
      "2023-12-31 20:31:19.894399: val_loss -0.8439\n",
      "2023-12-31 20:31:19.902397: Pseudo dice [0.9296, 0.9446, 0.9421]\n",
      "2023-12-31 20:31:19.908398: Epoch time: 125.13 s\n",
      "2023-12-31 20:31:20.892670: \n",
      "2023-12-31 20:31:20.897749: Epoch 101\n",
      "2023-12-31 20:31:20.906009: Current learning rate: 0.00909\n",
      "2023-12-31 20:33:26.196928: train_loss -0.8876\n",
      "2023-12-31 20:33:26.205926: val_loss -0.8465\n",
      "2023-12-31 20:33:26.211926: Pseudo dice [0.93, 0.9456, 0.9426]\n",
      "2023-12-31 20:33:26.217926: Epoch time: 125.31 s\n",
      "2023-12-31 20:33:27.215189: \n",
      "2023-12-31 20:33:27.227199: Epoch 102\n",
      "2023-12-31 20:33:27.236194: Current learning rate: 0.00908\n",
      "2023-12-31 20:35:32.504818: train_loss -0.8874\n",
      "2023-12-31 20:35:32.513819: val_loss -0.8408\n",
      "2023-12-31 20:35:32.520330: Pseudo dice [0.9244, 0.9427, 0.9412]\n",
      "2023-12-31 20:35:32.527332: Epoch time: 125.29 s\n",
      "2023-12-31 20:35:33.527996: \n",
      "2023-12-31 20:35:33.536034: Epoch 103\n",
      "2023-12-31 20:35:33.542011: Current learning rate: 0.00907\n",
      "2023-12-31 20:37:38.808460: train_loss -0.8901\n",
      "2023-12-31 20:37:38.817458: val_loss -0.837\n",
      "2023-12-31 20:37:38.825458: Pseudo dice [0.9199, 0.9422, 0.9404]\n",
      "2023-12-31 20:37:38.834459: Epoch time: 125.28 s\n",
      "2023-12-31 20:37:39.886099: \n",
      "2023-12-31 20:37:39.891099: Epoch 104\n",
      "2023-12-31 20:37:39.896099: Current learning rate: 0.00906\n",
      "2023-12-31 20:39:49.183024: train_loss -0.8894\n",
      "2023-12-31 20:39:49.196025: val_loss -0.8228\n",
      "2023-12-31 20:39:49.205024: Pseudo dice [0.9182, 0.938, 0.9354]\n",
      "2023-12-31 20:39:49.215024: Epoch time: 129.3 s\n",
      "2023-12-31 20:39:50.930719: \n",
      "2023-12-31 20:39:50.937282: Epoch 105\n",
      "2023-12-31 20:39:50.945277: Current learning rate: 0.00905\n",
      "2023-12-31 20:41:58.613788: train_loss -0.8915\n",
      "2023-12-31 20:41:58.624787: val_loss -0.8439\n",
      "2023-12-31 20:41:58.632787: Pseudo dice [0.9277, 0.9444, 0.9422]\n",
      "2023-12-31 20:41:58.640789: Epoch time: 127.69 s\n",
      "2023-12-31 20:41:59.710428: \n",
      "2023-12-31 20:41:59.715427: Epoch 106\n",
      "2023-12-31 20:41:59.721419: Current learning rate: 0.00904\n",
      "2023-12-31 20:44:10.422750: train_loss -0.8926\n",
      "2023-12-31 20:44:10.433973: val_loss -0.8437\n",
      "2023-12-31 20:44:10.441967: Pseudo dice [0.927, 0.9438, 0.9428]\n",
      "2023-12-31 20:44:10.451966: Epoch time: 130.71 s\n",
      "2023-12-31 20:44:12.315841: \n",
      "2023-12-31 20:44:12.325374: Epoch 107\n",
      "2023-12-31 20:44:12.332366: Current learning rate: 0.00903\n",
      "2023-12-31 20:46:19.116493: train_loss -0.8901\n",
      "2023-12-31 20:46:19.127462: val_loss -0.8468\n",
      "2023-12-31 20:46:19.135573: Pseudo dice [0.9273, 0.9449, 0.9448]\n",
      "2023-12-31 20:46:19.144710: Epoch time: 126.8 s\n",
      "2023-12-31 20:46:20.209981: \n",
      "2023-12-31 20:46:20.217982: Epoch 108\n",
      "2023-12-31 20:46:20.225489: Current learning rate: 0.00902\n",
      "2023-12-31 20:48:27.497142: train_loss -0.8912\n",
      "2023-12-31 20:48:27.506142: val_loss -0.8325\n",
      "2023-12-31 20:48:27.515142: Pseudo dice [0.9208, 0.9406, 0.9386]\n",
      "2023-12-31 20:48:27.523158: Epoch time: 127.29 s\n",
      "2023-12-31 20:48:28.627539: \n",
      "2023-12-31 20:48:28.633539: Epoch 109\n",
      "2023-12-31 20:48:28.638078: Current learning rate: 0.00901\n",
      "2023-12-31 20:50:37.767578: train_loss -0.8887\n",
      "2023-12-31 20:50:37.780208: val_loss -0.8443\n",
      "2023-12-31 20:50:37.789952: Pseudo dice [0.93, 0.9444, 0.9434]\n",
      "2023-12-31 20:50:37.801322: Epoch time: 129.14 s\n",
      "2023-12-31 20:50:39.140953: \n",
      "2023-12-31 20:50:39.147464: Epoch 110\n",
      "2023-12-31 20:50:39.153464: Current learning rate: 0.009\n",
      "2023-12-31 20:52:49.861366: train_loss -0.8906\n",
      "2023-12-31 20:52:49.872292: val_loss -0.8405\n",
      "2023-12-31 20:52:49.884410: Pseudo dice [0.9236, 0.9444, 0.9428]\n",
      "2023-12-31 20:52:49.894853: Epoch time: 130.72 s\n",
      "2023-12-31 20:52:51.150176: \n",
      "2023-12-31 20:52:51.158169: Epoch 111\n",
      "2023-12-31 20:52:51.166679: Current learning rate: 0.009\n",
      "2023-12-31 20:54:58.503730: train_loss -0.8889\n",
      "2023-12-31 20:54:58.514735: val_loss -0.8395\n",
      "2023-12-31 20:54:58.526730: Pseudo dice [0.9262, 0.9423, 0.9415]\n",
      "2023-12-31 20:54:58.536730: Epoch time: 127.35 s\n",
      "2023-12-31 20:54:59.608611: \n",
      "2023-12-31 20:54:59.614612: Epoch 112\n",
      "2023-12-31 20:54:59.620600: Current learning rate: 0.00899\n",
      "2023-12-31 20:57:09.202778: train_loss -0.8898\n",
      "2023-12-31 20:57:09.222289: val_loss -0.8464\n",
      "2023-12-31 20:57:09.240290: Pseudo dice [0.9284, 0.9449, 0.9437]\n",
      "2023-12-31 20:57:09.255290: Epoch time: 129.6 s\n",
      "2023-12-31 20:57:10.590345: \n",
      "2023-12-31 20:57:10.599361: Epoch 113\n",
      "2023-12-31 20:57:10.605358: Current learning rate: 0.00898\n",
      "2023-12-31 20:59:21.054839: train_loss -0.8917\n",
      "2023-12-31 20:59:21.067232: val_loss -0.8357\n",
      "2023-12-31 20:59:21.086284: Pseudo dice [0.9233, 0.9415, 0.9403]\n",
      "2023-12-31 20:59:21.101532: Epoch time: 130.47 s\n",
      "2023-12-31 20:59:23.050139: \n",
      "2023-12-31 20:59:23.058666: Epoch 114\n",
      "2023-12-31 20:59:23.068171: Current learning rate: 0.00897\n",
      "2023-12-31 21:01:46.003368: train_loss -0.8889\n",
      "2023-12-31 21:01:46.033643: val_loss -0.8227\n",
      "2023-12-31 21:01:46.047395: Pseudo dice [0.9158, 0.9379, 0.9355]\n",
      "2023-12-31 21:01:46.063841: Epoch time: 142.96 s\n",
      "2023-12-31 21:01:48.471144: \n",
      "2023-12-31 21:01:48.478502: Epoch 115\n",
      "2023-12-31 21:01:48.486890: Current learning rate: 0.00896\n",
      "2023-12-31 21:03:57.934862: train_loss -0.8929\n",
      "2023-12-31 21:03:57.944864: val_loss -0.8388\n",
      "2023-12-31 21:03:57.954371: Pseudo dice [0.9253, 0.9421, 0.9415]\n",
      "2023-12-31 21:03:57.962880: Epoch time: 129.47 s\n",
      "2023-12-31 21:03:59.268680: \n",
      "2023-12-31 21:03:59.274601: Epoch 116\n",
      "2023-12-31 21:03:59.279665: Current learning rate: 0.00895\n",
      "2023-12-31 21:06:09.067196: train_loss -0.8909\n",
      "2023-12-31 21:06:09.079351: val_loss -0.8392\n",
      "2023-12-31 21:06:09.089456: Pseudo dice [0.9264, 0.9433, 0.9427]\n",
      "2023-12-31 21:06:09.098969: Epoch time: 129.8 s\n",
      "2023-12-31 21:06:10.826797: \n",
      "2023-12-31 21:06:10.833807: Epoch 117\n",
      "2023-12-31 21:06:10.841829: Current learning rate: 0.00894\n",
      "2023-12-31 21:08:20.452415: train_loss -0.8914\n",
      "2023-12-31 21:08:20.467802: val_loss -0.8427\n",
      "2023-12-31 21:08:20.480727: Pseudo dice [0.9268, 0.9445, 0.9442]\n",
      "2023-12-31 21:08:20.538399: Epoch time: 129.63 s\n",
      "2023-12-31 21:08:22.208257: \n",
      "2023-12-31 21:08:22.215256: Epoch 118\n",
      "2023-12-31 21:08:22.223260: Current learning rate: 0.00893\n",
      "2023-12-31 21:10:32.050743: train_loss -0.8938\n",
      "2023-12-31 21:10:32.060244: val_loss -0.8385\n",
      "2023-12-31 21:10:32.068422: Pseudo dice [0.9268, 0.9432, 0.9426]\n",
      "2023-12-31 21:10:32.077239: Epoch time: 129.84 s\n",
      "2023-12-31 21:10:33.096450: \n",
      "2023-12-31 21:10:33.102679: Epoch 119\n",
      "2023-12-31 21:10:33.107684: Current learning rate: 0.00892\n",
      "2023-12-31 21:12:39.990009: train_loss -0.8938\n",
      "2023-12-31 21:12:40.000021: val_loss -0.8326\n",
      "2023-12-31 21:12:40.007366: Pseudo dice [0.9241, 0.9413, 0.94]\n",
      "2023-12-31 21:12:40.016367: Epoch time: 126.89 s\n",
      "2023-12-31 21:12:41.086946: \n",
      "2023-12-31 21:12:41.092935: Epoch 120\n",
      "2023-12-31 21:12:41.096943: Current learning rate: 0.00891\n",
      "2023-12-31 21:14:48.614865: train_loss -0.8926\n",
      "2023-12-31 21:14:48.626344: val_loss -0.8498\n",
      "2023-12-31 21:14:48.636344: Pseudo dice [0.9309, 0.9465, 0.9437]\n",
      "2023-12-31 21:14:48.647503: Epoch time: 127.53 s\n",
      "2023-12-31 21:14:49.710802: \n",
      "2023-12-31 21:14:49.716151: Epoch 121\n",
      "2023-12-31 21:14:49.721155: Current learning rate: 0.0089\n",
      "2023-12-31 21:16:58.216398: train_loss -0.8944\n",
      "2023-12-31 21:16:58.225398: val_loss -0.8368\n",
      "2023-12-31 21:16:58.233557: Pseudo dice [0.9264, 0.9423, 0.9406]\n",
      "2023-12-31 21:16:58.240840: Epoch time: 128.51 s\n",
      "2023-12-31 21:16:59.468401: \n",
      "2023-12-31 21:16:59.475589: Epoch 122\n",
      "2023-12-31 21:16:59.479666: Current learning rate: 0.00889\n",
      "2023-12-31 21:19:08.790508: train_loss -0.8934\n",
      "2023-12-31 21:19:08.800563: val_loss -0.8361\n",
      "2023-12-31 21:19:08.808804: Pseudo dice [0.9271, 0.942, 0.9411]\n",
      "2023-12-31 21:19:08.816805: Epoch time: 129.32 s\n",
      "2023-12-31 21:19:09.929071: \n",
      "2023-12-31 21:19:09.935072: Epoch 123\n",
      "2023-12-31 21:19:09.940075: Current learning rate: 0.00889\n",
      "2023-12-31 21:21:16.818639: train_loss -0.8917\n",
      "2023-12-31 21:21:16.827127: val_loss -0.8355\n",
      "2023-12-31 21:21:16.836126: Pseudo dice [0.9254, 0.9415, 0.942]\n",
      "2023-12-31 21:21:16.843136: Epoch time: 126.89 s\n",
      "2023-12-31 21:21:17.880025: \n",
      "2023-12-31 21:21:17.886176: Epoch 124\n",
      "2023-12-31 21:21:17.891251: Current learning rate: 0.00888\n",
      "2023-12-31 21:23:25.024500: train_loss -0.8947\n",
      "2023-12-31 21:23:25.035681: val_loss -0.8415\n",
      "2023-12-31 21:23:25.044756: Pseudo dice [0.9278, 0.9446, 0.9436]\n",
      "2023-12-31 21:23:25.052756: Epoch time: 127.15 s\n",
      "2023-12-31 21:23:26.135710: \n",
      "2023-12-31 21:23:26.144709: Epoch 125\n",
      "2023-12-31 21:23:26.151711: Current learning rate: 0.00887\n",
      "2023-12-31 21:25:35.638669: train_loss -0.8933\n",
      "2023-12-31 21:25:35.649919: val_loss -0.839\n",
      "2023-12-31 21:25:35.664308: Pseudo dice [0.9282, 0.9428, 0.9432]\n",
      "2023-12-31 21:25:35.681336: Epoch time: 129.5 s\n",
      "2023-12-31 21:25:37.707093: \n",
      "2023-12-31 21:25:37.713095: Epoch 126\n",
      "2023-12-31 21:25:37.719536: Current learning rate: 0.00886\n",
      "2023-12-31 21:27:44.870576: train_loss -0.8913\n",
      "2023-12-31 21:27:44.878913: val_loss -0.8326\n",
      "2023-12-31 21:27:44.887037: Pseudo dice [0.9224, 0.9406, 0.9392]\n",
      "2023-12-31 21:27:44.893302: Epoch time: 127.16 s\n",
      "2023-12-31 21:27:45.913368: \n",
      "2023-12-31 21:27:45.922750: Epoch 127\n",
      "2023-12-31 21:27:45.926823: Current learning rate: 0.00885\n",
      "2023-12-31 21:29:52.773520: train_loss -0.8963\n",
      "2023-12-31 21:29:52.784561: val_loss -0.8427\n",
      "2023-12-31 21:29:52.793570: Pseudo dice [0.9277, 0.9439, 0.9438]\n",
      "2023-12-31 21:29:52.802571: Epoch time: 126.86 s\n",
      "2023-12-31 21:29:54.314203: \n",
      "2023-12-31 21:29:54.321124: Epoch 128\n",
      "2023-12-31 21:29:54.329131: Current learning rate: 0.00884\n",
      "2023-12-31 21:32:01.992598: train_loss -0.8938\n",
      "2023-12-31 21:32:02.000596: val_loss -0.8376\n",
      "2023-12-31 21:32:02.010065: Pseudo dice [0.9241, 0.9426, 0.9407]\n",
      "2023-12-31 21:32:02.017069: Epoch time: 127.68 s\n",
      "2023-12-31 21:32:03.055413: \n",
      "2023-12-31 21:32:03.060913: Epoch 129\n",
      "2023-12-31 21:32:03.067913: Current learning rate: 0.00883\n",
      "2023-12-31 21:34:09.839410: train_loss -0.8923\n",
      "2023-12-31 21:34:09.848557: val_loss -0.8429\n",
      "2023-12-31 21:34:09.858735: Pseudo dice [0.9275, 0.944, 0.9425]\n",
      "2023-12-31 21:34:09.866780: Epoch time: 126.78 s\n",
      "2023-12-31 21:34:11.093397: \n",
      "2023-12-31 21:34:11.099656: Epoch 130\n",
      "2023-12-31 21:34:11.104533: Current learning rate: 0.00882\n",
      "2023-12-31 21:36:17.551608: train_loss -0.8941\n",
      "2023-12-31 21:36:17.558114: val_loss -0.8422\n",
      "2023-12-31 21:36:17.565129: Pseudo dice [0.9253, 0.9445, 0.9425]\n",
      "2023-12-31 21:36:17.570121: Epoch time: 126.46 s\n",
      "2023-12-31 21:36:18.720082: \n",
      "2023-12-31 21:36:18.734592: Epoch 131\n",
      "2023-12-31 21:36:18.739590: Current learning rate: 0.00881\n",
      "2023-12-31 21:38:24.880141: train_loss -0.8966\n",
      "2023-12-31 21:38:24.888142: val_loss -0.8424\n",
      "2023-12-31 21:38:24.895143: Pseudo dice [0.9266, 0.9453, 0.9443]\n",
      "2023-12-31 21:38:24.903141: Epoch time: 126.16 s\n",
      "2023-12-31 21:38:26.148990: \n",
      "2023-12-31 21:38:26.158151: Epoch 132\n",
      "2023-12-31 21:38:26.163175: Current learning rate: 0.0088\n",
      "2023-12-31 21:40:32.715776: train_loss -0.8938\n",
      "2023-12-31 21:40:32.724785: val_loss -0.8071\n",
      "2023-12-31 21:40:32.732775: Pseudo dice [0.9085, 0.9343, 0.9278]\n",
      "2023-12-31 21:40:32.740774: Epoch time: 126.57 s\n",
      "2023-12-31 21:40:33.948555: \n",
      "2023-12-31 21:40:33.958221: Epoch 133\n",
      "2023-12-31 21:40:33.964655: Current learning rate: 0.00879\n",
      "2023-12-31 21:42:40.544480: train_loss -0.8898\n",
      "2023-12-31 21:42:40.555472: val_loss -0.8334\n",
      "2023-12-31 21:42:40.562472: Pseudo dice [0.9214, 0.9422, 0.9397]\n",
      "2023-12-31 21:42:40.567473: Epoch time: 126.6 s\n",
      "2023-12-31 21:42:41.759402: \n",
      "2023-12-31 21:42:41.764406: Epoch 134\n",
      "2023-12-31 21:42:41.768917: Current learning rate: 0.00879\n",
      "2023-12-31 21:44:47.900404: train_loss -0.8899\n",
      "2023-12-31 21:44:47.907403: val_loss -0.8433\n",
      "2023-12-31 21:44:47.913473: Pseudo dice [0.9285, 0.945, 0.9443]\n",
      "2023-12-31 21:44:47.919488: Epoch time: 126.14 s\n",
      "2023-12-31 21:44:49.084668: \n",
      "2023-12-31 21:44:49.090659: Epoch 135\n",
      "2023-12-31 21:44:49.095657: Current learning rate: 0.00878\n",
      "2023-12-31 21:46:55.203287: train_loss -0.8952\n",
      "2023-12-31 21:46:55.210288: val_loss -0.8421\n",
      "2023-12-31 21:46:55.216288: Pseudo dice [0.9275, 0.9435, 0.944]\n",
      "2023-12-31 21:46:55.220288: Epoch time: 126.12 s\n",
      "2023-12-31 21:46:56.431967: \n",
      "2023-12-31 21:46:56.436968: Epoch 136\n",
      "2023-12-31 21:46:56.441985: Current learning rate: 0.00877\n",
      "2023-12-31 21:49:02.352464: train_loss -0.8947\n",
      "2023-12-31 21:49:02.360972: val_loss -0.8469\n",
      "2023-12-31 21:49:02.366972: Pseudo dice [0.9303, 0.9466, 0.9457]\n",
      "2023-12-31 21:49:02.372972: Epoch time: 125.92 s\n",
      "2023-12-31 21:49:03.536708: \n",
      "2023-12-31 21:49:03.541705: Epoch 137\n",
      "2023-12-31 21:49:03.547783: Current learning rate: 0.00876\n",
      "2023-12-31 21:51:09.653564: train_loss -0.8968\n",
      "2023-12-31 21:51:09.662077: val_loss -0.8436\n",
      "2023-12-31 21:51:09.668076: Pseudo dice [0.9308, 0.9432, 0.9425]\n",
      "2023-12-31 21:51:09.674076: Epoch time: 126.12 s\n",
      "2023-12-31 21:51:11.022629: \n",
      "2023-12-31 21:51:11.035706: Epoch 138\n",
      "2023-12-31 21:51:11.039634: Current learning rate: 0.00875\n",
      "2023-12-31 21:53:17.273862: train_loss -0.8946\n",
      "2023-12-31 21:53:17.281860: val_loss -0.8464\n",
      "2023-12-31 21:53:17.287863: Pseudo dice [0.9286, 0.9457, 0.9444]\n",
      "2023-12-31 21:53:17.293862: Epoch time: 126.25 s\n",
      "2023-12-31 21:53:18.585758: \n",
      "2023-12-31 21:53:18.590835: Epoch 139\n",
      "2023-12-31 21:53:18.594825: Current learning rate: 0.00874\n",
      "2023-12-31 21:55:25.007282: train_loss -0.8912\n",
      "2023-12-31 21:55:25.017289: val_loss -0.8443\n",
      "2023-12-31 21:55:25.027795: Pseudo dice [0.9287, 0.9452, 0.9448]\n",
      "2023-12-31 21:55:25.032795: Epoch time: 126.42 s\n",
      "2023-12-31 21:55:26.381274: \n",
      "2023-12-31 21:55:26.387275: Epoch 140\n",
      "2023-12-31 21:55:26.391341: Current learning rate: 0.00873\n",
      "2023-12-31 21:57:32.492116: train_loss -0.8951\n",
      "2023-12-31 21:57:32.500116: val_loss -0.8221\n",
      "2023-12-31 21:57:32.507116: Pseudo dice [0.9168, 0.9383, 0.9372]\n",
      "2023-12-31 21:57:32.513123: Epoch time: 126.11 s\n",
      "2023-12-31 21:57:33.711248: \n",
      "2023-12-31 21:57:33.723041: Epoch 141\n",
      "2023-12-31 21:57:33.728710: Current learning rate: 0.00872\n",
      "2023-12-31 21:59:40.112378: train_loss -0.8937\n",
      "2023-12-31 21:59:40.121383: val_loss -0.8349\n",
      "2023-12-31 21:59:40.127739: Pseudo dice [0.9221, 0.9431, 0.9424]\n",
      "2023-12-31 21:59:40.135496: Epoch time: 126.4 s\n",
      "2023-12-31 21:59:41.354609: \n",
      "2023-12-31 21:59:41.360605: Epoch 142\n",
      "2023-12-31 21:59:41.364606: Current learning rate: 0.00871\n",
      "2023-12-31 22:01:47.321946: train_loss -0.8952\n",
      "2023-12-31 22:01:47.328952: val_loss -0.8463\n",
      "2023-12-31 22:01:47.334956: Pseudo dice [0.933, 0.9458, 0.9453]\n",
      "2023-12-31 22:01:47.339956: Epoch time: 125.97 s\n",
      "2023-12-31 22:01:48.501014: \n",
      "2023-12-31 22:01:48.506654: Epoch 143\n",
      "2023-12-31 22:01:48.510760: Current learning rate: 0.0087\n",
      "2023-12-31 22:03:54.406702: train_loss -0.8957\n",
      "2023-12-31 22:03:54.412689: val_loss -0.842\n",
      "2023-12-31 22:03:54.419695: Pseudo dice [0.9258, 0.9439, 0.9424]\n",
      "2023-12-31 22:03:54.423698: Epoch time: 125.91 s\n",
      "2023-12-31 22:03:55.753896: \n",
      "2023-12-31 22:03:55.759967: Epoch 144\n",
      "2023-12-31 22:03:55.765972: Current learning rate: 0.00869\n",
      "2023-12-31 22:06:02.032223: train_loss -0.8935\n",
      "2023-12-31 22:06:02.038223: val_loss -0.8322\n",
      "2023-12-31 22:06:02.044224: Pseudo dice [0.9225, 0.9408, 0.9389]\n",
      "2023-12-31 22:06:02.049223: Epoch time: 126.28 s\n",
      "2023-12-31 22:06:03.384229: \n",
      "2023-12-31 22:06:03.389241: Epoch 145\n",
      "2023-12-31 22:06:03.394876: Current learning rate: 0.00868\n",
      "2023-12-31 22:08:09.293508: train_loss -0.8962\n",
      "2023-12-31 22:08:09.300508: val_loss -0.8386\n",
      "2023-12-31 22:08:09.306064: Pseudo dice [0.9243, 0.9458, 0.9445]\n",
      "2023-12-31 22:08:09.313357: Epoch time: 125.91 s\n",
      "2023-12-31 22:08:10.439426: \n",
      "2023-12-31 22:08:10.445424: Epoch 146\n",
      "2023-12-31 22:08:10.449418: Current learning rate: 0.00868\n",
      "2023-12-31 22:10:16.764915: train_loss -0.8942\n",
      "2023-12-31 22:10:16.773916: val_loss -0.8405\n",
      "2023-12-31 22:10:16.783241: Pseudo dice [0.9283, 0.9422, 0.9422]\n",
      "2023-12-31 22:10:16.788240: Epoch time: 126.33 s\n",
      "2023-12-31 22:10:18.068878: \n",
      "2023-12-31 22:10:18.075881: Epoch 147\n",
      "2023-12-31 22:10:18.080943: Current learning rate: 0.00867\n",
      "2023-12-31 22:12:24.011221: train_loss -0.8959\n",
      "2023-12-31 22:12:24.020217: val_loss -0.8314\n",
      "2023-12-31 22:12:24.025762: Pseudo dice [0.9211, 0.9406, 0.938]\n",
      "2023-12-31 22:12:24.032754: Epoch time: 125.94 s\n",
      "2023-12-31 22:12:25.147297: \n",
      "2023-12-31 22:12:25.152297: Epoch 148\n",
      "2023-12-31 22:12:25.157299: Current learning rate: 0.00866\n",
      "2023-12-31 22:14:31.165448: train_loss -0.8984\n",
      "2023-12-31 22:14:31.173705: val_loss -0.8392\n",
      "2023-12-31 22:14:31.180708: Pseudo dice [0.9254, 0.9431, 0.941]\n",
      "2023-12-31 22:14:31.188705: Epoch time: 126.02 s\n",
      "2023-12-31 22:14:32.407271: \n",
      "2023-12-31 22:14:32.412801: Epoch 149\n",
      "2023-12-31 22:14:32.417793: Current learning rate: 0.00865\n",
      "2023-12-31 22:16:38.737300: train_loss -0.8968\n",
      "2023-12-31 22:16:38.748295: val_loss -0.842\n",
      "2023-12-31 22:16:38.755296: Pseudo dice [0.928, 0.9435, 0.9447]\n",
      "2023-12-31 22:16:38.761809: Epoch time: 126.33 s\n",
      "2023-12-31 22:16:40.189313: \n",
      "2023-12-31 22:16:40.195304: Epoch 150\n",
      "2023-12-31 22:16:40.200303: Current learning rate: 0.00864\n",
      "2023-12-31 22:18:46.152540: train_loss -0.897\n",
      "2023-12-31 22:18:46.160539: val_loss -0.8327\n",
      "2023-12-31 22:18:46.166538: Pseudo dice [0.9234, 0.9411, 0.9413]\n",
      "2023-12-31 22:18:46.171536: Epoch time: 125.97 s\n",
      "2023-12-31 22:18:47.336515: \n",
      "2023-12-31 22:18:47.342507: Epoch 151\n",
      "2023-12-31 22:18:47.347508: Current learning rate: 0.00863\n",
      "2023-12-31 22:20:53.292240: train_loss -0.8975\n",
      "2023-12-31 22:20:53.299238: val_loss -0.8403\n",
      "2023-12-31 22:20:53.307454: Pseudo dice [0.9273, 0.9445, 0.9431]\n",
      "2023-12-31 22:20:53.312454: Epoch time: 125.96 s\n",
      "2023-12-31 22:20:54.679824: \n",
      "2023-12-31 22:20:54.684824: Epoch 152\n",
      "2023-12-31 22:20:54.689823: Current learning rate: 0.00862\n",
      "2023-12-31 22:23:00.552780: train_loss -0.8958\n",
      "2023-12-31 22:23:00.561779: val_loss -0.8423\n",
      "2023-12-31 22:23:00.570778: Pseudo dice [0.9275, 0.9444, 0.9433]\n",
      "2023-12-31 22:23:00.577778: Epoch time: 125.87 s\n",
      "2023-12-31 22:23:01.856589: \n",
      "2023-12-31 22:23:01.861588: Epoch 153\n",
      "2023-12-31 22:23:01.866589: Current learning rate: 0.00861\n",
      "2023-12-31 22:25:07.734372: train_loss -0.8966\n",
      "2023-12-31 22:25:07.743893: val_loss -0.8455\n",
      "2023-12-31 22:25:07.749888: Pseudo dice [0.9278, 0.9468, 0.9464]\n",
      "2023-12-31 22:25:07.754887: Epoch time: 125.88 s\n",
      "2023-12-31 22:25:09.045143: \n",
      "2023-12-31 22:25:09.050144: Epoch 154\n",
      "2023-12-31 22:25:09.055148: Current learning rate: 0.0086\n",
      "2023-12-31 22:27:15.218647: train_loss -0.8964\n",
      "2023-12-31 22:27:15.227645: val_loss -0.845\n",
      "2023-12-31 22:27:15.233647: Pseudo dice [0.9289, 0.9458, 0.9446]\n",
      "2023-12-31 22:27:15.241647: Epoch time: 126.18 s\n",
      "2023-12-31 22:27:16.474723: \n",
      "2023-12-31 22:27:16.480723: Epoch 155\n",
      "2023-12-31 22:27:16.484715: Current learning rate: 0.00859\n",
      "2023-12-31 22:29:22.156002: train_loss -0.8957\n",
      "2023-12-31 22:29:22.163000: val_loss -0.8449\n",
      "2023-12-31 22:29:22.169000: Pseudo dice [0.932, 0.9465, 0.9457]\n",
      "2023-12-31 22:29:22.175545: Epoch time: 125.68 s\n",
      "2023-12-31 22:29:23.449837: \n",
      "2023-12-31 22:29:23.461344: Epoch 156\n",
      "2023-12-31 22:29:23.466337: Current learning rate: 0.00858\n",
      "2023-12-31 22:31:29.147277: train_loss -0.8965\n",
      "2023-12-31 22:31:29.154277: val_loss -0.844\n",
      "2023-12-31 22:31:29.159277: Pseudo dice [0.9298, 0.9447, 0.944]\n",
      "2023-12-31 22:31:29.165278: Epoch time: 125.7 s\n",
      "2023-12-31 22:31:30.388211: \n",
      "2023-12-31 22:31:30.395293: Epoch 157\n",
      "2023-12-31 22:31:30.399352: Current learning rate: 0.00858\n",
      "2023-12-31 22:33:36.576908: train_loss -0.8966\n",
      "2023-12-31 22:33:36.583908: val_loss -0.8477\n",
      "2023-12-31 22:33:36.589907: Pseudo dice [0.9296, 0.947, 0.9463]\n",
      "2023-12-31 22:33:36.594907: Epoch time: 126.19 s\n",
      "2023-12-31 22:33:36.598907: Yayy! New best EMA pseudo Dice: 0.9384\n",
      "2023-12-31 22:33:38.077429: \n",
      "2023-12-31 22:33:38.085510: Epoch 158\n",
      "2023-12-31 22:33:38.089504: Current learning rate: 0.00857\n",
      "2023-12-31 22:35:43.960039: train_loss -0.8969\n",
      "2023-12-31 22:35:43.966037: val_loss -0.8407\n",
      "2023-12-31 22:35:43.972036: Pseudo dice [0.9285, 0.945, 0.9436]\n",
      "2023-12-31 22:35:43.978047: Epoch time: 125.88 s\n",
      "2023-12-31 22:35:43.983037: Yayy! New best EMA pseudo Dice: 0.9385\n",
      "2023-12-31 22:35:45.483081: \n",
      "2023-12-31 22:35:45.489074: Epoch 159\n",
      "2023-12-31 22:35:45.493080: Current learning rate: 0.00856\n",
      "2023-12-31 22:37:51.145957: train_loss -0.8996\n",
      "2023-12-31 22:37:51.151960: val_loss -0.846\n",
      "2023-12-31 22:37:51.158970: Pseudo dice [0.9326, 0.9461, 0.9445]\n",
      "2023-12-31 22:37:51.162961: Epoch time: 125.66 s\n",
      "2023-12-31 22:37:51.169956: Yayy! New best EMA pseudo Dice: 0.9387\n",
      "2023-12-31 22:37:52.847927: \n",
      "2023-12-31 22:37:52.852919: Epoch 160\n",
      "2023-12-31 22:37:52.857924: Current learning rate: 0.00855\n",
      "2023-12-31 22:39:58.908934: train_loss -0.8969\n",
      "2023-12-31 22:39:58.915935: val_loss -0.8423\n",
      "2023-12-31 22:39:58.925295: Pseudo dice [0.9302, 0.9456, 0.9442]\n",
      "2023-12-31 22:39:58.931885: Epoch time: 126.06 s\n",
      "2023-12-31 22:39:58.936889: Yayy! New best EMA pseudo Dice: 0.9389\n",
      "2023-12-31 22:40:00.324060: \n",
      "2023-12-31 22:40:00.330132: Epoch 161\n",
      "2023-12-31 22:40:00.334122: Current learning rate: 0.00854\n",
      "2023-12-31 22:42:06.101458: train_loss -0.8976\n",
      "2023-12-31 22:42:06.110457: val_loss -0.8413\n",
      "2023-12-31 22:42:06.120966: Pseudo dice [0.9329, 0.9443, 0.9434]\n",
      "2023-12-31 22:42:06.126968: Epoch time: 125.78 s\n",
      "2023-12-31 22:42:06.134972: Yayy! New best EMA pseudo Dice: 0.939\n",
      "2023-12-31 22:42:07.831151: \n",
      "2023-12-31 22:42:07.836157: Epoch 162\n",
      "2023-12-31 22:42:07.841099: Current learning rate: 0.00853\n",
      "2023-12-31 22:44:13.742111: train_loss -0.8986\n",
      "2023-12-31 22:44:13.749121: val_loss -0.8462\n",
      "2023-12-31 22:44:13.755126: Pseudo dice [0.9318, 0.9455, 0.9439]\n",
      "2023-12-31 22:44:13.760120: Epoch time: 125.91 s\n",
      "2023-12-31 22:44:13.765115: Yayy! New best EMA pseudo Dice: 0.9391\n",
      "2023-12-31 22:44:15.243856: \n",
      "2023-12-31 22:44:15.254797: Epoch 163\n",
      "2023-12-31 22:44:15.262789: Current learning rate: 0.00852\n",
      "2023-12-31 22:46:21.283463: train_loss -0.8998\n",
      "2023-12-31 22:46:21.293466: val_loss -0.8377\n",
      "2023-12-31 22:46:21.300464: Pseudo dice [0.9279, 0.9425, 0.9442]\n",
      "2023-12-31 22:46:21.306465: Epoch time: 126.04 s\n",
      "2023-12-31 22:46:22.594596: \n",
      "2023-12-31 22:46:22.605570: Epoch 164\n",
      "2023-12-31 22:46:22.610522: Current learning rate: 0.00851\n",
      "2023-12-31 22:48:28.517275: train_loss -0.8986\n",
      "2023-12-31 22:48:28.528275: val_loss -0.8424\n",
      "2023-12-31 22:48:28.538275: Pseudo dice [0.9318, 0.9447, 0.9434]\n",
      "2023-12-31 22:48:28.548276: Epoch time: 125.92 s\n",
      "2023-12-31 22:48:28.556275: Yayy! New best EMA pseudo Dice: 0.9391\n",
      "2023-12-31 22:48:30.342592: \n",
      "2023-12-31 22:48:30.348557: Epoch 165\n",
      "2023-12-31 22:48:30.353556: Current learning rate: 0.0085\n",
      "2023-12-31 22:50:36.305474: train_loss -0.8983\n",
      "2023-12-31 22:50:36.313474: val_loss -0.8451\n",
      "2023-12-31 22:50:36.320477: Pseudo dice [0.9311, 0.947, 0.9465]\n",
      "2023-12-31 22:50:36.325478: Epoch time: 125.96 s\n",
      "2023-12-31 22:50:36.335707: Yayy! New best EMA pseudo Dice: 0.9394\n",
      "2023-12-31 22:50:38.005810: \n",
      "2023-12-31 22:50:38.010870: Epoch 166\n",
      "2023-12-31 22:50:38.015882: Current learning rate: 0.00849\n",
      "2023-12-31 22:52:43.942194: train_loss -0.8983\n",
      "2023-12-31 22:52:43.950196: val_loss -0.8441\n",
      "2023-12-31 22:52:43.956204: Pseudo dice [0.9266, 0.9441, 0.9424]\n",
      "2023-12-31 22:52:43.962194: Epoch time: 125.94 s\n",
      "2023-12-31 22:52:45.224237: \n",
      "2023-12-31 22:52:45.229757: Epoch 167\n",
      "2023-12-31 22:52:45.234748: Current learning rate: 0.00848\n",
      "2023-12-31 22:54:51.230067: train_loss -0.8978\n",
      "2023-12-31 22:54:51.237067: val_loss -0.8461\n",
      "2023-12-31 22:54:51.245069: Pseudo dice [0.9313, 0.9469, 0.9464]\n",
      "2023-12-31 22:54:51.250068: Epoch time: 126.01 s\n",
      "2023-12-31 22:54:51.256068: Yayy! New best EMA pseudo Dice: 0.9394\n",
      "2023-12-31 22:54:52.873185: \n",
      "2023-12-31 22:54:52.878193: Epoch 168\n",
      "2023-12-31 22:54:52.882976: Current learning rate: 0.00847\n",
      "2023-12-31 22:56:58.359787: train_loss -0.9003\n",
      "2023-12-31 22:56:58.366786: val_loss -0.8426\n",
      "2023-12-31 22:56:58.373786: Pseudo dice [0.9299, 0.9451, 0.9445]\n",
      "2023-12-31 22:56:58.378790: Epoch time: 125.49 s\n",
      "2023-12-31 22:56:58.383790: Yayy! New best EMA pseudo Dice: 0.9395\n",
      "2023-12-31 22:56:59.911437: \n",
      "2023-12-31 22:56:59.916443: Epoch 169\n",
      "2023-12-31 22:56:59.921444: Current learning rate: 0.00847\n",
      "2023-12-31 22:59:06.229577: train_loss -0.8957\n",
      "2023-12-31 22:59:06.241576: val_loss -0.8301\n",
      "2023-12-31 22:59:06.249578: Pseudo dice [0.9227, 0.9416, 0.9386]\n",
      "2023-12-31 22:59:06.255845: Epoch time: 126.32 s\n",
      "2023-12-31 22:59:07.789552: \n",
      "2023-12-31 22:59:07.795550: Epoch 170\n",
      "2023-12-31 22:59:07.800545: Current learning rate: 0.00846\n",
      "2023-12-31 23:01:13.606165: train_loss -0.8918\n",
      "2023-12-31 23:01:13.613248: val_loss -0.8449\n",
      "2023-12-31 23:01:13.620239: Pseudo dice [0.9292, 0.9439, 0.9429]\n",
      "2023-12-31 23:01:13.629249: Epoch time: 125.82 s\n",
      "2023-12-31 23:01:14.796926: \n",
      "2023-12-31 23:01:14.803125: Epoch 171\n",
      "2023-12-31 23:01:14.809056: Current learning rate: 0.00845\n",
      "2023-12-31 23:03:20.493898: train_loss -0.8946\n",
      "2023-12-31 23:03:20.502898: val_loss -0.8291\n",
      "2023-12-31 23:03:20.507896: Pseudo dice [0.9186, 0.9426, 0.9392]\n",
      "2023-12-31 23:03:20.515901: Epoch time: 125.7 s\n",
      "2023-12-31 23:03:21.696843: \n",
      "2023-12-31 23:03:21.702840: Epoch 172\n",
      "2023-12-31 23:03:21.707839: Current learning rate: 0.00844\n",
      "2023-12-31 23:05:27.319916: train_loss -0.8918\n",
      "2023-12-31 23:05:27.326916: val_loss -0.8455\n",
      "2023-12-31 23:05:27.334921: Pseudo dice [0.9321, 0.9443, 0.9435]\n",
      "2023-12-31 23:05:27.339923: Epoch time: 125.62 s\n",
      "2023-12-31 23:05:28.503639: \n",
      "2023-12-31 23:05:28.509638: Epoch 173\n",
      "2023-12-31 23:05:28.513641: Current learning rate: 0.00843\n",
      "2023-12-31 23:07:34.193110: train_loss -0.8984\n",
      "2023-12-31 23:07:34.204110: val_loss -0.8441\n",
      "2023-12-31 23:07:34.212623: Pseudo dice [0.9259, 0.9454, 0.9442]\n",
      "2023-12-31 23:07:34.218622: Epoch time: 125.69 s\n",
      "2023-12-31 23:07:35.736314: \n",
      "2023-12-31 23:07:35.741313: Epoch 174\n",
      "2023-12-31 23:07:35.753321: Current learning rate: 0.00842\n",
      "2023-12-31 23:09:41.621508: train_loss -0.8979\n",
      "2023-12-31 23:09:41.628508: val_loss -0.8417\n",
      "2023-12-31 23:09:41.635508: Pseudo dice [0.9302, 0.9447, 0.9439]\n",
      "2023-12-31 23:09:41.650366: Epoch time: 125.89 s\n",
      "2023-12-31 23:09:42.809506: \n",
      "2023-12-31 23:09:42.815497: Epoch 175\n",
      "2023-12-31 23:09:42.819506: Current learning rate: 0.00841\n",
      "2023-12-31 23:11:48.254190: train_loss -0.8887\n",
      "2023-12-31 23:11:48.262235: val_loss -0.8463\n",
      "2023-12-31 23:11:48.271241: Pseudo dice [0.9308, 0.9446, 0.9441]\n",
      "2023-12-31 23:11:48.278236: Epoch time: 125.45 s\n",
      "2023-12-31 23:11:49.585924: \n",
      "2023-12-31 23:11:49.594925: Epoch 176\n",
      "2023-12-31 23:11:49.600919: Current learning rate: 0.0084\n",
      "2023-12-31 23:13:55.857538: train_loss -0.8896\n",
      "2023-12-31 23:13:55.865866: val_loss -0.8421\n",
      "2023-12-31 23:13:55.872866: Pseudo dice [0.9305, 0.9447, 0.9425]\n",
      "2023-12-31 23:13:55.879867: Epoch time: 126.27 s\n",
      "2023-12-31 23:13:57.150317: \n",
      "2023-12-31 23:13:57.156315: Epoch 177\n",
      "2023-12-31 23:13:57.161318: Current learning rate: 0.00839\n",
      "2023-12-31 23:16:02.431208: train_loss -0.8942\n",
      "2023-12-31 23:16:02.438710: val_loss -0.8369\n",
      "2023-12-31 23:16:02.445710: Pseudo dice [0.9262, 0.9437, 0.9417]\n",
      "2023-12-31 23:16:02.450711: Epoch time: 125.28 s\n",
      "2023-12-31 23:16:03.588321: \n",
      "2023-12-31 23:16:03.595392: Epoch 178\n",
      "2023-12-31 23:16:03.600377: Current learning rate: 0.00838\n",
      "2023-12-31 23:18:09.253200: train_loss -0.8968\n",
      "2023-12-31 23:18:09.259200: val_loss -0.8409\n",
      "2023-12-31 23:18:09.266205: Pseudo dice [0.9284, 0.944, 0.9426]\n",
      "2023-12-31 23:18:09.271210: Epoch time: 125.67 s\n",
      "2023-12-31 23:18:10.424998: \n",
      "2023-12-31 23:18:10.430254: Epoch 179\n",
      "2023-12-31 23:18:10.434254: Current learning rate: 0.00837\n",
      "2023-12-31 23:20:15.995983: train_loss -0.8962\n",
      "2023-12-31 23:20:16.005984: val_loss -0.8478\n",
      "2023-12-31 23:20:16.012982: Pseudo dice [0.9325, 0.9454, 0.9425]\n",
      "2023-12-31 23:20:16.018981: Epoch time: 125.57 s\n",
      "2023-12-31 23:20:17.361885: \n",
      "2023-12-31 23:20:17.367847: Epoch 180\n",
      "2023-12-31 23:20:17.372838: Current learning rate: 0.00836\n",
      "2023-12-31 23:22:23.121181: train_loss -0.897\n",
      "2023-12-31 23:22:23.128182: val_loss -0.8252\n",
      "2023-12-31 23:22:23.135182: Pseudo dice [0.92, 0.9401, 0.9379]\n",
      "2023-12-31 23:22:23.140188: Epoch time: 125.76 s\n",
      "2023-12-31 23:22:24.437210: \n",
      "2023-12-31 23:22:24.444210: Epoch 181\n",
      "2023-12-31 23:22:24.448210: Current learning rate: 0.00836\n",
      "2023-12-31 23:24:30.137932: train_loss -0.8974\n",
      "2023-12-31 23:24:30.144934: val_loss -0.8386\n",
      "2023-12-31 23:24:30.150934: Pseudo dice [0.9297, 0.9446, 0.9425]\n",
      "2023-12-31 23:24:30.160935: Epoch time: 125.7 s\n",
      "2023-12-31 23:24:31.638127: \n",
      "2023-12-31 23:24:31.643186: Epoch 182\n",
      "2023-12-31 23:24:31.648188: Current learning rate: 0.00835\n",
      "2023-12-31 23:26:37.595863: train_loss -0.8958\n",
      "2023-12-31 23:26:37.608112: val_loss -0.8366\n",
      "2023-12-31 23:26:37.623112: Pseudo dice [0.9247, 0.9427, 0.9403]\n",
      "2023-12-31 23:26:37.634480: Epoch time: 125.96 s\n",
      "2023-12-31 23:26:39.070589: \n",
      "2023-12-31 23:26:39.075242: Epoch 183\n",
      "2023-12-31 23:26:39.080315: Current learning rate: 0.00834\n",
      "2023-12-31 23:28:44.905368: train_loss -0.8997\n",
      "2023-12-31 23:28:44.914357: val_loss -0.8365\n",
      "2023-12-31 23:28:44.922358: Pseudo dice [0.9269, 0.9446, 0.9429]\n",
      "2023-12-31 23:28:44.928356: Epoch time: 125.84 s\n",
      "2023-12-31 23:28:46.230655: \n",
      "2023-12-31 23:28:46.236646: Epoch 184\n",
      "2023-12-31 23:28:46.240655: Current learning rate: 0.00833\n",
      "2023-12-31 23:30:52.028651: train_loss -0.8988\n",
      "2023-12-31 23:30:52.038651: val_loss -0.8279\n",
      "2023-12-31 23:30:52.045650: Pseudo dice [0.9216, 0.9418, 0.9409]\n",
      "2023-12-31 23:30:52.054655: Epoch time: 125.8 s\n",
      "2023-12-31 23:30:53.464370: \n",
      "2023-12-31 23:30:53.471876: Epoch 185\n",
      "2023-12-31 23:30:53.476876: Current learning rate: 0.00832\n",
      "2023-12-31 23:32:59.417641: train_loss -0.8983\n",
      "2023-12-31 23:32:59.424641: val_loss -0.8296\n",
      "2023-12-31 23:32:59.430641: Pseudo dice [0.9234, 0.9397, 0.9378]\n",
      "2023-12-31 23:32:59.435642: Epoch time: 125.95 s\n",
      "2023-12-31 23:33:00.665697: \n",
      "2023-12-31 23:33:00.672227: Epoch 186\n",
      "2023-12-31 23:33:00.677234: Current learning rate: 0.00831\n",
      "2023-12-31 23:35:06.543700: train_loss -0.8994\n",
      "2023-12-31 23:35:06.550699: val_loss -0.8424\n",
      "2023-12-31 23:35:06.557709: Pseudo dice [0.9267, 0.9445, 0.9434]\n",
      "2023-12-31 23:35:06.562710: Epoch time: 125.88 s\n",
      "2023-12-31 23:35:07.735073: \n",
      "2023-12-31 23:35:07.741348: Epoch 187\n",
      "2023-12-31 23:35:07.746347: Current learning rate: 0.0083\n",
      "2023-12-31 23:37:13.840654: train_loss -0.8978\n",
      "2023-12-31 23:37:13.847658: val_loss -0.8449\n",
      "2023-12-31 23:37:13.856829: Pseudo dice [0.9305, 0.9468, 0.9448]\n",
      "2023-12-31 23:37:13.863833: Epoch time: 126.11 s\n",
      "2023-12-31 23:37:15.141296: \n",
      "2023-12-31 23:37:15.148313: Epoch 188\n",
      "2023-12-31 23:37:15.152305: Current learning rate: 0.00829\n",
      "2023-12-31 23:39:21.207042: train_loss -0.8999\n",
      "2023-12-31 23:39:21.217041: val_loss -0.8235\n",
      "2023-12-31 23:39:21.225041: Pseudo dice [0.9217, 0.9403, 0.9379]\n",
      "2023-12-31 23:39:21.230042: Epoch time: 126.07 s\n",
      "2023-12-31 23:39:22.740550: \n",
      "2023-12-31 23:39:22.746550: Epoch 189\n",
      "2023-12-31 23:39:22.752552: Current learning rate: 0.00828\n",
      "2023-12-31 23:41:28.632732: train_loss -0.8998\n",
      "2023-12-31 23:41:28.640735: val_loss -0.8292\n",
      "2023-12-31 23:41:28.646736: Pseudo dice [0.9232, 0.94, 0.9374]\n",
      "2023-12-31 23:41:28.654736: Epoch time: 125.89 s\n",
      "2023-12-31 23:41:29.842810: \n",
      "2023-12-31 23:41:29.849824: Epoch 190\n",
      "2023-12-31 23:41:29.854828: Current learning rate: 0.00827\n",
      "2023-12-31 23:43:35.685852: train_loss -0.9013\n",
      "2023-12-31 23:43:35.693852: val_loss -0.8375\n",
      "2023-12-31 23:43:35.699856: Pseudo dice [0.9237, 0.9432, 0.9414]\n",
      "2023-12-31 23:43:35.705857: Epoch time: 125.84 s\n",
      "2023-12-31 23:43:37.008238: \n",
      "2023-12-31 23:43:37.012876: Epoch 191\n",
      "2023-12-31 23:43:37.017816: Current learning rate: 0.00826\n",
      "2023-12-31 23:45:42.565183: train_loss -0.9024\n",
      "2023-12-31 23:45:42.573184: val_loss -0.8361\n",
      "2023-12-31 23:45:42.580192: Pseudo dice [0.9265, 0.9438, 0.9419]\n",
      "2023-12-31 23:45:42.588192: Epoch time: 125.56 s\n",
      "2023-12-31 23:45:43.833569: \n",
      "2023-12-31 23:45:43.838568: Epoch 192\n",
      "2023-12-31 23:45:43.843560: Current learning rate: 0.00825\n",
      "2023-12-31 23:47:50.118097: train_loss -0.8998\n",
      "2023-12-31 23:47:50.127098: val_loss -0.8277\n",
      "2023-12-31 23:47:50.133609: Pseudo dice [0.9236, 0.9405, 0.9394]\n",
      "2023-12-31 23:47:50.140609: Epoch time: 126.29 s\n",
      "2023-12-31 23:47:51.399056: \n",
      "2023-12-31 23:47:51.406053: Epoch 193\n",
      "2023-12-31 23:47:51.410056: Current learning rate: 0.00824\n",
      "2023-12-31 23:49:57.395883: train_loss -0.8998\n",
      "2023-12-31 23:49:57.402882: val_loss -0.8358\n",
      "2023-12-31 23:49:57.408882: Pseudo dice [0.9283, 0.9437, 0.9409]\n",
      "2023-12-31 23:49:57.415883: Epoch time: 126.0 s\n",
      "2023-12-31 23:49:58.657474: \n",
      "2023-12-31 23:49:58.663465: Epoch 194\n",
      "2023-12-31 23:49:58.667474: Current learning rate: 0.00824\n",
      "2023-12-31 23:52:04.613486: train_loss -0.9004\n",
      "2023-12-31 23:52:04.620486: val_loss -0.8435\n",
      "2023-12-31 23:52:04.625486: Pseudo dice [0.9302, 0.9453, 0.943]\n",
      "2023-12-31 23:52:04.631486: Epoch time: 125.96 s\n",
      "2023-12-31 23:52:05.825676: \n",
      "2023-12-31 23:52:05.830675: Epoch 195\n",
      "2023-12-31 23:52:05.835667: Current learning rate: 0.00823\n",
      "2023-12-31 23:54:11.563822: train_loss -0.9035\n",
      "2023-12-31 23:54:11.570824: val_loss -0.8362\n",
      "2023-12-31 23:54:11.578822: Pseudo dice [0.9278, 0.9445, 0.9446]\n",
      "2023-12-31 23:54:11.583822: Epoch time: 125.74 s\n",
      "2023-12-31 23:54:12.830329: \n",
      "2023-12-31 23:54:12.835329: Epoch 196\n",
      "2023-12-31 23:54:12.839329: Current learning rate: 0.00822\n",
      "2023-12-31 23:56:18.673868: train_loss -0.9009\n",
      "2023-12-31 23:56:18.679868: val_loss -0.8412\n",
      "2023-12-31 23:56:18.686867: Pseudo dice [0.9296, 0.9453, 0.9441]\n",
      "2023-12-31 23:56:18.692867: Epoch time: 125.85 s\n",
      "2023-12-31 23:56:20.075479: \n",
      "2023-12-31 23:56:20.082486: Epoch 197\n",
      "2023-12-31 23:56:20.087476: Current learning rate: 0.00821\n",
      "2023-12-31 23:58:25.917723: train_loss -0.902\n",
      "2023-12-31 23:58:25.923723: val_loss -0.8452\n",
      "2023-12-31 23:58:25.928726: Pseudo dice [0.9304, 0.9467, 0.9447]\n",
      "2023-12-31 23:58:25.933723: Epoch time: 125.84 s\n",
      "2023-12-31 23:58:27.134184: \n",
      "2023-12-31 23:58:27.142181: Epoch 198\n",
      "2023-12-31 23:58:27.146180: Current learning rate: 0.0082\n",
      "2024-01-01 00:00:33.474914: train_loss -0.9001\n",
      "2024-01-01 00:00:33.482427: val_loss -0.846\n",
      "2024-01-01 00:00:33.489426: Pseudo dice [0.9296, 0.9463, 0.9449]\n",
      "2024-01-01 00:00:33.493430: Epoch time: 126.34 s\n",
      "2024-01-01 00:00:34.718890: \n",
      "2024-01-01 00:00:34.726894: Epoch 199\n",
      "2024-01-01 00:00:34.731916: Current learning rate: 0.00819\n",
      "2024-01-01 00:02:40.767733: train_loss -0.9002\n",
      "2024-01-01 00:02:40.775733: val_loss -0.8429\n",
      "2024-01-01 00:02:40.783733: Pseudo dice [0.9295, 0.9451, 0.9443]\n",
      "2024-01-01 00:02:40.788733: Epoch time: 126.05 s\n",
      "2024-01-01 00:02:42.280265: \n",
      "2024-01-01 00:02:42.285250: Epoch 200\n",
      "2024-01-01 00:02:42.290250: Current learning rate: 0.00818\n",
      "2024-01-01 00:04:48.785295: train_loss -0.897\n",
      "2024-01-01 00:04:48.793810: val_loss -0.8387\n",
      "2024-01-01 00:04:48.803806: Pseudo dice [0.9271, 0.9425, 0.9427]\n",
      "2024-01-01 00:04:48.812805: Epoch time: 126.51 s\n",
      "2024-01-01 00:04:50.094751: \n",
      "2024-01-01 00:04:50.100784: Epoch 201\n",
      "2024-01-01 00:04:50.104811: Current learning rate: 0.00817\n",
      "2024-01-01 00:06:55.981952: train_loss -0.9004\n",
      "2024-01-01 00:06:55.989951: val_loss -0.8381\n",
      "2024-01-01 00:06:55.995958: Pseudo dice [0.9246, 0.9434, 0.9428]\n",
      "2024-01-01 00:06:56.000954: Epoch time: 125.89 s\n",
      "2024-01-01 00:06:57.212797: \n",
      "2024-01-01 00:06:57.218804: Epoch 202\n",
      "2024-01-01 00:06:57.222809: Current learning rate: 0.00816\n",
      "2024-01-01 00:09:03.252276: train_loss -0.9027\n",
      "2024-01-01 00:09:03.258276: val_loss -0.8373\n",
      "2024-01-01 00:09:03.263276: Pseudo dice [0.927, 0.9425, 0.9403]\n",
      "2024-01-01 00:09:03.268276: Epoch time: 126.04 s\n",
      "2024-01-01 00:09:04.485548: \n",
      "2024-01-01 00:09:04.490548: Epoch 203\n",
      "2024-01-01 00:09:04.495552: Current learning rate: 0.00815\n",
      "2024-01-01 00:11:10.340495: train_loss -0.9005\n",
      "2024-01-01 00:11:10.349566: val_loss -0.8403\n",
      "2024-01-01 00:11:10.356493: Pseudo dice [0.9259, 0.9447, 0.9435]\n",
      "2024-01-01 00:11:10.362493: Epoch time: 125.86 s\n",
      "2024-01-01 00:11:11.723230: \n",
      "2024-01-01 00:11:11.728752: Epoch 204\n",
      "2024-01-01 00:11:11.732759: Current learning rate: 0.00814\n",
      "2024-01-01 00:13:17.607285: train_loss -0.9006\n",
      "2024-01-01 00:13:17.613285: val_loss -0.8396\n",
      "2024-01-01 00:13:17.619795: Pseudo dice [0.9269, 0.9434, 0.9424]\n",
      "2024-01-01 00:13:17.627795: Epoch time: 125.89 s\n",
      "2024-01-01 00:13:18.835717: \n",
      "2024-01-01 00:13:18.841706: Epoch 205\n",
      "2024-01-01 00:13:18.845716: Current learning rate: 0.00813\n",
      "2024-01-01 00:15:24.629353: train_loss -0.9043\n",
      "2024-01-01 00:15:24.636348: val_loss -0.8432\n",
      "2024-01-01 00:15:24.645346: Pseudo dice [0.93, 0.9451, 0.9429]\n",
      "2024-01-01 00:15:24.653348: Epoch time: 125.79 s\n",
      "2024-01-01 00:15:25.937090: \n",
      "2024-01-01 00:15:25.943092: Epoch 206\n",
      "2024-01-01 00:15:25.948101: Current learning rate: 0.00813\n",
      "2024-01-01 00:17:31.707906: train_loss -0.9051\n",
      "2024-01-01 00:17:31.717908: val_loss -0.8431\n",
      "2024-01-01 00:17:31.725906: Pseudo dice [0.9311, 0.9465, 0.9433]\n",
      "2024-01-01 00:17:31.731905: Epoch time: 125.77 s\n",
      "2024-01-01 00:17:32.919019: \n",
      "2024-01-01 00:17:32.925012: Epoch 207\n",
      "2024-01-01 00:17:32.930475: Current learning rate: 0.00812\n",
      "2024-01-01 00:19:38.632912: train_loss -0.9005\n",
      "2024-01-01 00:19:38.638916: val_loss -0.8404\n",
      "2024-01-01 00:19:38.644910: Pseudo dice [0.9291, 0.9463, 0.9452]\n",
      "2024-01-01 00:19:38.650908: Epoch time: 125.71 s\n",
      "2024-01-01 00:19:39.734813: \n",
      "2024-01-01 00:19:39.740856: Epoch 208\n",
      "2024-01-01 00:19:39.745879: Current learning rate: 0.00811\n",
      "2024-01-01 00:21:45.554431: train_loss -0.9058\n",
      "2024-01-01 00:21:45.561431: val_loss -0.8415\n",
      "2024-01-01 00:21:45.567434: Pseudo dice [0.9278, 0.946, 0.9437]\n",
      "2024-01-01 00:21:45.572432: Epoch time: 125.82 s\n",
      "2024-01-01 00:21:46.767719: \n",
      "2024-01-01 00:21:46.774723: Epoch 209\n",
      "2024-01-01 00:21:46.780731: Current learning rate: 0.0081\n",
      "2024-01-01 00:23:52.648850: train_loss -0.9026\n",
      "2024-01-01 00:23:52.661850: val_loss -0.8378\n",
      "2024-01-01 00:23:52.669019: Pseudo dice [0.9272, 0.946, 0.9444]\n",
      "2024-01-01 00:23:52.675025: Epoch time: 125.88 s\n",
      "2024-01-01 00:23:54.036107: \n",
      "2024-01-01 00:23:54.041115: Epoch 210\n",
      "2024-01-01 00:23:54.046115: Current learning rate: 0.00809\n",
      "2024-01-01 00:25:59.921738: train_loss -0.9012\n",
      "2024-01-01 00:25:59.928738: val_loss -0.8409\n",
      "2024-01-01 00:25:59.933747: Pseudo dice [0.9254, 0.944, 0.9434]\n",
      "2024-01-01 00:25:59.939739: Epoch time: 125.89 s\n",
      "2024-01-01 00:26:01.091528: \n",
      "2024-01-01 00:26:01.096527: Epoch 211\n",
      "2024-01-01 00:26:01.101593: Current learning rate: 0.00808\n",
      "2024-01-01 00:28:07.610930: train_loss -0.9007\n",
      "2024-01-01 00:28:07.617935: val_loss -0.8426\n",
      "2024-01-01 00:28:07.627643: Pseudo dice [0.9282, 0.9461, 0.9445]\n",
      "2024-01-01 00:28:07.633651: Epoch time: 126.52 s\n",
      "2024-01-01 00:28:09.077613: \n",
      "2024-01-01 00:28:09.083605: Epoch 212\n",
      "2024-01-01 00:28:09.087613: Current learning rate: 0.00807\n",
      "2024-01-01 00:30:14.855968: train_loss -0.9024\n",
      "2024-01-01 00:30:14.864977: val_loss -0.8332\n",
      "2024-01-01 00:30:14.869970: Pseudo dice [0.9276, 0.944, 0.9442]\n",
      "2024-01-01 00:30:14.873972: Epoch time: 125.78 s\n",
      "2024-01-01 00:30:16.041053: \n",
      "2024-01-01 00:30:16.048586: Epoch 213\n",
      "2024-01-01 00:30:16.052586: Current learning rate: 0.00806\n",
      "2024-01-01 00:32:22.205381: train_loss -0.9019\n",
      "2024-01-01 00:32:22.217381: val_loss -0.8444\n",
      "2024-01-01 00:32:22.227385: Pseudo dice [0.9303, 0.9467, 0.9446]\n",
      "2024-01-01 00:32:22.238897: Epoch time: 126.17 s\n",
      "2024-01-01 00:32:23.680329: \n",
      "2024-01-01 00:32:23.686332: Epoch 214\n",
      "2024-01-01 00:32:23.691330: Current learning rate: 0.00805\n",
      "2024-01-01 00:34:29.476759: train_loss -0.9042\n",
      "2024-01-01 00:34:29.482753: val_loss -0.8487\n",
      "2024-01-01 00:34:29.487751: Pseudo dice [0.9312, 0.9466, 0.9446]\n",
      "2024-01-01 00:34:29.493752: Epoch time: 125.8 s\n",
      "2024-01-01 00:34:30.672020: \n",
      "2024-01-01 00:34:30.678020: Epoch 215\n",
      "2024-01-01 00:34:30.684020: Current learning rate: 0.00804\n",
      "2024-01-01 00:36:36.572562: train_loss -0.9046\n",
      "2024-01-01 00:36:36.582789: val_loss -0.8402\n",
      "2024-01-01 00:36:36.589787: Pseudo dice [0.9283, 0.9447, 0.9444]\n",
      "2024-01-01 00:36:36.596790: Epoch time: 125.9 s\n",
      "2024-01-01 00:36:37.741872: \n",
      "2024-01-01 00:36:37.747863: Epoch 216\n",
      "2024-01-01 00:36:37.751872: Current learning rate: 0.00803\n",
      "2024-01-01 00:38:43.733135: train_loss -0.9031\n",
      "2024-01-01 00:38:43.740136: val_loss -0.8454\n",
      "2024-01-01 00:38:43.748136: Pseudo dice [0.9293, 0.9445, 0.9439]\n",
      "2024-01-01 00:38:43.755136: Epoch time: 125.99 s\n",
      "2024-01-01 00:38:44.872210: \n",
      "2024-01-01 00:38:44.878593: Epoch 217\n",
      "2024-01-01 00:38:44.882605: Current learning rate: 0.00802\n",
      "2024-01-01 00:40:51.013584: train_loss -0.9002\n",
      "2024-01-01 00:40:51.021593: val_loss -0.8417\n",
      "2024-01-01 00:40:51.029549: Pseudo dice [0.9293, 0.9465, 0.9459]\n",
      "2024-01-01 00:40:51.036537: Epoch time: 126.14 s\n",
      "2024-01-01 00:40:52.265134: \n",
      "2024-01-01 00:40:52.271053: Epoch 218\n",
      "2024-01-01 00:40:52.275062: Current learning rate: 0.00801\n",
      "2024-01-01 00:42:58.057304: train_loss -0.9061\n",
      "2024-01-01 00:42:58.064309: val_loss -0.84\n",
      "2024-01-01 00:42:58.069308: Pseudo dice [0.9287, 0.9446, 0.943]\n",
      "2024-01-01 00:42:58.073820: Epoch time: 125.79 s\n",
      "2024-01-01 00:42:59.467623: \n",
      "2024-01-01 00:42:59.474146: Epoch 219\n",
      "2024-01-01 00:42:59.479154: Current learning rate: 0.00801\n",
      "2024-01-01 00:45:06.156478: train_loss -0.9007\n",
      "2024-01-01 00:45:06.162478: val_loss -0.8504\n",
      "2024-01-01 00:45:06.169479: Pseudo dice [0.9298, 0.947, 0.9449]\n",
      "2024-01-01 00:45:06.176480: Epoch time: 126.69 s\n",
      "2024-01-01 00:45:07.418794: \n",
      "2024-01-01 00:45:07.423798: Epoch 220\n",
      "2024-01-01 00:45:07.428794: Current learning rate: 0.008\n",
      "2024-01-01 00:47:13.481315: train_loss -0.9027\n",
      "2024-01-01 00:47:13.490317: val_loss -0.8337\n",
      "2024-01-01 00:47:13.499317: Pseudo dice [0.9224, 0.9424, 0.9401]\n",
      "2024-01-01 00:47:13.507315: Epoch time: 126.06 s\n",
      "2024-01-01 00:47:14.797525: \n",
      "2024-01-01 00:47:14.803513: Epoch 221\n",
      "2024-01-01 00:47:14.809522: Current learning rate: 0.00799\n",
      "2024-01-01 00:49:20.762632: train_loss -0.9034\n",
      "2024-01-01 00:49:20.769634: val_loss -0.8239\n",
      "2024-01-01 00:49:20.776636: Pseudo dice [0.9184, 0.9391, 0.9368]\n",
      "2024-01-01 00:49:20.782634: Epoch time: 125.97 s\n",
      "2024-01-01 00:49:21.911944: \n",
      "2024-01-01 00:49:21.916925: Epoch 222\n",
      "2024-01-01 00:49:21.921874: Current learning rate: 0.00798\n",
      "2024-01-01 00:51:28.214908: train_loss -0.8989\n",
      "2024-01-01 00:51:28.221911: val_loss -0.8368\n",
      "2024-01-01 00:51:28.228911: Pseudo dice [0.9248, 0.9434, 0.939]\n",
      "2024-01-01 00:51:28.235909: Epoch time: 126.31 s\n",
      "2024-01-01 00:51:29.473301: \n",
      "2024-01-01 00:51:29.478301: Epoch 223\n",
      "2024-01-01 00:51:29.483301: Current learning rate: 0.00797\n",
      "2024-01-01 00:53:35.914303: train_loss -0.8987\n",
      "2024-01-01 00:53:35.921302: val_loss -0.8456\n",
      "2024-01-01 00:53:35.927302: Pseudo dice [0.9312, 0.9462, 0.9448]\n",
      "2024-01-01 00:53:35.931302: Epoch time: 126.44 s\n",
      "2024-01-01 00:53:37.113075: \n",
      "2024-01-01 00:53:37.118064: Epoch 224\n",
      "2024-01-01 00:53:37.122068: Current learning rate: 0.00796\n",
      "2024-01-01 00:55:43.621144: train_loss -0.8986\n",
      "2024-01-01 00:55:43.628145: val_loss -0.8489\n",
      "2024-01-01 00:55:43.635152: Pseudo dice [0.9295, 0.9468, 0.9453]\n",
      "2024-01-01 00:55:43.640151: Epoch time: 126.51 s\n",
      "2024-01-01 00:55:44.905156: \n",
      "2024-01-01 00:55:44.911151: Epoch 225\n",
      "2024-01-01 00:55:44.915151: Current learning rate: 0.00795\n",
      "2024-01-01 00:57:51.151556: train_loss -0.9011\n",
      "2024-01-01 00:57:51.157556: val_loss -0.8425\n",
      "2024-01-01 00:57:51.163556: Pseudo dice [0.9319, 0.9463, 0.946]\n",
      "2024-01-01 00:57:51.169556: Epoch time: 126.25 s\n",
      "2024-01-01 00:57:52.267739: \n",
      "2024-01-01 00:57:52.273812: Epoch 226\n",
      "2024-01-01 00:57:52.278796: Current learning rate: 0.00794\n",
      "2024-01-01 00:59:58.135927: train_loss -0.9055\n",
      "2024-01-01 00:59:58.143929: val_loss -0.8492\n",
      "2024-01-01 00:59:58.149928: Pseudo dice [0.9321, 0.9472, 0.9471]\n",
      "2024-01-01 00:59:58.155928: Epoch time: 125.87 s\n",
      "2024-01-01 00:59:59.477318: \n",
      "2024-01-01 00:59:59.483310: Epoch 227\n",
      "2024-01-01 00:59:59.487318: Current learning rate: 0.00793\n",
      "2024-01-01 01:02:05.439176: train_loss -0.9054\n",
      "2024-01-01 01:02:05.446218: val_loss -0.8396\n",
      "2024-01-01 01:02:05.453204: Pseudo dice [0.9304, 0.9455, 0.9443]\n",
      "2024-01-01 01:02:05.458205: Epoch time: 125.96 s\n",
      "2024-01-01 01:02:06.633318: \n",
      "2024-01-01 01:02:06.639309: Epoch 228\n",
      "2024-01-01 01:02:06.643318: Current learning rate: 0.00792\n",
      "2024-01-01 01:04:12.603975: train_loss -0.9049\n",
      "2024-01-01 01:04:12.613170: val_loss -0.8298\n",
      "2024-01-01 01:04:12.620168: Pseudo dice [0.9205, 0.9434, 0.9416]\n",
      "2024-01-01 01:04:12.628169: Epoch time: 125.97 s\n",
      "2024-01-01 01:04:13.685357: \n",
      "2024-01-01 01:04:13.690357: Epoch 229\n",
      "2024-01-01 01:04:13.694357: Current learning rate: 0.00791\n",
      "2024-01-01 01:06:19.566602: train_loss -0.9026\n",
      "2024-01-01 01:06:19.574666: val_loss -0.8467\n",
      "2024-01-01 01:06:19.581656: Pseudo dice [0.9311, 0.9464, 0.9454]\n",
      "2024-01-01 01:06:19.587658: Epoch time: 125.88 s\n",
      "2024-01-01 01:06:20.719226: \n",
      "2024-01-01 01:06:20.724221: Epoch 230\n",
      "2024-01-01 01:06:20.729220: Current learning rate: 0.0079\n",
      "2024-01-01 01:08:27.309399: train_loss -0.9022\n",
      "2024-01-01 01:08:27.318455: val_loss -0.8368\n",
      "2024-01-01 01:08:27.324449: Pseudo dice [0.927, 0.9447, 0.9449]\n",
      "2024-01-01 01:08:27.329448: Epoch time: 126.59 s\n",
      "2024-01-01 01:08:28.490868: \n",
      "2024-01-01 01:08:28.496866: Epoch 231\n",
      "2024-01-01 01:08:28.501874: Current learning rate: 0.00789\n",
      "2024-01-01 01:10:34.597175: train_loss -0.903\n",
      "2024-01-01 01:10:34.604178: val_loss -0.8473\n",
      "2024-01-01 01:10:34.612188: Pseudo dice [0.9306, 0.9457, 0.9442]\n",
      "2024-01-01 01:10:34.618175: Epoch time: 126.11 s\n",
      "2024-01-01 01:10:35.765710: \n",
      "2024-01-01 01:10:35.772280: Epoch 232\n",
      "2024-01-01 01:10:35.776289: Current learning rate: 0.00789\n",
      "2024-01-01 01:12:42.405149: train_loss -0.9014\n",
      "2024-01-01 01:12:42.416182: val_loss -0.8407\n",
      "2024-01-01 01:12:42.425180: Pseudo dice [0.9278, 0.9451, 0.9446]\n",
      "2024-01-01 01:12:42.435179: Epoch time: 126.64 s\n",
      "2024-01-01 01:12:43.616473: \n",
      "2024-01-01 01:12:43.621473: Epoch 233\n",
      "2024-01-01 01:12:43.628476: Current learning rate: 0.00788\n",
      "2024-01-01 01:14:49.768241: train_loss -0.9025\n",
      "2024-01-01 01:14:49.779241: val_loss -0.84\n",
      "2024-01-01 01:14:49.796937: Pseudo dice [0.9271, 0.9435, 0.9435]\n",
      "2024-01-01 01:14:49.803447: Epoch time: 126.15 s\n",
      "2024-01-01 01:14:50.938785: \n",
      "2024-01-01 01:14:50.945791: Epoch 234\n",
      "2024-01-01 01:14:50.950777: Current learning rate: 0.00787\n",
      "2024-01-01 01:16:56.813445: train_loss -0.9072\n",
      "2024-01-01 01:16:56.823068: val_loss -0.8489\n",
      "2024-01-01 01:16:56.832065: Pseudo dice [0.9318, 0.9487, 0.946]\n",
      "2024-01-01 01:16:56.842845: Epoch time: 125.88 s\n",
      "2024-01-01 01:16:58.244419: \n",
      "2024-01-01 01:16:58.251411: Epoch 235\n",
      "2024-01-01 01:16:58.255419: Current learning rate: 0.00786\n",
      "2024-01-01 01:19:04.253148: train_loss -0.9039\n",
      "2024-01-01 01:19:04.261142: val_loss -0.8416\n",
      "2024-01-01 01:19:04.267142: Pseudo dice [0.9304, 0.9461, 0.9449]\n",
      "2024-01-01 01:19:04.272142: Epoch time: 126.01 s\n",
      "2024-01-01 01:19:05.435289: \n",
      "2024-01-01 01:19:05.440814: Epoch 236\n",
      "2024-01-01 01:19:05.444814: Current learning rate: 0.00785\n",
      "2024-01-01 01:21:11.184497: train_loss -0.9064\n",
      "2024-01-01 01:21:11.192495: val_loss -0.846\n",
      "2024-01-01 01:21:11.199494: Pseudo dice [0.9292, 0.9458, 0.9445]\n",
      "2024-01-01 01:21:11.205494: Epoch time: 125.75 s\n",
      "2024-01-01 01:21:12.344116: \n",
      "2024-01-01 01:21:12.353116: Epoch 237\n",
      "2024-01-01 01:21:12.359116: Current learning rate: 0.00784\n",
      "2024-01-01 01:23:18.513268: train_loss -0.9049\n",
      "2024-01-01 01:23:18.523268: val_loss -0.8394\n",
      "2024-01-01 01:23:18.531268: Pseudo dice [0.9298, 0.9444, 0.9443]\n",
      "2024-01-01 01:23:18.536268: Epoch time: 126.17 s\n",
      "2024-01-01 01:23:19.920243: \n",
      "2024-01-01 01:23:19.925242: Epoch 238\n",
      "2024-01-01 01:23:19.930248: Current learning rate: 0.00783\n",
      "2024-01-01 01:25:26.548425: train_loss -0.902\n",
      "2024-01-01 01:25:26.558425: val_loss -0.8388\n",
      "2024-01-01 01:25:26.567433: Pseudo dice [0.9281, 0.9431, 0.9421]\n",
      "2024-01-01 01:25:26.574429: Epoch time: 126.63 s\n",
      "2024-01-01 01:25:27.738161: \n",
      "2024-01-01 01:25:27.744161: Epoch 239\n",
      "2024-01-01 01:25:27.755197: Current learning rate: 0.00782\n",
      "2024-01-01 01:27:33.852006: train_loss -0.9034\n",
      "2024-01-01 01:27:33.858515: val_loss -0.838\n",
      "2024-01-01 01:27:33.864517: Pseudo dice [0.9286, 0.9451, 0.9439]\n",
      "2024-01-01 01:27:33.870528: Epoch time: 126.11 s\n",
      "2024-01-01 01:27:34.996935: \n",
      "2024-01-01 01:27:35.002927: Epoch 240\n",
      "2024-01-01 01:27:35.006928: Current learning rate: 0.00781\n",
      "2024-01-01 01:29:41.195417: train_loss -0.9017\n",
      "2024-01-01 01:29:41.202417: val_loss -0.834\n",
      "2024-01-01 01:29:41.209419: Pseudo dice [0.9234, 0.9448, 0.9434]\n",
      "2024-01-01 01:29:41.215418: Epoch time: 126.2 s\n",
      "2024-01-01 01:29:42.425129: \n",
      "2024-01-01 01:29:42.430652: Epoch 241\n",
      "2024-01-01 01:29:42.435651: Current learning rate: 0.0078\n",
      "2024-01-01 01:31:48.538333: train_loss -0.9037\n",
      "2024-01-01 01:31:48.549339: val_loss -0.8394\n",
      "2024-01-01 01:31:48.558634: Pseudo dice [0.9256, 0.9454, 0.9427]\n",
      "2024-01-01 01:31:48.565146: Epoch time: 126.11 s\n",
      "2024-01-01 01:31:49.981878: \n",
      "2024-01-01 01:31:49.988878: Epoch 242\n",
      "2024-01-01 01:31:49.993869: Current learning rate: 0.00779\n",
      "2024-01-01 01:33:56.027801: train_loss -0.9054\n",
      "2024-01-01 01:33:56.034806: val_loss -0.8362\n",
      "2024-01-01 01:33:56.040315: Pseudo dice [0.9275, 0.944, 0.9425]\n",
      "2024-01-01 01:33:56.048320: Epoch time: 126.05 s\n",
      "2024-01-01 01:33:57.465067: \n",
      "2024-01-01 01:33:57.470067: Epoch 243\n",
      "2024-01-01 01:33:57.481067: Current learning rate: 0.00778\n",
      "2024-01-01 01:36:03.543178: train_loss -0.9026\n",
      "2024-01-01 01:36:03.551182: val_loss -0.8353\n",
      "2024-01-01 01:36:03.559186: Pseudo dice [0.9269, 0.9439, 0.9433]\n",
      "2024-01-01 01:36:03.564698: Epoch time: 126.08 s\n",
      "2024-01-01 01:36:04.851980: \n",
      "2024-01-01 01:36:04.858026: Epoch 244\n",
      "2024-01-01 01:36:04.862097: Current learning rate: 0.00777\n",
      "2024-01-01 01:38:10.803873: train_loss -0.9044\n",
      "2024-01-01 01:38:10.814880: val_loss -0.8349\n",
      "2024-01-01 01:38:10.822873: Pseudo dice [0.926, 0.9444, 0.9443]\n",
      "2024-01-01 01:38:10.828875: Epoch time: 125.95 s\n",
      "2024-01-01 01:38:11.979937: \n",
      "2024-01-01 01:38:11.985998: Epoch 245\n",
      "2024-01-01 01:38:11.990941: Current learning rate: 0.00777\n",
      "2024-01-01 01:40:18.058311: train_loss -0.9056\n",
      "2024-01-01 01:40:18.068356: val_loss -0.8137\n",
      "2024-01-01 01:40:18.075347: Pseudo dice [0.9182, 0.9396, 0.9382]\n",
      "2024-01-01 01:40:18.081103: Epoch time: 126.08 s\n",
      "2024-01-01 01:40:19.298312: \n",
      "2024-01-01 01:40:19.304317: Epoch 246\n",
      "2024-01-01 01:40:19.310310: Current learning rate: 0.00776\n",
      "2024-01-01 01:42:25.239522: train_loss -0.903\n",
      "2024-01-01 01:42:25.247520: val_loss -0.8187\n",
      "2024-01-01 01:42:25.255511: Pseudo dice [0.9201, 0.9403, 0.9384]\n",
      "2024-01-01 01:42:25.260511: Epoch time: 125.94 s\n",
      "2024-01-01 01:42:26.401881: \n",
      "2024-01-01 01:42:26.407885: Epoch 247\n",
      "2024-01-01 01:42:26.413892: Current learning rate: 0.00775\n",
      "2024-01-01 01:44:32.293256: train_loss -0.9064\n",
      "2024-01-01 01:44:32.300261: val_loss -0.8262\n",
      "2024-01-01 01:44:32.308254: Pseudo dice [0.9206, 0.9427, 0.939]\n",
      "2024-01-01 01:44:32.313905: Epoch time: 125.89 s\n",
      "2024-01-01 01:44:33.449477: \n",
      "2024-01-01 01:44:33.457479: Epoch 248\n",
      "2024-01-01 01:44:33.461487: Current learning rate: 0.00774\n",
      "2024-01-01 01:46:39.765571: train_loss -0.9032\n",
      "2024-01-01 01:46:39.776788: val_loss -0.8386\n",
      "2024-01-01 01:46:39.785789: Pseudo dice [0.9261, 0.9445, 0.9424]\n",
      "2024-01-01 01:46:39.791791: Epoch time: 126.32 s\n",
      "2024-01-01 01:46:41.097464: \n",
      "2024-01-01 01:46:41.103413: Epoch 249\n",
      "2024-01-01 01:46:41.107412: Current learning rate: 0.00773\n",
      "2024-01-01 01:48:47.354583: train_loss -0.905\n",
      "2024-01-01 01:48:47.362582: val_loss -0.8431\n",
      "2024-01-01 01:48:47.369582: Pseudo dice [0.9323, 0.9475, 0.9454]\n",
      "2024-01-01 01:48:47.376582: Epoch time: 126.26 s\n",
      "2024-01-01 01:48:48.860050: \n",
      "2024-01-01 01:48:48.865050: Epoch 250\n",
      "2024-01-01 01:48:48.870050: Current learning rate: 0.00772\n",
      "2024-01-01 01:50:54.892455: train_loss -0.9034\n",
      "2024-01-01 01:50:54.903455: val_loss -0.8329\n",
      "2024-01-01 01:50:54.914967: Pseudo dice [0.9269, 0.9455, 0.944]\n",
      "2024-01-01 01:50:54.922966: Epoch time: 126.03 s\n",
      "2024-01-01 01:50:56.386362: \n",
      "2024-01-01 01:50:56.393362: Epoch 251\n",
      "2024-01-01 01:50:56.398369: Current learning rate: 0.00771\n",
      "2024-01-01 01:53:02.581157: train_loss -0.9043\n",
      "2024-01-01 01:53:02.589162: val_loss -0.8317\n",
      "2024-01-01 01:53:02.598162: Pseudo dice [0.9246, 0.9417, 0.9405]\n",
      "2024-01-01 01:53:02.606681: Epoch time: 126.2 s\n",
      "2024-01-01 01:53:03.792816: \n",
      "2024-01-01 01:53:03.797807: Epoch 252\n",
      "2024-01-01 01:53:03.801815: Current learning rate: 0.0077\n",
      "2024-01-01 01:55:09.927462: train_loss -0.9023\n",
      "2024-01-01 01:55:09.934464: val_loss -0.8428\n",
      "2024-01-01 01:55:09.940462: Pseudo dice [0.9286, 0.9473, 0.9463]\n",
      "2024-01-01 01:55:09.945462: Epoch time: 126.14 s\n",
      "2024-01-01 01:55:11.118084: \n",
      "2024-01-01 01:55:11.124078: Epoch 253\n",
      "2024-01-01 01:55:11.129087: Current learning rate: 0.00769\n",
      "2024-01-01 01:57:17.144655: train_loss -0.9034\n",
      "2024-01-01 01:57:17.153659: val_loss -0.8158\n",
      "2024-01-01 01:57:17.158655: Pseudo dice [0.919, 0.9398, 0.9385]\n",
      "2024-01-01 01:57:17.166649: Epoch time: 126.03 s\n",
      "2024-01-01 01:57:18.356376: \n",
      "2024-01-01 01:57:18.363379: Epoch 254\n",
      "2024-01-01 01:57:18.369458: Current learning rate: 0.00768\n",
      "2024-01-01 01:59:24.521486: train_loss -0.9033\n",
      "2024-01-01 01:59:24.530484: val_loss -0.831\n",
      "2024-01-01 01:59:24.538486: Pseudo dice [0.9233, 0.9414, 0.9396]\n",
      "2024-01-01 01:59:24.547485: Epoch time: 126.17 s\n",
      "2024-01-01 01:59:25.745143: \n",
      "2024-01-01 01:59:25.751137: Epoch 255\n",
      "2024-01-01 01:59:25.756138: Current learning rate: 0.00767\n",
      "2024-01-01 02:01:31.989368: train_loss -0.9055\n",
      "2024-01-01 02:01:31.997368: val_loss -0.8262\n",
      "2024-01-01 02:01:32.005878: Pseudo dice [0.9173, 0.9411, 0.9394]\n",
      "2024-01-01 02:01:32.012878: Epoch time: 126.25 s\n",
      "2024-01-01 02:01:33.248634: \n",
      "2024-01-01 02:01:33.254629: Epoch 256\n",
      "2024-01-01 02:01:33.258629: Current learning rate: 0.00766\n",
      "2024-01-01 02:03:39.649988: train_loss -0.9026\n",
      "2024-01-01 02:03:39.659986: val_loss -0.8185\n",
      "2024-01-01 02:03:39.665987: Pseudo dice [0.9164, 0.9383, 0.9357]\n",
      "2024-01-01 02:03:39.674985: Epoch time: 126.4 s\n",
      "2024-01-01 02:03:40.815208: \n",
      "2024-01-01 02:03:40.821201: Epoch 257\n",
      "2024-01-01 02:03:40.825214: Current learning rate: 0.00765\n",
      "2024-01-01 02:05:46.492657: train_loss -0.9071\n",
      "2024-01-01 02:05:46.500663: val_loss -0.8422\n",
      "2024-01-01 02:05:46.508179: Pseudo dice [0.9276, 0.9447, 0.9432]\n",
      "2024-01-01 02:05:46.519708: Epoch time: 125.68 s\n",
      "2024-01-01 02:05:47.766529: \n",
      "2024-01-01 02:05:47.772619: Epoch 258\n",
      "2024-01-01 02:05:47.778539: Current learning rate: 0.00764\n",
      "2024-01-01 02:07:54.092837: train_loss -0.9036\n",
      "2024-01-01 02:07:54.103417: val_loss -0.8424\n",
      "2024-01-01 02:07:54.112670: Pseudo dice [0.9311, 0.9454, 0.9449]\n",
      "2024-01-01 02:07:54.119674: Epoch time: 126.33 s\n",
      "2024-01-01 02:07:55.526391: \n",
      "2024-01-01 02:07:55.531918: Epoch 259\n",
      "2024-01-01 02:07:55.535920: Current learning rate: 0.00764\n",
      "2024-01-01 02:10:01.951568: train_loss -0.9047\n",
      "2024-01-01 02:10:01.963075: val_loss -0.8391\n",
      "2024-01-01 02:10:01.972075: Pseudo dice [0.9266, 0.9457, 0.9459]\n",
      "2024-01-01 02:10:01.980077: Epoch time: 126.43 s\n",
      "2024-01-01 02:10:03.266739: \n",
      "2024-01-01 02:10:03.272738: Epoch 260\n",
      "2024-01-01 02:10:03.283776: Current learning rate: 0.00763\n",
      "2024-01-01 02:12:09.574266: train_loss -0.9035\n",
      "2024-01-01 02:12:09.581261: val_loss -0.7958\n",
      "2024-01-01 02:12:09.590261: Pseudo dice [0.9062, 0.9357, 0.9277]\n",
      "2024-01-01 02:12:09.597262: Epoch time: 126.31 s\n",
      "2024-01-01 02:12:10.746264: \n",
      "2024-01-01 02:12:10.757263: Epoch 261\n",
      "2024-01-01 02:12:10.761271: Current learning rate: 0.00762\n",
      "2024-01-01 02:14:16.450647: train_loss -0.8983\n",
      "2024-01-01 02:14:16.459646: val_loss -0.851\n",
      "2024-01-01 02:14:16.467771: Pseudo dice [0.9311, 0.9481, 0.9462]\n",
      "2024-01-01 02:14:16.472775: Epoch time: 125.71 s\n",
      "2024-01-01 02:14:17.644531: \n",
      "2024-01-01 02:14:17.651531: Epoch 262\n",
      "2024-01-01 02:14:17.656523: Current learning rate: 0.00761\n",
      "2024-01-01 02:16:23.653463: train_loss -0.8994\n",
      "2024-01-01 02:16:23.661462: val_loss -0.8424\n",
      "2024-01-01 02:16:23.668462: Pseudo dice [0.9255, 0.9451, 0.9418]\n",
      "2024-01-01 02:16:23.673461: Epoch time: 126.01 s\n",
      "2024-01-01 02:16:24.908435: \n",
      "2024-01-01 02:16:24.913435: Epoch 263\n",
      "2024-01-01 02:16:24.918428: Current learning rate: 0.0076\n",
      "2024-01-01 02:18:31.061834: train_loss -0.8933\n",
      "2024-01-01 02:18:31.072341: val_loss -0.8207\n",
      "2024-01-01 02:18:31.082342: Pseudo dice [0.9187, 0.9374, 0.9335]\n",
      "2024-01-01 02:18:31.091341: Epoch time: 126.16 s\n",
      "2024-01-01 02:18:32.381879: \n",
      "2024-01-01 02:18:32.391880: Epoch 264\n",
      "2024-01-01 02:18:32.396878: Current learning rate: 0.00759\n",
      "2024-01-01 02:20:37.958848: train_loss -0.9031\n",
      "2024-01-01 02:20:37.967849: val_loss -0.8434\n",
      "2024-01-01 02:20:37.974848: Pseudo dice [0.9271, 0.9461, 0.9444]\n",
      "2024-01-01 02:20:37.979849: Epoch time: 125.58 s\n",
      "2024-01-01 02:20:39.196913: \n",
      "2024-01-01 02:20:39.202909: Epoch 265\n",
      "2024-01-01 02:20:39.208909: Current learning rate: 0.00758\n",
      "2024-01-01 02:22:45.220049: train_loss -0.9048\n",
      "2024-01-01 02:22:45.231055: val_loss -0.8231\n",
      "2024-01-01 02:22:45.240053: Pseudo dice [0.9221, 0.9403, 0.9389]\n",
      "2024-01-01 02:22:45.248566: Epoch time: 126.02 s\n",
      "2024-01-01 02:22:46.727235: \n",
      "2024-01-01 02:22:46.732254: Epoch 266\n",
      "2024-01-01 02:22:46.737253: Current learning rate: 0.00757\n",
      "2024-01-01 02:24:52.732926: train_loss -0.9034\n",
      "2024-01-01 02:24:52.739488: val_loss -0.8315\n",
      "2024-01-01 02:24:52.744482: Pseudo dice [0.9214, 0.9421, 0.9404]\n",
      "2024-01-01 02:24:52.748482: Epoch time: 126.01 s\n",
      "2024-01-01 02:24:53.901306: \n",
      "2024-01-01 02:24:53.907297: Epoch 267\n",
      "2024-01-01 02:24:53.915310: Current learning rate: 0.00756\n",
      "2024-01-01 02:27:00.100857: train_loss -0.9056\n",
      "2024-01-01 02:27:00.108858: val_loss -0.8378\n",
      "2024-01-01 02:27:00.161347: Pseudo dice [0.9277, 0.9457, 0.9436]\n",
      "2024-01-01 02:27:00.168347: Epoch time: 126.2 s\n",
      "2024-01-01 02:27:01.580024: \n",
      "2024-01-01 02:27:01.585025: Epoch 268\n",
      "2024-01-01 02:27:01.590030: Current learning rate: 0.00755\n",
      "2024-01-01 02:29:07.240300: train_loss -0.9071\n",
      "2024-01-01 02:29:07.249445: val_loss -0.8263\n",
      "2024-01-01 02:29:07.259955: Pseudo dice [0.9199, 0.9408, 0.9391]\n",
      "2024-01-01 02:29:07.268953: Epoch time: 125.66 s\n",
      "2024-01-01 02:29:08.668032: \n",
      "2024-01-01 02:29:08.674032: Epoch 269\n",
      "2024-01-01 02:29:08.679024: Current learning rate: 0.00754\n",
      "2024-01-01 02:31:14.698266: train_loss -0.9046\n",
      "2024-01-01 02:31:14.705268: val_loss -0.8431\n",
      "2024-01-01 02:31:14.713266: Pseudo dice [0.9301, 0.9453, 0.9456]\n",
      "2024-01-01 02:31:14.720268: Epoch time: 126.03 s\n",
      "2024-01-01 02:31:15.926372: \n",
      "2024-01-01 02:31:15.931397: Epoch 270\n",
      "2024-01-01 02:31:15.935396: Current learning rate: 0.00753\n",
      "2024-01-01 02:33:21.700342: train_loss -0.9073\n",
      "2024-01-01 02:33:21.708342: val_loss -0.8362\n",
      "2024-01-01 02:33:21.715354: Pseudo dice [0.9275, 0.9453, 0.9436]\n",
      "2024-01-01 02:33:21.723354: Epoch time: 125.77 s\n",
      "2024-01-01 02:33:22.917791: \n",
      "2024-01-01 02:33:22.922855: Epoch 271\n",
      "2024-01-01 02:33:22.926854: Current learning rate: 0.00752\n",
      "2024-01-01 02:35:28.813114: train_loss -0.9081\n",
      "2024-01-01 02:35:28.820620: val_loss -0.8264\n",
      "2024-01-01 02:35:28.827618: Pseudo dice [0.9194, 0.9399, 0.9369]\n",
      "2024-01-01 02:35:28.835618: Epoch time: 125.9 s\n",
      "2024-01-01 02:35:30.093877: \n",
      "2024-01-01 02:35:30.099960: Epoch 272\n",
      "2024-01-01 02:35:30.104563: Current learning rate: 0.00751\n",
      "2024-01-01 02:37:36.432536: train_loss -0.9018\n",
      "2024-01-01 02:37:36.440536: val_loss -0.8356\n",
      "2024-01-01 02:37:36.450536: Pseudo dice [0.9277, 0.9434, 0.9432]\n",
      "2024-01-01 02:37:36.457536: Epoch time: 126.34 s\n",
      "2024-01-01 02:37:37.651798: \n",
      "2024-01-01 02:37:37.657860: Epoch 273\n",
      "2024-01-01 02:37:37.665864: Current learning rate: 0.00751\n",
      "2024-01-01 02:39:43.522205: train_loss -0.9052\n",
      "2024-01-01 02:39:43.529194: val_loss -0.8254\n",
      "2024-01-01 02:39:43.537192: Pseudo dice [0.9251, 0.9411, 0.9416]\n",
      "2024-01-01 02:39:43.545193: Epoch time: 125.87 s\n",
      "2024-01-01 02:39:44.751212: \n",
      "2024-01-01 02:39:44.756220: Epoch 274\n",
      "2024-01-01 02:39:44.765219: Current learning rate: 0.0075\n",
      "2024-01-01 02:41:50.698864: train_loss -0.9082\n",
      "2024-01-01 02:41:50.706367: val_loss -0.8365\n",
      "2024-01-01 02:41:50.711383: Pseudo dice [0.9266, 0.9444, 0.9438]\n",
      "2024-01-01 02:41:50.719372: Epoch time: 125.95 s\n",
      "2024-01-01 02:41:52.012730: \n",
      "2024-01-01 02:41:52.017734: Epoch 275\n",
      "2024-01-01 02:41:52.024729: Current learning rate: 0.00749\n",
      "2024-01-01 02:43:57.944446: train_loss -0.9057\n",
      "2024-01-01 02:43:57.955446: val_loss -0.813\n",
      "2024-01-01 02:43:57.966445: Pseudo dice [0.913, 0.9378, 0.9357]\n",
      "2024-01-01 02:43:57.975451: Epoch time: 125.93 s\n",
      "2024-01-01 02:43:59.545569: \n",
      "2024-01-01 02:43:59.551567: Epoch 276\n",
      "2024-01-01 02:43:59.557566: Current learning rate: 0.00748\n",
      "2024-01-01 02:46:05.572997: train_loss -0.9047\n",
      "2024-01-01 02:46:05.579998: val_loss -0.8171\n",
      "2024-01-01 02:46:05.586000: Pseudo dice [0.9183, 0.9394, 0.9352]\n",
      "2024-01-01 02:46:05.592999: Epoch time: 126.03 s\n",
      "2024-01-01 02:46:06.740985: \n",
      "2024-01-01 02:46:06.747061: Epoch 277\n",
      "2024-01-01 02:46:06.752117: Current learning rate: 0.00747\n",
      "2024-01-01 02:48:12.613566: train_loss -0.9047\n",
      "2024-01-01 02:48:12.624551: val_loss -0.8223\n",
      "2024-01-01 02:48:12.629552: Pseudo dice [0.9168, 0.9386, 0.9359]\n",
      "2024-01-01 02:48:12.636554: Epoch time: 125.87 s\n",
      "2024-01-01 02:48:13.882115: \n",
      "2024-01-01 02:48:13.887117: Epoch 278\n",
      "2024-01-01 02:48:13.892115: Current learning rate: 0.00746\n",
      "2024-01-01 02:50:20.342387: train_loss -0.9003\n",
      "2024-01-01 02:50:20.349394: val_loss -0.8182\n",
      "2024-01-01 02:50:20.354910: Pseudo dice [0.9167, 0.9375, 0.9359]\n",
      "2024-01-01 02:50:20.361916: Epoch time: 126.46 s\n",
      "2024-01-01 02:50:21.522418: \n",
      "2024-01-01 02:50:21.529419: Epoch 279\n",
      "2024-01-01 02:50:21.534410: Current learning rate: 0.00745\n",
      "2024-01-01 02:52:27.524713: train_loss -0.9032\n",
      "2024-01-01 02:52:27.533374: val_loss -0.8274\n",
      "2024-01-01 02:52:27.540377: Pseudo dice [0.9215, 0.9415, 0.938]\n",
      "2024-01-01 02:52:27.545377: Epoch time: 126.0 s\n",
      "2024-01-01 02:52:28.680516: \n",
      "2024-01-01 02:52:28.685516: Epoch 280\n",
      "2024-01-01 02:52:28.690516: Current learning rate: 0.00744\n",
      "2024-01-01 02:54:34.591890: train_loss -0.9074\n",
      "2024-01-01 02:54:34.597889: val_loss -0.8466\n",
      "2024-01-01 02:54:34.605895: Pseudo dice [0.931, 0.9468, 0.9457]\n",
      "2024-01-01 02:54:34.611332: Epoch time: 125.91 s\n",
      "2024-01-01 02:54:35.864000: \n",
      "2024-01-01 02:54:35.869066: Epoch 281\n",
      "2024-01-01 02:54:35.882762: Current learning rate: 0.00743\n",
      "2024-01-01 02:56:41.690378: train_loss -0.907\n",
      "2024-01-01 02:56:41.697376: val_loss -0.845\n",
      "2024-01-01 02:56:41.703883: Pseudo dice [0.9274, 0.9463, 0.945]\n",
      "2024-01-01 02:56:41.708886: Epoch time: 125.83 s\n",
      "2024-01-01 02:56:42.845299: \n",
      "2024-01-01 02:56:42.851293: Epoch 282\n",
      "2024-01-01 02:56:42.855293: Current learning rate: 0.00742\n",
      "2024-01-01 02:58:49.511821: train_loss -0.9065\n",
      "2024-01-01 02:58:49.518822: val_loss -0.8168\n",
      "2024-01-01 02:58:49.526121: Pseudo dice [0.9156, 0.9383, 0.9358]\n",
      "2024-01-01 02:58:49.532121: Epoch time: 126.67 s\n",
      "2024-01-01 02:58:51.124806: \n",
      "2024-01-01 02:58:51.129874: Epoch 283\n",
      "2024-01-01 02:58:51.134816: Current learning rate: 0.00741\n",
      "2024-01-01 03:00:57.752570: train_loss -0.9022\n",
      "2024-01-01 03:00:57.761583: val_loss -0.8331\n",
      "2024-01-01 03:00:57.769589: Pseudo dice [0.9271, 0.9463, 0.9442]\n",
      "2024-01-01 03:00:57.780095: Epoch time: 126.63 s\n",
      "2024-01-01 03:00:59.045959: \n",
      "2024-01-01 03:00:59.051958: Epoch 284\n",
      "2024-01-01 03:00:59.061142: Current learning rate: 0.0074\n",
      "2024-01-01 03:03:04.876740: train_loss -0.9057\n",
      "2024-01-01 03:03:04.884739: val_loss -0.8297\n",
      "2024-01-01 03:03:04.891738: Pseudo dice [0.9224, 0.9413, 0.9389]\n",
      "2024-01-01 03:03:04.898285: Epoch time: 125.83 s\n",
      "2024-01-01 03:03:06.115914: \n",
      "2024-01-01 03:03:06.120914: Epoch 285\n",
      "2024-01-01 03:03:06.127479: Current learning rate: 0.00739\n",
      "2024-01-01 03:05:12.244742: train_loss -0.9056\n",
      "2024-01-01 03:05:12.252742: val_loss -0.8272\n",
      "2024-01-01 03:05:12.259743: Pseudo dice [0.9219, 0.9441, 0.9414]\n",
      "2024-01-01 03:05:12.266744: Epoch time: 126.13 s\n",
      "2024-01-01 03:05:13.424135: \n",
      "2024-01-01 03:05:13.433146: Epoch 286\n",
      "2024-01-01 03:05:13.438174: Current learning rate: 0.00738\n",
      "2024-01-01 03:07:19.421680: train_loss -0.9082\n",
      "2024-01-01 03:07:19.429679: val_loss -0.8234\n",
      "2024-01-01 03:07:19.438701: Pseudo dice [0.9194, 0.9417, 0.9379]\n",
      "2024-01-01 03:07:19.445693: Epoch time: 126.0 s\n",
      "2024-01-01 03:07:20.629432: \n",
      "2024-01-01 03:07:20.634428: Epoch 287\n",
      "2024-01-01 03:07:20.639426: Current learning rate: 0.00738\n",
      "2024-01-01 03:09:26.583533: train_loss -0.9063\n",
      "2024-01-01 03:09:26.595535: val_loss -0.8137\n",
      "2024-01-01 03:09:26.601538: Pseudo dice [0.9173, 0.9381, 0.9356]\n",
      "2024-01-01 03:09:26.608535: Epoch time: 125.96 s\n",
      "2024-01-01 03:09:27.744155: \n",
      "2024-01-01 03:09:27.749152: Epoch 288\n",
      "2024-01-01 03:09:27.753152: Current learning rate: 0.00737\n",
      "2024-01-01 03:11:33.739738: train_loss -0.9085\n",
      "2024-01-01 03:11:33.749740: val_loss -0.8394\n",
      "2024-01-01 03:11:33.756737: Pseudo dice [0.9277, 0.9454, 0.9444]\n",
      "2024-01-01 03:11:33.765738: Epoch time: 126.0 s\n",
      "2024-01-01 03:11:34.983259: \n",
      "2024-01-01 03:11:34.989245: Epoch 289\n",
      "2024-01-01 03:11:34.994245: Current learning rate: 0.00736\n",
      "2024-01-01 03:13:41.018668: train_loss -0.9071\n",
      "2024-01-01 03:13:41.028670: val_loss -0.8413\n",
      "2024-01-01 03:13:41.037668: Pseudo dice [0.9287, 0.9447, 0.9441]\n",
      "2024-01-01 03:13:41.044671: Epoch time: 126.04 s\n",
      "2024-01-01 03:13:42.738147: \n",
      "2024-01-01 03:13:42.745148: Epoch 290\n",
      "2024-01-01 03:13:42.749146: Current learning rate: 0.00735\n",
      "2024-01-01 03:15:48.874358: train_loss -0.903\n",
      "2024-01-01 03:15:48.883870: val_loss -0.8152\n",
      "2024-01-01 03:15:48.888869: Pseudo dice [0.9139, 0.9395, 0.9377]\n",
      "2024-01-01 03:15:48.896868: Epoch time: 126.14 s\n",
      "2024-01-01 03:15:50.199219: \n",
      "2024-01-01 03:15:50.205226: Epoch 291\n",
      "2024-01-01 03:15:50.209219: Current learning rate: 0.00734\n",
      "2024-01-01 03:17:56.321012: train_loss -0.9086\n",
      "2024-01-01 03:17:56.327011: val_loss -0.8107\n",
      "2024-01-01 03:17:56.334011: Pseudo dice [0.9117, 0.9371, 0.934]\n",
      "2024-01-01 03:17:56.339011: Epoch time: 126.12 s\n",
      "2024-01-01 03:17:57.577590: \n",
      "2024-01-01 03:17:57.588588: Epoch 292\n",
      "2024-01-01 03:17:57.593580: Current learning rate: 0.00733\n",
      "2024-01-01 03:20:03.645408: train_loss -0.9029\n",
      "2024-01-01 03:20:03.655406: val_loss -0.8377\n",
      "2024-01-01 03:20:03.663916: Pseudo dice [0.9268, 0.9446, 0.9446]\n",
      "2024-01-01 03:20:03.669916: Epoch time: 126.07 s\n",
      "2024-01-01 03:20:04.821692: \n",
      "2024-01-01 03:20:04.827684: Epoch 293\n",
      "2024-01-01 03:20:04.832684: Current learning rate: 0.00732\n",
      "2024-01-01 03:22:10.855879: train_loss -0.9066\n",
      "2024-01-01 03:22:10.864900: val_loss -0.8437\n",
      "2024-01-01 03:22:10.872924: Pseudo dice [0.9314, 0.946, 0.9454]\n",
      "2024-01-01 03:22:10.879923: Epoch time: 126.04 s\n",
      "2024-01-01 03:22:12.080489: \n",
      "2024-01-01 03:22:12.087021: Epoch 294\n",
      "2024-01-01 03:22:12.092025: Current learning rate: 0.00731\n",
      "2024-01-01 03:24:18.506702: train_loss -0.9063\n",
      "2024-01-01 03:24:18.516898: val_loss -0.8399\n",
      "2024-01-01 03:24:18.526137: Pseudo dice [0.9297, 0.9468, 0.9457]\n",
      "2024-01-01 03:24:18.535135: Epoch time: 126.43 s\n",
      "2024-01-01 03:24:19.838746: \n",
      "2024-01-01 03:24:19.844740: Epoch 295\n",
      "2024-01-01 03:24:19.849747: Current learning rate: 0.0073\n",
      "2024-01-01 03:26:25.776987: train_loss -0.9076\n",
      "2024-01-01 03:26:25.782988: val_loss -0.8361\n",
      "2024-01-01 03:26:25.790071: Pseudo dice [0.9277, 0.9456, 0.9457]\n",
      "2024-01-01 03:26:25.794992: Epoch time: 125.94 s\n",
      "2024-01-01 03:26:26.988478: \n",
      "2024-01-01 03:26:26.996469: Epoch 296\n",
      "2024-01-01 03:26:27.002520: Current learning rate: 0.00729\n",
      "2024-01-01 03:28:33.278010: train_loss -0.906\n",
      "2024-01-01 03:28:33.286039: val_loss -0.8357\n",
      "2024-01-01 03:28:33.293027: Pseudo dice [0.9257, 0.9435, 0.9423]\n",
      "2024-01-01 03:28:33.301545: Epoch time: 126.29 s\n",
      "2024-01-01 03:28:34.500853: \n",
      "2024-01-01 03:28:34.510952: Epoch 297\n",
      "2024-01-01 03:28:34.515937: Current learning rate: 0.00728\n",
      "2024-01-01 03:30:41.072570: train_loss -0.9081\n",
      "2024-01-01 03:30:41.079576: val_loss -0.8334\n",
      "2024-01-01 03:30:41.087582: Pseudo dice [0.9238, 0.945, 0.9433]\n",
      "2024-01-01 03:30:41.092577: Epoch time: 126.57 s\n",
      "2024-01-01 03:30:42.439372: \n",
      "2024-01-01 03:30:42.447372: Epoch 298\n",
      "2024-01-01 03:30:42.452376: Current learning rate: 0.00727\n",
      "2024-01-01 03:32:48.851148: train_loss -0.905\n",
      "2024-01-01 03:32:48.861151: val_loss -0.8307\n",
      "2024-01-01 03:32:48.868151: Pseudo dice [0.9194, 0.9441, 0.9429]\n",
      "2024-01-01 03:32:48.876150: Epoch time: 126.41 s\n",
      "2024-01-01 03:32:50.096943: \n",
      "2024-01-01 03:32:50.101977: Epoch 299\n",
      "2024-01-01 03:32:50.106977: Current learning rate: 0.00726\n",
      "2024-01-01 03:34:57.034966: train_loss -0.9076\n",
      "2024-01-01 03:34:57.045185: val_loss -0.8408\n",
      "2024-01-01 03:34:57.056326: Pseudo dice [0.9266, 0.9448, 0.9434]\n",
      "2024-01-01 03:34:57.066335: Epoch time: 126.94 s\n",
      "2024-01-01 03:34:58.764905: \n",
      "2024-01-01 03:34:58.770896: Epoch 300\n",
      "2024-01-01 03:34:58.775900: Current learning rate: 0.00725\n",
      "2024-01-01 03:37:05.382057: train_loss -0.9001\n",
      "2024-01-01 03:37:05.390071: val_loss -0.834\n",
      "2024-01-01 03:37:05.400065: Pseudo dice [0.9224, 0.9438, 0.9421]\n",
      "2024-01-01 03:37:05.408574: Epoch time: 126.62 s\n",
      "2024-01-01 03:37:06.584017: \n",
      "2024-01-01 03:37:06.591036: Epoch 301\n",
      "2024-01-01 03:37:06.595035: Current learning rate: 0.00724\n",
      "2024-01-01 03:39:12.455816: train_loss -0.9083\n",
      "2024-01-01 03:39:12.465820: val_loss -0.8388\n",
      "2024-01-01 03:39:12.475939: Pseudo dice [0.9286, 0.9466, 0.9451]\n",
      "2024-01-01 03:39:12.480938: Epoch time: 125.87 s\n",
      "2024-01-01 03:39:13.910082: \n",
      "2024-01-01 03:39:13.916081: Epoch 302\n",
      "2024-01-01 03:39:13.921081: Current learning rate: 0.00724\n",
      "2024-01-01 03:41:19.836464: train_loss -0.9097\n",
      "2024-01-01 03:41:19.842459: val_loss -0.8445\n",
      "2024-01-01 03:41:19.849459: Pseudo dice [0.928, 0.9466, 0.9442]\n",
      "2024-01-01 03:41:19.857458: Epoch time: 125.93 s\n",
      "2024-01-01 03:41:21.033769: \n",
      "2024-01-01 03:41:21.039770: Epoch 303\n",
      "2024-01-01 03:41:21.044770: Current learning rate: 0.00723\n",
      "2024-01-01 03:43:27.237412: train_loss -0.9062\n",
      "2024-01-01 03:43:27.243416: val_loss -0.8373\n",
      "2024-01-01 03:43:27.250403: Pseudo dice [0.9258, 0.9442, 0.9437]\n",
      "2024-01-01 03:43:27.255403: Epoch time: 126.21 s\n",
      "2024-01-01 03:43:28.516176: \n",
      "2024-01-01 03:43:28.525172: Epoch 304\n",
      "2024-01-01 03:43:28.529168: Current learning rate: 0.00722\n",
      "2024-01-01 03:45:34.907897: train_loss -0.907\n",
      "2024-01-01 03:45:34.917898: val_loss -0.8411\n",
      "2024-01-01 03:45:34.925896: Pseudo dice [0.9291, 0.9477, 0.9457]\n",
      "2024-01-01 03:45:34.937903: Epoch time: 126.39 s\n",
      "2024-01-01 03:45:36.332335: \n",
      "2024-01-01 03:45:36.338353: Epoch 305\n",
      "2024-01-01 03:45:36.343351: Current learning rate: 0.00721\n",
      "2024-01-01 03:47:43.392430: train_loss -0.9064\n",
      "2024-01-01 03:47:43.404428: val_loss -0.8352\n",
      "2024-01-01 03:47:43.413814: Pseudo dice [0.9256, 0.9424, 0.9414]\n",
      "2024-01-01 03:47:43.419814: Epoch time: 127.06 s\n",
      "2024-01-01 03:47:44.803559: \n",
      "2024-01-01 03:47:44.809942: Epoch 306\n",
      "2024-01-01 03:47:44.815015: Current learning rate: 0.0072\n",
      "2024-01-01 03:49:51.635251: train_loss -0.9075\n",
      "2024-01-01 03:49:51.643249: val_loss -0.846\n",
      "2024-01-01 03:49:51.652250: Pseudo dice [0.9281, 0.9469, 0.945]\n",
      "2024-01-01 03:49:51.660268: Epoch time: 126.83 s\n",
      "2024-01-01 03:49:52.909308: \n",
      "2024-01-01 03:49:52.918629: Epoch 307\n",
      "2024-01-01 03:49:52.924550: Current learning rate: 0.00719\n",
      "2024-01-01 03:51:59.357940: train_loss -0.9075\n",
      "2024-01-01 03:51:59.366943: val_loss -0.8282\n",
      "2024-01-01 03:51:59.375949: Pseudo dice [0.9232, 0.9436, 0.9414]\n",
      "2024-01-01 03:51:59.384456: Epoch time: 126.45 s\n",
      "2024-01-01 03:52:00.681455: \n",
      "2024-01-01 03:52:00.686980: Epoch 308\n",
      "2024-01-01 03:52:00.690980: Current learning rate: 0.00718\n",
      "2024-01-01 03:54:06.467705: train_loss -0.9102\n",
      "2024-01-01 03:54:06.476712: val_loss -0.8377\n",
      "2024-01-01 03:54:06.483220: Pseudo dice [0.9285, 0.9454, 0.9444]\n",
      "2024-01-01 03:54:06.489224: Epoch time: 125.79 s\n",
      "2024-01-01 03:54:07.613248: \n",
      "2024-01-01 03:54:07.618249: Epoch 309\n",
      "2024-01-01 03:54:07.623253: Current learning rate: 0.00717\n",
      "2024-01-01 03:56:13.755991: train_loss -0.9099\n",
      "2024-01-01 03:56:13.765993: val_loss -0.8368\n",
      "2024-01-01 03:56:13.774498: Pseudo dice [0.9271, 0.9445, 0.9433]\n",
      "2024-01-01 03:56:13.781923: Epoch time: 126.14 s\n",
      "2024-01-01 03:56:14.841397: \n",
      "2024-01-01 03:56:14.846887: Epoch 310\n",
      "2024-01-01 03:56:14.850879: Current learning rate: 0.00716\n",
      "2024-01-01 03:58:21.772953: train_loss -0.9102\n",
      "2024-01-01 03:58:21.784955: val_loss -0.8375\n",
      "2024-01-01 03:58:21.796864: Pseudo dice [0.9271, 0.9457, 0.9446]\n",
      "2024-01-01 03:58:21.803015: Epoch time: 126.93 s\n",
      "2024-01-01 03:58:23.153036: \n",
      "2024-01-01 03:58:23.159044: Epoch 311\n",
      "2024-01-01 03:58:23.164045: Current learning rate: 0.00715\n",
      "2024-01-01 04:00:29.479050: train_loss -0.9127\n",
      "2024-01-01 04:00:29.492050: val_loss -0.8316\n",
      "2024-01-01 04:00:29.501058: Pseudo dice [0.9231, 0.9429, 0.9433]\n",
      "2024-01-01 04:00:29.513570: Epoch time: 126.33 s\n",
      "2024-01-01 04:00:30.779778: \n",
      "2024-01-01 04:00:30.785779: Epoch 312\n",
      "2024-01-01 04:00:30.789779: Current learning rate: 0.00714\n",
      "2024-01-01 04:02:37.541412: train_loss -0.9095\n",
      "2024-01-01 04:02:37.551411: val_loss -0.8461\n",
      "2024-01-01 04:02:37.557411: Pseudo dice [0.9322, 0.9484, 0.9463]\n",
      "2024-01-01 04:02:37.562411: Epoch time: 126.76 s\n",
      "2024-01-01 04:02:38.861182: \n",
      "2024-01-01 04:02:38.866182: Epoch 313\n",
      "2024-01-01 04:02:38.871182: Current learning rate: 0.00713\n",
      "2024-01-01 04:04:44.898426: train_loss -0.9101\n",
      "2024-01-01 04:04:44.905961: val_loss -0.8399\n",
      "2024-01-01 04:04:44.912954: Pseudo dice [0.9277, 0.9463, 0.9437]\n",
      "2024-01-01 04:04:44.918955: Epoch time: 126.04 s\n",
      "2024-01-01 04:04:46.088986: \n",
      "2024-01-01 04:04:46.093976: Epoch 314\n",
      "2024-01-01 04:04:46.097986: Current learning rate: 0.00712\n",
      "2024-01-01 04:06:52.260845: train_loss -0.9081\n",
      "2024-01-01 04:06:52.270842: val_loss -0.8356\n",
      "2024-01-01 04:06:52.277842: Pseudo dice [0.9234, 0.9435, 0.9427]\n",
      "2024-01-01 04:06:52.283845: Epoch time: 126.17 s\n",
      "2024-01-01 04:06:53.646664: \n",
      "2024-01-01 04:06:53.652677: Epoch 315\n",
      "2024-01-01 04:06:53.657678: Current learning rate: 0.00711\n",
      "2024-01-01 04:08:59.520558: train_loss -0.909\n",
      "2024-01-01 04:08:59.530560: val_loss -0.8365\n",
      "2024-01-01 04:08:59.537561: Pseudo dice [0.9255, 0.9438, 0.9422]\n",
      "2024-01-01 04:08:59.546560: Epoch time: 125.87 s\n",
      "2024-01-01 04:09:00.813636: \n",
      "2024-01-01 04:09:00.819701: Epoch 316\n",
      "2024-01-01 04:09:00.824699: Current learning rate: 0.0071\n",
      "2024-01-01 04:11:07.439342: train_loss -0.9053\n",
      "2024-01-01 04:11:07.449342: val_loss -0.8359\n",
      "2024-01-01 04:11:07.456343: Pseudo dice [0.9261, 0.9453, 0.9438]\n",
      "2024-01-01 04:11:07.461342: Epoch time: 126.63 s\n",
      "2024-01-01 04:11:08.577120: \n",
      "2024-01-01 04:11:08.582131: Epoch 317\n",
      "2024-01-01 04:11:08.589664: Current learning rate: 0.0071\n",
      "2024-01-01 04:13:14.623290: train_loss -0.9077\n",
      "2024-01-01 04:13:14.632290: val_loss -0.823\n",
      "2024-01-01 04:13:14.638297: Pseudo dice [0.9212, 0.942, 0.9399]\n",
      "2024-01-01 04:13:14.643293: Epoch time: 126.05 s\n",
      "2024-01-01 04:13:15.863397: \n",
      "2024-01-01 04:13:15.868404: Epoch 318\n",
      "2024-01-01 04:13:15.872934: Current learning rate: 0.00709\n",
      "2024-01-01 04:15:21.961857: train_loss -0.9078\n",
      "2024-01-01 04:15:21.970857: val_loss -0.8265\n",
      "2024-01-01 04:15:21.976856: Pseudo dice [0.922, 0.943, 0.9418]\n",
      "2024-01-01 04:15:21.983857: Epoch time: 126.1 s\n",
      "2024-01-01 04:15:23.214325: \n",
      "2024-01-01 04:15:23.220343: Epoch 319\n",
      "2024-01-01 04:15:23.225337: Current learning rate: 0.00708\n",
      "2024-01-01 04:17:29.639488: train_loss -0.9054\n",
      "2024-01-01 04:17:29.648487: val_loss -0.8211\n",
      "2024-01-01 04:17:29.657491: Pseudo dice [0.9213, 0.9401, 0.9396]\n",
      "2024-01-01 04:17:29.664489: Epoch time: 126.43 s\n",
      "2024-01-01 04:17:30.871378: \n",
      "2024-01-01 04:17:30.875369: Epoch 320\n",
      "2024-01-01 04:17:30.880377: Current learning rate: 0.00707\n",
      "2024-01-01 04:19:37.045348: train_loss -0.9104\n",
      "2024-01-01 04:19:37.054862: val_loss -0.8387\n",
      "2024-01-01 04:19:37.062861: Pseudo dice [0.9295, 0.9479, 0.9461]\n",
      "2024-01-01 04:19:37.072861: Epoch time: 126.17 s\n",
      "2024-01-01 04:19:38.685940: \n",
      "2024-01-01 04:19:38.691014: Epoch 321\n",
      "2024-01-01 04:19:38.696017: Current learning rate: 0.00706\n",
      "2024-01-01 04:21:44.736166: train_loss -0.9076\n",
      "2024-01-01 04:21:44.744165: val_loss -0.8391\n",
      "2024-01-01 04:21:44.750166: Pseudo dice [0.9273, 0.9469, 0.9445]\n",
      "2024-01-01 04:21:44.756166: Epoch time: 126.05 s\n",
      "2024-01-01 04:21:45.852833: \n",
      "2024-01-01 04:21:45.858833: Epoch 322\n",
      "2024-01-01 04:21:45.866836: Current learning rate: 0.00705\n",
      "2024-01-01 04:23:52.168477: train_loss -0.9067\n",
      "2024-01-01 04:23:52.178475: val_loss -0.8348\n",
      "2024-01-01 04:23:52.187478: Pseudo dice [0.9283, 0.9446, 0.9448]\n",
      "2024-01-01 04:23:52.194475: Epoch time: 126.32 s\n",
      "2024-01-01 04:23:53.559922: \n",
      "2024-01-01 04:23:53.564929: Epoch 323\n",
      "2024-01-01 04:23:53.569922: Current learning rate: 0.00704\n",
      "2024-01-01 04:25:59.529301: train_loss -0.9104\n",
      "2024-01-01 04:25:59.535301: val_loss -0.8366\n",
      "2024-01-01 04:25:59.543301: Pseudo dice [0.9287, 0.946, 0.9452]\n",
      "2024-01-01 04:25:59.547301: Epoch time: 125.97 s\n",
      "2024-01-01 04:26:00.712240: \n",
      "2024-01-01 04:26:00.720301: Epoch 324\n",
      "2024-01-01 04:26:00.725322: Current learning rate: 0.00703\n",
      "2024-01-01 04:28:06.946703: train_loss -0.9073\n",
      "2024-01-01 04:28:06.953703: val_loss -0.8373\n",
      "2024-01-01 04:28:06.960705: Pseudo dice [0.9294, 0.9457, 0.9469]\n",
      "2024-01-01 04:28:06.967704: Epoch time: 126.23 s\n",
      "2024-01-01 04:28:08.139323: \n",
      "2024-01-01 04:28:08.145323: Epoch 325\n",
      "2024-01-01 04:28:08.149323: Current learning rate: 0.00702\n",
      "2024-01-01 04:30:14.233718: train_loss -0.9105\n",
      "2024-01-01 04:30:14.243720: val_loss -0.8339\n",
      "2024-01-01 04:30:14.251718: Pseudo dice [0.9291, 0.946, 0.945]\n",
      "2024-01-01 04:30:14.259718: Epoch time: 126.09 s\n",
      "2024-01-01 04:30:15.658370: \n",
      "2024-01-01 04:30:15.665112: Epoch 326\n",
      "2024-01-01 04:30:15.670128: Current learning rate: 0.00701\n",
      "2024-01-01 04:32:21.865502: train_loss -0.9088\n",
      "2024-01-01 04:32:21.873520: val_loss -0.8321\n",
      "2024-01-01 04:32:21.880527: Pseudo dice [0.9248, 0.9445, 0.9437]\n",
      "2024-01-01 04:32:21.887528: Epoch time: 126.21 s\n",
      "2024-01-01 04:32:23.047237: \n",
      "2024-01-01 04:32:23.053313: Epoch 327\n",
      "2024-01-01 04:32:23.058364: Current learning rate: 0.007\n",
      "2024-01-01 04:34:29.365699: train_loss -0.908\n",
      "2024-01-01 04:34:29.371687: val_loss -0.8346\n",
      "2024-01-01 04:34:29.379689: Pseudo dice [0.9276, 0.9453, 0.9456]\n",
      "2024-01-01 04:34:29.385689: Epoch time: 126.32 s\n",
      "2024-01-01 04:34:30.763294: \n",
      "2024-01-01 04:34:30.768293: Epoch 328\n",
      "2024-01-01 04:34:30.773294: Current learning rate: 0.00699\n",
      "2024-01-01 04:36:36.887860: train_loss -0.9081\n",
      "2024-01-01 04:36:36.896862: val_loss -0.8397\n",
      "2024-01-01 04:36:36.905859: Pseudo dice [0.9259, 0.9454, 0.9438]\n",
      "2024-01-01 04:36:36.911860: Epoch time: 126.13 s\n",
      "2024-01-01 04:36:38.262004: \n",
      "2024-01-01 04:36:38.267995: Epoch 329\n",
      "2024-01-01 04:36:38.272995: Current learning rate: 0.00698\n",
      "2024-01-01 04:38:45.049408: train_loss -0.9072\n",
      "2024-01-01 04:38:45.057425: val_loss -0.8446\n",
      "2024-01-01 04:38:45.063413: Pseudo dice [0.93, 0.947, 0.9462]\n",
      "2024-01-01 04:38:45.069413: Epoch time: 126.79 s\n",
      "2024-01-01 04:38:46.259795: \n",
      "2024-01-01 04:38:46.265796: Epoch 330\n",
      "2024-01-01 04:38:46.270787: Current learning rate: 0.00697\n",
      "2024-01-01 04:40:52.276606: train_loss -0.9094\n",
      "2024-01-01 04:40:52.283599: val_loss -0.8402\n",
      "2024-01-01 04:40:52.289596: Pseudo dice [0.9287, 0.9462, 0.9441]\n",
      "2024-01-01 04:40:52.294596: Epoch time: 126.02 s\n",
      "2024-01-01 04:40:53.435287: \n",
      "2024-01-01 04:40:53.443287: Epoch 331\n",
      "2024-01-01 04:40:53.448285: Current learning rate: 0.00696\n",
      "2024-01-01 04:42:59.390959: train_loss -0.9089\n",
      "2024-01-01 04:42:59.401958: val_loss -0.8368\n",
      "2024-01-01 04:42:59.409958: Pseudo dice [0.9272, 0.9441, 0.9427]\n",
      "2024-01-01 04:42:59.415959: Epoch time: 125.96 s\n",
      "2024-01-01 04:43:00.691755: \n",
      "2024-01-01 04:43:00.696755: Epoch 332\n",
      "2024-01-01 04:43:00.701755: Current learning rate: 0.00696\n",
      "2024-01-01 04:45:07.944274: train_loss -0.9078\n",
      "2024-01-01 04:45:07.955275: val_loss -0.8343\n",
      "2024-01-01 04:45:07.964275: Pseudo dice [0.9264, 0.9456, 0.9447]\n",
      "2024-01-01 04:45:07.971275: Epoch time: 127.25 s\n",
      "2024-01-01 04:45:09.091311: \n",
      "2024-01-01 04:45:09.097311: Epoch 333\n",
      "2024-01-01 04:45:09.104320: Current learning rate: 0.00695\n",
      "2024-01-01 04:47:15.369197: train_loss -0.9096\n",
      "2024-01-01 04:47:15.382200: val_loss -0.8374\n",
      "2024-01-01 04:47:15.393713: Pseudo dice [0.9263, 0.9459, 0.9448]\n",
      "2024-01-01 04:47:15.404715: Epoch time: 126.28 s\n",
      "2024-01-01 04:47:16.487848: \n",
      "2024-01-01 04:47:16.493258: Epoch 334\n",
      "2024-01-01 04:47:16.499257: Current learning rate: 0.00694\n",
      "2024-01-01 04:49:23.121711: train_loss -0.9125\n",
      "2024-01-01 04:49:23.129717: val_loss -0.8408\n",
      "2024-01-01 04:49:23.136719: Pseudo dice [0.9255, 0.9457, 0.9447]\n",
      "2024-01-01 04:49:23.144717: Epoch time: 126.63 s\n",
      "2024-01-01 04:49:24.204291: \n",
      "2024-01-01 04:49:24.209287: Epoch 335\n",
      "2024-01-01 04:49:24.214285: Current learning rate: 0.00693\n",
      "2024-01-01 04:51:31.142713: train_loss -0.9082\n",
      "2024-01-01 04:51:31.153714: val_loss -0.8313\n",
      "2024-01-01 04:51:31.162714: Pseudo dice [0.9251, 0.9439, 0.9423]\n",
      "2024-01-01 04:51:31.171712: Epoch time: 126.94 s\n",
      "2024-01-01 04:51:32.680974: \n",
      "2024-01-01 04:51:32.687964: Epoch 336\n",
      "2024-01-01 04:51:32.693964: Current learning rate: 0.00692\n",
      "2024-01-01 04:53:39.015574: train_loss -0.9098\n",
      "2024-01-01 04:53:39.025092: val_loss -0.8335\n",
      "2024-01-01 04:53:39.033094: Pseudo dice [0.9282, 0.9455, 0.9446]\n",
      "2024-01-01 04:53:39.041098: Epoch time: 126.34 s\n",
      "2024-01-01 04:53:40.106256: \n",
      "2024-01-01 04:53:40.112258: Epoch 337\n",
      "2024-01-01 04:53:40.117251: Current learning rate: 0.00691\n",
      "2024-01-01 04:55:46.656813: train_loss -0.909\n",
      "2024-01-01 04:55:46.665821: val_loss -0.839\n",
      "2024-01-01 04:55:46.672822: Pseudo dice [0.9267, 0.9465, 0.9463]\n",
      "2024-01-01 04:55:46.680841: Epoch time: 126.55 s\n",
      "2024-01-01 04:55:47.814798: \n",
      "2024-01-01 04:55:47.821021: Epoch 338\n",
      "2024-01-01 04:55:47.826012: Current learning rate: 0.0069\n",
      "2024-01-01 04:57:53.357770: train_loss -0.9082\n",
      "2024-01-01 04:57:53.365767: val_loss -0.8305\n",
      "2024-01-01 04:57:53.372765: Pseudo dice [0.922, 0.9431, 0.9409]\n",
      "2024-01-01 04:57:53.381767: Epoch time: 125.54 s\n",
      "2024-01-01 04:57:54.434795: \n",
      "2024-01-01 04:57:54.440783: Epoch 339\n",
      "2024-01-01 04:57:54.446336: Current learning rate: 0.00689\n",
      "2024-01-01 05:00:01.023401: train_loss -0.908\n",
      "2024-01-01 05:00:01.032404: val_loss -0.8133\n",
      "2024-01-01 05:00:01.040400: Pseudo dice [0.9157, 0.9388, 0.936]\n",
      "2024-01-01 05:00:01.049400: Epoch time: 126.59 s\n",
      "2024-01-01 05:00:02.115051: \n",
      "2024-01-01 05:00:02.120051: Epoch 340\n",
      "2024-01-01 05:00:02.125052: Current learning rate: 0.00688\n",
      "2024-01-01 05:02:10.200567: train_loss -0.9101\n",
      "2024-01-01 05:02:10.210562: val_loss -0.8292\n",
      "2024-01-01 05:02:10.220563: Pseudo dice [0.9237, 0.9442, 0.9431]\n",
      "2024-01-01 05:02:10.228563: Epoch time: 128.09 s\n",
      "2024-01-01 05:02:11.638669: \n",
      "2024-01-01 05:02:11.644029: Epoch 341\n",
      "2024-01-01 05:02:11.650002: Current learning rate: 0.00687\n",
      "2024-01-01 05:04:18.923334: train_loss -0.9102\n",
      "2024-01-01 05:04:18.934843: val_loss -0.8285\n",
      "2024-01-01 05:04:18.945844: Pseudo dice [0.9243, 0.9434, 0.9436]\n",
      "2024-01-01 05:04:18.956848: Epoch time: 127.29 s\n",
      "2024-01-01 05:04:20.435041: \n",
      "2024-01-01 05:04:20.440557: Epoch 342\n",
      "2024-01-01 05:04:20.446516: Current learning rate: 0.00686\n",
      "2024-01-01 05:06:27.915856: train_loss -0.914\n",
      "2024-01-01 05:06:27.926855: val_loss -0.8387\n",
      "2024-01-01 05:06:27.935833: Pseudo dice [0.9275, 0.9438, 0.9434]\n",
      "2024-01-01 05:06:27.946106: Epoch time: 127.48 s\n",
      "2024-01-01 05:06:29.593255: \n",
      "2024-01-01 05:06:29.599255: Epoch 343\n",
      "2024-01-01 05:06:29.604255: Current learning rate: 0.00685\n",
      "2024-01-01 05:08:36.562204: train_loss -0.9089\n",
      "2024-01-01 05:08:36.571918: val_loss -0.8296\n",
      "2024-01-01 05:08:36.580343: Pseudo dice [0.9233, 0.9427, 0.9423]\n",
      "2024-01-01 05:08:36.588546: Epoch time: 126.97 s\n",
      "2024-01-01 05:08:38.165775: \n",
      "2024-01-01 05:08:38.172852: Epoch 344\n",
      "2024-01-01 05:08:38.177831: Current learning rate: 0.00684\n",
      "2024-01-01 05:10:46.068467: train_loss -0.9116\n",
      "2024-01-01 05:10:46.078295: val_loss -0.8339\n",
      "2024-01-01 05:10:46.086295: Pseudo dice [0.9239, 0.9449, 0.9422]\n",
      "2024-01-01 05:10:46.095134: Epoch time: 127.9 s\n",
      "2024-01-01 05:10:47.724205: \n",
      "2024-01-01 05:10:47.730205: Epoch 345\n",
      "2024-01-01 05:10:47.736205: Current learning rate: 0.00683\n",
      "2024-01-01 05:12:57.162575: train_loss -0.9072\n",
      "2024-01-01 05:12:57.178391: val_loss -0.8321\n",
      "2024-01-01 05:12:57.194901: Pseudo dice [0.9256, 0.9434, 0.9424]\n",
      "2024-01-01 05:12:57.210101: Epoch time: 129.44 s\n",
      "2024-01-01 05:12:59.094435: \n",
      "2024-01-01 05:12:59.105436: Epoch 346\n",
      "2024-01-01 05:12:59.113436: Current learning rate: 0.00682\n",
      "2024-01-01 05:15:07.706315: train_loss -0.9083\n",
      "2024-01-01 05:15:07.719382: val_loss -0.8348\n",
      "2024-01-01 05:15:07.728498: Pseudo dice [0.9261, 0.9443, 0.943]\n",
      "2024-01-01 05:15:07.736498: Epoch time: 128.61 s\n",
      "2024-01-01 05:15:08.924097: \n",
      "2024-01-01 05:15:08.930089: Epoch 347\n",
      "2024-01-01 05:15:08.937094: Current learning rate: 0.00681\n",
      "2024-01-01 05:17:17.830184: train_loss -0.9091\n",
      "2024-01-01 05:17:17.839332: val_loss -0.8289\n",
      "2024-01-01 05:17:17.850332: Pseudo dice [0.9243, 0.9445, 0.9431]\n",
      "2024-01-01 05:17:17.859346: Epoch time: 128.91 s\n",
      "2024-01-01 05:17:19.212884: \n",
      "2024-01-01 05:17:19.218807: Epoch 348\n",
      "2024-01-01 05:17:19.224573: Current learning rate: 0.0068\n",
      "2024-01-01 05:19:27.140664: train_loss -0.9104\n",
      "2024-01-01 05:19:27.150743: val_loss -0.8336\n",
      "2024-01-01 05:19:27.159795: Pseudo dice [0.9231, 0.9441, 0.9429]\n",
      "2024-01-01 05:19:27.177209: Epoch time: 127.93 s\n",
      "2024-01-01 05:19:28.367335: \n",
      "2024-01-01 05:19:28.372326: Epoch 349\n",
      "2024-01-01 05:19:28.378336: Current learning rate: 0.0068\n",
      "2024-01-01 05:21:36.443431: train_loss -0.9083\n",
      "2024-01-01 05:21:36.452353: val_loss -0.8391\n",
      "2024-01-01 05:21:36.462512: Pseudo dice [0.9291, 0.9465, 0.945]\n",
      "2024-01-01 05:21:36.472860: Epoch time: 128.08 s\n",
      "2024-01-01 05:21:37.907161: \n",
      "2024-01-01 05:21:37.914160: Epoch 350\n",
      "2024-01-01 05:21:37.919162: Current learning rate: 0.00679\n",
      "2024-01-01 05:23:47.367882: train_loss -0.9068\n",
      "2024-01-01 05:23:47.379283: val_loss -0.8427\n",
      "2024-01-01 05:23:47.390290: Pseudo dice [0.9231, 0.9463, 0.946]\n",
      "2024-01-01 05:23:47.401290: Epoch time: 129.46 s\n",
      "2024-01-01 05:23:49.137884: \n",
      "2024-01-01 05:23:49.143801: Epoch 351\n",
      "2024-01-01 05:23:49.149799: Current learning rate: 0.00678\n",
      "2024-01-01 05:26:18.007737: train_loss -0.9012\n",
      "2024-01-01 05:26:18.036737: val_loss -0.835\n",
      "2024-01-01 05:26:18.053735: Pseudo dice [0.9285, 0.9457, 0.9423]\n",
      "2024-01-01 05:26:18.065739: Epoch time: 148.87 s\n",
      "2024-01-01 05:26:20.186723: \n",
      "2024-01-01 05:26:20.202723: Epoch 352\n",
      "2024-01-01 05:26:20.214723: Current learning rate: 0.00677\n",
      "2024-01-01 05:28:31.648896: train_loss -0.9045\n",
      "2024-01-01 05:28:31.664435: val_loss -0.8155\n",
      "2024-01-01 05:28:31.677433: Pseudo dice [0.9162, 0.9385, 0.9369]\n",
      "2024-01-01 05:28:31.688431: Epoch time: 131.46 s\n",
      "2024-01-01 05:28:33.103634: \n",
      "2024-01-01 05:28:33.110622: Epoch 353\n",
      "2024-01-01 05:28:33.116623: Current learning rate: 0.00676\n",
      "2024-01-01 05:30:44.153394: train_loss -0.911\n",
      "2024-01-01 05:30:44.165395: val_loss -0.8349\n",
      "2024-01-01 05:30:44.177397: Pseudo dice [0.9239, 0.9443, 0.9431]\n",
      "2024-01-01 05:30:44.189394: Epoch time: 131.05 s\n",
      "2024-01-01 05:30:45.733655: \n",
      "2024-01-01 05:30:45.741674: Epoch 354\n",
      "2024-01-01 05:30:45.756177: Current learning rate: 0.00675\n",
      "2024-01-01 05:33:12.913122: train_loss -0.9057\n",
      "2024-01-01 05:33:12.936125: val_loss -0.8248\n",
      "2024-01-01 05:33:13.046580: Pseudo dice [0.9203, 0.943, 0.9396]\n",
      "2024-01-01 05:33:13.089714: Epoch time: 147.18 s\n",
      "2024-01-01 05:33:16.175363: \n",
      "2024-01-01 05:33:16.183366: Epoch 355\n",
      "2024-01-01 05:33:16.192374: Current learning rate: 0.00674\n",
      "2024-01-01 05:35:38.221084: train_loss -0.9079\n",
      "2024-01-01 05:35:38.233085: val_loss -0.8346\n",
      "2024-01-01 05:35:38.243091: Pseudo dice [0.9249, 0.946, 0.9434]\n",
      "2024-01-01 05:35:38.252599: Epoch time: 142.05 s\n",
      "2024-01-01 05:35:39.603822: \n",
      "2024-01-01 05:35:39.613811: Epoch 356\n",
      "2024-01-01 05:35:39.624826: Current learning rate: 0.00673\n",
      "2024-01-01 05:37:49.026192: train_loss -0.9062\n",
      "2024-01-01 05:37:49.036193: val_loss -0.8369\n",
      "2024-01-01 05:37:49.045192: Pseudo dice [0.9263, 0.9445, 0.9426]\n",
      "2024-01-01 05:37:49.054192: Epoch time: 129.43 s\n",
      "2024-01-01 05:37:50.267397: \n",
      "2024-01-01 05:37:50.275482: Epoch 357\n",
      "2024-01-01 05:37:50.282408: Current learning rate: 0.00672\n",
      "2024-01-01 05:40:01.992017: train_loss -0.9091\n",
      "2024-01-01 05:40:02.010530: val_loss -0.8234\n",
      "2024-01-01 05:40:02.021530: Pseudo dice [0.923, 0.9404, 0.9404]\n",
      "2024-01-01 05:40:02.031529: Epoch time: 131.73 s\n",
      "2024-01-01 05:40:05.017194: \n",
      "2024-01-01 05:40:05.036195: Epoch 358\n",
      "2024-01-01 05:40:05.057194: Current learning rate: 0.00671\n",
      "2024-01-01 05:42:18.861194: train_loss -0.9096\n",
      "2024-01-01 05:42:18.900232: val_loss -0.8233\n",
      "2024-01-01 05:42:18.916232: Pseudo dice [0.9197, 0.9411, 0.9401]\n",
      "2024-01-01 05:42:18.930235: Epoch time: 133.85 s\n",
      "2024-01-01 05:42:21.144997: \n",
      "2024-01-01 05:42:21.154981: Epoch 359\n",
      "2024-01-01 05:42:21.165100: Current learning rate: 0.0067\n",
      "2024-01-01 05:44:33.682301: train_loss -0.9119\n",
      "2024-01-01 05:44:33.694301: val_loss -0.8173\n",
      "2024-01-01 05:44:33.703302: Pseudo dice [0.9162, 0.9396, 0.9364]\n",
      "2024-01-01 05:44:33.714302: Epoch time: 132.54 s\n",
      "2024-01-01 05:44:35.301303: \n",
      "2024-01-01 05:44:35.307295: Epoch 360\n",
      "2024-01-01 05:44:35.313296: Current learning rate: 0.00669\n",
      "2024-01-01 05:46:46.254483: train_loss -0.9074\n",
      "2024-01-01 05:46:46.268481: val_loss -0.8335\n",
      "2024-01-01 05:46:46.286483: Pseudo dice [0.9285, 0.9438, 0.942]\n",
      "2024-01-01 05:46:46.299482: Epoch time: 130.96 s\n",
      "2024-01-01 05:46:48.122886: \n",
      "2024-01-01 05:46:48.129891: Epoch 361\n",
      "2024-01-01 05:46:48.135894: Current learning rate: 0.00668\n",
      "2024-01-01 05:48:56.260351: train_loss -0.912\n",
      "2024-01-01 05:48:56.272353: val_loss -0.8304\n",
      "2024-01-01 05:48:56.282352: Pseudo dice [0.9248, 0.9446, 0.9439]\n",
      "2024-01-01 05:48:56.292356: Epoch time: 128.14 s\n",
      "2024-01-01 05:48:57.655650: \n",
      "2024-01-01 05:48:57.661980: Epoch 362\n",
      "2024-01-01 05:48:57.667293: Current learning rate: 0.00667\n",
      "2024-01-01 05:51:05.063237: train_loss -0.9114\n",
      "2024-01-01 05:51:05.070237: val_loss -0.8302\n",
      "2024-01-01 05:51:05.075238: Pseudo dice [0.9218, 0.9432, 0.9413]\n",
      "2024-01-01 05:51:05.080239: Epoch time: 127.41 s\n",
      "2024-01-01 05:51:06.241291: \n",
      "2024-01-01 05:51:06.246288: Epoch 363\n",
      "2024-01-01 05:51:06.251280: Current learning rate: 0.00666\n",
      "2024-01-01 05:53:12.808067: train_loss -0.9126\n",
      "2024-01-01 05:53:12.815070: val_loss -0.8277\n",
      "2024-01-01 05:53:12.823070: Pseudo dice [0.9233, 0.9418, 0.9397]\n",
      "2024-01-01 05:53:12.832069: Epoch time: 126.57 s\n",
      "2024-01-01 05:53:14.176091: \n",
      "2024-01-01 05:53:14.183291: Epoch 364\n",
      "2024-01-01 05:53:14.188301: Current learning rate: 0.00665\n",
      "2024-01-01 05:55:21.403183: train_loss -0.9104\n",
      "2024-01-01 05:55:21.410177: val_loss -0.8186\n",
      "2024-01-01 05:55:21.419181: Pseudo dice [0.9189, 0.9391, 0.9365]\n",
      "2024-01-01 05:55:21.424178: Epoch time: 127.23 s\n",
      "2024-01-01 05:55:22.704916: \n",
      "2024-01-01 05:55:22.716918: Epoch 365\n",
      "2024-01-01 05:55:22.726915: Current learning rate: 0.00665\n",
      "2024-01-01 05:57:29.744349: train_loss -0.9073\n",
      "2024-01-01 05:57:29.754355: val_loss -0.821\n",
      "2024-01-01 05:57:29.762370: Pseudo dice [0.9179, 0.9403, 0.9389]\n",
      "2024-01-01 05:57:29.771370: Epoch time: 127.04 s\n",
      "2024-01-01 05:57:31.359336: \n",
      "2024-01-01 05:57:31.365665: Epoch 366\n",
      "2024-01-01 05:57:31.370663: Current learning rate: 0.00664\n",
      "2024-01-01 05:59:48.712726: train_loss -0.9087\n",
      "2024-01-01 05:59:48.723843: val_loss -0.8311\n",
      "2024-01-01 05:59:48.733844: Pseudo dice [0.9247, 0.9439, 0.9426]\n",
      "2024-01-01 05:59:48.743845: Epoch time: 137.35 s\n",
      "2024-01-01 05:59:50.598373: \n",
      "2024-01-01 05:59:50.606372: Epoch 367\n",
      "2024-01-01 05:59:50.613373: Current learning rate: 0.00663\n",
      "2024-01-01 06:02:02.293583: train_loss -0.9146\n",
      "2024-01-01 06:02:02.305094: val_loss -0.8397\n",
      "2024-01-01 06:02:02.316095: Pseudo dice [0.9282, 0.9447, 0.9417]\n",
      "2024-01-01 06:02:02.326093: Epoch time: 131.7 s\n",
      "2024-01-01 06:02:03.716428: \n",
      "2024-01-01 06:02:03.722746: Epoch 368\n",
      "2024-01-01 06:02:03.727765: Current learning rate: 0.00662\n",
      "2024-01-01 06:04:15.928063: train_loss -0.9086\n",
      "2024-01-01 06:04:15.938063: val_loss -0.8393\n",
      "2024-01-01 06:04:15.950383: Pseudo dice [0.9247, 0.9446, 0.9431]\n",
      "2024-01-01 06:04:15.959384: Epoch time: 132.21 s\n",
      "2024-01-01 06:04:17.266118: \n",
      "2024-01-01 06:04:17.273118: Epoch 369\n",
      "2024-01-01 06:04:17.281126: Current learning rate: 0.00661\n",
      "2024-01-01 06:06:25.745704: train_loss -0.9114\n",
      "2024-01-01 06:06:25.758703: val_loss -0.8385\n",
      "2024-01-01 06:06:25.768703: Pseudo dice [0.9283, 0.9459, 0.9448]\n",
      "2024-01-01 06:06:25.784923: Epoch time: 128.48 s\n",
      "2024-01-01 06:06:27.664227: \n",
      "2024-01-01 06:06:27.673224: Epoch 370\n",
      "2024-01-01 06:06:27.678224: Current learning rate: 0.0066\n",
      "2024-01-01 06:08:36.893260: train_loss -0.9107\n",
      "2024-01-01 06:08:36.904777: val_loss -0.8372\n",
      "2024-01-01 06:08:36.914768: Pseudo dice [0.9262, 0.9451, 0.9447]\n",
      "2024-01-01 06:08:36.922768: Epoch time: 129.23 s\n",
      "2024-01-01 06:08:38.425085: \n",
      "2024-01-01 06:08:38.432079: Epoch 371\n",
      "2024-01-01 06:08:38.437007: Current learning rate: 0.00659\n",
      "2024-01-01 06:10:46.699781: train_loss -0.9109\n",
      "2024-01-01 06:10:46.709782: val_loss -0.8389\n",
      "2024-01-01 06:10:46.718781: Pseudo dice [0.9258, 0.9464, 0.9446]\n",
      "2024-01-01 06:10:46.725785: Epoch time: 128.28 s\n",
      "2024-01-01 06:10:48.181166: \n",
      "2024-01-01 06:10:48.189163: Epoch 372\n",
      "2024-01-01 06:10:48.197676: Current learning rate: 0.00658\n",
      "2024-01-01 06:12:56.548827: train_loss -0.9123\n",
      "2024-01-01 06:12:56.564826: val_loss -0.8467\n",
      "2024-01-01 06:12:56.576833: Pseudo dice [0.9301, 0.9466, 0.9453]\n",
      "2024-01-01 06:12:56.588346: Epoch time: 128.37 s\n",
      "2024-01-01 06:12:58.458028: \n",
      "2024-01-01 06:12:58.464027: Epoch 373\n",
      "2024-01-01 06:12:58.470058: Current learning rate: 0.00657\n",
      "2024-01-01 06:15:09.011854: train_loss -0.91\n",
      "2024-01-01 06:15:09.027853: val_loss -0.8425\n",
      "2024-01-01 06:15:09.041854: Pseudo dice [0.9269, 0.9458, 0.9451]\n",
      "2024-01-01 06:15:09.057854: Epoch time: 130.55 s\n",
      "2024-01-01 06:15:11.112626: \n",
      "2024-01-01 06:15:11.124625: Epoch 374\n",
      "2024-01-01 06:15:11.133629: Current learning rate: 0.00656\n",
      "2024-01-01 06:17:21.000258: train_loss -0.9118\n",
      "2024-01-01 06:17:21.011258: val_loss -0.8399\n",
      "2024-01-01 06:17:21.021260: Pseudo dice [0.9273, 0.9454, 0.9437]\n",
      "2024-01-01 06:17:21.033499: Epoch time: 129.89 s\n",
      "2024-01-01 06:17:22.322983: \n",
      "2024-01-01 06:17:22.328978: Epoch 375\n",
      "2024-01-01 06:17:22.333979: Current learning rate: 0.00655\n",
      "2024-01-01 06:19:35.568459: train_loss -0.9087\n",
      "2024-01-01 06:19:35.580564: val_loss -0.8356\n",
      "2024-01-01 06:19:35.592168: Pseudo dice [0.9265, 0.9457, 0.9447]\n",
      "2024-01-01 06:19:35.603760: Epoch time: 133.25 s\n",
      "2024-01-01 06:19:36.963060: \n",
      "2024-01-01 06:19:36.970072: Epoch 376\n",
      "2024-01-01 06:19:36.974981: Current learning rate: 0.00654\n",
      "2024-01-01 06:22:02.188795: train_loss -0.9119\n",
      "2024-01-01 06:22:02.229942: val_loss -0.842\n",
      "2024-01-01 06:22:02.260628: Pseudo dice [0.9273, 0.9448, 0.9435]\n",
      "2024-01-01 06:22:02.297758: Epoch time: 145.23 s\n",
      "2024-01-01 06:22:04.475389: \n",
      "2024-01-01 06:22:04.485901: Epoch 377\n",
      "2024-01-01 06:22:04.498416: Current learning rate: 0.00653\n",
      "2024-01-01 06:24:22.727990: train_loss -0.909\n",
      "2024-01-01 06:24:22.738577: val_loss -0.8246\n",
      "2024-01-01 06:24:22.748099: Pseudo dice [0.9233, 0.9428, 0.9407]\n",
      "2024-01-01 06:24:22.757614: Epoch time: 138.26 s\n",
      "2024-01-01 06:24:24.209769: \n",
      "2024-01-01 06:24:24.218293: Epoch 378\n",
      "2024-01-01 06:24:24.223296: Current learning rate: 0.00652\n",
      "2024-01-01 06:26:34.307119: train_loss -0.9114\n",
      "2024-01-01 06:26:34.317647: val_loss -0.8235\n",
      "2024-01-01 06:26:34.328555: Pseudo dice [0.9237, 0.9441, 0.943]\n",
      "2024-01-01 06:26:34.340079: Epoch time: 130.1 s\n",
      "2024-01-01 06:26:35.745398: \n",
      "2024-01-01 06:26:35.752395: Epoch 379\n",
      "2024-01-01 06:26:35.758688: Current learning rate: 0.00651\n",
      "2024-01-01 06:28:43.915921: train_loss -0.9131\n",
      "2024-01-01 06:28:43.934651: val_loss -0.8211\n",
      "2024-01-01 06:28:43.952352: Pseudo dice [0.9179, 0.9418, 0.9402]\n",
      "2024-01-01 06:28:43.969019: Epoch time: 128.17 s\n",
      "2024-01-01 06:28:45.759939: \n",
      "2024-01-01 06:28:45.765472: Epoch 380\n",
      "2024-01-01 06:28:45.771480: Current learning rate: 0.0065\n",
      "2024-01-01 06:30:57.059418: train_loss -0.9113\n",
      "2024-01-01 06:30:57.074373: val_loss -0.8279\n",
      "2024-01-01 06:30:57.087421: Pseudo dice [0.921, 0.9421, 0.9413]\n",
      "2024-01-01 06:30:57.102947: Epoch time: 131.3 s\n",
      "2024-01-01 06:30:59.596915: \n",
      "2024-01-01 06:30:59.601915: Epoch 381\n",
      "2024-01-01 06:30:59.607431: Current learning rate: 0.00649\n",
      "2024-01-01 06:33:09.256381: train_loss -0.9122\n",
      "2024-01-01 06:33:09.266887: val_loss -0.8379\n",
      "2024-01-01 06:33:09.277406: Pseudo dice [0.9253, 0.9452, 0.9433]\n",
      "2024-01-01 06:33:09.286921: Epoch time: 129.66 s\n",
      "2024-01-01 06:33:10.867262: \n",
      "2024-01-01 06:33:10.872801: Epoch 382\n",
      "2024-01-01 06:33:10.877450: Current learning rate: 0.00648\n",
      "2024-01-01 06:35:21.548810: train_loss -0.9105\n",
      "2024-01-01 06:35:21.560927: val_loss -0.8414\n",
      "2024-01-01 06:35:21.574477: Pseudo dice [0.9283, 0.9472, 0.9462]\n",
      "2024-01-01 06:35:21.587550: Epoch time: 130.68 s\n",
      "2024-01-01 06:35:23.393105: \n",
      "2024-01-01 06:35:23.405165: Epoch 383\n",
      "2024-01-01 06:35:23.413179: Current learning rate: 0.00648\n",
      "2024-01-01 06:37:33.543428: train_loss -0.9104\n",
      "2024-01-01 06:37:33.556436: val_loss -0.8315\n",
      "2024-01-01 06:37:33.567435: Pseudo dice [0.9284, 0.9428, 0.9418]\n",
      "2024-01-01 06:37:33.580435: Epoch time: 130.15 s\n",
      "2024-01-01 06:37:35.231026: \n",
      "2024-01-01 06:37:35.240030: Epoch 384\n",
      "2024-01-01 06:37:35.246542: Current learning rate: 0.00647\n",
      "2024-01-01 06:39:42.872912: train_loss -0.9089\n",
      "2024-01-01 06:39:42.882912: val_loss -0.8291\n",
      "2024-01-01 06:39:42.892924: Pseudo dice [0.9254, 0.9444, 0.9434]\n",
      "2024-01-01 06:39:42.903441: Epoch time: 127.64 s\n",
      "2024-01-01 06:39:44.195928: \n",
      "2024-01-01 06:39:44.202687: Epoch 385\n",
      "2024-01-01 06:39:44.208580: Current learning rate: 0.00646\n",
      "2024-01-01 06:41:52.377139: train_loss -0.9105\n",
      "2024-01-01 06:41:52.388138: val_loss -0.8295\n",
      "2024-01-01 06:41:52.398137: Pseudo dice [0.9249, 0.9444, 0.9427]\n",
      "2024-01-01 06:41:52.407138: Epoch time: 128.18 s\n",
      "2024-01-01 06:41:53.506204: \n",
      "2024-01-01 06:41:53.512203: Epoch 386\n",
      "2024-01-01 06:41:53.517190: Current learning rate: 0.00645\n",
      "2024-01-01 06:44:00.988147: train_loss -0.9108\n",
      "2024-01-01 06:44:01.000277: val_loss -0.8351\n",
      "2024-01-01 06:44:01.009291: Pseudo dice [0.9281, 0.946, 0.9451]\n",
      "2024-01-01 06:44:01.017290: Epoch time: 127.48 s\n",
      "2024-01-01 06:44:02.110555: \n",
      "2024-01-01 06:44:02.115551: Epoch 387\n",
      "2024-01-01 06:44:02.120551: Current learning rate: 0.00644\n",
      "2024-01-01 06:46:08.742779: train_loss -0.9123\n",
      "2024-01-01 06:46:08.752296: val_loss -0.8278\n",
      "2024-01-01 06:46:08.762601: Pseudo dice [0.9199, 0.9427, 0.9419]\n",
      "2024-01-01 06:46:08.770806: Epoch time: 126.63 s\n",
      "2024-01-01 06:46:10.103267: \n",
      "2024-01-01 06:46:10.108261: Epoch 388\n",
      "2024-01-01 06:46:10.113260: Current learning rate: 0.00643\n",
      "2024-01-01 06:48:17.241957: train_loss -0.912\n",
      "2024-01-01 06:48:17.252177: val_loss -0.843\n",
      "2024-01-01 06:48:17.262172: Pseudo dice [0.9308, 0.9477, 0.9455]\n",
      "2024-01-01 06:48:17.270683: Epoch time: 127.14 s\n",
      "2024-01-01 06:48:18.438055: \n",
      "2024-01-01 06:48:18.444557: Epoch 389\n",
      "2024-01-01 06:48:18.448637: Current learning rate: 0.00642\n",
      "2024-01-01 06:50:24.102489: train_loss -0.9121\n",
      "2024-01-01 06:50:24.109497: val_loss -0.8335\n",
      "2024-01-01 06:50:24.117489: Pseudo dice [0.9268, 0.9458, 0.9444]\n",
      "2024-01-01 06:50:24.124491: Epoch time: 125.67 s\n",
      "2024-01-01 06:50:25.189535: \n",
      "2024-01-01 06:50:25.195527: Epoch 390\n",
      "2024-01-01 06:50:25.199532: Current learning rate: 0.00641\n",
      "2024-01-01 06:52:30.633461: train_loss -0.9133\n",
      "2024-01-01 06:52:30.641973: val_loss -0.8403\n",
      "2024-01-01 06:52:30.651134: Pseudo dice [0.9293, 0.9455, 0.9453]\n",
      "2024-01-01 06:52:30.657135: Epoch time: 125.44 s\n",
      "2024-01-01 06:52:31.711812: \n",
      "2024-01-01 06:52:31.716807: Epoch 391\n",
      "2024-01-01 06:52:31.722804: Current learning rate: 0.0064\n",
      "2024-01-01 06:54:37.321480: train_loss -0.9103\n",
      "2024-01-01 06:54:37.330997: val_loss -0.8346\n",
      "2024-01-01 06:54:37.338996: Pseudo dice [0.9256, 0.9453, 0.9445]\n",
      "2024-01-01 06:54:37.346997: Epoch time: 125.61 s\n",
      "2024-01-01 06:54:38.404675: \n",
      "2024-01-01 06:54:38.410665: Epoch 392\n",
      "2024-01-01 06:54:38.414687: Current learning rate: 0.00639\n",
      "2024-01-01 06:56:44.276862: train_loss -0.9088\n",
      "2024-01-01 06:56:44.285862: val_loss -0.8281\n",
      "2024-01-01 06:56:44.292863: Pseudo dice [0.9267, 0.9442, 0.9425]\n",
      "2024-01-01 06:56:44.300867: Epoch time: 125.87 s\n",
      "2024-01-01 06:56:45.359746: \n",
      "2024-01-01 06:56:45.364760: Epoch 393\n",
      "2024-01-01 06:56:45.369761: Current learning rate: 0.00638\n",
      "2024-01-01 06:58:50.767404: train_loss -0.9108\n",
      "2024-01-01 06:58:50.778481: val_loss -0.8272\n",
      "2024-01-01 06:58:50.785480: Pseudo dice [0.9254, 0.9441, 0.9443]\n",
      "2024-01-01 06:58:50.791481: Epoch time: 125.41 s\n",
      "2024-01-01 06:58:51.839472: \n",
      "2024-01-01 06:58:51.848485: Epoch 394\n",
      "2024-01-01 06:58:51.853494: Current learning rate: 0.00637\n",
      "2024-01-01 07:00:57.677403: train_loss -0.9096\n",
      "2024-01-01 07:00:57.685405: val_loss -0.8379\n",
      "2024-01-01 07:00:57.691403: Pseudo dice [0.9274, 0.9462, 0.9452]\n",
      "2024-01-01 07:00:57.701405: Epoch time: 125.84 s\n",
      "2024-01-01 07:00:58.762117: \n",
      "2024-01-01 07:00:58.770192: Epoch 395\n",
      "2024-01-01 07:00:58.775119: Current learning rate: 0.00636\n",
      "2024-01-01 07:03:04.205673: train_loss -0.9115\n",
      "2024-01-01 07:03:04.214672: val_loss -0.8365\n",
      "2024-01-01 07:03:04.221680: Pseudo dice [0.9274, 0.946, 0.9437]\n",
      "2024-01-01 07:03:04.227680: Epoch time: 125.44 s\n",
      "2024-01-01 07:03:05.273773: \n",
      "2024-01-01 07:03:05.285774: Epoch 396\n",
      "2024-01-01 07:03:05.293774: Current learning rate: 0.00635\n",
      "2024-01-01 07:05:10.750523: train_loss -0.9109\n",
      "2024-01-01 07:05:10.760524: val_loss -0.84\n",
      "2024-01-01 07:05:10.769525: Pseudo dice [0.9281, 0.9467, 0.9456]\n",
      "2024-01-01 07:05:10.774524: Epoch time: 125.48 s\n",
      "2024-01-01 07:05:11.821248: \n",
      "2024-01-01 07:05:11.827267: Epoch 397\n",
      "2024-01-01 07:05:11.831265: Current learning rate: 0.00634\n",
      "2024-01-01 07:07:17.227845: train_loss -0.912\n",
      "2024-01-01 07:07:17.236849: val_loss -0.8393\n",
      "2024-01-01 07:07:17.243847: Pseudo dice [0.9311, 0.9454, 0.9437]\n",
      "2024-01-01 07:07:17.250846: Epoch time: 125.41 s\n",
      "2024-01-01 07:07:18.304231: \n",
      "2024-01-01 07:07:18.312231: Epoch 398\n",
      "2024-01-01 07:07:18.317224: Current learning rate: 0.00633\n",
      "2024-01-01 07:09:23.580061: train_loss -0.9122\n",
      "2024-01-01 07:09:23.588571: val_loss -0.8343\n",
      "2024-01-01 07:09:23.596570: Pseudo dice [0.9251, 0.9454, 0.9445]\n",
      "2024-01-01 07:09:23.604570: Epoch time: 125.28 s\n",
      "2024-01-01 07:09:24.659677: \n",
      "2024-01-01 07:09:24.665691: Epoch 399\n",
      "2024-01-01 07:09:24.670684: Current learning rate: 0.00632\n",
      "2024-01-01 07:11:29.959795: train_loss -0.914\n",
      "2024-01-01 07:11:29.968791: val_loss -0.8266\n",
      "2024-01-01 07:11:29.974794: Pseudo dice [0.9222, 0.943, 0.9413]\n",
      "2024-01-01 07:11:29.980794: Epoch time: 125.3 s\n",
      "2024-01-01 07:11:31.301392: \n",
      "2024-01-01 07:11:31.309462: Epoch 400\n",
      "2024-01-01 07:11:31.314390: Current learning rate: 0.00631\n",
      "2024-01-01 07:13:36.576329: train_loss -0.9142\n",
      "2024-01-01 07:13:36.584328: val_loss -0.8295\n",
      "2024-01-01 07:13:36.591329: Pseudo dice [0.9263, 0.9441, 0.9434]\n",
      "2024-01-01 07:13:36.598329: Epoch time: 125.28 s\n",
      "2024-01-01 07:13:37.646899: \n",
      "2024-01-01 07:13:37.651905: Epoch 401\n",
      "2024-01-01 07:13:37.656971: Current learning rate: 0.0063\n",
      "2024-01-01 07:15:43.014043: train_loss -0.9089\n",
      "2024-01-01 07:15:43.021043: val_loss -0.8117\n",
      "2024-01-01 07:15:43.029048: Pseudo dice [0.9117, 0.9382, 0.9333]\n",
      "2024-01-01 07:15:43.036042: Epoch time: 125.37 s\n",
      "2024-01-01 07:15:44.089456: \n",
      "2024-01-01 07:15:44.099381: Epoch 402\n",
      "2024-01-01 07:15:44.104406: Current learning rate: 0.0063\n",
      "2024-01-01 07:17:49.543161: train_loss -0.9097\n",
      "2024-01-01 07:17:49.550161: val_loss -0.814\n",
      "2024-01-01 07:17:49.558161: Pseudo dice [0.9201, 0.941, 0.9394]\n",
      "2024-01-01 07:17:49.565160: Epoch time: 125.45 s\n",
      "2024-01-01 07:17:50.775210: \n",
      "2024-01-01 07:17:50.781119: Epoch 403\n",
      "2024-01-01 07:17:50.785127: Current learning rate: 0.00629\n",
      "2024-01-01 07:19:55.960764: train_loss -0.9139\n",
      "2024-01-01 07:19:55.969772: val_loss -0.8148\n",
      "2024-01-01 07:19:55.977773: Pseudo dice [0.916, 0.9404, 0.937]\n",
      "2024-01-01 07:19:55.988292: Epoch time: 125.19 s\n",
      "2024-01-01 07:19:57.034177: \n",
      "2024-01-01 07:19:57.043262: Epoch 404\n",
      "2024-01-01 07:19:57.048186: Current learning rate: 0.00628\n",
      "2024-01-01 07:22:02.388501: train_loss -0.9138\n",
      "2024-01-01 07:22:02.399502: val_loss -0.8155\n",
      "2024-01-01 07:22:02.408500: Pseudo dice [0.9161, 0.9379, 0.9346]\n",
      "2024-01-01 07:22:02.415505: Epoch time: 125.36 s\n",
      "2024-01-01 07:22:03.456218: \n",
      "2024-01-01 07:22:03.463276: Epoch 405\n",
      "2024-01-01 07:22:03.467288: Current learning rate: 0.00627\n",
      "2024-01-01 07:24:09.107641: train_loss -0.909\n",
      "2024-01-01 07:24:09.118645: val_loss -0.8117\n",
      "2024-01-01 07:24:09.127642: Pseudo dice [0.9173, 0.9391, 0.938]\n",
      "2024-01-01 07:24:09.134642: Epoch time: 125.65 s\n",
      "2024-01-01 07:24:10.183768: \n",
      "2024-01-01 07:24:10.191844: Epoch 406\n",
      "2024-01-01 07:24:10.196777: Current learning rate: 0.00626\n",
      "2024-01-01 07:26:15.607256: train_loss -0.9132\n",
      "2024-01-01 07:26:15.617974: val_loss -0.805\n",
      "2024-01-01 07:26:15.626496: Pseudo dice [0.9132, 0.9393, 0.9374]\n",
      "2024-01-01 07:26:15.633496: Epoch time: 125.42 s\n",
      "2024-01-01 07:26:16.740543: \n",
      "2024-01-01 07:26:16.752158: Epoch 407\n",
      "2024-01-01 07:26:16.759254: Current learning rate: 0.00625\n",
      "2024-01-01 07:28:22.395190: train_loss -0.9119\n",
      "2024-01-01 07:28:22.403205: val_loss -0.8091\n",
      "2024-01-01 07:28:22.412199: Pseudo dice [0.9139, 0.9406, 0.9381]\n",
      "2024-01-01 07:28:22.420714: Epoch time: 125.66 s\n",
      "2024-01-01 07:28:23.466578: \n",
      "2024-01-01 07:28:23.473583: Epoch 408\n",
      "2024-01-01 07:28:23.478591: Current learning rate: 0.00624\n",
      "2024-01-01 07:30:29.020098: train_loss -0.9133\n",
      "2024-01-01 07:30:29.027098: val_loss -0.8217\n",
      "2024-01-01 07:30:29.035099: Pseudo dice [0.9151, 0.9386, 0.9364]\n",
      "2024-01-01 07:30:29.043099: Epoch time: 125.55 s\n",
      "2024-01-01 07:30:30.101279: \n",
      "2024-01-01 07:30:30.108482: Epoch 409\n",
      "2024-01-01 07:30:30.113488: Current learning rate: 0.00623\n",
      "2024-01-01 07:32:35.551712: train_loss -0.9119\n",
      "2024-01-01 07:32:35.563708: val_loss -0.8091\n",
      "2024-01-01 07:32:35.575709: Pseudo dice [0.9143, 0.9396, 0.9375]\n",
      "2024-01-01 07:32:35.583710: Epoch time: 125.45 s\n",
      "2024-01-01 07:32:36.806355: \n",
      "2024-01-01 07:32:36.811356: Epoch 410\n",
      "2024-01-01 07:32:36.816355: Current learning rate: 0.00622\n",
      "2024-01-01 07:34:42.596565: train_loss -0.9119\n",
      "2024-01-01 07:34:42.604578: val_loss -0.8347\n",
      "2024-01-01 07:34:42.612566: Pseudo dice [0.9268, 0.9457, 0.9436]\n",
      "2024-01-01 07:34:42.619566: Epoch time: 125.79 s\n",
      "2024-01-01 07:34:43.593065: \n",
      "2024-01-01 07:34:43.599134: Epoch 411\n",
      "2024-01-01 07:34:43.604085: Current learning rate: 0.00621\n",
      "2024-01-01 07:36:48.912757: train_loss -0.9132\n",
      "2024-01-01 07:36:48.920764: val_loss -0.8232\n",
      "2024-01-01 07:36:48.928763: Pseudo dice [0.9217, 0.9425, 0.9417]\n",
      "2024-01-01 07:36:48.937762: Epoch time: 125.32 s\n",
      "2024-01-01 07:36:49.920042: \n",
      "2024-01-01 07:36:49.925025: Epoch 412\n",
      "2024-01-01 07:36:49.930029: Current learning rate: 0.0062\n",
      "2024-01-01 07:38:55.167866: train_loss -0.9151\n",
      "2024-01-01 07:38:55.174866: val_loss -0.8343\n",
      "2024-01-01 07:38:55.182373: Pseudo dice [0.925, 0.9456, 0.9439]\n",
      "2024-01-01 07:38:55.189375: Epoch time: 125.25 s\n",
      "2024-01-01 07:38:56.176714: \n",
      "2024-01-01 07:38:56.182482: Epoch 413\n",
      "2024-01-01 07:38:56.186485: Current learning rate: 0.00619\n",
      "2024-01-01 07:41:01.471836: train_loss -0.9119\n",
      "2024-01-01 07:41:01.480836: val_loss -0.8307\n",
      "2024-01-01 07:41:01.489839: Pseudo dice [0.9218, 0.9437, 0.9433]\n",
      "2024-01-01 07:41:01.495839: Epoch time: 125.3 s\n",
      "2024-01-01 07:41:02.479604: \n",
      "2024-01-01 07:41:02.485606: Epoch 414\n",
      "2024-01-01 07:41:02.490605: Current learning rate: 0.00618\n",
      "2024-01-01 07:43:07.763089: train_loss -0.9145\n",
      "2024-01-01 07:43:07.774089: val_loss -0.8239\n",
      "2024-01-01 07:43:07.780090: Pseudo dice [0.9216, 0.9411, 0.9403]\n",
      "2024-01-01 07:43:07.789092: Epoch time: 125.28 s\n",
      "2024-01-01 07:43:08.780075: \n",
      "2024-01-01 07:43:08.786074: Epoch 415\n",
      "2024-01-01 07:43:08.790085: Current learning rate: 0.00617\n",
      "2024-01-01 07:45:14.185323: train_loss -0.9117\n",
      "2024-01-01 07:45:14.192324: val_loss -0.8234\n",
      "2024-01-01 07:45:14.197323: Pseudo dice [0.9178, 0.9401, 0.9388]\n",
      "2024-01-01 07:45:14.203867: Epoch time: 125.41 s\n",
      "2024-01-01 07:45:15.198799: \n",
      "2024-01-01 07:45:15.203800: Epoch 416\n",
      "2024-01-01 07:45:15.208804: Current learning rate: 0.00616\n",
      "2024-01-01 07:47:20.467436: train_loss -0.9138\n",
      "2024-01-01 07:47:20.476438: val_loss -0.8163\n",
      "2024-01-01 07:47:20.484436: Pseudo dice [0.9178, 0.941, 0.9394]\n",
      "2024-01-01 07:47:20.492445: Epoch time: 125.27 s\n",
      "2024-01-01 07:47:21.499487: \n",
      "2024-01-01 07:47:21.504484: Epoch 417\n",
      "2024-01-01 07:47:21.509488: Current learning rate: 0.00615\n",
      "2024-01-01 07:49:26.867214: train_loss -0.9137\n",
      "2024-01-01 07:49:26.877213: val_loss -0.8085\n",
      "2024-01-01 07:49:26.886228: Pseudo dice [0.9138, 0.9383, 0.9346]\n",
      "2024-01-01 07:49:26.894229: Epoch time: 125.37 s\n",
      "2024-01-01 07:49:28.088639: \n",
      "2024-01-01 07:49:28.095705: Epoch 418\n",
      "2024-01-01 07:49:28.101286: Current learning rate: 0.00614\n",
      "2024-01-01 07:51:33.507268: train_loss -0.913\n",
      "2024-01-01 07:51:33.516260: val_loss -0.8277\n",
      "2024-01-01 07:51:33.522263: Pseudo dice [0.9215, 0.9438, 0.9432]\n",
      "2024-01-01 07:51:33.529258: Epoch time: 125.42 s\n",
      "2024-01-01 07:51:34.516073: \n",
      "2024-01-01 07:51:34.522079: Epoch 419\n",
      "2024-01-01 07:51:34.526078: Current learning rate: 0.00613\n",
      "2024-01-01 07:53:40.076425: train_loss -0.9103\n",
      "2024-01-01 07:53:40.085431: val_loss -0.8284\n",
      "2024-01-01 07:53:40.093978: Pseudo dice [0.9191, 0.9428, 0.9413]\n",
      "2024-01-01 07:53:40.100971: Epoch time: 125.56 s\n",
      "2024-01-01 07:53:41.098814: \n",
      "2024-01-01 07:53:41.104817: Epoch 420\n",
      "2024-01-01 07:53:41.115925: Current learning rate: 0.00612\n",
      "2024-01-01 07:55:46.464638: train_loss -0.9112\n",
      "2024-01-01 07:55:46.472639: val_loss -0.8286\n",
      "2024-01-01 07:55:46.480638: Pseudo dice [0.9247, 0.9458, 0.9436]\n",
      "2024-01-01 07:55:46.486638: Epoch time: 125.37 s\n",
      "2024-01-01 07:55:47.482698: \n",
      "2024-01-01 07:55:47.489697: Epoch 421\n",
      "2024-01-01 07:55:47.494700: Current learning rate: 0.00612\n",
      "2024-01-01 07:57:52.821238: train_loss -0.9132\n",
      "2024-01-01 07:57:52.829915: val_loss -0.8347\n",
      "2024-01-01 07:57:52.837915: Pseudo dice [0.9267, 0.9473, 0.9458]\n",
      "2024-01-01 07:57:52.842915: Epoch time: 125.34 s\n",
      "2024-01-01 07:57:53.832968: \n",
      "2024-01-01 07:57:53.838969: Epoch 422\n",
      "2024-01-01 07:57:53.842968: Current learning rate: 0.00611\n",
      "2024-01-01 07:59:59.345028: train_loss -0.9121\n",
      "2024-01-01 07:59:59.354032: val_loss -0.8011\n",
      "2024-01-01 07:59:59.362033: Pseudo dice [0.9148, 0.937, 0.9336]\n",
      "2024-01-01 07:59:59.370542: Epoch time: 125.51 s\n",
      "2024-01-01 08:00:00.397315: \n",
      "2024-01-01 08:00:00.403316: Epoch 423\n",
      "2024-01-01 08:00:00.409431: Current learning rate: 0.0061\n",
      "2024-01-01 08:02:06.042930: train_loss -0.9064\n",
      "2024-01-01 08:02:06.051930: val_loss -0.8255\n",
      "2024-01-01 08:02:06.058931: Pseudo dice [0.9215, 0.943, 0.94]\n",
      "2024-01-01 08:02:06.064931: Epoch time: 125.65 s\n",
      "2024-01-01 08:02:07.065695: \n",
      "2024-01-01 08:02:07.071762: Epoch 424\n",
      "2024-01-01 08:02:07.077772: Current learning rate: 0.00609\n",
      "2024-01-01 08:04:12.323053: train_loss -0.9113\n",
      "2024-01-01 08:04:12.335054: val_loss -0.82\n",
      "2024-01-01 08:04:12.344053: Pseudo dice [0.9198, 0.9401, 0.9389]\n",
      "2024-01-01 08:04:12.353052: Epoch time: 125.26 s\n",
      "2024-01-01 08:04:13.359893: \n",
      "2024-01-01 08:04:13.365890: Epoch 425\n",
      "2024-01-01 08:04:13.370914: Current learning rate: 0.00608\n",
      "2024-01-01 08:06:18.719999: train_loss -0.9112\n",
      "2024-01-01 08:06:18.727999: val_loss -0.8124\n",
      "2024-01-01 08:06:18.735003: Pseudo dice [0.9169, 0.9393, 0.9371]\n",
      "2024-01-01 08:06:18.741003: Epoch time: 125.36 s\n",
      "2024-01-01 08:06:19.910060: \n",
      "2024-01-01 08:06:19.916061: Epoch 426\n",
      "2024-01-01 08:06:19.921091: Current learning rate: 0.00607\n",
      "2024-01-01 08:08:25.414712: train_loss -0.9089\n",
      "2024-01-01 08:08:25.423714: val_loss -0.8226\n",
      "2024-01-01 08:08:25.431714: Pseudo dice [0.9176, 0.9401, 0.937]\n",
      "2024-01-01 08:08:25.437713: Epoch time: 125.5 s\n",
      "2024-01-01 08:08:26.425513: \n",
      "2024-01-01 08:08:26.430509: Epoch 427\n",
      "2024-01-01 08:08:26.435513: Current learning rate: 0.00606\n",
      "2024-01-01 08:10:31.666459: train_loss -0.9121\n",
      "2024-01-01 08:10:31.674458: val_loss -0.8037\n",
      "2024-01-01 08:10:31.681460: Pseudo dice [0.9166, 0.9369, 0.9349]\n",
      "2024-01-01 08:10:31.689461: Epoch time: 125.24 s\n",
      "2024-01-01 08:10:32.672023: \n",
      "2024-01-01 08:10:32.678023: Epoch 428\n",
      "2024-01-01 08:10:32.683039: Current learning rate: 0.00605\n",
      "2024-01-01 08:12:37.946504: train_loss -0.9114\n",
      "2024-01-01 08:12:37.954502: val_loss -0.8187\n",
      "2024-01-01 08:12:37.961503: Pseudo dice [0.9155, 0.9403, 0.9374]\n",
      "2024-01-01 08:12:37.969511: Epoch time: 125.28 s\n",
      "2024-01-01 08:12:38.967387: \n",
      "2024-01-01 08:12:38.972386: Epoch 429\n",
      "2024-01-01 08:12:38.977377: Current learning rate: 0.00604\n",
      "2024-01-01 08:14:44.425326: train_loss -0.9102\n",
      "2024-01-01 08:14:44.435326: val_loss -0.8167\n",
      "2024-01-01 08:14:44.444330: Pseudo dice [0.9163, 0.9391, 0.9382]\n",
      "2024-01-01 08:14:44.450332: Epoch time: 125.46 s\n",
      "2024-01-01 08:14:45.451850: \n",
      "2024-01-01 08:14:45.457355: Epoch 430\n",
      "2024-01-01 08:14:45.461367: Current learning rate: 0.00603\n",
      "2024-01-01 08:16:50.786918: train_loss -0.9118\n",
      "2024-01-01 08:16:50.793919: val_loss -0.8148\n",
      "2024-01-01 08:16:50.800926: Pseudo dice [0.9176, 0.939, 0.9378]\n",
      "2024-01-01 08:16:50.807919: Epoch time: 125.34 s\n",
      "2024-01-01 08:16:51.792665: \n",
      "2024-01-01 08:16:51.798664: Epoch 431\n",
      "2024-01-01 08:16:51.803665: Current learning rate: 0.00602\n",
      "2024-01-01 08:18:57.117762: train_loss -0.9101\n",
      "2024-01-01 08:18:57.125765: val_loss -0.7983\n",
      "2024-01-01 08:18:57.134273: Pseudo dice [0.9177, 0.9379, 0.9368]\n",
      "2024-01-01 08:18:57.141275: Epoch time: 125.33 s\n",
      "2024-01-01 08:18:58.127165: \n",
      "2024-01-01 08:18:58.132181: Epoch 432\n",
      "2024-01-01 08:18:58.137704: Current learning rate: 0.00601\n",
      "2024-01-01 08:21:03.699141: train_loss -0.9129\n",
      "2024-01-01 08:21:03.708147: val_loss -0.8106\n",
      "2024-01-01 08:21:03.715145: Pseudo dice [0.9158, 0.9403, 0.9368]\n",
      "2024-01-01 08:21:03.721648: Epoch time: 125.57 s\n",
      "2024-01-01 08:21:04.883319: \n",
      "2024-01-01 08:21:04.889362: Epoch 433\n",
      "2024-01-01 08:21:04.893449: Current learning rate: 0.006\n",
      "2024-01-01 08:23:10.457452: train_loss -0.9113\n",
      "2024-01-01 08:23:10.467458: val_loss -0.8109\n",
      "2024-01-01 08:23:10.475469: Pseudo dice [0.9143, 0.9396, 0.9375]\n",
      "2024-01-01 08:23:10.480468: Epoch time: 125.58 s\n",
      "2024-01-01 08:23:11.467064: \n",
      "2024-01-01 08:23:11.472063: Epoch 434\n",
      "2024-01-01 08:23:11.477067: Current learning rate: 0.00599\n",
      "2024-01-01 08:25:17.127497: train_loss -0.9105\n",
      "2024-01-01 08:25:17.134504: val_loss -0.8153\n",
      "2024-01-01 08:25:17.141497: Pseudo dice [0.914, 0.9406, 0.9377]\n",
      "2024-01-01 08:25:17.149497: Epoch time: 125.66 s\n",
      "2024-01-01 08:25:18.157493: \n",
      "2024-01-01 08:25:18.163510: Epoch 435\n",
      "2024-01-01 08:25:18.167510: Current learning rate: 0.00598\n",
      "2024-01-01 08:27:23.615232: train_loss -0.9121\n",
      "2024-01-01 08:27:23.626234: val_loss -0.819\n",
      "2024-01-01 08:27:23.634233: Pseudo dice [0.9153, 0.9392, 0.938]\n",
      "2024-01-01 08:27:23.641711: Epoch time: 125.46 s\n",
      "2024-01-01 08:27:24.653656: \n",
      "2024-01-01 08:27:24.659654: Epoch 436\n",
      "2024-01-01 08:27:24.664656: Current learning rate: 0.00597\n",
      "2024-01-01 08:29:29.678464: train_loss -0.9181\n",
      "2024-01-01 08:29:29.686471: val_loss -0.8198\n",
      "2024-01-01 08:29:29.694971: Pseudo dice [0.9179, 0.9414, 0.939]\n",
      "2024-01-01 08:29:29.702976: Epoch time: 125.03 s\n",
      "2024-01-01 08:29:30.701159: \n",
      "2024-01-01 08:29:30.706163: Epoch 437\n",
      "2024-01-01 08:29:30.711163: Current learning rate: 0.00596\n",
      "2024-01-01 08:31:36.081043: train_loss -0.9138\n",
      "2024-01-01 08:31:36.090044: val_loss -0.8091\n",
      "2024-01-01 08:31:36.098042: Pseudo dice [0.9129, 0.9372, 0.9346]\n",
      "2024-01-01 08:31:36.105042: Epoch time: 125.38 s\n",
      "2024-01-01 08:31:37.094929: \n",
      "2024-01-01 08:31:37.099930: Epoch 438\n",
      "2024-01-01 08:31:37.103930: Current learning rate: 0.00595\n",
      "2024-01-01 08:33:42.517907: train_loss -0.9112\n",
      "2024-01-01 08:33:42.525907: val_loss -0.8034\n",
      "2024-01-01 08:33:42.533907: Pseudo dice [0.9161, 0.9396, 0.938]\n",
      "2024-01-01 08:33:42.539907: Epoch time: 125.42 s\n",
      "2024-01-01 08:33:43.534513: \n",
      "2024-01-01 08:33:43.539506: Epoch 439\n",
      "2024-01-01 08:33:43.544506: Current learning rate: 0.00594\n",
      "2024-01-01 08:35:48.878082: train_loss -0.914\n",
      "2024-01-01 08:35:48.887081: val_loss -0.8185\n",
      "2024-01-01 08:35:48.895089: Pseudo dice [0.9159, 0.9414, 0.9382]\n",
      "2024-01-01 08:35:48.904084: Epoch time: 125.34 s\n",
      "2024-01-01 08:35:49.916341: \n",
      "2024-01-01 08:35:49.933940: Epoch 440\n",
      "2024-01-01 08:35:49.937933: Current learning rate: 0.00593\n",
      "2024-01-01 08:37:55.441668: train_loss -0.9124\n",
      "2024-01-01 08:37:55.451669: val_loss -0.8094\n",
      "2024-01-01 08:37:55.459668: Pseudo dice [0.9168, 0.9399, 0.9382]\n",
      "2024-01-01 08:37:55.467667: Epoch time: 125.53 s\n",
      "2024-01-01 08:37:56.645801: \n",
      "2024-01-01 08:37:56.650801: Epoch 441\n",
      "2024-01-01 08:37:56.656793: Current learning rate: 0.00592\n",
      "2024-01-01 08:40:02.114310: train_loss -0.9118\n",
      "2024-01-01 08:40:02.121307: val_loss -0.8021\n",
      "2024-01-01 08:40:02.130309: Pseudo dice [0.9169, 0.9395, 0.9374]\n",
      "2024-01-01 08:40:02.136308: Epoch time: 125.47 s\n",
      "2024-01-01 08:40:03.125233: \n",
      "2024-01-01 08:40:03.130229: Epoch 442\n",
      "2024-01-01 08:40:03.135230: Current learning rate: 0.00592\n",
      "2024-01-01 08:42:08.586986: train_loss -0.9123\n",
      "2024-01-01 08:42:08.596987: val_loss -0.8126\n",
      "2024-01-01 08:42:08.604986: Pseudo dice [0.9151, 0.9404, 0.9389]\n",
      "2024-01-01 08:42:08.612986: Epoch time: 125.46 s\n",
      "2024-01-01 08:42:09.629811: \n",
      "2024-01-01 08:42:09.635365: Epoch 443\n",
      "2024-01-01 08:42:09.640363: Current learning rate: 0.00591\n",
      "2024-01-01 08:44:15.029510: train_loss -0.9142\n",
      "2024-01-01 08:44:15.037791: val_loss -0.8052\n",
      "2024-01-01 08:44:15.044800: Pseudo dice [0.9156, 0.9398, 0.9366]\n",
      "2024-01-01 08:44:15.051790: Epoch time: 125.4 s\n",
      "2024-01-01 08:44:16.063665: \n",
      "2024-01-01 08:44:16.069667: Epoch 444\n",
      "2024-01-01 08:44:16.073665: Current learning rate: 0.0059\n",
      "2024-01-01 08:46:21.672312: train_loss -0.9122\n",
      "2024-01-01 08:46:21.680303: val_loss -0.8303\n",
      "2024-01-01 08:46:21.687302: Pseudo dice [0.928, 0.9434, 0.9433]\n",
      "2024-01-01 08:46:21.693316: Epoch time: 125.61 s\n",
      "2024-01-01 08:46:22.659673: \n",
      "2024-01-01 08:46:22.667673: Epoch 445\n",
      "2024-01-01 08:46:22.674664: Current learning rate: 0.00589\n",
      "2024-01-01 08:48:27.947810: train_loss -0.9136\n",
      "2024-01-01 08:48:27.957814: val_loss -0.7991\n",
      "2024-01-01 08:48:27.966898: Pseudo dice [0.9152, 0.9387, 0.9359]\n",
      "2024-01-01 08:48:27.975897: Epoch time: 125.29 s\n",
      "2024-01-01 08:48:28.967200: \n",
      "2024-01-01 08:48:28.972200: Epoch 446\n",
      "2024-01-01 08:48:28.977200: Current learning rate: 0.00588\n",
      "2024-01-01 08:50:34.270021: train_loss -0.9154\n",
      "2024-01-01 08:50:34.277028: val_loss -0.8111\n",
      "2024-01-01 08:50:34.285029: Pseudo dice [0.9169, 0.9386, 0.9368]\n",
      "2024-01-01 08:50:34.293499: Epoch time: 125.3 s\n",
      "2024-01-01 08:50:35.315855: \n",
      "2024-01-01 08:50:35.321167: Epoch 447\n",
      "2024-01-01 08:50:35.326175: Current learning rate: 0.00587\n",
      "2024-01-01 08:52:40.521004: train_loss -0.9131\n",
      "2024-01-01 08:52:40.530007: val_loss -0.8113\n",
      "2024-01-01 08:52:40.537003: Pseudo dice [0.9185, 0.9408, 0.9375]\n",
      "2024-01-01 08:52:40.544004: Epoch time: 125.21 s\n",
      "2024-01-01 08:52:41.526156: \n",
      "2024-01-01 08:52:41.532604: Epoch 448\n",
      "2024-01-01 08:52:41.537635: Current learning rate: 0.00586\n",
      "2024-01-01 08:54:46.740525: train_loss -0.9147\n",
      "2024-01-01 08:54:46.748525: val_loss -0.8215\n",
      "2024-01-01 08:54:46.757564: Pseudo dice [0.9219, 0.9417, 0.9388]\n",
      "2024-01-01 08:54:46.763560: Epoch time: 125.22 s\n",
      "2024-01-01 08:54:47.909540: \n",
      "2024-01-01 08:54:47.916539: Epoch 449\n",
      "2024-01-01 08:54:47.921540: Current learning rate: 0.00585\n",
      "2024-01-01 08:56:53.431560: train_loss -0.9128\n",
      "2024-01-01 08:56:53.439560: val_loss -0.8037\n",
      "2024-01-01 08:56:53.449560: Pseudo dice [0.9151, 0.9376, 0.9348]\n",
      "2024-01-01 08:56:53.456560: Epoch time: 125.52 s\n",
      "2024-01-01 08:56:54.680853: \n",
      "2024-01-01 08:56:54.686851: Epoch 450\n",
      "2024-01-01 08:56:54.690912: Current learning rate: 0.00584\n",
      "2024-01-01 08:59:00.030747: train_loss -0.9135\n",
      "2024-01-01 08:59:00.039749: val_loss -0.8098\n",
      "2024-01-01 08:59:00.048743: Pseudo dice [0.9201, 0.9414, 0.9387]\n",
      "2024-01-01 08:59:00.057746: Epoch time: 125.35 s\n",
      "2024-01-01 08:59:01.032474: \n",
      "2024-01-01 08:59:01.038510: Epoch 451\n",
      "2024-01-01 08:59:01.046668: Current learning rate: 0.00583\n",
      "2024-01-01 09:01:06.618428: train_loss -0.9094\n",
      "2024-01-01 09:01:06.625443: val_loss -0.804\n",
      "2024-01-01 09:01:06.633437: Pseudo dice [0.9148, 0.9388, 0.936]\n",
      "2024-01-01 09:01:06.639941: Epoch time: 125.59 s\n",
      "2024-01-01 09:01:07.660217: \n",
      "2024-01-01 09:01:07.666321: Epoch 452\n",
      "2024-01-01 09:01:07.671301: Current learning rate: 0.00582\n",
      "2024-01-01 09:03:13.167961: train_loss -0.9102\n",
      "2024-01-01 09:03:13.175954: val_loss -0.8196\n",
      "2024-01-01 09:03:13.181953: Pseudo dice [0.9208, 0.9425, 0.9404]\n",
      "2024-01-01 09:03:13.188461: Epoch time: 125.51 s\n",
      "2024-01-01 09:03:14.168135: \n",
      "2024-01-01 09:03:14.173168: Epoch 453\n",
      "2024-01-01 09:03:14.178171: Current learning rate: 0.00581\n",
      "2024-01-01 09:05:19.262599: train_loss -0.9157\n",
      "2024-01-01 09:05:19.270640: val_loss -0.8107\n",
      "2024-01-01 09:05:19.278647: Pseudo dice [0.9137, 0.9387, 0.9362]\n",
      "2024-01-01 09:05:19.287648: Epoch time: 125.1 s\n",
      "2024-01-01 09:05:20.302792: \n",
      "2024-01-01 09:05:20.308783: Epoch 454\n",
      "2024-01-01 09:05:20.313792: Current learning rate: 0.0058\n",
      "2024-01-01 09:07:25.774664: train_loss -0.9131\n",
      "2024-01-01 09:07:25.782665: val_loss -0.813\n",
      "2024-01-01 09:07:25.790174: Pseudo dice [0.913, 0.9375, 0.9346]\n",
      "2024-01-01 09:07:25.798175: Epoch time: 125.47 s\n",
      "2024-01-01 09:07:26.781818: \n",
      "2024-01-01 09:07:26.787418: Epoch 455\n",
      "2024-01-01 09:07:26.792354: Current learning rate: 0.00579\n",
      "2024-01-01 09:09:32.111455: train_loss -0.9136\n",
      "2024-01-01 09:09:32.121449: val_loss -0.8108\n",
      "2024-01-01 09:09:32.128449: Pseudo dice [0.917, 0.9404, 0.9376]\n",
      "2024-01-01 09:09:32.135954: Epoch time: 125.33 s\n",
      "2024-01-01 09:09:33.117735: \n",
      "2024-01-01 09:09:33.124816: Epoch 456\n",
      "2024-01-01 09:09:33.130741: Current learning rate: 0.00578\n",
      "2024-01-01 09:11:38.271338: train_loss -0.9157\n",
      "2024-01-01 09:11:38.281750: val_loss -0.8031\n",
      "2024-01-01 09:11:38.289262: Pseudo dice [0.9139, 0.9378, 0.9348]\n",
      "2024-01-01 09:11:38.296261: Epoch time: 125.15 s\n",
      "2024-01-01 09:11:39.453701: \n",
      "2024-01-01 09:11:39.460710: Epoch 457\n",
      "2024-01-01 09:11:39.464710: Current learning rate: 0.00577\n",
      "2024-01-01 09:13:45.090145: train_loss -0.9134\n",
      "2024-01-01 09:13:45.098144: val_loss -0.8088\n",
      "2024-01-01 09:13:45.106144: Pseudo dice [0.9144, 0.9389, 0.9354]\n",
      "2024-01-01 09:13:45.113149: Epoch time: 125.64 s\n",
      "2024-01-01 09:13:46.084586: \n",
      "2024-01-01 09:13:46.093653: Epoch 458\n",
      "2024-01-01 09:13:46.099592: Current learning rate: 0.00576\n",
      "2024-01-01 09:15:51.658839: train_loss -0.9108\n",
      "2024-01-01 09:15:51.669358: val_loss -0.8266\n",
      "2024-01-01 09:15:51.677357: Pseudo dice [0.918, 0.9404, 0.9375]\n",
      "2024-01-01 09:15:51.685358: Epoch time: 125.58 s\n",
      "2024-01-01 09:15:52.753439: \n",
      "2024-01-01 09:15:52.758512: Epoch 459\n",
      "2024-01-01 09:15:52.764448: Current learning rate: 0.00575\n",
      "2024-01-01 09:17:58.207214: train_loss -0.9122\n",
      "2024-01-01 09:17:58.215716: val_loss -0.811\n",
      "2024-01-01 09:17:58.223723: Pseudo dice [0.9169, 0.9387, 0.9369]\n",
      "2024-01-01 09:17:58.231721: Epoch time: 125.45 s\n",
      "2024-01-01 09:17:59.203842: \n",
      "2024-01-01 09:17:59.209479: Epoch 460\n",
      "2024-01-01 09:17:59.214487: Current learning rate: 0.00574\n",
      "2024-01-01 09:20:04.674801: train_loss -0.9115\n",
      "2024-01-01 09:20:04.682800: val_loss -0.8139\n",
      "2024-01-01 09:20:04.689762: Pseudo dice [0.917, 0.941, 0.9384]\n",
      "2024-01-01 09:20:04.694762: Epoch time: 125.47 s\n",
      "2024-01-01 09:20:05.702073: \n",
      "2024-01-01 09:20:05.708072: Epoch 461\n",
      "2024-01-01 09:20:05.712425: Current learning rate: 0.00573\n",
      "2024-01-01 09:22:10.997190: train_loss -0.9155\n",
      "2024-01-01 09:22:11.006193: val_loss -0.7995\n",
      "2024-01-01 09:22:11.014710: Pseudo dice [0.9149, 0.9387, 0.9352]\n",
      "2024-01-01 09:22:11.021716: Epoch time: 125.3 s\n",
      "2024-01-01 09:22:12.008475: \n",
      "2024-01-01 09:22:12.017206: Epoch 462\n",
      "2024-01-01 09:22:12.021191: Current learning rate: 0.00572\n",
      "2024-01-01 09:24:17.264517: train_loss -0.9162\n",
      "2024-01-01 09:24:17.272520: val_loss -0.814\n",
      "2024-01-01 09:24:17.280520: Pseudo dice [0.9122, 0.9389, 0.9357]\n",
      "2024-01-01 09:24:17.286518: Epoch time: 125.26 s\n",
      "2024-01-01 09:24:18.260975: \n",
      "2024-01-01 09:24:18.269057: Epoch 463\n",
      "2024-01-01 09:24:18.274971: Current learning rate: 0.00571\n",
      "2024-01-01 09:26:23.529833: train_loss -0.9163\n",
      "2024-01-01 09:26:23.538833: val_loss -0.8091\n",
      "2024-01-01 09:26:23.545835: Pseudo dice [0.9147, 0.9387, 0.9362]\n",
      "2024-01-01 09:26:23.554833: Epoch time: 125.27 s\n",
      "2024-01-01 09:26:24.538254: \n",
      "2024-01-01 09:26:24.543255: Epoch 464\n",
      "2024-01-01 09:26:24.548255: Current learning rate: 0.0057\n",
      "2024-01-01 09:28:30.155750: train_loss -0.911\n",
      "2024-01-01 09:28:30.166750: val_loss -0.816\n",
      "2024-01-01 09:28:30.176750: Pseudo dice [0.916, 0.9396, 0.9369]\n",
      "2024-01-01 09:28:30.184750: Epoch time: 125.62 s\n",
      "2024-01-01 09:28:31.349159: \n",
      "2024-01-01 09:28:31.358681: Epoch 465\n",
      "2024-01-01 09:28:31.364736: Current learning rate: 0.0057\n",
      "2024-01-01 09:30:36.711912: train_loss -0.9168\n",
      "2024-01-01 09:30:36.721933: val_loss -0.802\n",
      "2024-01-01 09:30:36.729921: Pseudo dice [0.9147, 0.9384, 0.9366]\n",
      "2024-01-01 09:30:36.738425: Epoch time: 125.36 s\n",
      "2024-01-01 09:30:37.724648: \n",
      "2024-01-01 09:30:37.732643: Epoch 466\n",
      "2024-01-01 09:30:37.737142: Current learning rate: 0.00569\n",
      "2024-01-01 09:32:43.087649: train_loss -0.9145\n",
      "2024-01-01 09:32:43.097649: val_loss -0.8041\n",
      "2024-01-01 09:32:43.105648: Pseudo dice [0.9161, 0.9403, 0.9379]\n",
      "2024-01-01 09:32:43.112649: Epoch time: 125.37 s\n",
      "2024-01-01 09:32:44.092842: \n",
      "2024-01-01 09:32:44.099842: Epoch 467\n",
      "2024-01-01 09:32:44.105041: Current learning rate: 0.00568\n",
      "2024-01-01 09:34:49.527275: train_loss -0.9139\n",
      "2024-01-01 09:34:49.536726: val_loss -0.8114\n",
      "2024-01-01 09:34:49.545732: Pseudo dice [0.9197, 0.9408, 0.9387]\n",
      "2024-01-01 09:34:49.552737: Epoch time: 125.44 s\n",
      "2024-01-01 09:34:50.568998: \n",
      "2024-01-01 09:34:50.573998: Epoch 468\n",
      "2024-01-01 09:34:50.578990: Current learning rate: 0.00567\n",
      "2024-01-01 09:36:55.912971: train_loss -0.9161\n",
      "2024-01-01 09:36:55.920968: val_loss -0.8129\n",
      "2024-01-01 09:36:55.929988: Pseudo dice [0.9181, 0.9412, 0.9389]\n",
      "2024-01-01 09:36:55.936993: Epoch time: 125.34 s\n",
      "2024-01-01 09:36:56.954156: \n",
      "2024-01-01 09:36:56.960150: Epoch 469\n",
      "2024-01-01 09:36:56.964669: Current learning rate: 0.00566\n",
      "2024-01-01 09:39:02.798471: train_loss -0.9093\n",
      "2024-01-01 09:39:02.807467: val_loss -0.8154\n",
      "2024-01-01 09:39:02.815740: Pseudo dice [0.9216, 0.9412, 0.9398]\n",
      "2024-01-01 09:39:02.821745: Epoch time: 125.85 s\n",
      "2024-01-01 09:39:03.797138: \n",
      "2024-01-01 09:39:03.804146: Epoch 470\n",
      "2024-01-01 09:39:03.809138: Current learning rate: 0.00565\n",
      "2024-01-01 09:41:09.206465: train_loss -0.9131\n",
      "2024-01-01 09:41:09.215466: val_loss -0.8144\n",
      "2024-01-01 09:41:09.224463: Pseudo dice [0.9155, 0.9392, 0.936]\n",
      "2024-01-01 09:41:09.232464: Epoch time: 125.41 s\n",
      "2024-01-01 09:41:10.207465: \n",
      "2024-01-01 09:41:10.218186: Epoch 471\n",
      "2024-01-01 09:41:10.223185: Current learning rate: 0.00564\n",
      "2024-01-01 09:43:15.744243: train_loss -0.9126\n",
      "2024-01-01 09:43:15.754243: val_loss -0.8055\n",
      "2024-01-01 09:43:15.762243: Pseudo dice [0.918, 0.9396, 0.9367]\n",
      "2024-01-01 09:43:15.772297: Epoch time: 125.54 s\n",
      "2024-01-01 09:43:16.795867: \n",
      "2024-01-01 09:43:16.801868: Epoch 472\n",
      "2024-01-01 09:43:16.806863: Current learning rate: 0.00563\n",
      "2024-01-01 09:45:21.918308: train_loss -0.9175\n",
      "2024-01-01 09:45:21.926311: val_loss -0.821\n",
      "2024-01-01 09:45:21.932316: Pseudo dice [0.9216, 0.9424, 0.9417]\n",
      "2024-01-01 09:45:21.937316: Epoch time: 125.12 s\n",
      "2024-01-01 09:45:22.922208: \n",
      "2024-01-01 09:45:22.928265: Epoch 473\n",
      "2024-01-01 09:45:22.932263: Current learning rate: 0.00562\n",
      "2024-01-01 09:47:28.414354: train_loss -0.9139\n",
      "2024-01-01 09:47:28.423355: val_loss -0.8165\n",
      "2024-01-01 09:47:28.432356: Pseudo dice [0.9185, 0.9409, 0.9401]\n",
      "2024-01-01 09:47:28.441365: Epoch time: 125.49 s\n",
      "2024-01-01 09:47:29.638706: \n",
      "2024-01-01 09:47:29.645707: Epoch 474\n",
      "2024-01-01 09:47:29.652459: Current learning rate: 0.00561\n",
      "2024-01-01 09:49:35.239706: train_loss -0.9112\n",
      "2024-01-01 09:49:35.249703: val_loss -0.8193\n",
      "2024-01-01 09:49:35.258703: Pseudo dice [0.921, 0.9423, 0.9407]\n",
      "2024-01-01 09:49:35.268705: Epoch time: 125.6 s\n",
      "2024-01-01 09:49:36.304977: \n",
      "2024-01-01 09:49:36.313986: Epoch 475\n",
      "2024-01-01 09:49:36.317989: Current learning rate: 0.0056\n",
      "2024-01-01 09:51:41.966450: train_loss -0.913\n",
      "2024-01-01 09:51:41.978450: val_loss -0.8168\n",
      "2024-01-01 09:51:41.987449: Pseudo dice [0.9142, 0.9389, 0.9365]\n",
      "2024-01-01 09:51:41.995455: Epoch time: 125.66 s\n",
      "2024-01-01 09:51:43.257558: \n",
      "2024-01-01 09:51:43.263552: Epoch 476\n",
      "2024-01-01 09:51:43.269551: Current learning rate: 0.00559\n",
      "2024-01-01 09:53:48.671110: train_loss -0.913\n",
      "2024-01-01 09:53:48.678256: val_loss -0.802\n",
      "2024-01-01 09:53:48.686256: Pseudo dice [0.9167, 0.9398, 0.9382]\n",
      "2024-01-01 09:53:48.694255: Epoch time: 125.41 s\n",
      "2024-01-01 09:53:49.667395: \n",
      "2024-01-01 09:53:49.673563: Epoch 477\n",
      "2024-01-01 09:53:49.677555: Current learning rate: 0.00558\n",
      "2024-01-01 09:55:54.984182: train_loss -0.9138\n",
      "2024-01-01 09:55:54.994184: val_loss -0.8148\n",
      "2024-01-01 09:55:55.004183: Pseudo dice [0.9144, 0.9379, 0.9346]\n",
      "2024-01-01 09:55:55.011184: Epoch time: 125.32 s\n",
      "2024-01-01 09:55:56.017157: \n",
      "2024-01-01 09:55:56.024158: Epoch 478\n",
      "2024-01-01 09:55:56.031167: Current learning rate: 0.00557\n",
      "2024-01-01 09:58:01.680210: train_loss -0.9128\n",
      "2024-01-01 09:58:01.688214: val_loss -0.8033\n",
      "2024-01-01 09:58:01.695214: Pseudo dice [0.9154, 0.9393, 0.9372]\n",
      "2024-01-01 09:58:01.701719: Epoch time: 125.67 s\n",
      "2024-01-01 09:58:02.751783: \n",
      "2024-01-01 09:58:02.758802: Epoch 479\n",
      "2024-01-01 09:58:02.763792: Current learning rate: 0.00556\n",
      "2024-01-01 10:00:08.180080: train_loss -0.9142\n",
      "2024-01-01 10:00:08.187080: val_loss -0.8064\n",
      "2024-01-01 10:00:08.193080: Pseudo dice [0.9161, 0.939, 0.9365]\n",
      "2024-01-01 10:00:08.200080: Epoch time: 125.43 s\n",
      "2024-01-01 10:00:09.224128: \n",
      "2024-01-01 10:00:09.230173: Epoch 480\n",
      "2024-01-01 10:00:09.234275: Current learning rate: 0.00555\n",
      "2024-01-01 10:02:14.540259: train_loss -0.9159\n",
      "2024-01-01 10:02:14.549258: val_loss -0.8149\n",
      "2024-01-01 10:02:14.556258: Pseudo dice [0.9193, 0.94, 0.9364]\n",
      "2024-01-01 10:02:14.565258: Epoch time: 125.32 s\n",
      "2024-01-01 10:02:15.560371: \n",
      "2024-01-01 10:02:15.567440: Epoch 481\n",
      "2024-01-01 10:02:15.572380: Current learning rate: 0.00554\n",
      "2024-01-01 10:04:20.971300: train_loss -0.9127\n",
      "2024-01-01 10:04:20.980807: val_loss -0.8037\n",
      "2024-01-01 10:04:20.988807: Pseudo dice [0.9192, 0.9413, 0.9404]\n",
      "2024-01-01 10:04:20.994807: Epoch time: 125.41 s\n",
      "2024-01-01 10:04:21.996336: \n",
      "2024-01-01 10:04:22.002335: Epoch 482\n",
      "2024-01-01 10:04:22.006418: Current learning rate: 0.00553\n",
      "2024-01-01 10:06:27.192872: train_loss -0.9129\n",
      "2024-01-01 10:06:27.201874: val_loss -0.8208\n",
      "2024-01-01 10:06:27.209872: Pseudo dice [0.9216, 0.9398, 0.9361]\n",
      "2024-01-01 10:06:27.216874: Epoch time: 125.2 s\n",
      "2024-01-01 10:06:28.248883: \n",
      "2024-01-01 10:06:28.254810: Epoch 483\n",
      "2024-01-01 10:06:28.258816: Current learning rate: 0.00552\n",
      "2024-01-01 10:08:33.799614: train_loss -0.9143\n",
      "2024-01-01 10:08:33.809617: val_loss -0.8256\n",
      "2024-01-01 10:08:33.818128: Pseudo dice [0.9231, 0.9427, 0.9414]\n",
      "2024-01-01 10:08:33.824716: Epoch time: 125.55 s\n",
      "2024-01-01 10:08:34.817160: \n",
      "2024-01-01 10:08:34.823190: Epoch 484\n",
      "2024-01-01 10:08:34.827220: Current learning rate: 0.00551\n",
      "2024-01-01 10:10:40.292770: train_loss -0.9141\n",
      "2024-01-01 10:10:40.301768: val_loss -0.8202\n",
      "2024-01-01 10:10:40.308768: Pseudo dice [0.9185, 0.9403, 0.9377]\n",
      "2024-01-01 10:10:40.314768: Epoch time: 125.48 s\n",
      "2024-01-01 10:10:41.298338: \n",
      "2024-01-01 10:10:41.303329: Epoch 485\n",
      "2024-01-01 10:10:41.308333: Current learning rate: 0.0055\n",
      "2024-01-01 10:12:46.697220: train_loss -0.9149\n",
      "2024-01-01 10:12:46.704221: val_loss -0.8087\n",
      "2024-01-01 10:12:46.712221: Pseudo dice [0.9147, 0.9403, 0.9379]\n",
      "2024-01-01 10:12:46.718219: Epoch time: 125.4 s\n",
      "2024-01-01 10:12:47.707312: \n",
      "2024-01-01 10:12:47.714311: Epoch 486\n",
      "2024-01-01 10:12:47.718314: Current learning rate: 0.00549\n",
      "2024-01-01 10:14:52.901140: train_loss -0.9133\n",
      "2024-01-01 10:14:52.910139: val_loss -0.8159\n",
      "2024-01-01 10:14:52.919139: Pseudo dice [0.9198, 0.9394, 0.939]\n",
      "2024-01-01 10:14:52.925139: Epoch time: 125.2 s\n",
      "2024-01-01 10:14:53.937588: \n",
      "2024-01-01 10:14:53.943681: Epoch 487\n",
      "2024-01-01 10:14:53.948594: Current learning rate: 0.00548\n",
      "2024-01-01 10:16:59.385664: train_loss -0.9157\n",
      "2024-01-01 10:16:59.393666: val_loss -0.8248\n",
      "2024-01-01 10:16:59.402668: Pseudo dice [0.9198, 0.9438, 0.9418]\n",
      "2024-01-01 10:16:59.411665: Epoch time: 125.45 s\n",
      "2024-01-01 10:17:00.408004: \n",
      "2024-01-01 10:17:00.416004: Epoch 488\n",
      "2024-01-01 10:17:00.421004: Current learning rate: 0.00547\n",
      "2024-01-01 10:19:05.828650: train_loss -0.9158\n",
      "2024-01-01 10:19:05.837861: val_loss -0.8159\n",
      "2024-01-01 10:19:05.843862: Pseudo dice [0.9205, 0.9418, 0.9391]\n",
      "2024-01-01 10:19:05.851871: Epoch time: 125.42 s\n",
      "2024-01-01 10:19:07.026086: \n",
      "2024-01-01 10:19:07.031024: Epoch 489\n",
      "2024-01-01 10:19:07.036015: Current learning rate: 0.00546\n",
      "2024-01-01 10:21:12.560089: train_loss -0.9109\n",
      "2024-01-01 10:21:12.569160: val_loss -0.8147\n",
      "2024-01-01 10:21:12.577159: Pseudo dice [0.9204, 0.9415, 0.9402]\n",
      "2024-01-01 10:21:12.585345: Epoch time: 125.54 s\n",
      "2024-01-01 10:21:13.585193: \n",
      "2024-01-01 10:21:13.590192: Epoch 490\n",
      "2024-01-01 10:21:13.595185: Current learning rate: 0.00546\n",
      "2024-01-01 10:23:19.069570: train_loss -0.9148\n",
      "2024-01-01 10:23:19.077587: val_loss -0.8267\n",
      "2024-01-01 10:23:19.086086: Pseudo dice [0.9255, 0.943, 0.9402]\n",
      "2024-01-01 10:23:19.094086: Epoch time: 125.49 s\n",
      "2024-01-01 10:23:20.121710: \n",
      "2024-01-01 10:23:20.127709: Epoch 491\n",
      "2024-01-01 10:23:20.134728: Current learning rate: 0.00545\n",
      "2024-01-01 10:25:25.778608: train_loss -0.9119\n",
      "2024-01-01 10:25:25.786607: val_loss -0.8175\n",
      "2024-01-01 10:25:25.794966: Pseudo dice [0.9226, 0.9416, 0.9393]\n",
      "2024-01-01 10:25:25.801960: Epoch time: 125.66 s\n",
      "2024-01-01 10:25:26.836254: \n",
      "2024-01-01 10:25:26.842381: Epoch 492\n",
      "2024-01-01 10:25:26.846456: Current learning rate: 0.00544\n",
      "2024-01-01 10:27:32.349491: train_loss -0.9157\n",
      "2024-01-01 10:27:32.357491: val_loss -0.8146\n",
      "2024-01-01 10:27:32.364495: Pseudo dice [0.917, 0.9409, 0.9389]\n",
      "2024-01-01 10:27:32.370494: Epoch time: 125.51 s\n",
      "2024-01-01 10:27:33.392762: \n",
      "2024-01-01 10:27:33.399761: Epoch 493\n",
      "2024-01-01 10:27:33.407767: Current learning rate: 0.00543\n",
      "2024-01-01 10:29:38.878151: train_loss -0.9141\n",
      "2024-01-01 10:29:38.887662: val_loss -0.8081\n",
      "2024-01-01 10:29:38.895660: Pseudo dice [0.9187, 0.9411, 0.9395]\n",
      "2024-01-01 10:29:38.903174: Epoch time: 125.49 s\n",
      "2024-01-01 10:29:39.896798: \n",
      "2024-01-01 10:29:39.902798: Epoch 494\n",
      "2024-01-01 10:29:39.907833: Current learning rate: 0.00542\n",
      "2024-01-01 10:31:45.582627: train_loss -0.9145\n",
      "2024-01-01 10:31:45.592628: val_loss -0.8048\n",
      "2024-01-01 10:31:45.601630: Pseudo dice [0.9167, 0.9373, 0.9363]\n",
      "2024-01-01 10:31:45.610628: Epoch time: 125.69 s\n",
      "2024-01-01 10:31:46.643681: \n",
      "2024-01-01 10:31:46.649054: Epoch 495\n",
      "2024-01-01 10:31:46.654069: Current learning rate: 0.00541\n",
      "2024-01-01 10:33:52.145376: train_loss -0.9111\n",
      "2024-01-01 10:33:52.155880: val_loss -0.8211\n",
      "2024-01-01 10:33:52.163880: Pseudo dice [0.9171, 0.94, 0.9379]\n",
      "2024-01-01 10:33:52.170880: Epoch time: 125.5 s\n",
      "2024-01-01 10:33:53.167469: \n",
      "2024-01-01 10:33:53.173458: Epoch 496\n",
      "2024-01-01 10:33:53.177466: Current learning rate: 0.0054\n",
      "2024-01-01 10:35:58.667667: train_loss -0.9122\n",
      "2024-01-01 10:35:58.676671: val_loss -0.8167\n",
      "2024-01-01 10:35:58.683666: Pseudo dice [0.9157, 0.94, 0.9373]\n",
      "2024-01-01 10:35:58.690676: Epoch time: 125.5 s\n",
      "2024-01-01 10:35:59.852671: \n",
      "2024-01-01 10:35:59.859856: Epoch 497\n",
      "2024-01-01 10:35:59.863801: Current learning rate: 0.00539\n",
      "2024-01-01 10:38:05.123450: train_loss -0.9145\n",
      "2024-01-01 10:38:05.132453: val_loss -0.8104\n",
      "2024-01-01 10:38:05.141983: Pseudo dice [0.9186, 0.9411, 0.938]\n",
      "2024-01-01 10:38:05.148985: Epoch time: 125.27 s\n",
      "2024-01-01 10:38:06.164363: \n",
      "2024-01-01 10:38:06.169358: Epoch 498\n",
      "2024-01-01 10:38:06.174363: Current learning rate: 0.00538\n",
      "2024-01-01 10:40:11.563156: train_loss -0.9142\n",
      "2024-01-01 10:40:11.572156: val_loss -0.8151\n",
      "2024-01-01 10:40:11.579157: Pseudo dice [0.918, 0.9426, 0.9409]\n",
      "2024-01-01 10:40:11.585163: Epoch time: 125.4 s\n",
      "2024-01-01 10:40:12.575348: \n",
      "2024-01-01 10:40:12.581336: Epoch 499\n",
      "2024-01-01 10:40:12.585338: Current learning rate: 0.00537\n",
      "2024-01-01 10:42:18.001603: train_loss -0.9133\n",
      "2024-01-01 10:42:18.008602: val_loss -0.8139\n",
      "2024-01-01 10:42:18.016604: Pseudo dice [0.9193, 0.9395, 0.9361]\n",
      "2024-01-01 10:42:18.022604: Epoch time: 125.43 s\n",
      "2024-01-01 10:42:19.294048: \n",
      "2024-01-01 10:42:19.300039: Epoch 500\n",
      "2024-01-01 10:42:19.305048: Current learning rate: 0.00536\n",
      "2024-01-01 10:44:24.849645: train_loss -0.9142\n",
      "2024-01-01 10:44:24.857645: val_loss -0.8056\n",
      "2024-01-01 10:44:24.864646: Pseudo dice [0.9118, 0.9385, 0.9357]\n",
      "2024-01-01 10:44:24.871644: Epoch time: 125.56 s\n",
      "2024-01-01 10:44:25.875472: \n",
      "2024-01-01 10:44:25.880464: Epoch 501\n",
      "2024-01-01 10:44:25.885463: Current learning rate: 0.00535\n",
      "2024-01-01 10:46:31.425606: train_loss -0.9137\n",
      "2024-01-01 10:46:31.435771: val_loss -0.8032\n",
      "2024-01-01 10:46:31.443769: Pseudo dice [0.9154, 0.9395, 0.9371]\n",
      "2024-01-01 10:46:31.449769: Epoch time: 125.55 s\n",
      "2024-01-01 10:46:32.455775: \n",
      "2024-01-01 10:46:32.462765: Epoch 502\n",
      "2024-01-01 10:46:32.467773: Current learning rate: 0.00534\n",
      "2024-01-01 10:48:38.064220: train_loss -0.9163\n",
      "2024-01-01 10:48:38.072215: val_loss -0.8097\n",
      "2024-01-01 10:48:38.079215: Pseudo dice [0.918, 0.9413, 0.9393]\n",
      "2024-01-01 10:48:38.084218: Epoch time: 125.61 s\n",
      "2024-01-01 10:48:39.084280: \n",
      "2024-01-01 10:48:39.090277: Epoch 503\n",
      "2024-01-01 10:48:39.094783: Current learning rate: 0.00533\n",
      "2024-01-01 10:50:44.521378: train_loss -0.9146\n",
      "2024-01-01 10:50:44.529378: val_loss -0.8032\n",
      "2024-01-01 10:50:44.537378: Pseudo dice [0.9158, 0.9386, 0.9387]\n",
      "2024-01-01 10:50:44.545381: Epoch time: 125.44 s\n",
      "2024-01-01 10:50:45.544654: \n",
      "2024-01-01 10:50:45.549654: Epoch 504\n",
      "2024-01-01 10:50:45.554649: Current learning rate: 0.00532\n",
      "2024-01-01 10:52:51.049413: train_loss -0.9133\n",
      "2024-01-01 10:52:51.057685: val_loss -0.8296\n",
      "2024-01-01 10:52:51.065181: Pseudo dice [0.9221, 0.9435, 0.9411]\n",
      "2024-01-01 10:52:51.070182: Epoch time: 125.51 s\n",
      "2024-01-01 10:52:52.242409: \n",
      "2024-01-01 10:52:52.247409: Epoch 505\n",
      "2024-01-01 10:52:52.252419: Current learning rate: 0.00531\n",
      "2024-01-01 10:54:57.636205: train_loss -0.9163\n",
      "2024-01-01 10:54:57.646209: val_loss -0.8078\n",
      "2024-01-01 10:54:57.653444: Pseudo dice [0.9192, 0.9407, 0.9397]\n",
      "2024-01-01 10:54:57.660439: Epoch time: 125.39 s\n",
      "2024-01-01 10:54:58.666303: \n",
      "2024-01-01 10:54:58.672295: Epoch 506\n",
      "2024-01-01 10:54:58.676303: Current learning rate: 0.0053\n",
      "2024-01-01 10:57:03.905674: train_loss -0.9135\n",
      "2024-01-01 10:57:03.914662: val_loss -0.8358\n",
      "2024-01-01 10:57:03.921679: Pseudo dice [0.9271, 0.9446, 0.9421]\n",
      "2024-01-01 10:57:03.927690: Epoch time: 125.24 s\n",
      "2024-01-01 10:57:04.940067: \n",
      "2024-01-01 10:57:04.946061: Epoch 507\n",
      "2024-01-01 10:57:04.950082: Current learning rate: 0.00529\n",
      "2024-01-01 10:59:10.410204: train_loss -0.9097\n",
      "2024-01-01 10:59:10.422716: val_loss -0.812\n",
      "2024-01-01 10:59:10.431716: Pseudo dice [0.9175, 0.9398, 0.9368]\n",
      "2024-01-01 10:59:10.440858: Epoch time: 125.47 s\n",
      "2024-01-01 10:59:11.451835: \n",
      "2024-01-01 10:59:11.458835: Epoch 508\n",
      "2024-01-01 10:59:11.464836: Current learning rate: 0.00528\n",
      "2024-01-01 11:01:16.554911: train_loss -0.9096\n",
      "2024-01-01 11:01:16.563908: val_loss -0.8183\n",
      "2024-01-01 11:01:16.571908: Pseudo dice [0.9165, 0.9389, 0.937]\n",
      "2024-01-01 11:01:16.578909: Epoch time: 125.1 s\n",
      "2024-01-01 11:01:17.592806: \n",
      "2024-01-01 11:01:17.597806: Epoch 509\n",
      "2024-01-01 11:01:17.602807: Current learning rate: 0.00527\n",
      "2024-01-01 11:03:22.893455: train_loss -0.912\n",
      "2024-01-01 11:03:22.900453: val_loss -0.8074\n",
      "2024-01-01 11:03:22.909454: Pseudo dice [0.9139, 0.9379, 0.9347]\n",
      "2024-01-01 11:03:22.917449: Epoch time: 125.3 s\n",
      "2024-01-01 11:03:23.922485: \n",
      "2024-01-01 11:03:23.927485: Epoch 510\n",
      "2024-01-01 11:03:23.932485: Current learning rate: 0.00526\n",
      "2024-01-01 11:05:29.138526: train_loss -0.914\n",
      "2024-01-01 11:05:29.146607: val_loss -0.8088\n",
      "2024-01-01 11:05:29.154606: Pseudo dice [0.9171, 0.9398, 0.9386]\n",
      "2024-01-01 11:05:29.161606: Epoch time: 125.22 s\n",
      "2024-01-01 11:05:30.170219: \n",
      "2024-01-01 11:05:30.175753: Epoch 511\n",
      "2024-01-01 11:05:30.180753: Current learning rate: 0.00525\n",
      "2024-01-01 11:07:35.426853: train_loss -0.915\n",
      "2024-01-01 11:07:35.435853: val_loss -0.8182\n",
      "2024-01-01 11:07:35.442857: Pseudo dice [0.916, 0.9394, 0.9358]\n",
      "2024-01-01 11:07:35.449859: Epoch time: 125.26 s\n",
      "2024-01-01 11:07:36.626184: \n",
      "2024-01-01 11:07:36.632184: Epoch 512\n",
      "2024-01-01 11:07:36.637179: Current learning rate: 0.00524\n",
      "2024-01-01 11:09:41.694624: train_loss -0.9039\n",
      "2024-01-01 11:09:41.703626: val_loss -0.8215\n",
      "2024-01-01 11:09:41.711630: Pseudo dice [0.9204, 0.9408, 0.9384]\n",
      "2024-01-01 11:09:41.717693: Epoch time: 125.07 s\n",
      "2024-01-01 11:09:42.715877: \n",
      "2024-01-01 11:09:42.721869: Epoch 513\n",
      "2024-01-01 11:09:42.725877: Current learning rate: 0.00523\n",
      "2024-01-01 11:11:48.009439: train_loss -0.9008\n",
      "2024-01-01 11:11:48.019443: val_loss -0.8156\n",
      "2024-01-01 11:11:48.028454: Pseudo dice [0.9125, 0.9389, 0.9351]\n",
      "2024-01-01 11:11:48.035973: Epoch time: 125.29 s\n",
      "2024-01-01 11:11:49.123861: \n",
      "2024-01-01 11:11:49.129369: Epoch 514\n",
      "2024-01-01 11:11:49.133387: Current learning rate: 0.00522\n",
      "2024-01-01 11:13:54.065182: train_loss -0.9096\n",
      "2024-01-01 11:13:54.075178: val_loss -0.8016\n",
      "2024-01-01 11:13:54.083177: Pseudo dice [0.912, 0.9374, 0.9345]\n",
      "2024-01-01 11:13:54.088172: Epoch time: 124.94 s\n",
      "2024-01-01 11:13:55.086687: \n",
      "2024-01-01 11:13:55.092678: Epoch 515\n",
      "2024-01-01 11:13:55.096687: Current learning rate: 0.00521\n",
      "2024-01-01 11:16:00.517444: train_loss -0.9116\n",
      "2024-01-01 11:16:00.524432: val_loss -0.8085\n",
      "2024-01-01 11:16:00.532443: Pseudo dice [0.9166, 0.9388, 0.9364]\n",
      "2024-01-01 11:16:00.542430: Epoch time: 125.43 s\n",
      "2024-01-01 11:16:01.543931: \n",
      "2024-01-01 11:16:01.548927: Epoch 516\n",
      "2024-01-01 11:16:01.553932: Current learning rate: 0.0052\n",
      "2024-01-01 11:18:06.718364: train_loss -0.9131\n",
      "2024-01-01 11:18:06.725371: val_loss -0.8024\n",
      "2024-01-01 11:18:06.732361: Pseudo dice [0.9142, 0.9384, 0.9361]\n",
      "2024-01-01 11:18:06.738361: Epoch time: 125.18 s\n",
      "2024-01-01 11:18:07.769740: \n",
      "2024-01-01 11:18:07.775288: Epoch 517\n",
      "2024-01-01 11:18:07.780287: Current learning rate: 0.00519\n",
      "2024-01-01 11:20:12.829372: train_loss -0.9146\n",
      "2024-01-01 11:20:12.841370: val_loss -0.8177\n",
      "2024-01-01 11:20:12.849372: Pseudo dice [0.9173, 0.9381, 0.9356]\n",
      "2024-01-01 11:20:12.855380: Epoch time: 125.06 s\n",
      "2024-01-01 11:20:13.862204: \n",
      "2024-01-01 11:20:13.868206: Epoch 518\n",
      "2024-01-01 11:20:13.873195: Current learning rate: 0.00518\n",
      "2024-01-01 11:22:19.129477: train_loss -0.9145\n",
      "2024-01-01 11:22:19.139477: val_loss -0.817\n",
      "2024-01-01 11:22:19.147477: Pseudo dice [0.9155, 0.9393, 0.9362]\n",
      "2024-01-01 11:22:19.155477: Epoch time: 125.27 s\n",
      "2024-01-01 11:22:20.143064: \n",
      "2024-01-01 11:22:20.149052: Epoch 519\n",
      "2024-01-01 11:22:20.153053: Current learning rate: 0.00518\n",
      "2024-01-01 11:24:25.283937: train_loss -0.9149\n",
      "2024-01-01 11:24:25.290935: val_loss -0.7995\n",
      "2024-01-01 11:24:25.298004: Pseudo dice [0.9139, 0.937, 0.9342]\n",
      "2024-01-01 11:24:25.305008: Epoch time: 125.14 s\n",
      "2024-01-01 11:24:26.483618: \n",
      "2024-01-01 11:24:26.488616: Epoch 520\n",
      "2024-01-01 11:24:26.493608: Current learning rate: 0.00517\n",
      "2024-01-01 11:26:31.973968: train_loss -0.9146\n",
      "2024-01-01 11:26:31.982959: val_loss -0.8195\n",
      "2024-01-01 11:26:31.990960: Pseudo dice [0.9175, 0.9414, 0.9392]\n",
      "2024-01-01 11:26:31.998958: Epoch time: 125.49 s\n",
      "2024-01-01 11:26:33.010238: \n",
      "2024-01-01 11:26:33.016233: Epoch 521\n",
      "2024-01-01 11:26:33.020233: Current learning rate: 0.00516\n",
      "2024-01-01 11:28:38.497922: train_loss -0.9122\n",
      "2024-01-01 11:28:38.505922: val_loss -0.8113\n",
      "2024-01-01 11:28:38.513927: Pseudo dice [0.9173, 0.9412, 0.9388]\n",
      "2024-01-01 11:28:38.522926: Epoch time: 125.49 s\n",
      "2024-01-01 11:28:39.524370: \n",
      "2024-01-01 11:28:39.529903: Epoch 522\n",
      "2024-01-01 11:28:39.533906: Current learning rate: 0.00515\n",
      "2024-01-01 11:30:45.332991: train_loss -0.9139\n",
      "2024-01-01 11:30:45.341497: val_loss -0.8083\n",
      "2024-01-01 11:30:45.349500: Pseudo dice [0.9183, 0.9392, 0.9369]\n",
      "2024-01-01 11:30:45.357497: Epoch time: 125.81 s\n",
      "2024-01-01 11:30:46.453840: \n",
      "2024-01-01 11:30:46.459362: Epoch 523\n",
      "2024-01-01 11:30:46.463362: Current learning rate: 0.00514\n",
      "2024-01-01 11:32:52.127194: train_loss -0.9109\n",
      "2024-01-01 11:32:52.136266: val_loss -0.8008\n",
      "2024-01-01 11:32:52.143272: Pseudo dice [0.9138, 0.9407, 0.9376]\n",
      "2024-01-01 11:32:52.161307: Epoch time: 125.67 s\n",
      "2024-01-01 11:32:53.164390: \n",
      "2024-01-01 11:32:53.169390: Epoch 524\n",
      "2024-01-01 11:32:53.174454: Current learning rate: 0.00513\n",
      "2024-01-01 11:34:58.235906: train_loss -0.9169\n",
      "2024-01-01 11:34:58.244018: val_loss -0.8188\n",
      "2024-01-01 11:34:58.250034: Pseudo dice [0.9209, 0.9412, 0.9395]\n",
      "2024-01-01 11:34:58.257032: Epoch time: 125.07 s\n",
      "2024-01-01 11:34:59.245912: \n",
      "2024-01-01 11:34:59.252064: Epoch 525\n",
      "2024-01-01 11:34:59.256984: Current learning rate: 0.00512\n",
      "2024-01-01 11:37:04.325870: train_loss -0.9145\n",
      "2024-01-01 11:37:04.332869: val_loss -0.8293\n",
      "2024-01-01 11:37:04.340869: Pseudo dice [0.9294, 0.9455, 0.9438]\n",
      "2024-01-01 11:37:04.346869: Epoch time: 125.08 s\n",
      "2024-01-01 11:37:05.349926: \n",
      "2024-01-01 11:37:05.356945: Epoch 526\n",
      "2024-01-01 11:37:05.362935: Current learning rate: 0.00511\n",
      "2024-01-01 11:39:10.415344: train_loss -0.9177\n",
      "2024-01-01 11:39:10.423344: val_loss -0.8307\n",
      "2024-01-01 11:39:10.431344: Pseudo dice [0.9257, 0.9441, 0.9447]\n",
      "2024-01-01 11:39:10.438344: Epoch time: 125.07 s\n",
      "2024-01-01 11:39:11.440793: \n",
      "2024-01-01 11:39:11.445793: Epoch 527\n",
      "2024-01-01 11:39:11.450794: Current learning rate: 0.0051\n",
      "2024-01-01 11:41:16.584558: train_loss -0.915\n",
      "2024-01-01 11:41:16.593557: val_loss -0.8378\n",
      "2024-01-01 11:41:16.601557: Pseudo dice [0.9274, 0.9462, 0.9446]\n",
      "2024-01-01 11:41:16.609560: Epoch time: 125.14 s\n",
      "2024-01-01 11:41:17.814562: \n",
      "2024-01-01 11:41:17.819563: Epoch 528\n",
      "2024-01-01 11:41:17.824560: Current learning rate: 0.00509\n",
      "2024-01-01 11:43:23.203350: train_loss -0.9167\n",
      "2024-01-01 11:43:23.212342: val_loss -0.8256\n",
      "2024-01-01 11:43:23.220342: Pseudo dice [0.9272, 0.945, 0.9451]\n",
      "2024-01-01 11:43:23.228342: Epoch time: 125.39 s\n",
      "2024-01-01 11:43:24.221412: \n",
      "2024-01-01 11:43:24.227457: Epoch 529\n",
      "2024-01-01 11:43:24.231414: Current learning rate: 0.00508\n",
      "2024-01-01 11:45:29.691709: train_loss -0.9143\n",
      "2024-01-01 11:45:29.699710: val_loss -0.8347\n",
      "2024-01-01 11:45:29.708208: Pseudo dice [0.927, 0.9456, 0.9449]\n",
      "2024-01-01 11:45:29.716213: Epoch time: 125.47 s\n",
      "2024-01-01 11:45:30.743032: \n",
      "2024-01-01 11:45:30.748034: Epoch 530\n",
      "2024-01-01 11:45:30.752101: Current learning rate: 0.00507\n",
      "2024-01-01 11:47:35.957765: train_loss -0.9165\n",
      "2024-01-01 11:47:35.969771: val_loss -0.8395\n",
      "2024-01-01 11:47:35.979550: Pseudo dice [0.929, 0.9453, 0.9445]\n",
      "2024-01-01 11:47:35.986549: Epoch time: 125.22 s\n",
      "2024-01-01 11:47:36.987391: \n",
      "2024-01-01 11:47:36.993453: Epoch 531\n",
      "2024-01-01 11:47:36.997396: Current learning rate: 0.00506\n",
      "2024-01-01 11:49:42.446876: train_loss -0.9146\n",
      "2024-01-01 11:49:42.455879: val_loss -0.8339\n",
      "2024-01-01 11:49:42.464877: Pseudo dice [0.9275, 0.9434, 0.9429]\n",
      "2024-01-01 11:49:42.471877: Epoch time: 125.46 s\n",
      "2024-01-01 11:49:43.526973: \n",
      "2024-01-01 11:49:43.532973: Epoch 532\n",
      "2024-01-01 11:49:43.537973: Current learning rate: 0.00505\n",
      "2024-01-01 11:51:48.726863: train_loss -0.917\n",
      "2024-01-01 11:51:48.733866: val_loss -0.8333\n",
      "2024-01-01 11:51:48.740864: Pseudo dice [0.9241, 0.9449, 0.9439]\n",
      "2024-01-01 11:51:48.746865: Epoch time: 125.2 s\n",
      "2024-01-01 11:51:49.741430: \n",
      "2024-01-01 11:51:49.746430: Epoch 533\n",
      "2024-01-01 11:51:49.751422: Current learning rate: 0.00504\n",
      "2024-01-01 11:53:55.170338: train_loss -0.9177\n",
      "2024-01-01 11:53:55.179341: val_loss -0.832\n",
      "2024-01-01 11:53:55.186337: Pseudo dice [0.9278, 0.9433, 0.9433]\n",
      "2024-01-01 11:53:55.192338: Epoch time: 125.43 s\n",
      "2024-01-01 11:53:56.196060: \n",
      "2024-01-01 11:53:56.206768: Epoch 534\n",
      "2024-01-01 11:53:56.212772: Current learning rate: 0.00503\n",
      "2024-01-01 11:56:01.673865: train_loss -0.9144\n",
      "2024-01-01 11:56:01.683974: val_loss -0.8248\n",
      "2024-01-01 11:56:01.693972: Pseudo dice [0.9277, 0.9445, 0.944]\n",
      "2024-01-01 11:56:01.699980: Epoch time: 125.48 s\n",
      "2024-01-01 11:56:02.713853: \n",
      "2024-01-01 11:56:02.719849: Epoch 535\n",
      "2024-01-01 11:56:02.723854: Current learning rate: 0.00502\n",
      "2024-01-01 11:58:08.189854: train_loss -0.9157\n",
      "2024-01-01 11:58:08.198858: val_loss -0.8388\n",
      "2024-01-01 11:58:08.206143: Pseudo dice [0.9227, 0.9431, 0.9437]\n",
      "2024-01-01 11:58:08.213652: Epoch time: 125.48 s\n",
      "2024-01-01 11:58:09.393908: \n",
      "2024-01-01 11:58:09.399927: Epoch 536\n",
      "2024-01-01 11:58:09.403918: Current learning rate: 0.00501\n",
      "2024-01-01 12:00:14.777151: train_loss -0.9154\n",
      "2024-01-01 12:00:14.788199: val_loss -0.8288\n",
      "2024-01-01 12:00:14.797704: Pseudo dice [0.9287, 0.9453, 0.9434]\n",
      "2024-01-01 12:00:14.806711: Epoch time: 125.38 s\n",
      "2024-01-01 12:00:15.877622: \n",
      "2024-01-01 12:00:15.885619: Epoch 537\n",
      "2024-01-01 12:00:15.891619: Current learning rate: 0.005\n",
      "2024-01-01 12:02:21.688872: train_loss -0.9138\n",
      "2024-01-01 12:02:21.697871: val_loss -0.8317\n",
      "2024-01-01 12:02:21.704874: Pseudo dice [0.9283, 0.9461, 0.9453]\n",
      "2024-01-01 12:02:21.711883: Epoch time: 125.81 s\n",
      "2024-01-01 12:02:22.718167: \n",
      "2024-01-01 12:02:22.728213: Epoch 538\n",
      "2024-01-01 12:02:22.733225: Current learning rate: 0.00499\n",
      "2024-01-01 12:04:28.066024: train_loss -0.9141\n",
      "2024-01-01 12:04:28.076025: val_loss -0.834\n",
      "2024-01-01 12:04:28.084029: Pseudo dice [0.9266, 0.9458, 0.9438]\n",
      "2024-01-01 12:04:28.094023: Epoch time: 125.35 s\n",
      "2024-01-01 12:04:29.188208: \n",
      "2024-01-01 12:04:29.194197: Epoch 539\n",
      "2024-01-01 12:04:29.198206: Current learning rate: 0.00498\n",
      "2024-01-01 12:06:34.919192: train_loss -0.9153\n",
      "2024-01-01 12:06:34.928212: val_loss -0.8388\n",
      "2024-01-01 12:06:34.935213: Pseudo dice [0.9274, 0.9462, 0.9448]\n",
      "2024-01-01 12:06:34.943252: Epoch time: 125.73 s\n",
      "2024-01-01 12:06:35.934867: \n",
      "2024-01-01 12:06:35.940386: Epoch 540\n",
      "2024-01-01 12:06:35.945384: Current learning rate: 0.00497\n",
      "2024-01-01 12:08:41.757552: train_loss -0.9156\n",
      "2024-01-01 12:08:41.768554: val_loss -0.8282\n",
      "2024-01-01 12:08:41.780308: Pseudo dice [0.9264, 0.945, 0.9443]\n",
      "2024-01-01 12:08:41.789316: Epoch time: 125.82 s\n",
      "2024-01-01 12:08:42.937460: \n",
      "2024-01-01 12:08:42.943463: Epoch 541\n",
      "2024-01-01 12:08:42.947531: Current learning rate: 0.00496\n",
      "2024-01-01 12:10:48.427963: train_loss -0.9117\n",
      "2024-01-01 12:10:48.435963: val_loss -0.8052\n",
      "2024-01-01 12:10:48.442960: Pseudo dice [0.9142, 0.9387, 0.937]\n",
      "2024-01-01 12:10:48.447960: Epoch time: 125.49 s\n",
      "2024-01-01 12:10:49.449136: \n",
      "2024-01-01 12:10:49.457135: Epoch 542\n",
      "2024-01-01 12:10:49.462135: Current learning rate: 0.00495\n",
      "2024-01-01 12:12:54.961358: train_loss -0.9126\n",
      "2024-01-01 12:12:54.971348: val_loss -0.8185\n",
      "2024-01-01 12:12:54.980348: Pseudo dice [0.9219, 0.9407, 0.939]\n",
      "2024-01-01 12:12:54.987348: Epoch time: 125.51 s\n",
      "2024-01-01 12:12:55.983389: \n",
      "2024-01-01 12:12:55.989390: Epoch 543\n",
      "2024-01-01 12:12:55.994396: Current learning rate: 0.00494\n",
      "2024-01-01 12:15:01.077295: train_loss -0.9123\n",
      "2024-01-01 12:15:01.084665: val_loss -0.7943\n",
      "2024-01-01 12:15:01.093174: Pseudo dice [0.9118, 0.9372, 0.9343]\n",
      "2024-01-01 12:15:01.099173: Epoch time: 125.09 s\n",
      "2024-01-01 12:15:02.273747: \n",
      "2024-01-01 12:15:02.278744: Epoch 544\n",
      "2024-01-01 12:15:02.283744: Current learning rate: 0.00493\n",
      "2024-01-01 12:17:07.390615: train_loss -0.9153\n",
      "2024-01-01 12:17:07.399615: val_loss -0.8137\n",
      "2024-01-01 12:17:07.407617: Pseudo dice [0.9135, 0.9367, 0.9336]\n",
      "2024-01-01 12:17:07.413617: Epoch time: 125.12 s\n",
      "2024-01-01 12:17:08.411455: \n",
      "2024-01-01 12:17:08.419456: Epoch 545\n",
      "2024-01-01 12:17:08.424455: Current learning rate: 0.00492\n",
      "2024-01-01 12:19:14.062749: train_loss -0.9097\n",
      "2024-01-01 12:19:14.069750: val_loss -0.8166\n",
      "2024-01-01 12:19:14.077746: Pseudo dice [0.9172, 0.94, 0.9384]\n",
      "2024-01-01 12:19:14.085747: Epoch time: 125.65 s\n",
      "2024-01-01 12:19:15.092308: \n",
      "2024-01-01 12:19:15.097307: Epoch 546\n",
      "2024-01-01 12:19:15.102299: Current learning rate: 0.00491\n",
      "2024-01-01 12:21:20.305779: train_loss -0.9103\n",
      "2024-01-01 12:21:20.316783: val_loss -0.8304\n",
      "2024-01-01 12:21:20.323801: Pseudo dice [0.9252, 0.9444, 0.9455]\n",
      "2024-01-01 12:21:20.331791: Epoch time: 125.21 s\n",
      "2024-01-01 12:21:21.327908: \n",
      "2024-01-01 12:21:21.332906: Epoch 547\n",
      "2024-01-01 12:21:21.337898: Current learning rate: 0.0049\n",
      "2024-01-01 12:23:26.880599: train_loss -0.9113\n",
      "2024-01-01 12:23:26.888599: val_loss -0.8347\n",
      "2024-01-01 12:23:26.895606: Pseudo dice [0.9255, 0.9454, 0.9442]\n",
      "2024-01-01 12:23:26.903634: Epoch time: 125.55 s\n",
      "2024-01-01 12:23:27.913861: \n",
      "2024-01-01 12:23:27.918865: Epoch 548\n",
      "2024-01-01 12:23:27.922861: Current learning rate: 0.00489\n",
      "2024-01-01 12:25:33.501722: train_loss -0.9105\n",
      "2024-01-01 12:25:33.510229: val_loss -0.8088\n",
      "2024-01-01 12:25:33.519240: Pseudo dice [0.9132, 0.9384, 0.936]\n",
      "2024-01-01 12:25:33.528237: Epoch time: 125.59 s\n",
      "2024-01-01 12:25:34.580982: \n",
      "2024-01-01 12:25:34.589883: Epoch 549\n",
      "2024-01-01 12:25:34.594935: Current learning rate: 0.00488\n",
      "2024-01-01 12:27:40.085620: train_loss -0.912\n",
      "2024-01-01 12:27:40.094610: val_loss -0.8272\n",
      "2024-01-01 12:27:40.102621: Pseudo dice [0.9296, 0.9453, 0.9443]\n",
      "2024-01-01 12:27:40.111616: Epoch time: 125.51 s\n",
      "2024-01-01 12:27:41.467853: \n",
      "2024-01-01 12:27:41.473859: Epoch 550\n",
      "2024-01-01 12:27:41.477848: Current learning rate: 0.00487\n",
      "2024-01-01 12:29:46.740027: train_loss -0.9146\n",
      "2024-01-01 12:29:46.749036: val_loss -0.8321\n",
      "2024-01-01 12:29:46.756028: Pseudo dice [0.9299, 0.9462, 0.9446]\n",
      "2024-01-01 12:29:46.764028: Epoch time: 125.27 s\n",
      "2024-01-01 12:29:47.931258: \n",
      "2024-01-01 12:29:47.937245: Epoch 551\n",
      "2024-01-01 12:29:47.942245: Current learning rate: 0.00486\n",
      "2024-01-01 12:31:53.210757: train_loss -0.9159\n",
      "2024-01-01 12:31:53.220757: val_loss -0.8321\n",
      "2024-01-01 12:31:53.229033: Pseudo dice [0.926, 0.9442, 0.9447]\n",
      "2024-01-01 12:31:53.237030: Epoch time: 125.28 s\n",
      "2024-01-01 12:31:54.289184: \n",
      "2024-01-01 12:31:54.297184: Epoch 552\n",
      "2024-01-01 12:31:54.302181: Current learning rate: 0.00485\n",
      "2024-01-01 12:33:59.585996: train_loss -0.9139\n",
      "2024-01-01 12:33:59.594007: val_loss -0.8214\n",
      "2024-01-01 12:33:59.601002: Pseudo dice [0.9233, 0.9435, 0.9429]\n",
      "2024-01-01 12:33:59.609001: Epoch time: 125.3 s\n",
      "2024-01-01 12:34:00.621028: \n",
      "2024-01-01 12:34:00.626537: Epoch 553\n",
      "2024-01-01 12:34:00.631536: Current learning rate: 0.00484\n",
      "2024-01-01 12:36:06.198019: train_loss -0.9156\n",
      "2024-01-01 12:36:06.208020: val_loss -0.8343\n",
      "2024-01-01 12:36:06.216023: Pseudo dice [0.9305, 0.945, 0.9436]\n",
      "2024-01-01 12:36:06.224025: Epoch time: 125.58 s\n",
      "2024-01-01 12:36:07.250985: \n",
      "2024-01-01 12:36:07.261009: Epoch 554\n",
      "2024-01-01 12:36:07.268999: Current learning rate: 0.00484\n",
      "2024-01-01 12:38:12.478920: train_loss -0.9168\n",
      "2024-01-01 12:38:12.489923: val_loss -0.823\n",
      "2024-01-01 12:38:12.500921: Pseudo dice [0.9263, 0.9456, 0.9437]\n",
      "2024-01-01 12:38:12.508926: Epoch time: 125.23 s\n",
      "2024-01-01 12:38:13.536374: \n",
      "2024-01-01 12:38:13.542374: Epoch 555\n",
      "2024-01-01 12:38:13.548373: Current learning rate: 0.00483\n",
      "2024-01-01 12:40:19.006238: train_loss -0.914\n",
      "2024-01-01 12:40:19.016251: val_loss -0.8398\n",
      "2024-01-01 12:40:19.023250: Pseudo dice [0.9299, 0.9468, 0.9448]\n",
      "2024-01-01 12:40:19.030756: Epoch time: 125.47 s\n",
      "2024-01-01 12:40:20.031474: \n",
      "2024-01-01 12:40:20.039548: Epoch 556\n",
      "2024-01-01 12:40:20.044483: Current learning rate: 0.00482\n",
      "2024-01-01 12:42:25.522753: train_loss -0.9122\n",
      "2024-01-01 12:42:25.532752: val_loss -0.8308\n",
      "2024-01-01 12:42:25.538753: Pseudo dice [0.9275, 0.9453, 0.9448]\n",
      "2024-01-01 12:42:25.547752: Epoch time: 125.49 s\n",
      "2024-01-01 12:42:26.537470: \n",
      "2024-01-01 12:42:26.543586: Epoch 557\n",
      "2024-01-01 12:42:26.548496: Current learning rate: 0.00481\n",
      "2024-01-01 12:44:31.735986: train_loss -0.9162\n",
      "2024-01-01 12:44:31.744990: val_loss -0.8412\n",
      "2024-01-01 12:44:31.752990: Pseudo dice [0.9297, 0.9459, 0.9458]\n",
      "2024-01-01 12:44:31.759986: Epoch time: 125.2 s\n",
      "2024-01-01 12:44:32.758959: \n",
      "2024-01-01 12:44:32.765038: Epoch 558\n",
      "2024-01-01 12:44:32.769036: Current learning rate: 0.0048\n",
      "2024-01-01 12:46:38.005760: train_loss -0.9178\n",
      "2024-01-01 12:46:38.012770: val_loss -0.8393\n",
      "2024-01-01 12:46:38.020764: Pseudo dice [0.9274, 0.9461, 0.9447]\n",
      "2024-01-01 12:46:38.027760: Epoch time: 125.25 s\n",
      "2024-01-01 12:46:39.212148: \n",
      "2024-01-01 12:46:39.218151: Epoch 559\n",
      "2024-01-01 12:46:39.222220: Current learning rate: 0.00479\n",
      "2024-01-01 12:48:44.396456: train_loss -0.9194\n",
      "2024-01-01 12:48:44.405461: val_loss -0.8275\n",
      "2024-01-01 12:48:44.413975: Pseudo dice [0.9229, 0.9429, 0.9406]\n",
      "2024-01-01 12:48:44.421973: Epoch time: 125.19 s\n",
      "2024-01-01 12:48:45.466815: \n",
      "2024-01-01 12:48:45.472883: Epoch 560\n",
      "2024-01-01 12:48:45.477812: Current learning rate: 0.00478\n",
      "2024-01-01 12:50:50.725727: train_loss -0.916\n",
      "2024-01-01 12:50:50.732727: val_loss -0.8316\n",
      "2024-01-01 12:50:50.740725: Pseudo dice [0.925, 0.9438, 0.9436]\n",
      "2024-01-01 12:50:50.746725: Epoch time: 125.26 s\n",
      "2024-01-01 12:50:51.768478: \n",
      "2024-01-01 12:50:51.776479: Epoch 561\n",
      "2024-01-01 12:50:51.781548: Current learning rate: 0.00477\n",
      "2024-01-01 12:52:57.076816: train_loss -0.9141\n",
      "2024-01-01 12:52:57.085816: val_loss -0.8363\n",
      "2024-01-01 12:52:57.093825: Pseudo dice [0.9277, 0.947, 0.9456]\n",
      "2024-01-01 12:52:57.102329: Epoch time: 125.31 s\n",
      "2024-01-01 12:52:58.123642: \n",
      "2024-01-01 12:52:58.129725: Epoch 562\n",
      "2024-01-01 12:52:58.134659: Current learning rate: 0.00476\n",
      "2024-01-01 12:55:04.088059: train_loss -0.9154\n",
      "2024-01-01 12:55:04.097062: val_loss -0.8357\n",
      "2024-01-01 12:55:04.105060: Pseudo dice [0.9261, 0.9463, 0.9455]\n",
      "2024-01-01 12:55:04.113068: Epoch time: 125.97 s\n",
      "2024-01-01 12:55:05.139770: \n",
      "2024-01-01 12:55:05.145774: Epoch 563\n",
      "2024-01-01 12:55:05.150486: Current learning rate: 0.00475\n",
      "2024-01-01 12:57:10.387036: train_loss -0.9169\n",
      "2024-01-01 12:57:10.394045: val_loss -0.831\n",
      "2024-01-01 12:57:10.401044: Pseudo dice [0.9234, 0.9446, 0.9428]\n",
      "2024-01-01 12:57:10.408559: Epoch time: 125.25 s\n",
      "2024-01-01 12:57:11.412236: \n",
      "2024-01-01 12:57:11.418246: Epoch 564\n",
      "2024-01-01 12:57:11.423234: Current learning rate: 0.00474\n",
      "2024-01-01 12:59:16.829096: train_loss -0.9166\n",
      "2024-01-01 12:59:16.837103: val_loss -0.8333\n",
      "2024-01-01 12:59:16.846103: Pseudo dice [0.9272, 0.9456, 0.9452]\n",
      "2024-01-01 12:59:16.853610: Epoch time: 125.42 s\n",
      "2024-01-01 12:59:17.847298: \n",
      "2024-01-01 12:59:17.860809: Epoch 565\n",
      "2024-01-01 12:59:17.866816: Current learning rate: 0.00473\n",
      "2024-01-01 13:01:23.102524: train_loss -0.9134\n",
      "2024-01-01 13:01:23.109527: val_loss -0.8225\n",
      "2024-01-01 13:01:23.117524: Pseudo dice [0.9235, 0.9453, 0.9443]\n",
      "2024-01-01 13:01:23.125525: Epoch time: 125.26 s\n",
      "2024-01-01 13:01:24.185816: \n",
      "2024-01-01 13:01:24.190887: Epoch 566\n",
      "2024-01-01 13:01:24.195885: Current learning rate: 0.00472\n",
      "2024-01-01 13:03:29.588490: train_loss -0.9175\n",
      "2024-01-01 13:03:29.598491: val_loss -0.8332\n",
      "2024-01-01 13:03:29.605491: Pseudo dice [0.9251, 0.9457, 0.9445]\n",
      "2024-01-01 13:03:29.612492: Epoch time: 125.4 s\n",
      "2024-01-01 13:03:30.795554: \n",
      "2024-01-01 13:03:30.802559: Epoch 567\n",
      "2024-01-01 13:03:30.807554: Current learning rate: 0.00471\n",
      "2024-01-01 13:05:35.945625: train_loss -0.9134\n",
      "2024-01-01 13:05:35.954689: val_loss -0.835\n",
      "2024-01-01 13:05:35.963196: Pseudo dice [0.9275, 0.9454, 0.9436]\n",
      "2024-01-01 13:05:35.971203: Epoch time: 125.15 s\n",
      "2024-01-01 13:05:36.967471: \n",
      "2024-01-01 13:05:36.978505: Epoch 568\n",
      "2024-01-01 13:05:36.983049: Current learning rate: 0.0047\n",
      "2024-01-01 13:07:42.167598: train_loss -0.9148\n",
      "2024-01-01 13:07:42.175603: val_loss -0.8245\n",
      "2024-01-01 13:07:42.182599: Pseudo dice [0.9235, 0.9423, 0.9401]\n",
      "2024-01-01 13:07:42.190600: Epoch time: 125.2 s\n",
      "2024-01-01 13:07:43.194963: \n",
      "2024-01-01 13:07:43.200963: Epoch 569\n",
      "2024-01-01 13:07:43.205955: Current learning rate: 0.00469\n",
      "2024-01-01 13:09:48.466489: train_loss -0.9176\n",
      "2024-01-01 13:09:48.476695: val_loss -0.8277\n",
      "2024-01-01 13:09:48.484710: Pseudo dice [0.9281, 0.945, 0.9443]\n",
      "2024-01-01 13:09:48.491701: Epoch time: 125.27 s\n",
      "2024-01-01 13:09:49.501297: \n",
      "2024-01-01 13:09:49.510288: Epoch 570\n",
      "2024-01-01 13:09:49.518313: Current learning rate: 0.00468\n",
      "2024-01-01 13:11:54.883744: train_loss -0.915\n",
      "2024-01-01 13:11:54.891745: val_loss -0.8374\n",
      "2024-01-01 13:11:54.900747: Pseudo dice [0.9293, 0.9456, 0.9434]\n",
      "2024-01-01 13:11:54.908747: Epoch time: 125.38 s\n",
      "2024-01-01 13:11:55.957475: \n",
      "2024-01-01 13:11:55.963479: Epoch 571\n",
      "2024-01-01 13:11:55.968477: Current learning rate: 0.00467\n",
      "2024-01-01 13:14:01.318374: train_loss -0.9182\n",
      "2024-01-01 13:14:01.326375: val_loss -0.8346\n",
      "2024-01-01 13:14:01.333374: Pseudo dice [0.9258, 0.9459, 0.9445]\n",
      "2024-01-01 13:14:01.340374: Epoch time: 125.36 s\n",
      "2024-01-01 13:14:02.364636: \n",
      "2024-01-01 13:14:02.377168: Epoch 572\n",
      "2024-01-01 13:14:02.382164: Current learning rate: 0.00466\n",
      "2024-01-01 13:16:07.933783: train_loss -0.9143\n",
      "2024-01-01 13:16:07.941783: val_loss -0.8258\n",
      "2024-01-01 13:16:07.949783: Pseudo dice [0.9262, 0.9445, 0.9436]\n",
      "2024-01-01 13:16:07.956783: Epoch time: 125.57 s\n",
      "2024-01-01 13:16:08.975643: \n",
      "2024-01-01 13:16:08.981643: Epoch 573\n",
      "2024-01-01 13:16:08.986635: Current learning rate: 0.00465\n",
      "2024-01-01 13:18:14.252764: train_loss -0.9181\n",
      "2024-01-01 13:18:14.261769: val_loss -0.8247\n",
      "2024-01-01 13:18:14.270799: Pseudo dice [0.9242, 0.9442, 0.9428]\n",
      "2024-01-01 13:18:14.278797: Epoch time: 125.28 s\n",
      "2024-01-01 13:18:15.472528: \n",
      "2024-01-01 13:18:15.480531: Epoch 574\n",
      "2024-01-01 13:18:15.487526: Current learning rate: 0.00464\n",
      "2024-01-01 13:20:20.932461: train_loss -0.9169\n",
      "2024-01-01 13:20:20.940460: val_loss -0.8369\n",
      "2024-01-01 13:20:20.947465: Pseudo dice [0.9305, 0.9449, 0.9434]\n",
      "2024-01-01 13:20:20.954294: Epoch time: 125.46 s\n",
      "2024-01-01 13:20:21.965720: \n",
      "2024-01-01 13:20:21.974729: Epoch 575\n",
      "2024-01-01 13:20:21.979729: Current learning rate: 0.00463\n",
      "2024-01-01 13:22:27.535222: train_loss -0.9143\n",
      "2024-01-01 13:22:27.543229: val_loss -0.8151\n",
      "2024-01-01 13:22:27.555731: Pseudo dice [0.9191, 0.9422, 0.9395]\n",
      "2024-01-01 13:22:27.564735: Epoch time: 125.57 s\n",
      "2024-01-01 13:22:28.581218: \n",
      "2024-01-01 13:22:28.587298: Epoch 576\n",
      "2024-01-01 13:22:28.592236: Current learning rate: 0.00462\n",
      "2024-01-01 13:24:33.765368: train_loss -0.9147\n",
      "2024-01-01 13:24:33.773368: val_loss -0.824\n",
      "2024-01-01 13:24:33.781368: Pseudo dice [0.924, 0.9434, 0.9412]\n",
      "2024-01-01 13:24:33.789371: Epoch time: 125.19 s\n",
      "2024-01-01 13:24:34.799155: \n",
      "2024-01-01 13:24:34.806171: Epoch 577\n",
      "2024-01-01 13:24:34.815163: Current learning rate: 0.00461\n",
      "2024-01-01 13:26:40.415650: train_loss -0.9147\n",
      "2024-01-01 13:26:40.425651: val_loss -0.8003\n",
      "2024-01-01 13:26:40.434658: Pseudo dice [0.9148, 0.9398, 0.9368]\n",
      "2024-01-01 13:26:40.441662: Epoch time: 125.62 s\n",
      "2024-01-01 13:26:41.467619: \n",
      "2024-01-01 13:26:41.476622: Epoch 578\n",
      "2024-01-01 13:26:41.481629: Current learning rate: 0.0046\n",
      "2024-01-01 13:28:47.028804: train_loss -0.9134\n",
      "2024-01-01 13:28:47.036802: val_loss -0.808\n",
      "2024-01-01 13:28:47.046802: Pseudo dice [0.9196, 0.9405, 0.9391]\n",
      "2024-01-01 13:28:47.053802: Epoch time: 125.56 s\n",
      "2024-01-01 13:28:48.081028: \n",
      "2024-01-01 13:28:48.087021: Epoch 579\n",
      "2024-01-01 13:28:48.092013: Current learning rate: 0.00459\n",
      "2024-01-01 13:30:53.574241: train_loss -0.9141\n",
      "2024-01-01 13:30:53.583241: val_loss -0.8195\n",
      "2024-01-01 13:30:53.592246: Pseudo dice [0.9201, 0.942, 0.9392]\n",
      "2024-01-01 13:30:53.598250: Epoch time: 125.49 s\n",
      "2024-01-01 13:30:54.609542: \n",
      "2024-01-01 13:30:54.615736: Epoch 580\n",
      "2024-01-01 13:30:54.619745: Current learning rate: 0.00458\n",
      "2024-01-01 13:32:59.904917: train_loss -0.9157\n",
      "2024-01-01 13:32:59.913953: val_loss -0.8223\n",
      "2024-01-01 13:32:59.920463: Pseudo dice [0.9244, 0.9417, 0.9398]\n",
      "2024-01-01 13:32:59.928528: Epoch time: 125.3 s\n",
      "2024-01-01 13:33:00.962155: \n",
      "2024-01-01 13:33:00.970155: Epoch 581\n",
      "2024-01-01 13:33:00.975154: Current learning rate: 0.00457\n",
      "2024-01-01 13:35:06.489586: train_loss -0.9187\n",
      "2024-01-01 13:35:06.496584: val_loss -0.8215\n",
      "2024-01-01 13:35:06.503585: Pseudo dice [0.9211, 0.9428, 0.9405]\n",
      "2024-01-01 13:35:06.508585: Epoch time: 125.53 s\n",
      "2024-01-01 13:35:07.714543: \n",
      "2024-01-01 13:35:07.724038: Epoch 582\n",
      "2024-01-01 13:35:07.728046: Current learning rate: 0.00456\n",
      "2024-01-01 13:37:12.741919: train_loss -0.9206\n",
      "2024-01-01 13:37:12.750921: val_loss -0.8215\n",
      "2024-01-01 13:37:12.760919: Pseudo dice [0.9226, 0.9411, 0.9402]\n",
      "2024-01-01 13:37:12.768931: Epoch time: 125.03 s\n",
      "2024-01-01 13:37:13.801489: \n",
      "2024-01-01 13:37:13.807490: Epoch 583\n",
      "2024-01-01 13:37:13.812499: Current learning rate: 0.00455\n",
      "2024-01-01 13:39:19.112063: train_loss -0.9169\n",
      "2024-01-01 13:39:19.122060: val_loss -0.8269\n",
      "2024-01-01 13:39:19.130577: Pseudo dice [0.9253, 0.9442, 0.9442]\n",
      "2024-01-01 13:39:19.139572: Epoch time: 125.31 s\n",
      "2024-01-01 13:39:20.176672: \n",
      "2024-01-01 13:39:20.184678: Epoch 584\n",
      "2024-01-01 13:39:20.191670: Current learning rate: 0.00454\n",
      "2024-01-01 13:41:25.749904: train_loss -0.9195\n",
      "2024-01-01 13:41:25.758904: val_loss -0.818\n",
      "2024-01-01 13:41:25.764903: Pseudo dice [0.9206, 0.9424, 0.9399]\n",
      "2024-01-01 13:41:25.771903: Epoch time: 125.57 s\n",
      "2024-01-01 13:41:26.795928: \n",
      "2024-01-01 13:41:26.801930: Epoch 585\n",
      "2024-01-01 13:41:26.806930: Current learning rate: 0.00453\n",
      "2024-01-01 13:43:32.063245: train_loss -0.9163\n",
      "2024-01-01 13:43:32.072241: val_loss -0.8141\n",
      "2024-01-01 13:43:32.080245: Pseudo dice [0.9209, 0.9438, 0.942]\n",
      "2024-01-01 13:43:32.088241: Epoch time: 125.27 s\n",
      "2024-01-01 13:43:33.095510: \n",
      "2024-01-01 13:43:33.102514: Epoch 586\n",
      "2024-01-01 13:43:33.107509: Current learning rate: 0.00452\n",
      "2024-01-01 13:45:38.248709: train_loss -0.9171\n",
      "2024-01-01 13:45:38.256708: val_loss -0.8105\n",
      "2024-01-01 13:45:38.301965: Pseudo dice [0.919, 0.9417, 0.9394]\n",
      "2024-01-01 13:45:38.310963: Epoch time: 125.16 s\n",
      "2024-01-01 13:45:39.314350: \n",
      "2024-01-01 13:45:39.319353: Epoch 587\n",
      "2024-01-01 13:45:39.325350: Current learning rate: 0.00451\n",
      "2024-01-01 13:47:44.521037: train_loss -0.9178\n",
      "2024-01-01 13:47:44.531039: val_loss -0.813\n",
      "2024-01-01 13:47:44.538042: Pseudo dice [0.9233, 0.9438, 0.9416]\n",
      "2024-01-01 13:47:44.547038: Epoch time: 125.21 s\n",
      "2024-01-01 13:47:45.619074: \n",
      "2024-01-01 13:47:45.625584: Epoch 588\n",
      "2024-01-01 13:47:45.629588: Current learning rate: 0.0045\n",
      "2024-01-01 13:49:50.969521: train_loss -0.917\n",
      "2024-01-01 13:49:50.979517: val_loss -0.8286\n",
      "2024-01-01 13:49:50.988517: Pseudo dice [0.9231, 0.9443, 0.9442]\n",
      "2024-01-01 13:49:50.997517: Epoch time: 125.35 s\n",
      "2024-01-01 13:49:52.021205: \n",
      "2024-01-01 13:49:52.027270: Epoch 589\n",
      "2024-01-01 13:49:52.031210: Current learning rate: 0.00449\n",
      "2024-01-01 13:51:57.107098: train_loss -0.9192\n",
      "2024-01-01 13:51:57.116607: val_loss -0.82\n",
      "2024-01-01 13:51:57.123607: Pseudo dice [0.9221, 0.9426, 0.9412]\n",
      "2024-01-01 13:51:57.131605: Epoch time: 125.09 s\n",
      "2024-01-01 13:51:58.343838: \n",
      "2024-01-01 13:51:58.349841: Epoch 590\n",
      "2024-01-01 13:51:58.353842: Current learning rate: 0.00448\n",
      "2024-01-01 13:54:03.591773: train_loss -0.9168\n",
      "2024-01-01 13:54:03.599773: val_loss -0.8196\n",
      "2024-01-01 13:54:03.607772: Pseudo dice [0.9263, 0.945, 0.9418]\n",
      "2024-01-01 13:54:03.615773: Epoch time: 125.25 s\n",
      "2024-01-01 13:54:04.624853: \n",
      "2024-01-01 13:54:04.629858: Epoch 591\n",
      "2024-01-01 13:54:04.634853: Current learning rate: 0.00447\n",
      "2024-01-01 13:56:10.086001: train_loss -0.916\n",
      "2024-01-01 13:56:10.094005: val_loss -0.8308\n",
      "2024-01-01 13:56:10.101512: Pseudo dice [0.9247, 0.944, 0.9432]\n",
      "2024-01-01 13:56:10.107517: Epoch time: 125.46 s\n",
      "2024-01-01 13:56:11.126127: \n",
      "2024-01-01 13:56:11.131132: Epoch 592\n",
      "2024-01-01 13:56:11.136190: Current learning rate: 0.00446\n",
      "2024-01-01 13:58:16.487695: train_loss -0.917\n",
      "2024-01-01 13:58:16.496695: val_loss -0.8262\n",
      "2024-01-01 13:58:16.506694: Pseudo dice [0.9259, 0.9454, 0.9451]\n",
      "2024-01-01 13:58:16.514694: Epoch time: 125.36 s\n",
      "2024-01-01 13:58:17.544797: \n",
      "2024-01-01 13:58:17.550802: Epoch 593\n",
      "2024-01-01 13:58:17.555798: Current learning rate: 0.00445\n",
      "2024-01-01 14:00:22.923091: train_loss -0.9172\n",
      "2024-01-01 14:00:22.931095: val_loss -0.8168\n",
      "2024-01-01 14:00:22.947469: Pseudo dice [0.9203, 0.9409, 0.9389]\n",
      "2024-01-01 14:00:22.955968: Epoch time: 125.38 s\n",
      "2024-01-01 14:00:24.012382: \n",
      "2024-01-01 14:00:24.018449: Epoch 594\n",
      "2024-01-01 14:00:24.023382: Current learning rate: 0.00444\n",
      "2024-01-01 14:02:29.120577: train_loss -0.9198\n",
      "2024-01-01 14:02:29.128582: val_loss -0.802\n",
      "2024-01-01 14:02:29.136582: Pseudo dice [0.9169, 0.9404, 0.938]\n",
      "2024-01-01 14:02:29.144091: Epoch time: 125.11 s\n",
      "2024-01-01 14:02:30.167387: \n",
      "2024-01-01 14:02:30.172391: Epoch 595\n",
      "2024-01-01 14:02:30.177387: Current learning rate: 0.00443\n",
      "2024-01-01 14:04:35.419734: train_loss -0.9174\n",
      "2024-01-01 14:04:35.428226: val_loss -0.8252\n",
      "2024-01-01 14:04:35.434230: Pseudo dice [0.9225, 0.9428, 0.9423]\n",
      "2024-01-01 14:04:35.441230: Epoch time: 125.25 s\n",
      "2024-01-01 14:04:36.463778: \n",
      "2024-01-01 14:04:36.469807: Epoch 596\n",
      "2024-01-01 14:04:36.475806: Current learning rate: 0.00442\n",
      "2024-01-01 14:06:41.772515: train_loss -0.9156\n",
      "2024-01-01 14:06:41.782514: val_loss -0.83\n",
      "2024-01-01 14:06:41.790517: Pseudo dice [0.9317, 0.9443, 0.9432]\n",
      "2024-01-01 14:06:41.799516: Epoch time: 125.31 s\n",
      "2024-01-01 14:06:42.848233: \n",
      "2024-01-01 14:06:42.854237: Epoch 597\n",
      "2024-01-01 14:06:42.859233: Current learning rate: 0.00441\n",
      "2024-01-01 14:08:48.001110: train_loss -0.9186\n",
      "2024-01-01 14:08:48.009111: val_loss -0.8291\n",
      "2024-01-01 14:08:48.015110: Pseudo dice [0.9269, 0.9453, 0.9436]\n",
      "2024-01-01 14:08:48.023113: Epoch time: 125.15 s\n",
      "2024-01-01 14:08:49.222027: \n",
      "2024-01-01 14:08:49.228026: Epoch 598\n",
      "2024-01-01 14:08:49.232028: Current learning rate: 0.0044\n",
      "2024-01-01 14:10:54.625600: train_loss -0.9147\n",
      "2024-01-01 14:10:54.633613: val_loss -0.8303\n",
      "2024-01-01 14:10:54.641599: Pseudo dice [0.9283, 0.9461, 0.9451]\n",
      "2024-01-01 14:10:54.649599: Epoch time: 125.4 s\n",
      "2024-01-01 14:10:55.685803: \n",
      "2024-01-01 14:10:55.691847: Epoch 599\n",
      "2024-01-01 14:10:55.696804: Current learning rate: 0.00439\n",
      "2024-01-01 14:13:01.138903: train_loss -0.9135\n",
      "2024-01-01 14:13:01.147906: val_loss -0.8315\n",
      "2024-01-01 14:13:01.155903: Pseudo dice [0.9267, 0.9437, 0.943]\n",
      "2024-01-01 14:13:01.161909: Epoch time: 125.45 s\n",
      "2024-01-01 14:13:02.457893: \n",
      "2024-01-01 14:13:02.464011: Epoch 600\n",
      "2024-01-01 14:13:02.468902: Current learning rate: 0.00438\n",
      "2024-01-01 14:15:07.717597: train_loss -0.9184\n",
      "2024-01-01 14:15:07.725594: val_loss -0.8338\n",
      "2024-01-01 14:15:07.733102: Pseudo dice [0.9257, 0.9445, 0.943]\n",
      "2024-01-01 14:15:07.740101: Epoch time: 125.26 s\n",
      "2024-01-01 14:15:08.758734: \n",
      "2024-01-01 14:15:08.764729: Epoch 601\n",
      "2024-01-01 14:15:08.769725: Current learning rate: 0.00437\n",
      "2024-01-01 14:17:14.030462: train_loss -0.9174\n",
      "2024-01-01 14:17:14.041463: val_loss -0.8363\n",
      "2024-01-01 14:17:14.050462: Pseudo dice [0.9276, 0.9476, 0.9459]\n",
      "2024-01-01 14:17:14.061463: Epoch time: 125.27 s\n",
      "2024-01-01 14:17:15.113156: \n",
      "2024-01-01 14:17:15.121226: Epoch 602\n",
      "2024-01-01 14:17:15.126185: Current learning rate: 0.00436\n",
      "2024-01-01 14:19:20.365672: train_loss -0.9182\n",
      "2024-01-01 14:19:20.372672: val_loss -0.8266\n",
      "2024-01-01 14:19:20.380673: Pseudo dice [0.929, 0.945, 0.9445]\n",
      "2024-01-01 14:19:20.386672: Epoch time: 125.25 s\n",
      "2024-01-01 14:19:21.415555: \n",
      "2024-01-01 14:19:21.421628: Epoch 603\n",
      "2024-01-01 14:19:21.427835: Current learning rate: 0.00435\n",
      "2024-01-01 14:21:26.927532: train_loss -0.9156\n",
      "2024-01-01 14:21:26.938535: val_loss -0.8275\n",
      "2024-01-01 14:21:26.947534: Pseudo dice [0.9276, 0.9449, 0.9447]\n",
      "2024-01-01 14:21:26.957533: Epoch time: 125.51 s\n",
      "2024-01-01 14:21:28.011559: \n",
      "2024-01-01 14:21:28.016652: Epoch 604\n",
      "2024-01-01 14:21:28.022566: Current learning rate: 0.00434\n",
      "2024-01-01 14:23:33.392698: train_loss -0.9144\n",
      "2024-01-01 14:23:33.402696: val_loss -0.824\n",
      "2024-01-01 14:23:33.412696: Pseudo dice [0.9231, 0.9453, 0.9428]\n",
      "2024-01-01 14:23:33.420696: Epoch time: 125.38 s\n",
      "2024-01-01 14:23:34.455234: \n",
      "2024-01-01 14:23:34.460303: Epoch 605\n",
      "2024-01-01 14:23:34.465306: Current learning rate: 0.00433\n",
      "2024-01-01 14:25:39.830330: train_loss -0.9152\n",
      "2024-01-01 14:25:39.839330: val_loss -0.8279\n",
      "2024-01-01 14:25:39.847332: Pseudo dice [0.9262, 0.9431, 0.942]\n",
      "2024-01-01 14:25:39.854330: Epoch time: 125.38 s\n",
      "2024-01-01 14:25:40.867890: \n",
      "2024-01-01 14:25:40.873892: Epoch 606\n",
      "2024-01-01 14:25:40.878892: Current learning rate: 0.00432\n",
      "2024-01-01 14:27:46.088073: train_loss -0.9197\n",
      "2024-01-01 14:27:46.097069: val_loss -0.8295\n",
      "2024-01-01 14:27:46.106068: Pseudo dice [0.9244, 0.9435, 0.9426]\n",
      "2024-01-01 14:27:46.113068: Epoch time: 125.22 s\n",
      "2024-01-01 14:27:47.142093: \n",
      "2024-01-01 14:27:47.153104: Epoch 607\n",
      "2024-01-01 14:27:47.160102: Current learning rate: 0.00431\n",
      "2024-01-01 14:29:52.379609: train_loss -0.9162\n",
      "2024-01-01 14:29:52.388610: val_loss -0.8289\n",
      "2024-01-01 14:29:52.395610: Pseudo dice [0.9229, 0.9436, 0.9425]\n",
      "2024-01-01 14:29:52.401611: Epoch time: 125.24 s\n",
      "2024-01-01 14:29:53.435633: \n",
      "2024-01-01 14:29:53.440688: Epoch 608\n",
      "2024-01-01 14:29:53.445798: Current learning rate: 0.0043\n",
      "2024-01-01 14:31:58.805311: train_loss -0.9154\n",
      "2024-01-01 14:31:58.815322: val_loss -0.8278\n",
      "2024-01-01 14:31:58.823426: Pseudo dice [0.9278, 0.9453, 0.9443]\n",
      "2024-01-01 14:31:58.831426: Epoch time: 125.37 s\n",
      "2024-01-01 14:31:59.892675: \n",
      "2024-01-01 14:31:59.905746: Epoch 609\n",
      "2024-01-01 14:31:59.910672: Current learning rate: 0.00429\n",
      "2024-01-01 14:34:05.261948: train_loss -0.9174\n",
      "2024-01-01 14:34:05.270948: val_loss -0.8293\n",
      "2024-01-01 14:34:05.279948: Pseudo dice [0.9264, 0.9439, 0.9414]\n",
      "2024-01-01 14:34:05.289948: Epoch time: 125.37 s\n",
      "2024-01-01 14:34:06.304010: \n",
      "2024-01-01 14:34:06.309086: Epoch 610\n",
      "2024-01-01 14:34:06.314199: Current learning rate: 0.00429\n",
      "2024-01-01 14:36:11.643817: train_loss -0.9191\n",
      "2024-01-01 14:36:11.653819: val_loss -0.8329\n",
      "2024-01-01 14:36:11.661818: Pseudo dice [0.9244, 0.944, 0.9428]\n",
      "2024-01-01 14:36:11.669817: Epoch time: 125.34 s\n",
      "2024-01-01 14:36:12.695477: \n",
      "2024-01-01 14:36:12.701483: Epoch 611\n",
      "2024-01-01 14:36:12.706477: Current learning rate: 0.00428\n",
      "2024-01-01 14:38:18.137362: train_loss -0.9174\n",
      "2024-01-01 14:38:18.146362: val_loss -0.8337\n",
      "2024-01-01 14:38:18.154361: Pseudo dice [0.9239, 0.9446, 0.9446]\n",
      "2024-01-01 14:38:18.161363: Epoch time: 125.44 s\n",
      "2024-01-01 14:38:19.180045: \n",
      "2024-01-01 14:38:19.186035: Epoch 612\n",
      "2024-01-01 14:38:19.191042: Current learning rate: 0.00427\n",
      "2024-01-01 14:40:24.290191: train_loss -0.9192\n",
      "2024-01-01 14:40:24.299191: val_loss -0.8333\n",
      "2024-01-01 14:40:24.307191: Pseudo dice [0.9262, 0.9468, 0.9455]\n",
      "2024-01-01 14:40:24.314191: Epoch time: 125.11 s\n",
      "2024-01-01 14:40:25.510995: \n",
      "2024-01-01 14:40:25.518566: Epoch 613\n",
      "2024-01-01 14:40:25.525651: Current learning rate: 0.00426\n",
      "2024-01-01 14:42:30.583816: train_loss -0.9189\n",
      "2024-01-01 14:42:30.592243: val_loss -0.8339\n",
      "2024-01-01 14:42:30.599247: Pseudo dice [0.9254, 0.9449, 0.9442]\n",
      "2024-01-01 14:42:30.607463: Epoch time: 125.07 s\n",
      "2024-01-01 14:42:31.633621: \n",
      "2024-01-01 14:42:31.639617: Epoch 614\n",
      "2024-01-01 14:42:31.644613: Current learning rate: 0.00425\n",
      "2024-01-01 14:44:37.117381: train_loss -0.916\n",
      "2024-01-01 14:44:37.126390: val_loss -0.8284\n",
      "2024-01-01 14:44:37.135392: Pseudo dice [0.9272, 0.9442, 0.9441]\n",
      "2024-01-01 14:44:37.141389: Epoch time: 125.48 s\n",
      "2024-01-01 14:44:38.163569: \n",
      "2024-01-01 14:44:38.169566: Epoch 615\n",
      "2024-01-01 14:44:38.173584: Current learning rate: 0.00424\n",
      "2024-01-01 14:46:43.056924: train_loss -0.919\n",
      "2024-01-01 14:46:43.068045: val_loss -0.8305\n",
      "2024-01-01 14:46:43.076054: Pseudo dice [0.9283, 0.9435, 0.9422]\n",
      "2024-01-01 14:46:43.086054: Epoch time: 124.89 s\n",
      "2024-01-01 14:46:44.126629: \n",
      "2024-01-01 14:46:44.132620: Epoch 616\n",
      "2024-01-01 14:46:44.137624: Current learning rate: 0.00423\n",
      "2024-01-01 14:48:49.463212: train_loss -0.9193\n",
      "2024-01-01 14:48:49.472237: val_loss -0.8366\n",
      "2024-01-01 14:48:49.481598: Pseudo dice [0.929, 0.9454, 0.9452]\n",
      "2024-01-01 14:48:49.489599: Epoch time: 125.34 s\n",
      "2024-01-01 14:48:50.542481: \n",
      "2024-01-01 14:48:50.554644: Epoch 617\n",
      "2024-01-01 14:48:50.559632: Current learning rate: 0.00422\n",
      "2024-01-01 14:50:55.889674: train_loss -0.9181\n",
      "2024-01-01 14:50:55.896675: val_loss -0.8374\n",
      "2024-01-01 14:50:55.905676: Pseudo dice [0.9274, 0.9446, 0.9455]\n",
      "2024-01-01 14:50:55.912675: Epoch time: 125.35 s\n",
      "2024-01-01 14:50:56.927925: \n",
      "2024-01-01 14:50:56.934997: Epoch 618\n",
      "2024-01-01 14:50:56.942051: Current learning rate: 0.00421\n",
      "2024-01-01 14:53:02.147124: train_loss -0.9198\n",
      "2024-01-01 14:53:02.154119: val_loss -0.822\n",
      "2024-01-01 14:53:02.162121: Pseudo dice [0.9248, 0.9441, 0.9445]\n",
      "2024-01-01 14:53:02.170121: Epoch time: 125.22 s\n",
      "2024-01-01 14:53:03.195575: \n",
      "2024-01-01 14:53:03.201667: Epoch 619\n",
      "2024-01-01 14:53:03.206571: Current learning rate: 0.0042\n",
      "2024-01-01 14:55:08.562811: train_loss -0.918\n",
      "2024-01-01 14:55:08.569811: val_loss -0.8286\n",
      "2024-01-01 14:55:08.576811: Pseudo dice [0.9253, 0.9459, 0.9461]\n",
      "2024-01-01 14:55:08.582811: Epoch time: 125.37 s\n",
      "2024-01-01 14:55:09.609591: \n",
      "2024-01-01 14:55:09.615679: Epoch 620\n",
      "2024-01-01 14:55:09.620674: Current learning rate: 0.00419\n",
      "2024-01-01 14:57:15.060007: train_loss -0.9173\n",
      "2024-01-01 14:57:15.070013: val_loss -0.8278\n",
      "2024-01-01 14:57:15.078521: Pseudo dice [0.9299, 0.9461, 0.9446]\n",
      "2024-01-01 14:57:15.085521: Epoch time: 125.45 s\n",
      "2024-01-01 14:57:16.323528: \n",
      "2024-01-01 14:57:16.330595: Epoch 621\n",
      "2024-01-01 14:57:16.336528: Current learning rate: 0.00418\n",
      "2024-01-01 14:59:22.192944: train_loss -0.914\n",
      "2024-01-01 14:59:22.201944: val_loss -0.8303\n",
      "2024-01-01 14:59:22.207947: Pseudo dice [0.9263, 0.9444, 0.9437]\n",
      "2024-01-01 14:59:22.213947: Epoch time: 125.87 s\n",
      "2024-01-01 14:59:23.245323: \n",
      "2024-01-01 14:59:23.251323: Epoch 622\n",
      "2024-01-01 14:59:23.257314: Current learning rate: 0.00417\n",
      "2024-01-01 15:01:28.547874: train_loss -0.9151\n",
      "2024-01-01 15:01:28.557878: val_loss -0.8327\n",
      "2024-01-01 15:01:28.565878: Pseudo dice [0.9284, 0.9464, 0.9449]\n",
      "2024-01-01 15:01:28.574385: Epoch time: 125.3 s\n",
      "2024-01-01 15:01:29.629129: \n",
      "2024-01-01 15:01:29.635200: Epoch 623\n",
      "2024-01-01 15:01:29.640134: Current learning rate: 0.00416\n",
      "2024-01-01 15:03:34.792832: train_loss -0.9204\n",
      "2024-01-01 15:03:34.800845: val_loss -0.8312\n",
      "2024-01-01 15:03:34.807843: Pseudo dice [0.9245, 0.9443, 0.9431]\n",
      "2024-01-01 15:03:34.815348: Epoch time: 125.16 s\n",
      "2024-01-01 15:03:35.827058: \n",
      "2024-01-01 15:03:35.833136: Epoch 624\n",
      "2024-01-01 15:03:35.838117: Current learning rate: 0.00415\n",
      "2024-01-01 15:05:43.962214: train_loss -0.9176\n",
      "2024-01-01 15:05:43.974219: val_loss -0.8288\n",
      "2024-01-01 15:05:43.984728: Pseudo dice [0.9274, 0.945, 0.944]\n",
      "2024-01-01 15:05:43.994727: Epoch time: 128.14 s\n",
      "2024-01-01 15:05:45.265408: \n",
      "2024-01-01 15:05:45.272401: Epoch 625\n",
      "2024-01-01 15:05:45.279906: Current learning rate: 0.00414\n",
      "2024-01-01 15:07:51.695966: train_loss -0.9174\n",
      "2024-01-01 15:07:51.705966: val_loss -0.8355\n",
      "2024-01-01 15:07:51.714966: Pseudo dice [0.9274, 0.9464, 0.9451]\n",
      "2024-01-01 15:07:51.722970: Epoch time: 126.43 s\n",
      "2024-01-01 15:07:52.900142: \n",
      "2024-01-01 15:07:52.906697: Epoch 626\n",
      "2024-01-01 15:07:52.911754: Current learning rate: 0.00413\n",
      "2024-01-01 15:09:59.025749: train_loss -0.9187\n",
      "2024-01-01 15:09:59.036080: val_loss -0.8279\n",
      "2024-01-01 15:09:59.044080: Pseudo dice [0.9294, 0.9452, 0.9437]\n",
      "2024-01-01 15:09:59.053416: Epoch time: 126.13 s\n",
      "2024-01-01 15:10:00.113914: \n",
      "2024-01-01 15:10:00.119940: Epoch 627\n",
      "2024-01-01 15:10:00.124938: Current learning rate: 0.00412\n",
      "2024-01-01 15:12:05.797608: train_loss -0.918\n",
      "2024-01-01 15:12:05.808608: val_loss -0.8269\n",
      "2024-01-01 15:12:05.815608: Pseudo dice [0.9251, 0.9441, 0.9443]\n",
      "2024-01-01 15:12:05.822607: Epoch time: 125.68 s\n",
      "2024-01-01 15:12:07.033196: \n",
      "2024-01-01 15:12:07.038197: Epoch 628\n",
      "2024-01-01 15:12:07.043189: Current learning rate: 0.00411\n",
      "2024-01-01 15:14:12.484542: train_loss -0.917\n",
      "2024-01-01 15:14:12.495538: val_loss -0.8318\n",
      "2024-01-01 15:14:12.503902: Pseudo dice [0.9269, 0.9466, 0.9445]\n",
      "2024-01-01 15:14:12.511904: Epoch time: 125.45 s\n",
      "2024-01-01 15:14:13.557434: \n",
      "2024-01-01 15:14:13.565462: Epoch 629\n",
      "2024-01-01 15:14:13.570986: Current learning rate: 0.0041\n",
      "2024-01-01 15:16:18.860788: train_loss -0.9177\n",
      "2024-01-01 15:16:18.868788: val_loss -0.8301\n",
      "2024-01-01 15:16:18.876295: Pseudo dice [0.9231, 0.9429, 0.9416]\n",
      "2024-01-01 15:16:18.882319: Epoch time: 125.3 s\n",
      "2024-01-01 15:16:19.902972: \n",
      "2024-01-01 15:16:19.911967: Epoch 630\n",
      "2024-01-01 15:16:19.917966: Current learning rate: 0.00409\n",
      "2024-01-01 15:18:25.015041: train_loss -0.9197\n",
      "2024-01-01 15:18:25.023041: val_loss -0.7982\n",
      "2024-01-01 15:18:25.032043: Pseudo dice [0.9157, 0.9386, 0.9362]\n",
      "2024-01-01 15:18:25.041041: Epoch time: 125.11 s\n",
      "2024-01-01 15:18:26.057159: \n",
      "2024-01-01 15:18:26.062160: Epoch 631\n",
      "2024-01-01 15:18:26.067160: Current learning rate: 0.00408\n",
      "2024-01-01 15:20:31.163242: train_loss -0.918\n",
      "2024-01-01 15:20:31.171243: val_loss -0.8267\n",
      "2024-01-01 15:20:31.179252: Pseudo dice [0.9243, 0.9442, 0.9436]\n",
      "2024-01-01 15:20:31.186250: Epoch time: 125.11 s\n",
      "2024-01-01 15:20:32.205792: \n",
      "2024-01-01 15:20:32.211998: Epoch 632\n",
      "2024-01-01 15:20:32.217001: Current learning rate: 0.00407\n",
      "2024-01-01 15:22:37.700012: train_loss -0.9147\n",
      "2024-01-01 15:22:37.709514: val_loss -0.7968\n",
      "2024-01-01 15:22:37.717520: Pseudo dice [0.9123, 0.9379, 0.9361]\n",
      "2024-01-01 15:22:37.725516: Epoch time: 125.5 s\n",
      "2024-01-01 15:22:38.821655: \n",
      "2024-01-01 15:22:38.827653: Epoch 633\n",
      "2024-01-01 15:22:38.832652: Current learning rate: 0.00406\n",
      "2024-01-01 15:24:44.098514: train_loss -0.9142\n",
      "2024-01-01 15:24:44.107521: val_loss -0.81\n",
      "2024-01-01 15:24:44.116586: Pseudo dice [0.9181, 0.9388, 0.9372]\n",
      "2024-01-01 15:24:44.125101: Epoch time: 125.28 s\n",
      "2024-01-01 15:24:45.147979: \n",
      "2024-01-01 15:24:45.153970: Epoch 634\n",
      "2024-01-01 15:24:45.157979: Current learning rate: 0.00405\n",
      "2024-01-01 15:26:50.575976: train_loss -0.9188\n",
      "2024-01-01 15:26:50.585974: val_loss -0.8215\n",
      "2024-01-01 15:26:50.593975: Pseudo dice [0.9271, 0.9445, 0.9431]\n",
      "2024-01-01 15:26:50.602008: Epoch time: 125.43 s\n",
      "2024-01-01 15:26:51.624421: \n",
      "2024-01-01 15:26:51.631429: Epoch 635\n",
      "2024-01-01 15:26:51.635438: Current learning rate: 0.00404\n",
      "2024-01-01 15:28:58.426881: train_loss -0.9149\n",
      "2024-01-01 15:28:58.438391: val_loss -0.8295\n",
      "2024-01-01 15:28:58.448388: Pseudo dice [0.9277, 0.9454, 0.9445]\n",
      "2024-01-01 15:28:58.458389: Epoch time: 126.8 s\n",
      "2024-01-01 15:28:59.868774: \n",
      "2024-01-01 15:28:59.880547: Epoch 636\n",
      "2024-01-01 15:28:59.887561: Current learning rate: 0.00403\n",
      "2024-01-01 15:31:15.056246: train_loss -0.9152\n",
      "2024-01-01 15:31:15.109536: val_loss -0.8262\n",
      "2024-01-01 15:31:15.131573: Pseudo dice [0.9239, 0.9437, 0.9417]\n",
      "2024-01-01 15:31:15.148620: Epoch time: 135.19 s\n",
      "2024-01-01 15:31:17.435127: \n",
      "2024-01-01 15:31:17.444707: Epoch 637\n",
      "2024-01-01 15:31:17.454663: Current learning rate: 0.00402\n",
      "2024-01-01 15:33:28.083788: train_loss -0.92\n",
      "2024-01-01 15:33:28.101979: val_loss -0.8254\n",
      "2024-01-01 15:33:28.117022: Pseudo dice [0.9286, 0.9449, 0.9447]\n",
      "2024-01-01 15:33:28.130549: Epoch time: 130.65 s\n",
      "2024-01-01 15:33:29.781165: \n",
      "2024-01-01 15:33:29.789084: Epoch 638\n",
      "2024-01-01 15:33:29.795174: Current learning rate: 0.00401\n",
      "2024-01-01 15:35:40.458095: train_loss -0.9206\n",
      "2024-01-01 15:35:40.474096: val_loss -0.8249\n",
      "2024-01-01 15:35:40.488378: Pseudo dice [0.9261, 0.9439, 0.9433]\n",
      "2024-01-01 15:35:40.506377: Epoch time: 130.68 s\n",
      "2024-01-01 15:35:42.760242: \n",
      "2024-01-01 15:35:42.771236: Epoch 639\n",
      "2024-01-01 15:35:42.779236: Current learning rate: 0.004\n",
      "2024-01-01 15:37:59.627784: train_loss -0.9179\n",
      "2024-01-01 15:37:59.672876: val_loss -0.8269\n",
      "2024-01-01 15:37:59.743193: Pseudo dice [0.9256, 0.9448, 0.9441]\n",
      "2024-01-01 15:37:59.773747: Epoch time: 136.87 s\n",
      "2024-01-01 15:38:02.114480: \n",
      "2024-01-01 15:38:02.120990: Epoch 640\n",
      "2024-01-01 15:38:02.128497: Current learning rate: 0.00399\n",
      "2024-01-01 15:40:29.063557: train_loss -0.9139\n",
      "2024-01-01 15:40:29.082558: val_loss -0.8073\n",
      "2024-01-01 15:40:29.096565: Pseudo dice [0.9145, 0.939, 0.9358]\n",
      "2024-01-01 15:40:29.108567: Epoch time: 146.95 s\n",
      "2024-01-01 15:40:30.875345: \n",
      "2024-01-01 15:40:30.885344: Epoch 641\n",
      "2024-01-01 15:40:30.895345: Current learning rate: 0.00398\n",
      "2024-01-01 15:42:39.399053: train_loss -0.9171\n",
      "2024-01-01 15:42:39.411054: val_loss -0.8371\n",
      "2024-01-01 15:42:39.423053: Pseudo dice [0.928, 0.9457, 0.9451]\n",
      "2024-01-01 15:42:39.433055: Epoch time: 128.53 s\n",
      "2024-01-01 15:42:40.604997: \n",
      "2024-01-01 15:42:40.610993: Epoch 642\n",
      "2024-01-01 15:42:40.615993: Current learning rate: 0.00397\n",
      "2024-01-01 15:44:48.180939: train_loss -0.922\n",
      "2024-01-01 15:44:48.189478: val_loss -0.827\n",
      "2024-01-01 15:44:48.199469: Pseudo dice [0.9284, 0.9452, 0.945]\n",
      "2024-01-01 15:44:48.207467: Epoch time: 127.58 s\n",
      "2024-01-01 15:44:49.533767: \n",
      "2024-01-01 15:44:49.539767: Epoch 643\n",
      "2024-01-01 15:44:49.544768: Current learning rate: 0.00396\n",
      "2024-01-01 15:47:14.924003: train_loss -0.9202\n",
      "2024-01-01 15:47:14.937150: val_loss -0.8252\n",
      "2024-01-01 15:47:14.950413: Pseudo dice [0.9264, 0.9441, 0.9449]\n",
      "2024-01-01 15:47:14.965809: Epoch time: 145.39 s\n",
      "2024-01-01 15:47:16.698567: \n",
      "2024-01-01 15:47:16.710364: Epoch 644\n",
      "2024-01-01 15:47:16.720366: Current learning rate: 0.00395\n",
      "2024-01-01 15:49:25.065908: train_loss -0.9193\n",
      "2024-01-01 15:49:25.078134: val_loss -0.8294\n",
      "2024-01-01 15:49:25.094138: Pseudo dice [0.9259, 0.9461, 0.9448]\n",
      "2024-01-01 15:49:25.107749: Epoch time: 128.37 s\n",
      "2024-01-01 15:49:26.322529: \n",
      "2024-01-01 15:49:26.331533: Epoch 645\n",
      "2024-01-01 15:49:26.337534: Current learning rate: 0.00394\n",
      "2024-01-01 15:51:37.200967: train_loss -0.9174\n",
      "2024-01-01 15:51:37.222220: val_loss -0.8332\n",
      "2024-01-01 15:51:37.240218: Pseudo dice [0.9263, 0.944, 0.9428]\n",
      "2024-01-01 15:51:37.274833: Epoch time: 130.88 s\n",
      "2024-01-01 15:51:39.131981: \n",
      "2024-01-01 15:51:39.141982: Epoch 646\n",
      "2024-01-01 15:51:39.149985: Current learning rate: 0.00393\n",
      "2024-01-01 15:53:48.130214: train_loss -0.9173\n",
      "2024-01-01 15:53:48.144214: val_loss -0.8301\n",
      "2024-01-01 15:53:48.156215: Pseudo dice [0.9287, 0.9463, 0.9457]\n",
      "2024-01-01 15:53:48.166010: Epoch time: 129.0 s\n",
      "2024-01-01 15:53:49.574611: \n",
      "2024-01-01 15:53:49.581609: Epoch 647\n",
      "2024-01-01 15:53:49.588608: Current learning rate: 0.00392\n",
      "2024-01-01 15:55:57.750939: train_loss -0.9166\n",
      "2024-01-01 15:55:57.762797: val_loss -0.8283\n",
      "2024-01-01 15:55:57.772222: Pseudo dice [0.9271, 0.9454, 0.9442]\n",
      "2024-01-01 15:55:57.782217: Epoch time: 128.18 s\n",
      "2024-01-01 15:55:59.002080: \n",
      "2024-01-01 15:55:59.009096: Epoch 648\n",
      "2024-01-01 15:55:59.014097: Current learning rate: 0.00391\n",
      "2024-01-01 15:58:08.314013: train_loss -0.9173\n",
      "2024-01-01 15:58:08.325663: val_loss -0.833\n",
      "2024-01-01 15:58:08.335193: Pseudo dice [0.9264, 0.9445, 0.9445]\n",
      "2024-01-01 15:58:08.345193: Epoch time: 129.31 s\n",
      "2024-01-01 15:58:09.546059: \n",
      "2024-01-01 15:58:09.553059: Epoch 649\n",
      "2024-01-01 15:58:09.559459: Current learning rate: 0.0039\n",
      "2024-01-01 16:00:18.027252: train_loss -0.9168\n",
      "2024-01-01 16:00:18.037776: val_loss -0.8331\n",
      "2024-01-01 16:00:18.046837: Pseudo dice [0.9272, 0.9459, 0.9447]\n",
      "2024-01-01 16:00:18.055836: Epoch time: 128.48 s\n",
      "2024-01-01 16:00:19.677114: \n",
      "2024-01-01 16:00:19.684117: Epoch 650\n",
      "2024-01-01 16:00:19.690114: Current learning rate: 0.00389\n",
      "2024-01-01 16:02:32.597118: train_loss -0.9181\n",
      "2024-01-01 16:02:32.616304: val_loss -0.8272\n",
      "2024-01-01 16:02:32.632590: Pseudo dice [0.924, 0.9454, 0.9444]\n",
      "2024-01-01 16:02:32.649951: Epoch time: 132.92 s\n",
      "2024-01-01 16:02:34.272939: \n",
      "2024-01-01 16:02:34.279929: Epoch 651\n",
      "2024-01-01 16:02:34.285474: Current learning rate: 0.00388\n",
      "2024-01-01 16:04:42.980095: train_loss -0.9197\n",
      "2024-01-01 16:04:42.989094: val_loss -0.8256\n",
      "2024-01-01 16:04:42.999225: Pseudo dice [0.9272, 0.9453, 0.9448]\n",
      "2024-01-01 16:04:43.009224: Epoch time: 128.71 s\n",
      "2024-01-01 16:04:44.253967: \n",
      "2024-01-01 16:04:44.260909: Epoch 652\n",
      "2024-01-01 16:04:44.266913: Current learning rate: 0.00387\n",
      "2024-01-01 16:06:54.007590: train_loss -0.9169\n",
      "2024-01-01 16:06:54.019586: val_loss -0.8361\n",
      "2024-01-01 16:06:54.029586: Pseudo dice [0.9275, 0.9448, 0.9429]\n",
      "2024-01-01 16:06:54.039587: Epoch time: 129.75 s\n",
      "2024-01-01 16:06:55.307516: \n",
      "2024-01-01 16:06:55.313515: Epoch 653\n",
      "2024-01-01 16:06:55.318517: Current learning rate: 0.00386\n",
      "2024-01-01 16:09:04.953334: train_loss -0.9169\n",
      "2024-01-01 16:09:04.965337: val_loss -0.8308\n",
      "2024-01-01 16:09:04.974898: Pseudo dice [0.9249, 0.9442, 0.943]\n",
      "2024-01-01 16:09:04.985402: Epoch time: 129.65 s\n",
      "2024-01-01 16:09:06.434085: \n",
      "2024-01-01 16:09:06.440092: Epoch 654\n",
      "2024-01-01 16:09:06.449080: Current learning rate: 0.00385\n",
      "2024-01-01 16:11:14.631053: train_loss -0.9197\n",
      "2024-01-01 16:11:14.642053: val_loss -0.8277\n",
      "2024-01-01 16:11:14.652054: Pseudo dice [0.9261, 0.9456, 0.9452]\n",
      "2024-01-01 16:11:14.662055: Epoch time: 128.2 s\n",
      "2024-01-01 16:11:16.143220: \n",
      "2024-01-01 16:11:16.151222: Epoch 655\n",
      "2024-01-01 16:11:16.157214: Current learning rate: 0.00384\n",
      "2024-01-01 16:13:25.449535: train_loss -0.9199\n",
      "2024-01-01 16:13:25.460798: val_loss -0.8299\n",
      "2024-01-01 16:13:25.469797: Pseudo dice [0.9255, 0.9446, 0.9439]\n",
      "2024-01-01 16:13:25.479402: Epoch time: 129.31 s\n",
      "2024-01-01 16:13:26.692886: \n",
      "2024-01-01 16:13:26.698426: Epoch 656\n",
      "2024-01-01 16:13:26.703426: Current learning rate: 0.00383\n",
      "2024-01-01 16:15:36.674219: train_loss -0.9202\n",
      "2024-01-01 16:15:36.684218: val_loss -0.832\n",
      "2024-01-01 16:15:36.695218: Pseudo dice [0.9271, 0.946, 0.9442]\n",
      "2024-01-01 16:15:36.706219: Epoch time: 129.98 s\n",
      "2024-01-01 16:15:37.912027: \n",
      "2024-01-01 16:15:37.919064: Epoch 657\n",
      "2024-01-01 16:15:37.925063: Current learning rate: 0.00382\n",
      "2024-01-01 16:17:49.780514: train_loss -0.9156\n",
      "2024-01-01 16:17:49.866908: val_loss -0.8298\n",
      "2024-01-01 16:17:49.879905: Pseudo dice [0.9285, 0.9456, 0.9445]\n",
      "2024-01-01 16:17:49.891906: Epoch time: 131.87 s\n",
      "2024-01-01 16:17:53.199854: \n",
      "2024-01-01 16:17:53.213859: Epoch 658\n",
      "2024-01-01 16:17:53.225869: Current learning rate: 0.00381\n",
      "2024-01-01 16:20:13.985173: train_loss -0.9188\n",
      "2024-01-01 16:20:14.000184: val_loss -0.8325\n",
      "2024-01-01 16:20:14.012696: Pseudo dice [0.9252, 0.9457, 0.9442]\n",
      "2024-01-01 16:20:14.024695: Epoch time: 140.79 s\n",
      "2024-01-01 16:20:15.920059: \n",
      "2024-01-01 16:20:15.934057: Epoch 659\n",
      "2024-01-01 16:20:15.945301: Current learning rate: 0.0038\n",
      "2024-01-01 16:22:24.902677: train_loss -0.9167\n",
      "2024-01-01 16:22:24.945747: val_loss -0.8322\n",
      "2024-01-01 16:22:24.970751: Pseudo dice [0.927, 0.9455, 0.9442]\n",
      "2024-01-01 16:22:24.986264: Epoch time: 128.98 s\n",
      "2024-01-01 16:22:26.132545: \n",
      "2024-01-01 16:22:26.138534: Epoch 660\n",
      "2024-01-01 16:22:26.144533: Current learning rate: 0.00379\n",
      "2024-01-01 16:24:53.233368: train_loss -0.9177\n",
      "2024-01-01 16:24:53.259887: val_loss -0.8259\n",
      "2024-01-01 16:24:53.276886: Pseudo dice [0.9272, 0.9452, 0.9455]\n",
      "2024-01-01 16:24:53.288775: Epoch time: 147.1 s\n",
      "2024-01-01 16:24:55.409901: \n",
      "2024-01-01 16:24:55.424904: Epoch 661\n",
      "2024-01-01 16:24:55.436945: Current learning rate: 0.00378\n",
      "2024-01-01 16:27:05.204986: train_loss -0.9185\n",
      "2024-01-01 16:27:05.220904: val_loss -0.8306\n",
      "2024-01-01 16:27:05.234515: Pseudo dice [0.9293, 0.9458, 0.9447]\n",
      "2024-01-01 16:27:05.246515: Epoch time: 129.8 s\n",
      "2024-01-01 16:27:07.086187: \n",
      "2024-01-01 16:27:07.093181: Epoch 662\n",
      "2024-01-01 16:27:07.099187: Current learning rate: 0.00377\n",
      "2024-01-01 16:29:41.654343: train_loss -0.9167\n",
      "2024-01-01 16:29:41.676595: val_loss -0.8354\n",
      "2024-01-01 16:29:41.701896: Pseudo dice [0.9267, 0.9449, 0.9442]\n",
      "2024-01-01 16:29:41.721404: Epoch time: 154.57 s\n",
      "2024-01-01 16:29:44.650229: \n",
      "2024-01-01 16:29:44.662220: Epoch 663\n",
      "2024-01-01 16:29:44.677217: Current learning rate: 0.00376\n",
      "2024-01-01 16:32:38.550445: train_loss -0.9171\n",
      "2024-01-01 16:32:38.599270: val_loss -0.8302\n",
      "2024-01-01 16:32:38.638271: Pseudo dice [0.9272, 0.9455, 0.9452]\n",
      "2024-01-01 16:32:38.695797: Epoch time: 173.9 s\n",
      "2024-01-01 16:32:42.688517: \n",
      "2024-01-01 16:32:42.703320: Epoch 664\n",
      "2024-01-01 16:32:42.718232: Current learning rate: 0.00375\n",
      "2024-01-01 16:35:35.436102: train_loss -0.9162\n",
      "2024-01-01 16:35:35.505326: val_loss -0.8395\n",
      "2024-01-01 16:35:35.571918: Pseudo dice [0.9276, 0.9465, 0.9452]\n",
      "2024-01-01 16:35:35.659120: Epoch time: 172.75 s\n",
      "2024-01-01 16:35:39.073745: \n",
      "2024-01-01 16:35:39.083752: Epoch 665\n",
      "2024-01-01 16:35:39.096746: Current learning rate: 0.00374\n",
      "2024-01-01 16:38:20.617038: train_loss -0.9187\n",
      "2024-01-01 16:38:20.633035: val_loss -0.8374\n",
      "2024-01-01 16:38:20.646036: Pseudo dice [0.9258, 0.9468, 0.9451]\n",
      "2024-01-01 16:38:20.657043: Epoch time: 161.55 s\n",
      "2024-01-01 16:38:23.872968: \n",
      "2024-01-01 16:38:23.883967: Epoch 666\n",
      "2024-01-01 16:38:23.897274: Current learning rate: 0.00373\n",
      "2024-01-01 16:41:14.418532: train_loss -0.9205\n",
      "2024-01-01 16:41:14.596439: val_loss -0.8313\n",
      "2024-01-01 16:41:14.772469: Pseudo dice [0.9258, 0.9452, 0.9445]\n",
      "2024-01-01 16:41:14.936908: Epoch time: 170.55 s\n",
      "2024-01-01 16:41:18.426322: \n",
      "2024-01-01 16:41:18.433338: Epoch 667\n",
      "2024-01-01 16:41:18.440327: Current learning rate: 0.00372\n",
      "2024-01-01 16:44:06.488475: train_loss -0.9178\n",
      "2024-01-01 16:44:06.571127: val_loss -0.8293\n",
      "2024-01-01 16:44:06.699649: Pseudo dice [0.9255, 0.9461, 0.9451]\n",
      "2024-01-01 16:44:06.812472: Epoch time: 168.06 s\n",
      "2024-01-01 16:44:10.340900: \n",
      "2024-01-01 16:44:10.351913: Epoch 668\n",
      "2024-01-01 16:44:10.364911: Current learning rate: 0.00371\n",
      "2024-01-01 16:46:47.561304: train_loss -0.9188\n",
      "2024-01-01 16:46:47.693823: val_loss -0.8308\n",
      "2024-01-01 16:46:47.789338: Pseudo dice [0.9273, 0.9443, 0.9443]\n",
      "2024-01-01 16:46:47.898165: Epoch time: 157.22 s\n",
      "2024-01-01 16:46:50.776341: \n",
      "2024-01-01 16:46:50.783349: Epoch 669\n",
      "2024-01-01 16:46:50.792404: Current learning rate: 0.0037\n",
      "2024-01-01 16:49:24.765549: train_loss -0.9195\n",
      "2024-01-01 16:49:24.941947: val_loss -0.8315\n",
      "2024-01-01 16:49:25.096983: Pseudo dice [0.9263, 0.9457, 0.9439]\n",
      "2024-01-01 16:49:25.213513: Epoch time: 153.99 s\n",
      "2024-01-01 16:49:28.359237: \n",
      "2024-01-01 16:49:28.368741: Epoch 670\n",
      "2024-01-01 16:49:28.378838: Current learning rate: 0.00369\n",
      "2024-01-01 16:52:09.630878: train_loss -0.9119\n",
      "2024-01-01 16:52:09.640877: val_loss -0.8422\n",
      "2024-01-01 16:52:09.651879: Pseudo dice [0.9268, 0.9453, 0.9439]\n",
      "2024-01-01 16:52:09.661878: Epoch time: 161.27 s\n",
      "2024-01-01 16:52:11.807319: \n",
      "2024-01-01 16:52:11.817393: Epoch 671\n",
      "2024-01-01 16:52:11.824322: Current learning rate: 0.00368\n",
      "2024-01-01 16:54:45.743628: train_loss -0.9166\n",
      "2024-01-01 16:54:45.810606: val_loss -0.8334\n",
      "2024-01-01 16:54:45.841607: Pseudo dice [0.9246, 0.9445, 0.9426]\n",
      "2024-01-01 16:54:45.870606: Epoch time: 153.94 s\n",
      "2024-01-01 16:54:48.736158: \n",
      "2024-01-01 16:54:48.747161: Epoch 672\n",
      "2024-01-01 16:54:48.758160: Current learning rate: 0.00367\n",
      "2024-01-01 16:57:20.902407: train_loss -0.919\n",
      "2024-01-01 16:57:20.913571: val_loss -0.8333\n",
      "2024-01-01 16:57:20.926569: Pseudo dice [0.9258, 0.9446, 0.9442]\n",
      "2024-01-01 16:57:20.937917: Epoch time: 152.17 s\n",
      "2024-01-01 16:57:23.588284: \n",
      "2024-01-01 16:57:23.599794: Epoch 673\n",
      "2024-01-01 16:57:23.610794: Current learning rate: 0.00366\n",
      "2024-01-01 16:59:58.919952: train_loss -0.9186\n",
      "2024-01-01 16:59:59.000473: val_loss -0.8266\n",
      "2024-01-01 16:59:59.014472: Pseudo dice [0.9278, 0.9456, 0.944]\n",
      "2024-01-01 16:59:59.026474: Epoch time: 155.33 s\n",
      "2024-01-01 17:00:01.755365: \n",
      "2024-01-01 17:00:01.767485: Epoch 674\n",
      "2024-01-01 17:00:01.778545: Current learning rate: 0.00365\n",
      "2024-01-01 17:02:42.281116: train_loss -0.9172\n",
      "2024-01-01 17:02:42.299124: val_loss -0.8235\n",
      "2024-01-01 17:02:42.317639: Pseudo dice [0.9199, 0.942, 0.9417]\n",
      "2024-01-01 17:02:42.332798: Epoch time: 160.53 s\n",
      "2024-01-01 17:02:44.701553: \n",
      "2024-01-01 17:02:44.713070: Epoch 675\n",
      "2024-01-01 17:02:44.725070: Current learning rate: 0.00364\n",
      "2024-01-01 17:05:21.051348: train_loss -0.9198\n",
      "2024-01-01 17:05:21.066509: val_loss -0.8328\n",
      "2024-01-01 17:05:21.078510: Pseudo dice [0.9271, 0.9449, 0.9445]\n",
      "2024-01-01 17:05:21.098023: Epoch time: 156.35 s\n",
      "2024-01-01 17:05:24.194350: \n",
      "2024-01-01 17:05:24.205862: Epoch 676\n",
      "2024-01-01 17:05:24.216864: Current learning rate: 0.00363\n",
      "2024-01-01 17:07:49.387076: train_loss -0.9173\n",
      "2024-01-01 17:07:49.427076: val_loss -0.8309\n",
      "2024-01-01 17:07:49.455076: Pseudo dice [0.9251, 0.9435, 0.9431]\n",
      "2024-01-01 17:07:49.475594: Epoch time: 145.19 s\n",
      "2024-01-01 17:07:51.940729: \n",
      "2024-01-01 17:07:51.950729: Epoch 677\n",
      "2024-01-01 17:07:51.960734: Current learning rate: 0.00362\n",
      "2024-01-01 17:10:15.528293: train_loss -0.9199\n",
      "2024-01-01 17:10:15.551377: val_loss -0.8255\n",
      "2024-01-01 17:10:15.583130: Pseudo dice [0.9263, 0.9448, 0.9443]\n",
      "2024-01-01 17:10:15.600129: Epoch time: 143.59 s\n",
      "2024-01-01 17:10:17.896839: \n",
      "2024-01-01 17:10:17.908235: Epoch 678\n",
      "2024-01-01 17:10:17.917905: Current learning rate: 0.00361\n",
      "2024-01-01 17:12:33.037073: train_loss -0.9187\n",
      "2024-01-01 17:12:33.080074: val_loss -0.8361\n",
      "2024-01-01 17:12:33.107590: Pseudo dice [0.927, 0.946, 0.9448]\n",
      "2024-01-01 17:12:33.123589: Epoch time: 135.14 s\n",
      "2024-01-01 17:12:35.556903: \n",
      "2024-01-01 17:12:35.567915: Epoch 679\n",
      "2024-01-01 17:12:35.578904: Current learning rate: 0.0036\n",
      "2024-01-01 17:14:57.031723: train_loss -0.9188\n",
      "2024-01-01 17:14:57.045252: val_loss -0.8341\n",
      "2024-01-01 17:14:57.056250: Pseudo dice [0.9256, 0.9458, 0.945]\n",
      "2024-01-01 17:14:57.066257: Epoch time: 141.48 s\n",
      "2024-01-01 17:14:59.368123: \n",
      "2024-01-01 17:14:59.377633: Epoch 680\n",
      "2024-01-01 17:14:59.385634: Current learning rate: 0.00359\n",
      "2024-01-01 17:17:26.369066: train_loss -0.9184\n",
      "2024-01-01 17:17:26.403067: val_loss -0.8237\n",
      "2024-01-01 17:17:26.417065: Pseudo dice [0.9221, 0.9418, 0.9406]\n",
      "2024-01-01 17:17:26.433071: Epoch time: 147.0 s\n",
      "2024-01-01 17:17:28.255330: \n",
      "2024-01-01 17:17:28.265328: Epoch 681\n",
      "2024-01-01 17:17:28.277330: Current learning rate: 0.00358\n",
      "2024-01-01 17:19:56.509115: train_loss -0.9173\n",
      "2024-01-01 17:19:56.520116: val_loss -0.8187\n",
      "2024-01-01 17:19:56.529629: Pseudo dice [0.92, 0.9427, 0.941]\n",
      "2024-01-01 17:19:56.540629: Epoch time: 148.25 s\n",
      "2024-01-01 17:19:58.514184: \n",
      "2024-01-01 17:19:58.521699: Epoch 682\n",
      "2024-01-01 17:19:58.528700: Current learning rate: 0.00357\n",
      "2024-01-01 17:22:23.682920: train_loss -0.9195\n",
      "2024-01-01 17:22:23.699435: val_loss -0.8303\n",
      "2024-01-01 17:22:23.713435: Pseudo dice [0.9239, 0.9442, 0.9432]\n",
      "2024-01-01 17:22:23.728435: Epoch time: 145.17 s\n",
      "2024-01-01 17:22:25.824545: \n",
      "2024-01-01 17:22:25.834545: Epoch 683\n",
      "2024-01-01 17:22:25.845548: Current learning rate: 0.00356\n",
      "2024-01-01 17:24:49.034715: train_loss -0.9182\n",
      "2024-01-01 17:24:49.058723: val_loss -0.8217\n",
      "2024-01-01 17:24:49.075235: Pseudo dice [0.9208, 0.9415, 0.9414]\n",
      "2024-01-01 17:24:49.105237: Epoch time: 143.21 s\n",
      "2024-01-01 17:24:50.975314: \n",
      "2024-01-01 17:24:50.985314: Epoch 684\n",
      "2024-01-01 17:24:50.993316: Current learning rate: 0.00355\n",
      "2024-01-01 17:27:10.097447: train_loss -0.92\n",
      "2024-01-01 17:27:10.115458: val_loss -0.8252\n",
      "2024-01-01 17:27:10.126971: Pseudo dice [0.926, 0.9442, 0.9435]\n",
      "2024-01-01 17:27:10.138969: Epoch time: 139.12 s\n",
      "2024-01-01 17:27:12.905996: \n",
      "2024-01-01 17:27:12.918000: Epoch 685\n",
      "2024-01-01 17:27:12.927504: Current learning rate: 0.00354\n",
      "2024-01-01 17:29:37.291751: train_loss -0.92\n",
      "2024-01-01 17:29:37.437775: val_loss -0.8285\n",
      "2024-01-01 17:29:37.542573: Pseudo dice [0.927, 0.9442, 0.9435]\n",
      "2024-01-01 17:29:37.633089: Epoch time: 144.39 s\n",
      "2024-01-01 17:29:40.478883: \n",
      "2024-01-01 17:29:40.490890: Epoch 686\n",
      "2024-01-01 17:29:40.503400: Current learning rate: 0.00353\n",
      "2024-01-01 17:32:52.284404: train_loss -0.9179\n",
      "2024-01-01 17:32:52.319402: val_loss -0.8246\n",
      "2024-01-01 17:32:52.332878: Pseudo dice [0.9244, 0.9429, 0.9428]\n",
      "2024-01-01 17:32:52.347394: Epoch time: 191.81 s\n",
      "2024-01-01 17:32:55.637729: \n",
      "2024-01-01 17:32:55.650245: Epoch 687\n",
      "2024-01-01 17:32:55.661242: Current learning rate: 0.00352\n",
      "2024-01-01 17:35:58.029114: train_loss -0.9169\n",
      "2024-01-01 17:35:58.086629: val_loss -0.8304\n",
      "2024-01-01 17:35:58.132370: Pseudo dice [0.9249, 0.9461, 0.9451]\n",
      "2024-01-01 17:35:58.165881: Epoch time: 182.39 s\n",
      "2024-01-01 17:36:00.910196: \n",
      "2024-01-01 17:36:00.935456: Epoch 688\n",
      "2024-01-01 17:36:00.956459: Current learning rate: 0.00351\n",
      "2024-01-01 17:38:43.449116: train_loss -0.9202\n",
      "2024-01-01 17:38:43.466112: val_loss -0.8351\n",
      "2024-01-01 17:38:43.483109: Pseudo dice [0.9279, 0.9439, 0.9433]\n",
      "2024-01-01 17:38:43.593302: Epoch time: 162.54 s\n",
      "2024-01-01 17:38:46.412169: \n",
      "2024-01-01 17:38:46.423169: Epoch 689\n",
      "2024-01-01 17:38:46.434169: Current learning rate: 0.0035\n",
      "2024-01-01 17:41:22.352786: train_loss -0.9186\n",
      "2024-01-01 17:41:22.368830: val_loss -0.8254\n",
      "2024-01-01 17:41:22.384854: Pseudo dice [0.9243, 0.944, 0.9424]\n",
      "2024-01-01 17:41:22.404911: Epoch time: 155.94 s\n",
      "2024-01-01 17:41:25.693492: \n",
      "2024-01-01 17:41:25.701802: Epoch 690\n",
      "2024-01-01 17:41:25.708811: Current learning rate: 0.00349\n",
      "2024-01-01 17:44:10.242524: train_loss -0.9191\n",
      "2024-01-01 17:44:10.323343: val_loss -0.828\n",
      "2024-01-01 17:44:10.378001: Pseudo dice [0.9269, 0.9451, 0.9451]\n",
      "2024-01-01 17:44:10.462342: Epoch time: 164.55 s\n",
      "2024-01-01 17:44:13.774019: \n",
      "2024-01-01 17:44:13.786762: Epoch 691\n",
      "2024-01-01 17:44:13.799379: Current learning rate: 0.00348\n",
      "2024-01-01 17:46:58.106983: train_loss -0.9208\n",
      "2024-01-01 17:46:58.144981: val_loss -0.8322\n",
      "2024-01-01 17:46:58.175497: Pseudo dice [0.9251, 0.9443, 0.9425]\n",
      "2024-01-01 17:46:58.205497: Epoch time: 164.34 s\n",
      "2024-01-01 17:47:00.825053: \n",
      "2024-01-01 17:47:00.843054: Epoch 692\n",
      "2024-01-01 17:47:00.855056: Current learning rate: 0.00346\n",
      "2024-01-01 17:49:54.694007: train_loss -0.9208\n",
      "2024-01-01 17:49:54.865049: val_loss -0.8275\n",
      "2024-01-01 17:49:55.003113: Pseudo dice [0.9267, 0.9442, 0.9442]\n",
      "2024-01-01 17:49:55.117860: Epoch time: 173.87 s\n",
      "2024-01-01 17:49:57.984525: \n",
      "2024-01-01 17:49:57.993923: Epoch 693\n",
      "2024-01-01 17:49:58.002143: Current learning rate: 0.00345\n",
      "2024-01-01 17:52:53.555844: train_loss -0.9175\n",
      "2024-01-01 17:52:53.595426: val_loss -0.8333\n",
      "2024-01-01 17:52:53.639764: Pseudo dice [0.9268, 0.946, 0.9453]\n",
      "2024-01-01 17:52:53.690184: Epoch time: 175.57 s\n",
      "2024-01-01 17:52:56.172171: \n",
      "2024-01-01 17:52:56.183356: Epoch 694\n",
      "2024-01-01 17:52:56.190401: Current learning rate: 0.00344\n",
      "2024-01-01 17:55:49.987044: train_loss -0.9189\n",
      "2024-01-01 17:55:50.037100: val_loss -0.8324\n",
      "2024-01-01 17:55:50.077717: Pseudo dice [0.9265, 0.9448, 0.9439]\n",
      "2024-01-01 17:55:50.134456: Epoch time: 173.82 s\n",
      "2024-01-01 17:55:54.186295: \n",
      "2024-01-01 17:55:54.194372: Epoch 695\n",
      "2024-01-01 17:55:54.204428: Current learning rate: 0.00343\n",
      "2024-01-01 17:58:43.145756: train_loss -0.9207\n",
      "2024-01-01 17:58:43.162757: val_loss -0.8341\n",
      "2024-01-01 17:58:43.221033: Pseudo dice [0.9268, 0.9453, 0.9447]\n",
      "2024-01-01 17:58:43.273526: Epoch time: 168.96 s\n",
      "2024-01-01 17:58:46.777912: \n",
      "2024-01-01 17:58:46.790913: Epoch 696\n",
      "2024-01-01 17:58:46.809425: Current learning rate: 0.00342\n",
      "2024-01-01 18:01:18.793820: train_loss -0.9228\n",
      "2024-01-01 18:01:18.806300: val_loss -0.8294\n",
      "2024-01-01 18:01:18.871980: Pseudo dice [0.9269, 0.9451, 0.9439]\n",
      "2024-01-01 18:01:19.013768: Epoch time: 152.02 s\n",
      "2024-01-01 18:01:22.725697: \n",
      "2024-01-01 18:01:22.737659: Epoch 697\n",
      "2024-01-01 18:01:22.749659: Current learning rate: 0.00341\n",
      "2024-01-01 18:03:53.920747: train_loss -0.9207\n",
      "2024-01-01 18:03:53.998265: val_loss -0.8302\n",
      "2024-01-01 18:03:54.047803: Pseudo dice [0.9268, 0.9451, 0.9433]\n",
      "2024-01-01 18:03:54.062800: Epoch time: 151.2 s\n",
      "2024-01-01 18:03:56.538778: \n",
      "2024-01-01 18:03:56.545768: Epoch 698\n",
      "2024-01-01 18:03:56.552296: Current learning rate: 0.0034\n",
      "2024-01-01 18:06:19.486316: train_loss -0.9192\n",
      "2024-01-01 18:06:19.565428: val_loss -0.8196\n",
      "2024-01-01 18:06:19.609431: Pseudo dice [0.9265, 0.9442, 0.9441]\n",
      "2024-01-01 18:06:19.690506: Epoch time: 142.95 s\n",
      "2024-01-01 18:06:22.306370: \n",
      "2024-01-01 18:06:22.318370: Epoch 699\n",
      "2024-01-01 18:06:22.327370: Current learning rate: 0.00339\n",
      "2024-01-01 18:08:59.498808: train_loss -0.9173\n",
      "2024-01-01 18:08:59.580497: val_loss -0.8218\n",
      "2024-01-01 18:08:59.744816: Pseudo dice [0.9263, 0.9445, 0.9421]\n",
      "2024-01-01 18:08:59.922844: Epoch time: 157.2 s\n",
      "2024-01-01 18:09:03.514225: \n",
      "2024-01-01 18:09:03.526226: Epoch 700\n",
      "2024-01-01 18:09:03.534394: Current learning rate: 0.00338\n",
      "2024-01-01 18:11:34.321609: train_loss -0.9187\n",
      "2024-01-01 18:11:34.332610: val_loss -0.8289\n",
      "2024-01-01 18:11:34.341611: Pseudo dice [0.9271, 0.9462, 0.9446]\n",
      "2024-01-01 18:11:34.354618: Epoch time: 150.81 s\n",
      "2024-01-01 18:11:36.808774: \n",
      "2024-01-01 18:11:36.817775: Epoch 701\n",
      "2024-01-01 18:11:36.826784: Current learning rate: 0.00337\n",
      "2024-01-01 18:14:11.420991: train_loss -0.9184\n",
      "2024-01-01 18:14:11.509516: val_loss -0.8296\n",
      "2024-01-01 18:14:11.563513: Pseudo dice [0.9249, 0.9455, 0.9454]\n",
      "2024-01-01 18:14:11.595033: Epoch time: 154.61 s\n",
      "2024-01-01 18:14:14.373402: \n",
      "2024-01-01 18:14:14.382919: Epoch 702\n",
      "2024-01-01 18:14:14.388918: Current learning rate: 0.00336\n",
      "2024-01-01 18:16:51.847509: train_loss -0.9151\n",
      "2024-01-01 18:16:51.894027: val_loss -0.8325\n",
      "2024-01-01 18:16:51.959024: Pseudo dice [0.9278, 0.9452, 0.945]\n",
      "2024-01-01 18:16:52.018559: Epoch time: 157.48 s\n",
      "2024-01-01 18:16:54.663235: \n",
      "2024-01-01 18:16:54.674239: Epoch 703\n",
      "2024-01-01 18:16:54.683252: Current learning rate: 0.00335\n",
      "2024-01-01 18:19:23.546148: train_loss -0.9185\n",
      "2024-01-01 18:19:23.562149: val_loss -0.8339\n",
      "2024-01-01 18:19:23.573146: Pseudo dice [0.9261, 0.9463, 0.9443]\n",
      "2024-01-01 18:19:23.592156: Epoch time: 148.88 s\n",
      "2024-01-01 18:19:25.789931: \n",
      "2024-01-01 18:19:25.799590: Epoch 704\n",
      "2024-01-01 18:19:25.804595: Current learning rate: 0.00334\n",
      "2024-01-01 18:21:31.745584: train_loss -0.9186\n",
      "2024-01-01 18:21:31.761094: val_loss -0.8271\n",
      "2024-01-01 18:21:31.772103: Pseudo dice [0.927, 0.9462, 0.9452]\n",
      "2024-01-01 18:21:31.778101: Epoch time: 125.96 s\n",
      "2024-01-01 18:21:33.108073: \n",
      "2024-01-01 18:21:33.114136: Epoch 705\n",
      "2024-01-01 18:21:33.120076: Current learning rate: 0.00333\n",
      "2024-01-01 18:23:39.243032: train_loss -0.9219\n",
      "2024-01-01 18:23:39.251034: val_loss -0.8401\n",
      "2024-01-01 18:23:39.257024: Pseudo dice [0.9294, 0.945, 0.944]\n",
      "2024-01-01 18:23:39.263030: Epoch time: 126.14 s\n",
      "2024-01-01 18:23:40.590184: \n",
      "2024-01-01 18:23:40.597253: Epoch 706\n",
      "2024-01-01 18:23:40.602183: Current learning rate: 0.00332\n",
      "2024-01-01 18:25:47.500460: train_loss -0.9183\n",
      "2024-01-01 18:25:47.512460: val_loss -0.8288\n",
      "2024-01-01 18:25:47.521464: Pseudo dice [0.9269, 0.9448, 0.9437]\n",
      "2024-01-01 18:25:47.529462: Epoch time: 126.91 s\n",
      "2024-01-01 18:25:49.172325: \n",
      "2024-01-01 18:25:49.178329: Epoch 707\n",
      "2024-01-01 18:25:49.185345: Current learning rate: 0.00331\n",
      "2024-01-01 18:27:55.394268: train_loss -0.917\n",
      "2024-01-01 18:27:55.404274: val_loss -0.8268\n",
      "2024-01-01 18:27:55.414269: Pseudo dice [0.9266, 0.9449, 0.9446]\n",
      "2024-01-01 18:27:55.423269: Epoch time: 126.22 s\n",
      "2024-01-01 18:27:56.824317: \n",
      "2024-01-01 18:27:56.830331: Epoch 708\n",
      "2024-01-01 18:27:56.835331: Current learning rate: 0.0033\n",
      "2024-01-01 18:30:03.650461: train_loss -0.9175\n",
      "2024-01-01 18:30:03.661459: val_loss -0.8244\n",
      "2024-01-01 18:30:03.670464: Pseudo dice [0.9268, 0.9448, 0.9433]\n",
      "2024-01-01 18:30:03.676463: Epoch time: 126.83 s\n",
      "2024-01-01 18:30:05.025390: \n",
      "2024-01-01 18:30:05.037850: Epoch 709\n",
      "2024-01-01 18:30:05.042916: Current learning rate: 0.00329\n",
      "2024-01-01 18:32:15.497857: train_loss -0.9176\n",
      "2024-01-01 18:32:15.510687: val_loss -0.8309\n",
      "2024-01-01 18:32:15.518193: Pseudo dice [0.9282, 0.9455, 0.9446]\n",
      "2024-01-01 18:32:15.528193: Epoch time: 130.47 s\n",
      "2024-01-01 18:32:17.321065: \n",
      "2024-01-01 18:32:17.330069: Epoch 710\n",
      "2024-01-01 18:32:17.337065: Current learning rate: 0.00328\n",
      "2024-01-01 18:34:24.888898: train_loss -0.9217\n",
      "2024-01-01 18:34:24.898258: val_loss -0.8331\n",
      "2024-01-01 18:34:24.907345: Pseudo dice [0.9277, 0.9447, 0.9445]\n",
      "2024-01-01 18:34:24.917343: Epoch time: 127.57 s\n",
      "2024-01-01 18:34:26.148218: \n",
      "2024-01-01 18:34:26.159219: Epoch 711\n",
      "2024-01-01 18:34:26.166867: Current learning rate: 0.00327\n",
      "2024-01-01 18:36:46.682465: train_loss -0.9181\n",
      "2024-01-01 18:36:46.696464: val_loss -0.8325\n",
      "2024-01-01 18:36:46.706464: Pseudo dice [0.9275, 0.9458, 0.9449]\n",
      "2024-01-01 18:36:46.720445: Epoch time: 140.54 s\n",
      "2024-01-01 18:36:49.851676: \n",
      "2024-01-01 18:36:49.864756: Epoch 712\n",
      "2024-01-01 18:36:49.874748: Current learning rate: 0.00326\n",
      "2024-01-01 18:39:22.047866: train_loss -0.9167\n",
      "2024-01-01 18:39:22.198811: val_loss -0.8331\n",
      "2024-01-01 18:39:22.282346: Pseudo dice [0.927, 0.9468, 0.9453]\n",
      "2024-01-01 18:39:22.373596: Epoch time: 152.2 s\n",
      "2024-01-01 18:39:25.218116: \n",
      "2024-01-01 18:39:25.230471: Epoch 713\n",
      "2024-01-01 18:39:25.240455: Current learning rate: 0.00325\n",
      "2024-01-01 18:41:43.971089: train_loss -0.922\n",
      "2024-01-01 18:41:43.986090: val_loss -0.8301\n",
      "2024-01-01 18:41:43.999089: Pseudo dice [0.9287, 0.9464, 0.9449]\n",
      "2024-01-01 18:41:44.010130: Epoch time: 138.75 s\n",
      "2024-01-01 18:41:46.098072: \n",
      "2024-01-01 18:41:46.109510: Epoch 714\n",
      "2024-01-01 18:41:46.119005: Current learning rate: 0.00324\n",
      "2024-01-01 18:44:10.603974: train_loss -0.9191\n",
      "2024-01-01 18:44:10.654830: val_loss -0.8356\n",
      "2024-01-01 18:44:10.724666: Pseudo dice [0.9265, 0.9475, 0.9458]\n",
      "2024-01-01 18:44:10.821238: Epoch time: 144.51 s\n",
      "2024-01-01 18:44:13.537202: \n",
      "2024-01-01 18:44:13.552244: Epoch 715\n",
      "2024-01-01 18:44:13.565781: Current learning rate: 0.00323\n",
      "2024-01-01 18:46:32.258795: train_loss -0.9184\n",
      "2024-01-01 18:46:32.271842: val_loss -0.8371\n",
      "2024-01-01 18:46:32.282842: Pseudo dice [0.9293, 0.9453, 0.9444]\n",
      "2024-01-01 18:46:32.293842: Epoch time: 138.72 s\n",
      "2024-01-01 18:46:33.568879: \n",
      "2024-01-01 18:46:33.579366: Epoch 716\n",
      "2024-01-01 18:46:33.584446: Current learning rate: 0.00322\n",
      "2024-01-01 18:48:42.133455: train_loss -0.9198\n",
      "2024-01-01 18:48:42.149460: val_loss -0.8316\n",
      "2024-01-01 18:48:42.163977: Pseudo dice [0.9234, 0.9441, 0.9434]\n",
      "2024-01-01 18:48:42.177058: Epoch time: 128.57 s\n",
      "2024-01-01 18:48:43.827498: \n",
      "2024-01-01 18:48:43.841491: Epoch 717\n",
      "2024-01-01 18:48:43.848495: Current learning rate: 0.00321\n",
      "2024-01-01 18:50:51.896804: train_loss -0.9195\n",
      "2024-01-01 18:50:51.909558: val_loss -0.826\n",
      "2024-01-01 18:50:51.921052: Pseudo dice [0.9259, 0.9449, 0.9438]\n",
      "2024-01-01 18:50:51.932670: Epoch time: 128.07 s\n",
      "2024-01-01 18:50:53.522168: \n",
      "2024-01-01 18:50:53.529164: Epoch 718\n",
      "2024-01-01 18:50:53.534169: Current learning rate: 0.0032\n",
      "2024-01-01 18:53:03.871665: train_loss -0.9176\n",
      "2024-01-01 18:53:03.883952: val_loss -0.8317\n",
      "2024-01-01 18:53:03.894033: Pseudo dice [0.9235, 0.9442, 0.9432]\n",
      "2024-01-01 18:53:03.906213: Epoch time: 130.35 s\n",
      "2024-01-01 18:53:05.556471: \n",
      "2024-01-01 18:53:05.566169: Epoch 719\n",
      "2024-01-01 18:53:05.574130: Current learning rate: 0.00319\n",
      "2024-01-01 18:55:15.059653: train_loss -0.9208\n",
      "2024-01-01 18:55:15.069656: val_loss -0.8305\n",
      "2024-01-01 18:55:15.078712: Pseudo dice [0.9276, 0.9455, 0.9439]\n",
      "2024-01-01 18:55:15.088358: Epoch time: 129.51 s\n",
      "2024-01-01 18:55:17.073324: \n",
      "2024-01-01 18:55:17.082326: Epoch 720\n",
      "2024-01-01 18:55:17.089133: Current learning rate: 0.00318\n",
      "2024-01-01 18:57:28.051225: train_loss -0.921\n",
      "2024-01-01 18:57:28.063525: val_loss -0.8263\n",
      "2024-01-01 18:57:28.075438: Pseudo dice [0.9264, 0.944, 0.943]\n",
      "2024-01-01 18:57:28.096125: Epoch time: 130.98 s\n",
      "2024-01-01 18:57:29.911589: \n",
      "2024-01-01 18:57:29.920590: Epoch 721\n",
      "2024-01-01 18:57:29.930292: Current learning rate: 0.00317\n",
      "2024-01-01 18:59:41.731157: train_loss -0.9193\n",
      "2024-01-01 18:59:41.777537: val_loss -0.8291\n",
      "2024-01-01 18:59:41.829968: Pseudo dice [0.9259, 0.9468, 0.945]\n",
      "2024-01-01 18:59:41.875010: Epoch time: 131.82 s\n",
      "2024-01-01 18:59:43.706740: \n",
      "2024-01-01 18:59:43.716727: Epoch 722\n",
      "2024-01-01 18:59:43.725743: Current learning rate: 0.00316\n",
      "2024-01-01 19:01:51.948664: train_loss -0.9224\n",
      "2024-01-01 19:01:51.963118: val_loss -0.8345\n",
      "2024-01-01 19:01:51.972118: Pseudo dice [0.9267, 0.9442, 0.9442]\n",
      "2024-01-01 19:01:51.983454: Epoch time: 128.24 s\n",
      "2024-01-01 19:01:53.119892: \n",
      "2024-01-01 19:01:53.126447: Epoch 723\n",
      "2024-01-01 19:01:53.132382: Current learning rate: 0.00315\n",
      "2024-01-01 19:04:01.412545: train_loss -0.9198\n",
      "2024-01-01 19:04:01.423868: val_loss -0.8266\n",
      "2024-01-01 19:04:01.434034: Pseudo dice [0.9244, 0.9439, 0.9425]\n",
      "2024-01-01 19:04:01.443552: Epoch time: 128.29 s\n",
      "2024-01-01 19:04:02.801238: \n",
      "2024-01-01 19:04:02.808223: Epoch 724\n",
      "2024-01-01 19:04:02.814764: Current learning rate: 0.00314\n",
      "2024-01-01 19:06:09.774117: train_loss -0.9216\n",
      "2024-01-01 19:06:09.785484: val_loss -0.8251\n",
      "2024-01-01 19:06:09.795930: Pseudo dice [0.9257, 0.9457, 0.9451]\n",
      "2024-01-01 19:06:09.806929: Epoch time: 126.97 s\n",
      "2024-01-01 19:06:11.188904: \n",
      "2024-01-01 19:06:11.195899: Epoch 725\n",
      "2024-01-01 19:06:11.201897: Current learning rate: 0.00313\n",
      "2024-01-01 19:08:22.279521: train_loss -0.9207\n",
      "2024-01-01 19:08:22.294713: val_loss -0.8279\n",
      "2024-01-01 19:08:22.306314: Pseudo dice [0.9248, 0.947, 0.9455]\n",
      "2024-01-01 19:08:22.316809: Epoch time: 131.09 s\n",
      "2024-01-01 19:08:24.115776: \n",
      "2024-01-01 19:08:24.124314: Epoch 726\n",
      "2024-01-01 19:08:24.130821: Current learning rate: 0.00312\n",
      "2024-01-01 19:10:35.015658: train_loss -0.9214\n",
      "2024-01-01 19:10:35.026536: val_loss -0.8271\n",
      "2024-01-01 19:10:35.037077: Pseudo dice [0.9253, 0.9444, 0.9432]\n",
      "2024-01-01 19:10:35.047743: Epoch time: 130.9 s\n",
      "2024-01-01 19:10:36.503197: \n",
      "2024-01-01 19:10:36.513710: Epoch 727\n",
      "2024-01-01 19:10:36.520712: Current learning rate: 0.00311\n",
      "2024-01-01 19:12:47.631451: train_loss -0.9219\n",
      "2024-01-01 19:12:47.642495: val_loss -0.8256\n",
      "2024-01-01 19:12:47.651347: Pseudo dice [0.9239, 0.9425, 0.9416]\n",
      "2024-01-01 19:12:47.661713: Epoch time: 131.13 s\n",
      "2024-01-01 19:12:49.127308: \n",
      "2024-01-01 19:12:49.133315: Epoch 728\n",
      "2024-01-01 19:12:49.139332: Current learning rate: 0.0031\n",
      "2024-01-01 19:14:57.070656: train_loss -0.9197\n",
      "2024-01-01 19:14:57.081655: val_loss -0.8318\n",
      "2024-01-01 19:14:57.091048: Pseudo dice [0.926, 0.9457, 0.9442]\n",
      "2024-01-01 19:14:57.103049: Epoch time: 127.95 s\n",
      "2024-01-01 19:14:58.466243: \n",
      "2024-01-01 19:14:58.482316: Epoch 729\n",
      "2024-01-01 19:14:58.497192: Current learning rate: 0.00309\n",
      "2024-01-01 19:17:09.017077: train_loss -0.9196\n",
      "2024-01-01 19:17:09.031202: val_loss -0.8297\n",
      "2024-01-01 19:17:09.042384: Pseudo dice [0.9254, 0.9456, 0.9452]\n",
      "2024-01-01 19:17:09.053394: Epoch time: 130.55 s\n",
      "2024-01-01 19:17:11.083297: \n",
      "2024-01-01 19:17:11.090308: Epoch 730\n",
      "2024-01-01 19:17:11.096298: Current learning rate: 0.00308\n",
      "2024-01-01 19:19:21.202438: train_loss -0.9201\n",
      "2024-01-01 19:19:21.213327: val_loss -0.8221\n",
      "2024-01-01 19:19:21.221377: Pseudo dice [0.9277, 0.9446, 0.9439]\n",
      "2024-01-01 19:19:21.231423: Epoch time: 130.12 s\n",
      "2024-01-01 19:19:23.045357: \n",
      "2024-01-01 19:19:23.054565: Epoch 731\n",
      "2024-01-01 19:19:23.060581: Current learning rate: 0.00307\n",
      "2024-01-01 19:21:35.247079: train_loss -0.9219\n",
      "2024-01-01 19:21:35.270538: val_loss -0.8265\n",
      "2024-01-01 19:21:35.288706: Pseudo dice [0.9267, 0.9443, 0.9432]\n",
      "2024-01-01 19:21:35.307177: Epoch time: 132.2 s\n",
      "2024-01-01 19:21:37.433912: \n",
      "2024-01-01 19:21:37.440911: Epoch 732\n",
      "2024-01-01 19:21:37.446912: Current learning rate: 0.00306\n",
      "2024-01-01 19:23:53.478018: train_loss -0.9206\n",
      "2024-01-01 19:23:53.493517: val_loss -0.8295\n",
      "2024-01-01 19:23:53.505756: Pseudo dice [0.9263, 0.9455, 0.9448]\n",
      "2024-01-01 19:23:53.515605: Epoch time: 136.05 s\n",
      "2024-01-01 19:23:55.458972: \n",
      "2024-01-01 19:23:55.466436: Epoch 733\n",
      "2024-01-01 19:23:55.473560: Current learning rate: 0.00305\n",
      "2024-01-01 19:26:08.852440: train_loss -0.9223\n",
      "2024-01-01 19:26:08.878397: val_loss -0.8274\n",
      "2024-01-01 19:26:08.901494: Pseudo dice [0.9254, 0.9457, 0.9451]\n",
      "2024-01-01 19:26:08.916796: Epoch time: 133.4 s\n",
      "2024-01-01 19:26:10.908730: \n",
      "2024-01-01 19:26:10.918739: Epoch 734\n",
      "2024-01-01 19:26:10.926728: Current learning rate: 0.00304\n",
      "2024-01-01 19:28:19.514540: train_loss -0.921\n",
      "2024-01-01 19:28:19.523542: val_loss -0.8279\n",
      "2024-01-01 19:28:19.532072: Pseudo dice [0.9252, 0.9437, 0.9423]\n",
      "2024-01-01 19:28:19.538783: Epoch time: 128.61 s\n",
      "2024-01-01 19:28:21.012873: \n",
      "2024-01-01 19:28:21.019874: Epoch 735\n",
      "2024-01-01 19:28:21.024873: Current learning rate: 0.00303\n",
      "2024-01-01 19:30:29.755591: train_loss -0.9195\n",
      "2024-01-01 19:30:29.769047: val_loss -0.8268\n",
      "2024-01-01 19:30:29.778517: Pseudo dice [0.9295, 0.9455, 0.9445]\n",
      "2024-01-01 19:30:29.793265: Epoch time: 128.74 s\n",
      "2024-01-01 19:30:31.346844: \n",
      "2024-01-01 19:30:31.352846: Epoch 736\n",
      "2024-01-01 19:30:31.358845: Current learning rate: 0.00302\n",
      "2024-01-01 19:32:52.096406: train_loss -0.9198\n",
      "2024-01-01 19:32:52.176554: val_loss -0.8294\n",
      "2024-01-01 19:32:52.242741: Pseudo dice [0.9277, 0.9451, 0.945]\n",
      "2024-01-01 19:32:52.340245: Epoch time: 140.75 s\n",
      "2024-01-01 19:32:54.979350: \n",
      "2024-01-01 19:32:54.987883: Epoch 737\n",
      "2024-01-01 19:32:54.995448: Current learning rate: 0.00301\n",
      "2024-01-01 19:35:03.975113: train_loss -0.9211\n",
      "2024-01-01 19:35:03.992115: val_loss -0.8135\n",
      "2024-01-01 19:35:04.008115: Pseudo dice [0.9209, 0.9432, 0.9419]\n",
      "2024-01-01 19:35:04.018112: Epoch time: 129.0 s\n",
      "2024-01-01 19:35:05.600057: \n",
      "2024-01-01 19:35:05.610006: Epoch 738\n",
      "2024-01-01 19:35:05.617006: Current learning rate: 0.003\n",
      "2024-01-01 19:37:13.123240: train_loss -0.9221\n",
      "2024-01-01 19:37:13.135026: val_loss -0.8235\n",
      "2024-01-01 19:37:13.146227: Pseudo dice [0.9273, 0.9442, 0.9433]\n",
      "2024-01-01 19:37:13.153232: Epoch time: 127.52 s\n",
      "2024-01-01 19:37:14.800684: \n",
      "2024-01-01 19:37:14.807487: Epoch 739\n",
      "2024-01-01 19:37:14.813498: Current learning rate: 0.00299\n",
      "2024-01-01 19:39:23.343739: train_loss -0.9183\n",
      "2024-01-01 19:39:23.355738: val_loss -0.8299\n",
      "2024-01-01 19:39:23.366755: Pseudo dice [0.9271, 0.9453, 0.9441]\n",
      "2024-01-01 19:39:23.376257: Epoch time: 128.55 s\n",
      "2024-01-01 19:39:24.664608: \n",
      "2024-01-01 19:39:24.672700: Epoch 740\n",
      "2024-01-01 19:39:24.678131: Current learning rate: 0.00297\n",
      "2024-01-01 19:41:33.849856: train_loss -0.9214\n",
      "2024-01-01 19:41:33.861242: val_loss -0.8264\n",
      "2024-01-01 19:41:33.872242: Pseudo dice [0.9274, 0.9444, 0.9449]\n",
      "2024-01-01 19:41:33.882824: Epoch time: 129.19 s\n",
      "2024-01-01 19:41:35.884866: \n",
      "2024-01-01 19:41:35.895878: Epoch 741\n",
      "2024-01-01 19:41:35.903865: Current learning rate: 0.00296\n",
      "2024-01-01 19:43:47.818549: train_loss -0.9203\n",
      "2024-01-01 19:43:47.830544: val_loss -0.819\n",
      "2024-01-01 19:43:47.841551: Pseudo dice [0.9217, 0.9434, 0.9429]\n",
      "2024-01-01 19:43:47.851544: Epoch time: 131.93 s\n",
      "2024-01-01 19:43:49.629629: \n",
      "2024-01-01 19:43:49.638642: Epoch 742\n",
      "2024-01-01 19:43:49.647635: Current learning rate: 0.00295\n",
      "2024-01-01 19:46:05.166760: train_loss -0.9212\n",
      "2024-01-01 19:46:05.181271: val_loss -0.8281\n",
      "2024-01-01 19:46:05.198274: Pseudo dice [0.9257, 0.9451, 0.9444]\n",
      "2024-01-01 19:46:05.211273: Epoch time: 135.54 s\n",
      "2024-01-01 19:46:07.308890: \n",
      "2024-01-01 19:46:07.315891: Epoch 743\n",
      "2024-01-01 19:46:07.322890: Current learning rate: 0.00294\n",
      "2024-01-01 19:48:18.785231: train_loss -0.9192\n",
      "2024-01-01 19:48:18.800230: val_loss -0.8302\n",
      "2024-01-01 19:48:18.814231: Pseudo dice [0.926, 0.945, 0.9446]\n",
      "2024-01-01 19:48:18.825231: Epoch time: 131.48 s\n",
      "2024-01-01 19:48:20.342265: \n",
      "2024-01-01 19:48:20.350778: Epoch 744\n",
      "2024-01-01 19:48:20.355320: Current learning rate: 0.00293\n",
      "2024-01-01 19:50:28.197549: train_loss -0.9203\n",
      "2024-01-01 19:50:28.210547: val_loss -0.8277\n",
      "2024-01-01 19:50:28.220554: Pseudo dice [0.9269, 0.9444, 0.9446]\n",
      "2024-01-01 19:50:28.229553: Epoch time: 127.86 s\n",
      "2024-01-01 19:50:29.354402: \n",
      "2024-01-01 19:50:29.362170: Epoch 745\n",
      "2024-01-01 19:50:29.368179: Current learning rate: 0.00292\n",
      "2024-01-01 19:52:37.418569: train_loss -0.923\n",
      "2024-01-01 19:52:37.430572: val_loss -0.8244\n",
      "2024-01-01 19:52:37.441091: Pseudo dice [0.922, 0.9429, 0.9409]\n",
      "2024-01-01 19:52:37.452086: Epoch time: 128.07 s\n",
      "2024-01-01 19:52:38.922993: \n",
      "2024-01-01 19:52:38.929991: Epoch 746\n",
      "2024-01-01 19:52:38.937514: Current learning rate: 0.00291\n",
      "2024-01-01 19:54:48.144699: train_loss -0.9213\n",
      "2024-01-01 19:54:48.157697: val_loss -0.8291\n",
      "2024-01-01 19:54:48.170897: Pseudo dice [0.9288, 0.9459, 0.9441]\n",
      "2024-01-01 19:54:48.188006: Epoch time: 129.22 s\n",
      "2024-01-01 19:54:49.797759: \n",
      "2024-01-01 19:54:49.805765: Epoch 747\n",
      "2024-01-01 19:54:49.812765: Current learning rate: 0.0029\n",
      "2024-01-01 19:57:00.166364: train_loss -0.9182\n",
      "2024-01-01 19:57:00.193864: val_loss -0.8293\n",
      "2024-01-01 19:57:00.209076: Pseudo dice [0.9281, 0.9449, 0.9438]\n",
      "2024-01-01 19:57:00.223901: Epoch time: 130.37 s\n",
      "2024-01-01 19:57:02.266376: \n",
      "2024-01-01 19:57:02.273368: Epoch 748\n",
      "2024-01-01 19:57:02.279369: Current learning rate: 0.00289\n",
      "2024-01-01 19:59:13.034533: train_loss -0.9209\n",
      "2024-01-01 19:59:13.052180: val_loss -0.8281\n",
      "2024-01-01 19:59:13.078873: Pseudo dice [0.9255, 0.9458, 0.9451]\n",
      "2024-01-01 19:59:13.099469: Epoch time: 130.77 s\n",
      "2024-01-01 19:59:15.176282: \n",
      "2024-01-01 19:59:15.185282: Epoch 749\n",
      "2024-01-01 19:59:15.192752: Current learning rate: 0.00288\n",
      "2024-01-01 20:01:24.127187: train_loss -0.921\n",
      "2024-01-01 20:01:24.142191: val_loss -0.8312\n",
      "2024-01-01 20:01:24.153187: Pseudo dice [0.9245, 0.9437, 0.9433]\n",
      "2024-01-01 20:01:24.163192: Epoch time: 128.95 s\n",
      "2024-01-01 20:01:25.945202: \n",
      "2024-01-01 20:01:25.952204: Epoch 750\n",
      "2024-01-01 20:01:25.958286: Current learning rate: 0.00287\n",
      "2024-01-01 20:03:41.404047: train_loss -0.9211\n",
      "2024-01-01 20:03:41.539612: val_loss -0.8273\n",
      "2024-01-01 20:03:41.587717: Pseudo dice [0.9277, 0.9462, 0.9447]\n",
      "2024-01-01 20:03:41.599234: Epoch time: 135.46 s\n",
      "2024-01-01 20:03:44.521303: \n",
      "2024-01-01 20:03:44.531076: Epoch 751\n",
      "2024-01-01 20:03:44.543301: Current learning rate: 0.00286\n",
      "2024-01-01 20:06:08.480440: train_loss -0.9181\n",
      "2024-01-01 20:06:08.510051: val_loss -0.8291\n",
      "2024-01-01 20:06:08.589756: Pseudo dice [0.9262, 0.9448, 0.9436]\n",
      "2024-01-01 20:06:08.630877: Epoch time: 143.96 s\n",
      "2024-01-01 20:06:11.228762: \n",
      "2024-01-01 20:06:11.243152: Epoch 752\n",
      "2024-01-01 20:06:11.261340: Current learning rate: 0.00285\n",
      "2024-01-01 20:08:21.428842: train_loss -0.9201\n",
      "2024-01-01 20:08:21.447110: val_loss -0.8284\n",
      "2024-01-01 20:08:21.458551: Pseudo dice [0.9246, 0.9456, 0.9445]\n",
      "2024-01-01 20:08:21.471066: Epoch time: 130.2 s\n",
      "2024-01-01 20:08:23.253320: \n",
      "2024-01-01 20:08:23.261348: Epoch 753\n",
      "2024-01-01 20:08:23.267937: Current learning rate: 0.00284\n",
      "2024-01-01 20:10:37.625147: train_loss -0.9208\n",
      "2024-01-01 20:10:37.640578: val_loss -0.8232\n",
      "2024-01-01 20:10:37.662197: Pseudo dice [0.925, 0.9447, 0.9448]\n",
      "2024-01-01 20:10:37.688763: Epoch time: 134.37 s\n",
      "2024-01-01 20:10:39.743091: \n",
      "2024-01-01 20:10:39.754695: Epoch 754\n",
      "2024-01-01 20:10:39.768842: Current learning rate: 0.00283\n",
      "2024-01-01 20:12:59.005471: train_loss -0.9192\n",
      "2024-01-01 20:12:59.041727: val_loss -0.8253\n",
      "2024-01-01 20:12:59.071298: Pseudo dice [0.9267, 0.9445, 0.9439]\n",
      "2024-01-01 20:12:59.193953: Epoch time: 139.26 s\n",
      "2024-01-01 20:13:02.001739: \n",
      "2024-01-01 20:13:02.014812: Epoch 755\n",
      "2024-01-01 20:13:02.027907: Current learning rate: 0.00282\n",
      "2024-01-01 20:15:16.400162: train_loss -0.9199\n",
      "2024-01-01 20:15:16.419169: val_loss -0.8249\n",
      "2024-01-01 20:15:16.433936: Pseudo dice [0.9271, 0.9437, 0.943]\n",
      "2024-01-01 20:15:16.445934: Epoch time: 134.4 s\n",
      "2024-01-01 20:15:18.259487: \n",
      "2024-01-01 20:15:18.266502: Epoch 756\n",
      "2024-01-01 20:15:18.272487: Current learning rate: 0.00281\n",
      "2024-01-01 20:17:34.382869: train_loss -0.9201\n",
      "2024-01-01 20:17:34.394280: val_loss -0.8313\n",
      "2024-01-01 20:17:34.405281: Pseudo dice [0.9241, 0.9445, 0.944]\n",
      "2024-01-01 20:17:34.417291: Epoch time: 136.13 s\n",
      "2024-01-01 20:17:36.006744: \n",
      "2024-01-01 20:17:36.018748: Epoch 757\n",
      "2024-01-01 20:17:36.025761: Current learning rate: 0.0028\n",
      "2024-01-01 20:19:53.290790: train_loss -0.9212\n",
      "2024-01-01 20:19:53.309880: val_loss -0.8274\n",
      "2024-01-01 20:19:53.329089: Pseudo dice [0.9269, 0.9453, 0.9439]\n",
      "2024-01-01 20:19:53.347314: Epoch time: 137.29 s\n",
      "2024-01-01 20:19:55.412180: \n",
      "2024-01-01 20:19:55.420171: Epoch 758\n",
      "2024-01-01 20:19:55.430180: Current learning rate: 0.00279\n",
      "2024-01-01 20:22:03.593189: train_loss -0.9206\n",
      "2024-01-01 20:22:03.604187: val_loss -0.8255\n",
      "2024-01-01 20:22:03.615333: Pseudo dice [0.9252, 0.9452, 0.9446]\n",
      "2024-01-01 20:22:03.626296: Epoch time: 128.18 s\n",
      "2024-01-01 20:22:05.139082: \n",
      "2024-01-01 20:22:05.146070: Epoch 759\n",
      "2024-01-01 20:22:05.156238: Current learning rate: 0.00278\n",
      "2024-01-01 20:24:15.135970: train_loss -0.9203\n",
      "2024-01-01 20:24:15.156168: val_loss -0.8246\n",
      "2024-01-01 20:24:15.188219: Pseudo dice [0.9259, 0.9443, 0.9438]\n",
      "2024-01-01 20:24:15.213530: Epoch time: 130.0 s\n",
      "2024-01-01 20:24:17.701170: \n",
      "2024-01-01 20:24:17.708256: Epoch 760\n",
      "2024-01-01 20:24:17.715170: Current learning rate: 0.00277\n",
      "2024-01-01 20:26:26.124193: train_loss -0.9191\n",
      "2024-01-01 20:26:26.136208: val_loss -0.8286\n",
      "2024-01-01 20:26:26.146129: Pseudo dice [0.9249, 0.9456, 0.9442]\n",
      "2024-01-01 20:26:26.155888: Epoch time: 128.42 s\n",
      "2024-01-01 20:26:27.449835: \n",
      "2024-01-01 20:26:27.462985: Epoch 761\n",
      "2024-01-01 20:26:27.468982: Current learning rate: 0.00276\n",
      "2024-01-01 20:28:36.425266: train_loss -0.9216\n",
      "2024-01-01 20:28:36.439778: val_loss -0.8213\n",
      "2024-01-01 20:28:36.450779: Pseudo dice [0.9265, 0.9441, 0.9435]\n",
      "2024-01-01 20:28:36.462779: Epoch time: 128.98 s\n",
      "2024-01-01 20:28:37.704155: \n",
      "2024-01-01 20:28:37.719438: Epoch 762\n",
      "2024-01-01 20:28:37.730448: Current learning rate: 0.00275\n",
      "2024-01-01 20:30:48.764669: train_loss -0.9214\n",
      "2024-01-01 20:30:48.777669: val_loss -0.8298\n",
      "2024-01-01 20:30:48.790875: Pseudo dice [0.9273, 0.9455, 0.9442]\n",
      "2024-01-01 20:30:48.802884: Epoch time: 131.06 s\n",
      "2024-01-01 20:30:50.623229: \n",
      "2024-01-01 20:30:50.633232: Epoch 763\n",
      "2024-01-01 20:30:50.643742: Current learning rate: 0.00274\n",
      "2024-01-01 20:33:00.682417: train_loss -0.9198\n",
      "2024-01-01 20:33:00.695422: val_loss -0.8299\n",
      "2024-01-01 20:33:00.707417: Pseudo dice [0.9286, 0.9446, 0.9433]\n",
      "2024-01-01 20:33:00.719421: Epoch time: 130.06 s\n",
      "2024-01-01 20:33:02.039455: \n",
      "2024-01-01 20:33:02.046531: Epoch 764\n",
      "2024-01-01 20:33:02.052525: Current learning rate: 0.00273\n",
      "2024-01-01 20:35:09.580162: train_loss -0.92\n",
      "2024-01-01 20:35:09.591162: val_loss -0.8317\n",
      "2024-01-01 20:35:09.600166: Pseudo dice [0.9276, 0.9448, 0.9438]\n",
      "2024-01-01 20:35:09.608166: Epoch time: 127.54 s\n",
      "2024-01-01 20:35:10.800923: \n",
      "2024-01-01 20:35:10.806921: Epoch 765\n",
      "2024-01-01 20:35:10.812914: Current learning rate: 0.00272\n",
      "2024-01-01 20:37:17.715891: train_loss -0.9215\n",
      "2024-01-01 20:37:17.725891: val_loss -0.8234\n",
      "2024-01-01 20:37:17.734891: Pseudo dice [0.925, 0.9446, 0.9451]\n",
      "2024-01-01 20:37:17.743186: Epoch time: 126.92 s\n",
      "2024-01-01 20:37:18.930243: \n",
      "2024-01-01 20:37:18.938243: Epoch 766\n",
      "2024-01-01 20:37:18.946247: Current learning rate: 0.00271\n",
      "2024-01-01 20:39:24.735333: train_loss -0.9204\n",
      "2024-01-01 20:39:24.745334: val_loss -0.827\n",
      "2024-01-01 20:39:24.754334: Pseudo dice [0.9245, 0.9446, 0.9438]\n",
      "2024-01-01 20:39:24.761335: Epoch time: 125.81 s\n",
      "2024-01-01 20:39:26.034366: \n",
      "2024-01-01 20:39:26.040362: Epoch 767\n",
      "2024-01-01 20:39:26.045362: Current learning rate: 0.0027\n",
      "2024-01-01 20:41:31.581896: train_loss -0.9226\n",
      "2024-01-01 20:41:31.592896: val_loss -0.8336\n",
      "2024-01-01 20:41:31.602901: Pseudo dice [0.9248, 0.9449, 0.9432]\n",
      "2024-01-01 20:41:31.610897: Epoch time: 125.55 s\n",
      "2024-01-01 20:41:32.694089: \n",
      "2024-01-01 20:41:32.700090: Epoch 768\n",
      "2024-01-01 20:41:32.706175: Current learning rate: 0.00268\n",
      "2024-01-01 20:43:38.522855: train_loss -0.9208\n",
      "2024-01-01 20:43:38.534856: val_loss -0.8277\n",
      "2024-01-01 20:43:38.542744: Pseudo dice [0.9259, 0.9453, 0.9443]\n",
      "2024-01-01 20:43:38.549744: Epoch time: 125.83 s\n",
      "2024-01-01 20:43:39.632176: \n",
      "2024-01-01 20:43:39.638176: Epoch 769\n",
      "2024-01-01 20:43:39.643180: Current learning rate: 0.00267\n",
      "2024-01-01 20:45:45.309436: train_loss -0.9199\n",
      "2024-01-01 20:45:45.318436: val_loss -0.828\n",
      "2024-01-01 20:45:45.329435: Pseudo dice [0.9267, 0.9452, 0.9443]\n",
      "2024-01-01 20:45:45.337435: Epoch time: 125.68 s\n",
      "2024-01-01 20:45:46.436285: \n",
      "2024-01-01 20:45:46.444380: Epoch 770\n",
      "2024-01-01 20:45:46.450395: Current learning rate: 0.00266\n",
      "2024-01-01 20:47:51.943488: train_loss -0.922\n",
      "2024-01-01 20:47:51.954489: val_loss -0.8203\n",
      "2024-01-01 20:47:51.964494: Pseudo dice [0.9257, 0.9443, 0.9442]\n",
      "2024-01-01 20:47:51.973494: Epoch time: 125.51 s\n",
      "2024-01-01 20:47:53.111569: \n",
      "2024-01-01 20:47:53.117558: Epoch 771\n",
      "2024-01-01 20:47:53.123592: Current learning rate: 0.00265\n",
      "2024-01-01 20:49:58.768741: train_loss -0.9194\n",
      "2024-01-01 20:49:58.777748: val_loss -0.8289\n",
      "2024-01-01 20:49:58.786742: Pseudo dice [0.9257, 0.945, 0.9444]\n",
      "2024-01-01 20:49:58.793746: Epoch time: 125.66 s\n",
      "2024-01-01 20:49:59.896583: \n",
      "2024-01-01 20:49:59.902625: Epoch 772\n",
      "2024-01-01 20:49:59.907583: Current learning rate: 0.00264\n",
      "2024-01-01 20:52:05.484967: train_loss -0.9214\n",
      "2024-01-01 20:52:05.494966: val_loss -0.831\n",
      "2024-01-01 20:52:05.503965: Pseudo dice [0.9251, 0.9443, 0.9435]\n",
      "2024-01-01 20:52:05.512965: Epoch time: 125.59 s\n",
      "2024-01-01 20:52:06.650392: \n",
      "2024-01-01 20:52:06.656397: Epoch 773\n",
      "2024-01-01 20:52:06.661397: Current learning rate: 0.00263\n",
      "2024-01-01 20:54:12.249154: train_loss -0.9224\n",
      "2024-01-01 20:54:12.259155: val_loss -0.8277\n",
      "2024-01-01 20:54:12.268154: Pseudo dice [0.9259, 0.9458, 0.9454]\n",
      "2024-01-01 20:54:12.278156: Epoch time: 125.6 s\n",
      "2024-01-01 20:54:13.417148: \n",
      "2024-01-01 20:54:13.423251: Epoch 774\n",
      "2024-01-01 20:54:13.428152: Current learning rate: 0.00262\n",
      "2024-01-01 20:56:19.015225: train_loss -0.9197\n",
      "2024-01-01 20:56:19.023223: val_loss -0.8264\n",
      "2024-01-01 20:56:19.032224: Pseudo dice [0.9275, 0.946, 0.9456]\n",
      "2024-01-01 20:56:19.039225: Epoch time: 125.6 s\n",
      "2024-01-01 20:56:20.308818: \n",
      "2024-01-01 20:56:20.315820: Epoch 775\n",
      "2024-01-01 20:56:20.320820: Current learning rate: 0.00261\n",
      "2024-01-01 20:58:26.296726: train_loss -0.9221\n",
      "2024-01-01 20:58:26.307727: val_loss -0.8291\n",
      "2024-01-01 20:58:26.316741: Pseudo dice [0.9257, 0.9447, 0.9443]\n",
      "2024-01-01 20:58:26.324735: Epoch time: 125.99 s\n",
      "2024-01-01 20:58:27.430927: \n",
      "2024-01-01 20:58:27.436015: Epoch 776\n",
      "2024-01-01 20:58:27.440925: Current learning rate: 0.0026\n",
      "2024-01-01 21:00:32.737989: train_loss -0.9198\n",
      "2024-01-01 21:00:32.747991: val_loss -0.8342\n",
      "2024-01-01 21:00:32.757991: Pseudo dice [0.925, 0.946, 0.9459]\n",
      "2024-01-01 21:00:32.764997: Epoch time: 125.31 s\n",
      "2024-01-01 21:00:33.858016: \n",
      "2024-01-01 21:00:33.864100: Epoch 777\n",
      "2024-01-01 21:00:33.869088: Current learning rate: 0.00259\n",
      "2024-01-01 21:02:39.615253: train_loss -0.9227\n",
      "2024-01-01 21:02:39.627773: val_loss -0.8321\n",
      "2024-01-01 21:02:39.638159: Pseudo dice [0.9267, 0.9458, 0.9453]\n",
      "2024-01-01 21:02:39.647157: Epoch time: 125.76 s\n",
      "2024-01-01 21:02:40.787807: \n",
      "2024-01-01 21:02:40.793822: Epoch 778\n",
      "2024-01-01 21:02:40.799824: Current learning rate: 0.00258\n",
      "2024-01-01 21:04:46.250106: train_loss -0.9205\n",
      "2024-01-01 21:04:46.261111: val_loss -0.8332\n",
      "2024-01-01 21:04:46.269619: Pseudo dice [0.9266, 0.9444, 0.9447]\n",
      "2024-01-01 21:04:46.278619: Epoch time: 125.46 s\n",
      "2024-01-01 21:04:47.369634: \n",
      "2024-01-01 21:04:47.375708: Epoch 779\n",
      "2024-01-01 21:04:47.384651: Current learning rate: 0.00257\n",
      "2024-01-01 21:06:53.069380: train_loss -0.922\n",
      "2024-01-01 21:06:53.079389: val_loss -0.8266\n",
      "2024-01-01 21:06:53.088441: Pseudo dice [0.9248, 0.9418, 0.9436]\n",
      "2024-01-01 21:06:53.097439: Epoch time: 125.7 s\n",
      "2024-01-01 21:06:54.219589: \n",
      "2024-01-01 21:06:54.226592: Epoch 780\n",
      "2024-01-01 21:06:54.231665: Current learning rate: 0.00256\n",
      "2024-01-01 21:08:59.782300: train_loss -0.9204\n",
      "2024-01-01 21:08:59.791300: val_loss -0.8227\n",
      "2024-01-01 21:08:59.801299: Pseudo dice [0.9254, 0.9444, 0.9439]\n",
      "2024-01-01 21:08:59.810301: Epoch time: 125.56 s\n",
      "2024-01-01 21:09:00.995693: \n",
      "2024-01-01 21:09:01.002695: Epoch 781\n",
      "2024-01-01 21:09:01.007694: Current learning rate: 0.00255\n",
      "2024-01-01 21:11:06.631718: train_loss -0.9218\n",
      "2024-01-01 21:11:06.641723: val_loss -0.8272\n",
      "2024-01-01 21:11:06.649717: Pseudo dice [0.9267, 0.9441, 0.9437]\n",
      "2024-01-01 21:11:06.657717: Epoch time: 125.64 s\n",
      "2024-01-01 21:11:07.902748: \n",
      "2024-01-01 21:11:07.907751: Epoch 782\n",
      "2024-01-01 21:11:07.917339: Current learning rate: 0.00254\n",
      "2024-01-01 21:13:13.436928: train_loss -0.923\n",
      "2024-01-01 21:13:13.444935: val_loss -0.8313\n",
      "2024-01-01 21:13:13.451948: Pseudo dice [0.9265, 0.9451, 0.9441]\n",
      "2024-01-01 21:13:13.460439: Epoch time: 125.54 s\n",
      "2024-01-01 21:13:14.546250: \n",
      "2024-01-01 21:13:14.552256: Epoch 783\n",
      "2024-01-01 21:13:14.557255: Current learning rate: 0.00253\n",
      "2024-01-01 21:15:20.031014: train_loss -0.9214\n",
      "2024-01-01 21:15:20.040016: val_loss -0.8244\n",
      "2024-01-01 21:15:20.048015: Pseudo dice [0.925, 0.9453, 0.9445]\n",
      "2024-01-01 21:15:20.055015: Epoch time: 125.49 s\n",
      "2024-01-01 21:15:21.168648: \n",
      "2024-01-01 21:15:21.181717: Epoch 784\n",
      "2024-01-01 21:15:21.187678: Current learning rate: 0.00252\n",
      "2024-01-01 21:17:26.683832: train_loss -0.9242\n",
      "2024-01-01 21:17:26.693834: val_loss -0.8246\n",
      "2024-01-01 21:17:26.703834: Pseudo dice [0.9263, 0.9454, 0.9442]\n",
      "2024-01-01 21:17:26.712832: Epoch time: 125.52 s\n",
      "2024-01-01 21:17:27.802243: \n",
      "2024-01-01 21:17:27.808235: Epoch 785\n",
      "2024-01-01 21:17:27.813452: Current learning rate: 0.00251\n",
      "2024-01-01 21:19:33.455810: train_loss -0.9218\n",
      "2024-01-01 21:19:33.465810: val_loss -0.8276\n",
      "2024-01-01 21:19:33.475810: Pseudo dice [0.9256, 0.9453, 0.9447]\n",
      "2024-01-01 21:19:33.483809: Epoch time: 125.65 s\n",
      "2024-01-01 21:19:34.656434: \n",
      "2024-01-01 21:19:34.661434: Epoch 786\n",
      "2024-01-01 21:19:34.667439: Current learning rate: 0.0025\n",
      "2024-01-01 21:21:40.093071: train_loss -0.9227\n",
      "2024-01-01 21:21:40.103070: val_loss -0.8272\n",
      "2024-01-01 21:21:40.112069: Pseudo dice [0.9269, 0.9457, 0.9449]\n",
      "2024-01-01 21:21:40.121070: Epoch time: 125.44 s\n",
      "2024-01-01 21:21:41.201944: \n",
      "2024-01-01 21:21:41.209943: Epoch 787\n",
      "2024-01-01 21:21:41.218009: Current learning rate: 0.00249\n",
      "2024-01-01 21:23:46.425212: train_loss -0.9243\n",
      "2024-01-01 21:23:46.436213: val_loss -0.8265\n",
      "2024-01-01 21:23:46.445210: Pseudo dice [0.9266, 0.9451, 0.9445]\n",
      "2024-01-01 21:23:46.453211: Epoch time: 125.22 s\n",
      "2024-01-01 21:23:47.550345: \n",
      "2024-01-01 21:23:47.556346: Epoch 788\n",
      "2024-01-01 21:23:47.561346: Current learning rate: 0.00248\n",
      "2024-01-01 21:25:53.492737: train_loss -0.9201\n",
      "2024-01-01 21:25:53.501739: val_loss -0.8332\n",
      "2024-01-01 21:25:53.510740: Pseudo dice [0.9268, 0.9443, 0.9445]\n",
      "2024-01-01 21:25:53.519739: Epoch time: 125.94 s\n",
      "2024-01-01 21:25:54.813996: \n",
      "2024-01-01 21:25:54.820001: Epoch 789\n",
      "2024-01-01 21:25:54.824996: Current learning rate: 0.00247\n",
      "2024-01-01 21:28:00.363292: train_loss -0.9201\n",
      "2024-01-01 21:28:00.373293: val_loss -0.8223\n",
      "2024-01-01 21:28:00.382303: Pseudo dice [0.9243, 0.944, 0.9441]\n",
      "2024-01-01 21:28:00.389296: Epoch time: 125.55 s\n",
      "2024-01-01 21:28:01.474839: \n",
      "2024-01-01 21:28:01.480875: Epoch 790\n",
      "2024-01-01 21:28:01.485841: Current learning rate: 0.00245\n",
      "2024-01-01 21:30:07.113069: train_loss -0.9231\n",
      "2024-01-01 21:30:07.124067: val_loss -0.825\n",
      "2024-01-01 21:30:07.134070: Pseudo dice [0.9254, 0.9445, 0.9436]\n",
      "2024-01-01 21:30:07.143075: Epoch time: 125.64 s\n",
      "2024-01-01 21:30:08.268116: \n",
      "2024-01-01 21:30:08.274121: Epoch 791\n",
      "2024-01-01 21:30:08.280188: Current learning rate: 0.00244\n",
      "2024-01-01 21:32:14.073848: train_loss -0.9204\n",
      "2024-01-01 21:32:14.085847: val_loss -0.8239\n",
      "2024-01-01 21:32:14.095851: Pseudo dice [0.9272, 0.9458, 0.9452]\n",
      "2024-01-01 21:32:14.105360: Epoch time: 125.81 s\n",
      "2024-01-01 21:32:15.298138: \n",
      "2024-01-01 21:32:15.309668: Epoch 792\n",
      "2024-01-01 21:32:15.316666: Current learning rate: 0.00243\n",
      "2024-01-01 21:34:20.892072: train_loss -0.9201\n",
      "2024-01-01 21:34:20.903074: val_loss -0.8299\n",
      "2024-01-01 21:34:20.911074: Pseudo dice [0.9245, 0.9447, 0.9435]\n",
      "2024-01-01 21:34:20.920079: Epoch time: 125.59 s\n",
      "2024-01-01 21:34:22.083429: \n",
      "2024-01-01 21:34:22.089434: Epoch 793\n",
      "2024-01-01 21:34:22.094433: Current learning rate: 0.00242\n",
      "2024-01-01 21:36:27.485981: train_loss -0.9207\n",
      "2024-01-01 21:36:27.495983: val_loss -0.8319\n",
      "2024-01-01 21:36:27.504495: Pseudo dice [0.9263, 0.9463, 0.9455]\n",
      "2024-01-01 21:36:27.511494: Epoch time: 125.4 s\n",
      "2024-01-01 21:36:28.593982: \n",
      "2024-01-01 21:36:28.600922: Epoch 794\n",
      "2024-01-01 21:36:28.606451: Current learning rate: 0.00241\n",
      "2024-01-01 21:38:34.049847: train_loss -0.9242\n",
      "2024-01-01 21:38:34.061359: val_loss -0.8239\n",
      "2024-01-01 21:38:34.069360: Pseudo dice [0.9256, 0.9441, 0.9434]\n",
      "2024-01-01 21:38:34.076360: Epoch time: 125.46 s\n",
      "2024-01-01 21:38:35.181613: \n",
      "2024-01-01 21:38:35.187612: Epoch 795\n",
      "2024-01-01 21:38:35.193604: Current learning rate: 0.0024\n",
      "2024-01-01 21:40:41.201360: train_loss -0.9186\n",
      "2024-01-01 21:40:41.210359: val_loss -0.8346\n",
      "2024-01-01 21:40:41.218379: Pseudo dice [0.9278, 0.9464, 0.9448]\n",
      "2024-01-01 21:40:41.226359: Epoch time: 126.02 s\n",
      "2024-01-01 21:40:42.328853: \n",
      "2024-01-01 21:40:42.336910: Epoch 796\n",
      "2024-01-01 21:40:42.345859: Current learning rate: 0.00239\n",
      "2024-01-01 21:42:47.806709: train_loss -0.9224\n",
      "2024-01-01 21:42:47.819709: val_loss -0.8274\n",
      "2024-01-01 21:42:47.828709: Pseudo dice [0.9257, 0.9457, 0.9442]\n",
      "2024-01-01 21:42:47.837714: Epoch time: 125.48 s\n",
      "2024-01-01 21:42:49.146724: \n",
      "2024-01-01 21:42:49.154273: Epoch 797\n",
      "2024-01-01 21:42:49.160302: Current learning rate: 0.00238\n",
      "2024-01-01 21:44:54.574837: train_loss -0.9215\n",
      "2024-01-01 21:44:54.583843: val_loss -0.8209\n",
      "2024-01-01 21:44:54.592849: Pseudo dice [0.9256, 0.9461, 0.9456]\n",
      "2024-01-01 21:44:54.600363: Epoch time: 125.43 s\n",
      "2024-01-01 21:44:55.737267: \n",
      "2024-01-01 21:44:55.743268: Epoch 798\n",
      "2024-01-01 21:44:55.749328: Current learning rate: 0.00237\n",
      "2024-01-01 21:47:00.965845: train_loss -0.9259\n",
      "2024-01-01 21:47:00.973846: val_loss -0.8268\n",
      "2024-01-01 21:47:00.982843: Pseudo dice [0.9246, 0.9444, 0.9442]\n",
      "2024-01-01 21:47:00.989841: Epoch time: 125.23 s\n",
      "2024-01-01 21:47:02.079202: \n",
      "2024-01-01 21:47:02.085193: Epoch 799\n",
      "2024-01-01 21:47:02.090198: Current learning rate: 0.00236\n",
      "2024-01-01 21:49:07.635024: train_loss -0.9243\n",
      "2024-01-01 21:49:07.646022: val_loss -0.8218\n",
      "2024-01-01 21:49:07.654022: Pseudo dice [0.9265, 0.9445, 0.9436]\n",
      "2024-01-01 21:49:07.661022: Epoch time: 125.56 s\n",
      "2024-01-01 21:49:09.036782: \n",
      "2024-01-01 21:49:09.043303: Epoch 800\n",
      "2024-01-01 21:49:09.049307: Current learning rate: 0.00235\n",
      "2024-01-01 21:51:14.760128: train_loss -0.9239\n",
      "2024-01-01 21:51:14.772130: val_loss -0.8274\n",
      "2024-01-01 21:51:14.781128: Pseudo dice [0.9257, 0.9457, 0.9444]\n",
      "2024-01-01 21:51:14.791128: Epoch time: 125.72 s\n",
      "2024-01-01 21:51:16.043461: \n",
      "2024-01-01 21:51:16.052973: Epoch 801\n",
      "2024-01-01 21:51:16.061971: Current learning rate: 0.00234\n",
      "2024-01-01 21:53:21.440576: train_loss -0.9217\n",
      "2024-01-01 21:53:21.449575: val_loss -0.8254\n",
      "2024-01-01 21:53:21.458575: Pseudo dice [0.9247, 0.9441, 0.9432]\n",
      "2024-01-01 21:53:21.464575: Epoch time: 125.4 s\n",
      "2024-01-01 21:53:22.547170: \n",
      "2024-01-01 21:53:22.553691: Epoch 802\n",
      "2024-01-01 21:53:22.558692: Current learning rate: 0.00233\n",
      "2024-01-01 21:55:28.238720: train_loss -0.9231\n",
      "2024-01-01 21:55:28.249232: val_loss -0.825\n",
      "2024-01-01 21:55:28.259229: Pseudo dice [0.9281, 0.9454, 0.9452]\n",
      "2024-01-01 21:55:28.267229: Epoch time: 125.69 s\n",
      "2024-01-01 21:55:29.363426: \n",
      "2024-01-01 21:55:29.369427: Epoch 803\n",
      "2024-01-01 21:55:29.374429: Current learning rate: 0.00232\n",
      "2024-01-01 21:57:37.841538: train_loss -0.9223\n",
      "2024-01-01 21:57:37.858537: val_loss -0.8274\n",
      "2024-01-01 21:57:37.871734: Pseudo dice [0.9255, 0.9454, 0.9446]\n",
      "2024-01-01 21:57:37.885026: Epoch time: 128.48 s\n",
      "2024-01-01 21:57:39.606623: \n",
      "2024-01-01 21:57:39.612628: Epoch 804\n",
      "2024-01-01 21:57:39.618629: Current learning rate: 0.00231\n",
      "2024-01-01 21:59:48.964239: train_loss -0.9183\n",
      "2024-01-01 21:59:48.979239: val_loss -0.8299\n",
      "2024-01-01 21:59:48.990239: Pseudo dice [0.9271, 0.9441, 0.9447]\n",
      "2024-01-01 21:59:49.000238: Epoch time: 129.36 s\n",
      "2024-01-01 21:59:50.332684: \n",
      "2024-01-01 21:59:50.343695: Epoch 805\n",
      "2024-01-01 21:59:50.352618: Current learning rate: 0.0023\n",
      "2024-01-01 22:01:58.364301: train_loss -0.9225\n",
      "2024-01-01 22:01:58.375364: val_loss -0.8299\n",
      "2024-01-01 22:01:58.386439: Pseudo dice [0.9258, 0.9457, 0.9446]\n",
      "2024-01-01 22:01:58.397443: Epoch time: 128.03 s\n",
      "2024-01-01 22:01:59.872394: \n",
      "2024-01-01 22:01:59.881388: Epoch 806\n",
      "2024-01-01 22:01:59.891407: Current learning rate: 0.00229\n",
      "2024-01-01 22:04:06.685115: train_loss -0.921\n",
      "2024-01-01 22:04:06.693115: val_loss -0.8312\n",
      "2024-01-01 22:04:06.702116: Pseudo dice [0.9283, 0.9451, 0.944]\n",
      "2024-01-01 22:04:06.711117: Epoch time: 126.81 s\n",
      "2024-01-01 22:04:07.819690: \n",
      "2024-01-01 22:04:07.826682: Epoch 807\n",
      "2024-01-01 22:04:07.831681: Current learning rate: 0.00228\n",
      "2024-01-01 22:06:13.581406: train_loss -0.9213\n",
      "2024-01-01 22:06:13.592407: val_loss -0.8301\n",
      "2024-01-01 22:06:13.602108: Pseudo dice [0.9272, 0.9457, 0.9449]\n",
      "2024-01-01 22:06:13.612139: Epoch time: 125.76 s\n",
      "2024-01-01 22:06:14.704046: \n",
      "2024-01-01 22:06:14.709568: Epoch 808\n",
      "2024-01-01 22:06:14.715647: Current learning rate: 0.00226\n",
      "2024-01-01 22:08:20.244620: train_loss -0.9258\n",
      "2024-01-01 22:08:20.253626: val_loss -0.8247\n",
      "2024-01-01 22:08:20.262132: Pseudo dice [0.9242, 0.9456, 0.9455]\n",
      "2024-01-01 22:08:20.269128: Epoch time: 125.54 s\n",
      "2024-01-01 22:08:21.386444: \n",
      "2024-01-01 22:08:21.392444: Epoch 809\n",
      "2024-01-01 22:08:21.398454: Current learning rate: 0.00225\n",
      "2024-01-01 22:10:27.321821: train_loss -0.9236\n",
      "2024-01-01 22:10:27.330821: val_loss -0.8275\n",
      "2024-01-01 22:10:27.338821: Pseudo dice [0.9271, 0.9461, 0.9454]\n",
      "2024-01-01 22:10:27.347822: Epoch time: 125.94 s\n",
      "2024-01-01 22:10:28.453110: \n",
      "2024-01-01 22:10:28.459157: Epoch 810\n",
      "2024-01-01 22:10:28.464116: Current learning rate: 0.00224\n",
      "2024-01-01 22:12:34.058933: train_loss -0.9254\n",
      "2024-01-01 22:12:34.068935: val_loss -0.8254\n",
      "2024-01-01 22:12:34.077933: Pseudo dice [0.9249, 0.9444, 0.9435]\n",
      "2024-01-01 22:12:34.085935: Epoch time: 125.61 s\n",
      "2024-01-01 22:12:35.401130: \n",
      "2024-01-01 22:12:35.407135: Epoch 811\n",
      "2024-01-01 22:12:35.413132: Current learning rate: 0.00223\n",
      "2024-01-01 22:14:40.926733: train_loss -0.9225\n",
      "2024-01-01 22:14:40.936733: val_loss -0.8285\n",
      "2024-01-01 22:14:40.947738: Pseudo dice [0.9253, 0.9449, 0.9445]\n",
      "2024-01-01 22:14:40.954741: Epoch time: 125.53 s\n",
      "2024-01-01 22:14:42.044789: \n",
      "2024-01-01 22:14:42.051786: Epoch 812\n",
      "2024-01-01 22:14:42.056790: Current learning rate: 0.00222\n",
      "2024-01-01 22:16:48.686826: train_loss -0.9216\n",
      "2024-01-01 22:16:48.698824: val_loss -0.8189\n",
      "2024-01-01 22:16:48.707826: Pseudo dice [0.9231, 0.9446, 0.9437]\n",
      "2024-01-01 22:16:48.716829: Epoch time: 126.64 s\n",
      "2024-01-01 22:16:49.857081: \n",
      "2024-01-01 22:16:49.866593: Epoch 813\n",
      "2024-01-01 22:16:49.871593: Current learning rate: 0.00221\n",
      "2024-01-01 22:18:55.909337: train_loss -0.9217\n",
      "2024-01-01 22:18:55.919343: val_loss -0.8284\n",
      "2024-01-01 22:18:55.928398: Pseudo dice [0.9276, 0.9448, 0.944]\n",
      "2024-01-01 22:18:55.938398: Epoch time: 126.05 s\n",
      "2024-01-01 22:18:57.044256: \n",
      "2024-01-01 22:18:57.051245: Epoch 814\n",
      "2024-01-01 22:18:57.057243: Current learning rate: 0.0022\n",
      "2024-01-01 22:21:03.215925: train_loss -0.9235\n",
      "2024-01-01 22:21:03.225925: val_loss -0.826\n",
      "2024-01-01 22:21:03.236264: Pseudo dice [0.9256, 0.9444, 0.9448]\n",
      "2024-01-01 22:21:03.244269: Epoch time: 126.17 s\n",
      "2024-01-01 22:21:04.370927: \n",
      "2024-01-01 22:21:04.376929: Epoch 815\n",
      "2024-01-01 22:21:04.381930: Current learning rate: 0.00219\n",
      "2024-01-01 22:23:10.672431: train_loss -0.9219\n",
      "2024-01-01 22:23:10.682434: val_loss -0.8255\n",
      "2024-01-01 22:23:10.690442: Pseudo dice [0.9246, 0.9444, 0.9436]\n",
      "2024-01-01 22:23:10.697437: Epoch time: 126.3 s\n",
      "2024-01-01 22:23:11.836299: \n",
      "2024-01-01 22:23:11.846304: Epoch 816\n",
      "2024-01-01 22:23:11.852286: Current learning rate: 0.00218\n",
      "2024-01-01 22:25:17.594636: train_loss -0.9196\n",
      "2024-01-01 22:25:17.604639: val_loss -0.8296\n",
      "2024-01-01 22:25:17.613635: Pseudo dice [0.926, 0.945, 0.9438]\n",
      "2024-01-01 22:25:17.620636: Epoch time: 125.76 s\n",
      "2024-01-01 22:25:18.710359: \n",
      "2024-01-01 22:25:18.716362: Epoch 817\n",
      "2024-01-01 22:25:18.721361: Current learning rate: 0.00217\n",
      "2024-01-01 22:27:24.527184: train_loss -0.9208\n",
      "2024-01-01 22:27:24.537180: val_loss -0.8226\n",
      "2024-01-01 22:27:24.545177: Pseudo dice [0.9255, 0.9453, 0.9446]\n",
      "2024-01-01 22:27:24.554176: Epoch time: 125.82 s\n",
      "2024-01-01 22:27:25.677412: \n",
      "2024-01-01 22:27:25.690497: Epoch 818\n",
      "2024-01-01 22:27:25.695424: Current learning rate: 0.00216\n",
      "2024-01-01 22:29:31.192100: train_loss -0.9239\n",
      "2024-01-01 22:29:31.202103: val_loss -0.8258\n",
      "2024-01-01 22:29:31.211099: Pseudo dice [0.9247, 0.944, 0.9434]\n",
      "2024-01-01 22:29:31.219098: Epoch time: 125.51 s\n",
      "2024-01-01 22:29:32.481590: \n",
      "2024-01-01 22:29:32.488587: Epoch 819\n",
      "2024-01-01 22:29:32.494580: Current learning rate: 0.00215\n",
      "2024-01-01 22:31:37.821772: train_loss -0.9272\n",
      "2024-01-01 22:31:37.829772: val_loss -0.8273\n",
      "2024-01-01 22:31:37.838772: Pseudo dice [0.9246, 0.9447, 0.9443]\n",
      "2024-01-01 22:31:37.845771: Epoch time: 125.34 s\n",
      "2024-01-01 22:31:38.865259: \n",
      "2024-01-01 22:31:38.872252: Epoch 820\n",
      "2024-01-01 22:31:38.877252: Current learning rate: 0.00214\n",
      "2024-01-01 22:33:44.562906: train_loss -0.9233\n",
      "2024-01-01 22:33:44.571906: val_loss -0.8334\n",
      "2024-01-01 22:33:44.578906: Pseudo dice [0.9282, 0.9452, 0.9435]\n",
      "2024-01-01 22:33:44.586906: Epoch time: 125.7 s\n",
      "2024-01-01 22:33:45.610154: \n",
      "2024-01-01 22:33:45.616253: Epoch 821\n",
      "2024-01-01 22:33:45.622154: Current learning rate: 0.00213\n",
      "2024-01-01 22:35:51.672872: train_loss -0.9248\n",
      "2024-01-01 22:35:51.682870: val_loss -0.827\n",
      "2024-01-01 22:35:51.692877: Pseudo dice [0.9264, 0.9457, 0.9448]\n",
      "2024-01-01 22:35:51.704387: Epoch time: 126.06 s\n",
      "2024-01-01 22:35:53.158539: \n",
      "2024-01-01 22:35:53.166966: Epoch 822\n",
      "2024-01-01 22:35:53.175965: Current learning rate: 0.00212\n",
      "2024-01-01 22:38:10.191732: train_loss -0.9229\n",
      "2024-01-01 22:38:10.206301: val_loss -0.8241\n",
      "2024-01-01 22:38:10.216831: Pseudo dice [0.9247, 0.9437, 0.9422]\n",
      "2024-01-01 22:38:10.229346: Epoch time: 137.03 s\n",
      "2024-01-01 22:38:12.771651: \n",
      "2024-01-01 22:38:12.781673: Epoch 823\n",
      "2024-01-01 22:38:12.791698: Current learning rate: 0.0021\n",
      "2024-01-01 22:40:24.005318: train_loss -0.9232\n",
      "2024-01-01 22:40:24.023284: val_loss -0.8262\n",
      "2024-01-01 22:40:24.038878: Pseudo dice [0.925, 0.9442, 0.9433]\n",
      "2024-01-01 22:40:24.053230: Epoch time: 131.23 s\n",
      "2024-01-01 22:40:26.029898: \n",
      "2024-01-01 22:40:26.038250: Epoch 824\n",
      "2024-01-01 22:40:26.045768: Current learning rate: 0.00209\n",
      "2024-01-01 22:42:42.949490: train_loss -0.9237\n",
      "2024-01-01 22:42:42.966505: val_loss -0.823\n",
      "2024-01-01 22:42:42.987020: Pseudo dice [0.9266, 0.9447, 0.9446]\n",
      "2024-01-01 22:42:42.999020: Epoch time: 136.92 s\n",
      "2024-01-01 22:42:44.890936: \n",
      "2024-01-01 22:42:44.903929: Epoch 825\n",
      "2024-01-01 22:42:44.909928: Current learning rate: 0.00208\n",
      "2024-01-01 22:44:53.801740: train_loss -0.9225\n",
      "2024-01-01 22:44:53.817740: val_loss -0.8269\n",
      "2024-01-01 22:44:53.827737: Pseudo dice [0.9258, 0.9448, 0.9441]\n",
      "2024-01-01 22:44:53.838743: Epoch time: 128.91 s\n",
      "2024-01-01 22:44:55.673288: \n",
      "2024-01-01 22:44:55.686367: Epoch 826\n",
      "2024-01-01 22:44:55.692680: Current learning rate: 0.00207\n",
      "2024-01-01 22:47:07.074773: train_loss -0.9208\n",
      "2024-01-01 22:47:07.112080: val_loss -0.8297\n",
      "2024-01-01 22:47:07.135394: Pseudo dice [0.9253, 0.9445, 0.9442]\n",
      "2024-01-01 22:47:07.147558: Epoch time: 131.4 s\n",
      "2024-01-01 22:47:09.500882: \n",
      "2024-01-01 22:47:09.545010: Epoch 827\n",
      "2024-01-01 22:47:09.574355: Current learning rate: 0.00206\n",
      "2024-01-01 22:49:29.380394: train_loss -0.9231\n",
      "2024-01-01 22:49:29.396927: val_loss -0.83\n",
      "2024-01-01 22:49:29.423694: Pseudo dice [0.926, 0.9448, 0.9435]\n",
      "2024-01-01 22:49:29.484100: Epoch time: 139.88 s\n",
      "2024-01-01 22:49:31.983381: \n",
      "2024-01-01 22:49:32.000383: Epoch 828\n",
      "2024-01-01 22:49:32.008379: Current learning rate: 0.00205\n",
      "2024-01-01 22:51:44.056978: train_loss -0.9213\n",
      "2024-01-01 22:51:44.072703: val_loss -0.8252\n",
      "2024-01-01 22:51:44.087702: Pseudo dice [0.9255, 0.9447, 0.9439]\n",
      "2024-01-01 22:51:44.102710: Epoch time: 132.07 s\n",
      "2024-01-01 22:51:45.920105: \n",
      "2024-01-01 22:51:45.929102: Epoch 829\n",
      "2024-01-01 22:51:45.937102: Current learning rate: 0.00204\n",
      "2024-01-01 22:53:56.854586: train_loss -0.9221\n",
      "2024-01-01 22:53:56.870587: val_loss -0.8202\n",
      "2024-01-01 22:53:56.882586: Pseudo dice [0.923, 0.9438, 0.9424]\n",
      "2024-01-01 22:53:56.895587: Epoch time: 130.94 s\n",
      "2024-01-01 22:53:58.499720: \n",
      "2024-01-01 22:53:58.509831: Epoch 830\n",
      "2024-01-01 22:53:58.521392: Current learning rate: 0.00203\n",
      "2024-01-01 22:56:07.579818: train_loss -0.9236\n",
      "2024-01-01 22:56:07.600349: val_loss -0.8202\n",
      "2024-01-01 22:56:07.615545: Pseudo dice [0.9232, 0.9446, 0.9436]\n",
      "2024-01-01 22:56:07.628264: Epoch time: 129.08 s\n",
      "2024-01-01 22:56:09.112750: \n",
      "2024-01-01 22:56:09.123228: Epoch 831\n",
      "2024-01-01 22:56:09.133226: Current learning rate: 0.00202\n",
      "2024-01-01 22:58:28.501206: train_loss -0.9208\n",
      "2024-01-01 22:58:28.636025: val_loss -0.8362\n",
      "2024-01-01 22:58:28.741254: Pseudo dice [0.9275, 0.9445, 0.9451]\n",
      "2024-01-01 22:58:28.841955: Epoch time: 139.39 s\n",
      "2024-01-01 22:58:31.951249: \n",
      "2024-01-01 22:58:31.959778: Epoch 832\n",
      "2024-01-01 22:58:31.969780: Current learning rate: 0.00201\n",
      "2024-01-01 23:00:44.658175: train_loss -0.9244\n",
      "2024-01-01 23:00:44.670465: val_loss -0.8292\n",
      "2024-01-01 23:00:44.678985: Pseudo dice [0.9272, 0.9448, 0.9436]\n",
      "2024-01-01 23:00:44.690438: Epoch time: 132.71 s\n",
      "2024-01-01 23:00:46.184731: \n",
      "2024-01-01 23:00:46.191723: Epoch 833\n",
      "2024-01-01 23:00:46.197739: Current learning rate: 0.002\n",
      "2024-01-01 23:02:57.119062: train_loss -0.9236\n",
      "2024-01-01 23:02:57.137063: val_loss -0.8208\n",
      "2024-01-01 23:02:57.182061: Pseudo dice [0.9256, 0.9445, 0.9447]\n",
      "2024-01-01 23:02:57.198282: Epoch time: 130.94 s\n",
      "2024-01-01 23:02:59.734832: \n",
      "2024-01-01 23:02:59.744291: Epoch 834\n",
      "2024-01-01 23:02:59.755293: Current learning rate: 0.00199\n",
      "2024-01-01 23:05:10.100290: train_loss -0.9236\n",
      "2024-01-01 23:05:10.113290: val_loss -0.8307\n",
      "2024-01-01 23:05:10.124017: Pseudo dice [0.925, 0.9449, 0.9439]\n",
      "2024-01-01 23:05:10.135172: Epoch time: 130.37 s\n",
      "2024-01-01 23:05:11.726753: \n",
      "2024-01-01 23:05:11.732825: Epoch 835\n",
      "2024-01-01 23:05:11.739828: Current learning rate: 0.00198\n",
      "2024-01-01 23:07:20.121400: train_loss -0.923\n",
      "2024-01-01 23:07:20.132390: val_loss -0.8267\n",
      "2024-01-01 23:07:20.143594: Pseudo dice [0.9242, 0.9439, 0.9429]\n",
      "2024-01-01 23:07:20.152596: Epoch time: 128.4 s\n",
      "2024-01-01 23:07:21.425660: \n",
      "2024-01-01 23:07:21.432659: Epoch 836\n",
      "2024-01-01 23:07:21.438691: Current learning rate: 0.00196\n",
      "2024-01-01 23:09:29.724918: train_loss -0.9219\n",
      "2024-01-01 23:09:29.734919: val_loss -0.8136\n",
      "2024-01-01 23:09:29.746444: Pseudo dice [0.9192, 0.942, 0.9404]\n",
      "2024-01-01 23:09:29.755439: Epoch time: 128.3 s\n",
      "2024-01-01 23:09:31.065817: \n",
      "2024-01-01 23:09:31.071831: Epoch 837\n",
      "2024-01-01 23:09:31.077842: Current learning rate: 0.00195\n",
      "2024-01-01 23:11:39.442906: train_loss -0.9277\n",
      "2024-01-01 23:11:39.454959: val_loss -0.8224\n",
      "2024-01-01 23:11:39.468965: Pseudo dice [0.9218, 0.9431, 0.9426]\n",
      "2024-01-01 23:11:39.484878: Epoch time: 128.38 s\n",
      "2024-01-01 23:11:41.458259: \n",
      "2024-01-01 23:11:41.465257: Epoch 838\n",
      "2024-01-01 23:11:41.471263: Current learning rate: 0.00194\n",
      "2024-01-01 23:13:49.775608: train_loss -0.9213\n",
      "2024-01-01 23:13:49.787611: val_loss -0.8201\n",
      "2024-01-01 23:13:49.798640: Pseudo dice [0.9243, 0.9437, 0.9432]\n",
      "2024-01-01 23:13:49.807638: Epoch time: 128.32 s\n",
      "2024-01-01 23:13:51.057101: \n",
      "2024-01-01 23:13:51.063070: Epoch 839\n",
      "2024-01-01 23:13:51.068169: Current learning rate: 0.00193\n",
      "2024-01-01 23:15:57.665958: train_loss -0.9229\n",
      "2024-01-01 23:15:57.677965: val_loss -0.8369\n",
      "2024-01-01 23:15:57.686469: Pseudo dice [0.9271, 0.9459, 0.9455]\n",
      "2024-01-01 23:15:57.694977: Epoch time: 126.61 s\n",
      "2024-01-01 23:15:58.860082: \n",
      "2024-01-01 23:15:58.866077: Epoch 840\n",
      "2024-01-01 23:15:58.872069: Current learning rate: 0.00192\n",
      "2024-01-01 23:18:05.134340: train_loss -0.9222\n",
      "2024-01-01 23:18:05.142342: val_loss -0.8297\n",
      "2024-01-01 23:18:05.150348: Pseudo dice [0.926, 0.9448, 0.944]\n",
      "2024-01-01 23:18:05.158342: Epoch time: 126.28 s\n",
      "2024-01-01 23:18:06.378312: \n",
      "2024-01-01 23:18:06.385322: Epoch 841\n",
      "2024-01-01 23:18:06.396328: Current learning rate: 0.00191\n",
      "2024-01-01 23:20:12.593909: train_loss -0.9239\n",
      "2024-01-01 23:20:12.606908: val_loss -0.8213\n",
      "2024-01-01 23:20:12.617918: Pseudo dice [0.9247, 0.9442, 0.944]\n",
      "2024-01-01 23:20:12.624945: Epoch time: 126.22 s\n",
      "2024-01-01 23:20:14.043089: \n",
      "2024-01-01 23:20:14.050092: Epoch 842\n",
      "2024-01-01 23:20:14.055093: Current learning rate: 0.0019\n",
      "2024-01-01 23:22:22.017517: train_loss -0.9221\n",
      "2024-01-01 23:22:22.029640: val_loss -0.8241\n",
      "2024-01-01 23:22:22.040762: Pseudo dice [0.9233, 0.9442, 0.9429]\n",
      "2024-01-01 23:22:22.050765: Epoch time: 127.98 s\n",
      "2024-01-01 23:22:23.159349: \n",
      "2024-01-01 23:22:23.165687: Epoch 843\n",
      "2024-01-01 23:22:23.171680: Current learning rate: 0.00189\n",
      "2024-01-01 23:24:32.000846: train_loss -0.9215\n",
      "2024-01-01 23:24:32.016772: val_loss -0.826\n",
      "2024-01-01 23:24:32.030904: Pseudo dice [0.9251, 0.9439, 0.9431]\n",
      "2024-01-01 23:24:32.046914: Epoch time: 128.84 s\n",
      "2024-01-01 23:24:34.009507: \n",
      "2024-01-01 23:24:34.018503: Epoch 844\n",
      "2024-01-01 23:24:34.024511: Current learning rate: 0.00188\n",
      "2024-01-01 23:26:43.328127: train_loss -0.9241\n",
      "2024-01-01 23:26:43.342268: val_loss -0.8186\n",
      "2024-01-01 23:26:43.356269: Pseudo dice [0.9234, 0.9435, 0.9439]\n",
      "2024-01-01 23:26:43.372269: Epoch time: 129.32 s\n",
      "2024-01-01 23:26:45.144825: \n",
      "2024-01-01 23:26:45.150829: Epoch 845\n",
      "2024-01-01 23:26:45.157827: Current learning rate: 0.00187\n",
      "2024-01-01 23:28:51.623212: train_loss -0.9249\n",
      "2024-01-01 23:28:51.632215: val_loss -0.8257\n",
      "2024-01-01 23:28:51.640215: Pseudo dice [0.9253, 0.9434, 0.9431]\n",
      "2024-01-01 23:28:51.647218: Epoch time: 126.48 s\n",
      "2024-01-01 23:28:52.891107: \n",
      "2024-01-01 23:28:52.901777: Epoch 846\n",
      "2024-01-01 23:28:52.910782: Current learning rate: 0.00186\n",
      "2024-01-01 23:31:00.735302: train_loss -0.9211\n",
      "2024-01-01 23:31:00.745303: val_loss -0.8133\n",
      "2024-01-01 23:31:00.756467: Pseudo dice [0.9179, 0.9405, 0.9386]\n",
      "2024-01-01 23:31:00.766982: Epoch time: 127.85 s\n",
      "2024-01-01 23:31:02.519489: \n",
      "2024-01-01 23:31:02.526489: Epoch 847\n",
      "2024-01-01 23:31:02.531991: Current learning rate: 0.00185\n",
      "2024-01-01 23:33:08.929543: train_loss -0.9226\n",
      "2024-01-01 23:33:08.935544: val_loss -0.819\n",
      "2024-01-01 23:33:08.944549: Pseudo dice [0.9204, 0.9424, 0.9415]\n",
      "2024-01-01 23:33:08.952613: Epoch time: 126.41 s\n",
      "2024-01-01 23:33:10.034676: \n",
      "2024-01-01 23:33:10.043495: Epoch 848\n",
      "2024-01-01 23:33:10.049426: Current learning rate: 0.00184\n",
      "2024-01-01 23:35:15.758179: train_loss -0.9235\n",
      "2024-01-01 23:35:15.767179: val_loss -0.8211\n",
      "2024-01-01 23:35:15.775689: Pseudo dice [0.9235, 0.9434, 0.9428]\n",
      "2024-01-01 23:35:15.781691: Epoch time: 125.72 s\n",
      "2024-01-01 23:35:16.911502: \n",
      "2024-01-01 23:35:16.917671: Epoch 849\n",
      "2024-01-01 23:35:16.926750: Current learning rate: 0.00182\n",
      "2024-01-01 23:37:22.623518: train_loss -0.9241\n",
      "2024-01-01 23:37:22.632514: val_loss -0.8211\n",
      "2024-01-01 23:37:22.639514: Pseudo dice [0.9213, 0.9416, 0.9396]\n",
      "2024-01-01 23:37:22.647514: Epoch time: 125.71 s\n",
      "2024-01-01 23:37:24.306596: \n",
      "2024-01-01 23:37:24.312463: Epoch 850\n",
      "2024-01-01 23:37:24.317476: Current learning rate: 0.00181\n",
      "2024-01-01 23:39:30.932704: train_loss -0.9212\n",
      "2024-01-01 23:39:30.943709: val_loss -0.8255\n",
      "2024-01-01 23:39:30.949710: Pseudo dice [0.9239, 0.9434, 0.9429]\n",
      "2024-01-01 23:39:30.957221: Epoch time: 126.63 s\n",
      "2024-01-01 23:39:32.076282: \n",
      "2024-01-01 23:39:32.083349: Epoch 851\n",
      "2024-01-01 23:39:32.088420: Current learning rate: 0.0018\n",
      "2024-01-01 23:41:38.671649: train_loss -0.9222\n",
      "2024-01-01 23:41:38.679647: val_loss -0.8215\n",
      "2024-01-01 23:41:38.687719: Pseudo dice [0.9229, 0.9448, 0.9439]\n",
      "2024-01-01 23:41:38.694649: Epoch time: 126.6 s\n",
      "2024-01-01 23:41:39.845853: \n",
      "2024-01-01 23:41:39.858531: Epoch 852\n",
      "2024-01-01 23:41:39.863750: Current learning rate: 0.00179\n",
      "2024-01-01 23:43:46.060131: train_loss -0.9251\n",
      "2024-01-01 23:43:46.070129: val_loss -0.8185\n",
      "2024-01-01 23:43:46.076120: Pseudo dice [0.9233, 0.9435, 0.9425]\n",
      "2024-01-01 23:43:46.084115: Epoch time: 126.22 s\n",
      "2024-01-01 23:43:47.232360: \n",
      "2024-01-01 23:43:47.238219: Epoch 853\n",
      "2024-01-01 23:43:47.249530: Current learning rate: 0.00178\n",
      "2024-01-01 23:45:52.943707: train_loss -0.9242\n",
      "2024-01-01 23:45:52.952716: val_loss -0.829\n",
      "2024-01-01 23:45:52.960716: Pseudo dice [0.9274, 0.9441, 0.9429]\n",
      "2024-01-01 23:45:52.966707: Epoch time: 125.71 s\n",
      "2024-01-01 23:45:54.065743: \n",
      "2024-01-01 23:45:54.074830: Epoch 854\n",
      "2024-01-01 23:45:54.079902: Current learning rate: 0.00177\n",
      "2024-01-01 23:47:59.822566: train_loss -0.9248\n",
      "2024-01-01 23:47:59.830569: val_loss -0.8225\n",
      "2024-01-01 23:47:59.837569: Pseudo dice [0.9231, 0.9432, 0.942]\n",
      "2024-01-01 23:47:59.843074: Epoch time: 125.76 s\n",
      "2024-01-01 23:48:00.941486: \n",
      "2024-01-01 23:48:00.949715: Epoch 855\n",
      "2024-01-01 23:48:00.954708: Current learning rate: 0.00176\n",
      "2024-01-01 23:50:06.663855: train_loss -0.9214\n",
      "2024-01-01 23:50:06.670853: val_loss -0.8302\n",
      "2024-01-01 23:50:06.676853: Pseudo dice [0.9267, 0.944, 0.9432]\n",
      "2024-01-01 23:50:06.683857: Epoch time: 125.72 s\n",
      "2024-01-01 23:50:07.813720: \n",
      "2024-01-01 23:50:07.820661: Epoch 856\n",
      "2024-01-01 23:50:07.825769: Current learning rate: 0.00175\n",
      "2024-01-01 23:52:13.157897: train_loss -0.9259\n",
      "2024-01-01 23:52:13.166906: val_loss -0.8216\n",
      "2024-01-01 23:52:13.173916: Pseudo dice [0.9229, 0.9444, 0.9441]\n",
      "2024-01-01 23:52:13.181210: Epoch time: 125.35 s\n",
      "2024-01-01 23:52:14.297432: \n",
      "2024-01-01 23:52:14.304092: Epoch 857\n",
      "2024-01-01 23:52:14.309084: Current learning rate: 0.00174\n",
      "2024-01-01 23:54:19.647829: train_loss -0.9242\n",
      "2024-01-01 23:54:19.656822: val_loss -0.8255\n",
      "2024-01-01 23:54:19.662821: Pseudo dice [0.9256, 0.945, 0.9447]\n",
      "2024-01-01 23:54:19.667830: Epoch time: 125.35 s\n",
      "2024-01-01 23:54:20.903975: \n",
      "2024-01-01 23:54:20.909270: Epoch 858\n",
      "2024-01-01 23:54:20.914222: Current learning rate: 0.00173\n",
      "2024-01-01 23:56:26.568607: train_loss -0.9224\n",
      "2024-01-01 23:56:26.575603: val_loss -0.8264\n",
      "2024-01-01 23:56:26.582611: Pseudo dice [0.9254, 0.9455, 0.9446]\n",
      "2024-01-01 23:56:26.591610: Epoch time: 125.67 s\n",
      "2024-01-01 23:56:27.672965: \n",
      "2024-01-01 23:56:27.682100: Epoch 859\n",
      "2024-01-01 23:56:27.686847: Current learning rate: 0.00172\n",
      "2024-01-01 23:58:33.153234: train_loss -0.9249\n",
      "2024-01-01 23:58:33.160231: val_loss -0.8275\n",
      "2024-01-01 23:58:33.165232: Pseudo dice [0.9246, 0.9443, 0.9431]\n",
      "2024-01-01 23:58:33.171246: Epoch time: 125.48 s\n",
      "2024-01-01 23:58:34.270711: \n",
      "2024-01-01 23:58:34.277318: Epoch 860\n",
      "2024-01-01 23:58:34.287393: Current learning rate: 0.0017\n",
      "2024-01-02 00:00:39.849937: train_loss -0.9248\n",
      "2024-01-02 00:00:39.858054: val_loss -0.8316\n",
      "2024-01-02 00:00:39.865063: Pseudo dice [0.9275, 0.9455, 0.9446]\n",
      "2024-01-02 00:00:39.874063: Epoch time: 125.58 s\n",
      "2024-01-02 00:00:41.026382: \n",
      "2024-01-02 00:00:41.032102: Epoch 861\n",
      "2024-01-02 00:00:41.037166: Current learning rate: 0.00169\n",
      "2024-01-02 00:02:46.357198: train_loss -0.9271\n",
      "2024-01-02 00:02:46.364709: val_loss -0.8233\n",
      "2024-01-02 00:02:46.373782: Pseudo dice [0.9267, 0.9447, 0.9439]\n",
      "2024-01-02 00:02:46.381783: Epoch time: 125.33 s\n",
      "2024-01-02 00:02:47.520705: \n",
      "2024-01-02 00:02:47.531354: Epoch 862\n",
      "2024-01-02 00:02:47.537354: Current learning rate: 0.00168\n",
      "2024-01-02 00:04:53.371109: train_loss -0.923\n",
      "2024-01-02 00:04:53.381109: val_loss -0.8292\n",
      "2024-01-02 00:04:53.387115: Pseudo dice [0.9254, 0.9453, 0.9441]\n",
      "2024-01-02 00:04:53.393114: Epoch time: 125.85 s\n",
      "2024-01-02 00:04:54.529470: \n",
      "2024-01-02 00:04:54.536000: Epoch 863\n",
      "2024-01-02 00:04:54.544068: Current learning rate: 0.00167\n",
      "2024-01-02 00:07:00.138057: train_loss -0.9251\n",
      "2024-01-02 00:07:00.145053: val_loss -0.8389\n",
      "2024-01-02 00:07:00.150053: Pseudo dice [0.9272, 0.9443, 0.9443]\n",
      "2024-01-02 00:07:00.156053: Epoch time: 125.61 s\n",
      "2024-01-02 00:07:01.254038: \n",
      "2024-01-02 00:07:01.263027: Epoch 864\n",
      "2024-01-02 00:07:01.267100: Current learning rate: 0.00166\n",
      "2024-01-02 00:09:06.969319: train_loss -0.9247\n",
      "2024-01-02 00:09:06.979321: val_loss -0.8222\n",
      "2024-01-02 00:09:06.984319: Pseudo dice [0.9248, 0.9442, 0.9433]\n",
      "2024-01-02 00:09:06.991320: Epoch time: 125.72 s\n",
      "2024-01-02 00:09:08.111382: \n",
      "2024-01-02 00:09:08.120386: Epoch 865\n",
      "2024-01-02 00:09:08.128323: Current learning rate: 0.00165\n",
      "2024-01-02 00:11:13.592528: train_loss -0.9243\n",
      "2024-01-02 00:11:13.600537: val_loss -0.8245\n",
      "2024-01-02 00:11:13.607531: Pseudo dice [0.9258, 0.9454, 0.9441]\n",
      "2024-01-02 00:11:13.615532: Epoch time: 125.48 s\n",
      "2024-01-02 00:11:14.858128: \n",
      "2024-01-02 00:11:14.863136: Epoch 866\n",
      "2024-01-02 00:11:14.868134: Current learning rate: 0.00164\n",
      "2024-01-02 00:13:20.289418: train_loss -0.9265\n",
      "2024-01-02 00:13:20.296420: val_loss -0.8232\n",
      "2024-01-02 00:13:20.301418: Pseudo dice [0.9257, 0.9464, 0.9456]\n",
      "2024-01-02 00:13:20.309431: Epoch time: 125.43 s\n",
      "2024-01-02 00:13:21.457883: \n",
      "2024-01-02 00:13:21.466961: Epoch 867\n",
      "2024-01-02 00:13:21.471943: Current learning rate: 0.00163\n",
      "2024-01-02 00:15:27.056999: train_loss -0.9222\n",
      "2024-01-02 00:15:27.064993: val_loss -0.8272\n",
      "2024-01-02 00:15:27.070992: Pseudo dice [0.9261, 0.9451, 0.9443]\n",
      "2024-01-02 00:15:27.076993: Epoch time: 125.6 s\n",
      "2024-01-02 00:15:28.226319: \n",
      "2024-01-02 00:15:28.232090: Epoch 868\n",
      "2024-01-02 00:15:28.237156: Current learning rate: 0.00162\n",
      "2024-01-02 00:17:33.930908: train_loss -0.9229\n",
      "2024-01-02 00:17:33.939899: val_loss -0.8213\n",
      "2024-01-02 00:17:33.945905: Pseudo dice [0.923, 0.9437, 0.9424]\n",
      "2024-01-02 00:17:33.953494: Epoch time: 125.71 s\n",
      "2024-01-02 00:17:35.084075: \n",
      "2024-01-02 00:17:35.091134: Epoch 869\n",
      "2024-01-02 00:17:35.096692: Current learning rate: 0.00161\n",
      "2024-01-02 00:19:40.980917: train_loss -0.9228\n",
      "2024-01-02 00:19:40.989940: val_loss -0.8222\n",
      "2024-01-02 00:19:40.995925: Pseudo dice [0.9257, 0.9455, 0.9444]\n",
      "2024-01-02 00:19:41.003603: Epoch time: 125.9 s\n",
      "2024-01-02 00:19:42.087813: \n",
      "2024-01-02 00:19:42.094809: Epoch 870\n",
      "2024-01-02 00:19:42.100314: Current learning rate: 0.00159\n",
      "2024-01-02 00:21:47.481169: train_loss -0.9266\n",
      "2024-01-02 00:21:47.489165: val_loss -0.8269\n",
      "2024-01-02 00:21:47.494165: Pseudo dice [0.9264, 0.9447, 0.9436]\n",
      "2024-01-02 00:21:47.499169: Epoch time: 125.39 s\n",
      "2024-01-02 00:21:48.654505: \n",
      "2024-01-02 00:21:48.659696: Epoch 871\n",
      "2024-01-02 00:21:48.664665: Current learning rate: 0.00158\n",
      "2024-01-02 00:23:54.301339: train_loss -0.9236\n",
      "2024-01-02 00:23:54.311900: val_loss -0.8201\n",
      "2024-01-02 00:23:54.319898: Pseudo dice [0.9249, 0.9441, 0.9436]\n",
      "2024-01-02 00:23:54.326899: Epoch time: 125.65 s\n",
      "2024-01-02 00:23:55.438562: \n",
      "2024-01-02 00:23:55.444562: Epoch 872\n",
      "2024-01-02 00:23:55.454561: Current learning rate: 0.00157\n",
      "2024-01-02 00:26:01.187843: train_loss -0.9249\n",
      "2024-01-02 00:26:01.197408: val_loss -0.8238\n",
      "2024-01-02 00:26:01.203411: Pseudo dice [0.9246, 0.9448, 0.9443]\n",
      "2024-01-02 00:26:01.210405: Epoch time: 125.75 s\n",
      "2024-01-02 00:26:02.311845: \n",
      "2024-01-02 00:26:02.318833: Epoch 873\n",
      "2024-01-02 00:26:02.323583: Current learning rate: 0.00156\n",
      "2024-01-02 00:28:08.010978: train_loss -0.9255\n",
      "2024-01-02 00:28:08.017484: val_loss -0.827\n",
      "2024-01-02 00:28:08.024484: Pseudo dice [0.9249, 0.943, 0.9424]\n",
      "2024-01-02 00:28:08.029997: Epoch time: 125.7 s\n",
      "2024-01-02 00:28:09.426242: \n",
      "2024-01-02 00:28:09.432360: Epoch 874\n",
      "2024-01-02 00:28:09.437364: Current learning rate: 0.00155\n",
      "2024-01-02 00:30:15.170092: train_loss -0.9253\n",
      "2024-01-02 00:30:15.180108: val_loss -0.8184\n",
      "2024-01-02 00:30:15.190116: Pseudo dice [0.9248, 0.9453, 0.9445]\n",
      "2024-01-02 00:30:15.199642: Epoch time: 125.74 s\n",
      "2024-01-02 00:30:16.367388: \n",
      "2024-01-02 00:30:16.380002: Epoch 875\n",
      "2024-01-02 00:30:16.388056: Current learning rate: 0.00154\n",
      "2024-01-02 00:32:22.289167: train_loss -0.9216\n",
      "2024-01-02 00:32:22.296167: val_loss -0.8214\n",
      "2024-01-02 00:32:22.301720: Pseudo dice [0.926, 0.9443, 0.9432]\n",
      "2024-01-02 00:32:22.306816: Epoch time: 125.92 s\n",
      "2024-01-02 00:32:23.499622: \n",
      "2024-01-02 00:32:23.507151: Epoch 876\n",
      "2024-01-02 00:32:23.511602: Current learning rate: 0.00153\n",
      "2024-01-02 00:34:28.784044: train_loss -0.9253\n",
      "2024-01-02 00:34:28.794046: val_loss -0.83\n",
      "2024-01-02 00:34:28.802051: Pseudo dice [0.9268, 0.944, 0.9431]\n",
      "2024-01-02 00:34:28.808048: Epoch time: 125.29 s\n",
      "2024-01-02 00:34:29.865007: \n",
      "2024-01-02 00:34:29.871047: Epoch 877\n",
      "2024-01-02 00:34:29.876039: Current learning rate: 0.00152\n",
      "2024-01-02 00:36:35.417711: train_loss -0.9252\n",
      "2024-01-02 00:36:35.427713: val_loss -0.8246\n",
      "2024-01-02 00:36:35.435713: Pseudo dice [0.9246, 0.9433, 0.9429]\n",
      "2024-01-02 00:36:35.443714: Epoch time: 125.55 s\n",
      "2024-01-02 00:36:36.617902: \n",
      "2024-01-02 00:36:36.624258: Epoch 878\n",
      "2024-01-02 00:36:36.632305: Current learning rate: 0.00151\n",
      "2024-01-02 00:38:42.429457: train_loss -0.9236\n",
      "2024-01-02 00:38:42.438522: val_loss -0.8286\n",
      "2024-01-02 00:38:42.443521: Pseudo dice [0.9253, 0.9437, 0.9431]\n",
      "2024-01-02 00:38:42.449519: Epoch time: 125.81 s\n",
      "2024-01-02 00:38:43.555340: \n",
      "2024-01-02 00:38:43.563334: Epoch 879\n",
      "2024-01-02 00:38:43.571275: Current learning rate: 0.00149\n",
      "2024-01-02 00:40:49.648246: train_loss -0.9201\n",
      "2024-01-02 00:40:49.655260: val_loss -0.8238\n",
      "2024-01-02 00:40:49.660243: Pseudo dice [0.9258, 0.9449, 0.9441]\n",
      "2024-01-02 00:40:49.665747: Epoch time: 126.09 s\n",
      "2024-01-02 00:40:50.726022: \n",
      "2024-01-02 00:40:50.731415: Epoch 880\n",
      "2024-01-02 00:40:50.736473: Current learning rate: 0.00148\n",
      "2024-01-02 00:42:56.226068: train_loss -0.9235\n",
      "2024-01-02 00:42:56.235066: val_loss -0.8253\n",
      "2024-01-02 00:42:56.241578: Pseudo dice [0.9254, 0.9451, 0.9445]\n",
      "2024-01-02 00:42:56.246575: Epoch time: 125.5 s\n",
      "2024-01-02 00:42:57.525102: \n",
      "2024-01-02 00:42:57.530355: Epoch 881\n",
      "2024-01-02 00:42:57.536073: Current learning rate: 0.00147\n",
      "2024-01-02 00:45:02.953063: train_loss -0.9258\n",
      "2024-01-02 00:45:02.962064: val_loss -0.8226\n",
      "2024-01-02 00:45:02.969064: Pseudo dice [0.9263, 0.9454, 0.9446]\n",
      "2024-01-02 00:45:02.976837: Epoch time: 125.43 s\n",
      "2024-01-02 00:45:04.137235: \n",
      "2024-01-02 00:45:04.144547: Epoch 882\n",
      "2024-01-02 00:45:04.149455: Current learning rate: 0.00146\n",
      "2024-01-02 00:47:09.774145: train_loss -0.9243\n",
      "2024-01-02 00:47:09.786661: val_loss -0.811\n",
      "2024-01-02 00:47:09.794664: Pseudo dice [0.9199, 0.9402, 0.9386]\n",
      "2024-01-02 00:47:09.799672: Epoch time: 125.64 s\n",
      "2024-01-02 00:47:10.888792: \n",
      "2024-01-02 00:47:10.894907: Epoch 883\n",
      "2024-01-02 00:47:10.899908: Current learning rate: 0.00145\n",
      "2024-01-02 00:49:16.745981: train_loss -0.9235\n",
      "2024-01-02 00:49:16.754981: val_loss -0.8262\n",
      "2024-01-02 00:49:16.763987: Pseudo dice [0.9227, 0.9425, 0.9416]\n",
      "2024-01-02 00:49:16.769987: Epoch time: 125.86 s\n",
      "2024-01-02 00:49:17.906489: \n",
      "2024-01-02 00:49:17.917496: Epoch 884\n",
      "2024-01-02 00:49:17.923472: Current learning rate: 0.00144\n",
      "2024-01-02 00:51:23.365590: train_loss -0.9263\n",
      "2024-01-02 00:51:23.372589: val_loss -0.8219\n",
      "2024-01-02 00:51:23.379008: Pseudo dice [0.923, 0.9437, 0.9429]\n",
      "2024-01-02 00:51:23.384018: Epoch time: 125.46 s\n",
      "2024-01-02 00:51:24.482446: \n",
      "2024-01-02 00:51:24.489538: Epoch 885\n",
      "2024-01-02 00:51:24.494620: Current learning rate: 0.00143\n",
      "2024-01-02 00:53:30.080686: train_loss -0.9241\n",
      "2024-01-02 00:53:30.090675: val_loss -0.8176\n",
      "2024-01-02 00:53:30.099684: Pseudo dice [0.9242, 0.944, 0.9432]\n",
      "2024-01-02 00:53:30.107674: Epoch time: 125.6 s\n",
      "2024-01-02 00:53:31.275095: \n",
      "2024-01-02 00:53:31.281905: Epoch 886\n",
      "2024-01-02 00:53:31.286970: Current learning rate: 0.00142\n",
      "2024-01-02 00:55:36.956723: train_loss -0.9257\n",
      "2024-01-02 00:55:36.963728: val_loss -0.8256\n",
      "2024-01-02 00:55:36.970735: Pseudo dice [0.9257, 0.9454, 0.9443]\n",
      "2024-01-02 00:55:36.979726: Epoch time: 125.68 s\n",
      "2024-01-02 00:55:38.129414: \n",
      "2024-01-02 00:55:38.137881: Epoch 887\n",
      "2024-01-02 00:55:38.143611: Current learning rate: 0.00141\n",
      "2024-01-02 00:57:43.877063: train_loss -0.9275\n",
      "2024-01-02 00:57:43.884064: val_loss -0.8217\n",
      "2024-01-02 00:57:43.890736: Pseudo dice [0.9247, 0.9445, 0.9441]\n",
      "2024-01-02 00:57:43.900759: Epoch time: 125.75 s\n",
      "2024-01-02 00:57:45.007947: \n",
      "2024-01-02 00:57:45.020177: Epoch 888\n",
      "2024-01-02 00:57:45.026775: Current learning rate: 0.00139\n",
      "2024-01-02 00:59:50.657365: train_loss -0.9245\n",
      "2024-01-02 00:59:50.665366: val_loss -0.8312\n",
      "2024-01-02 00:59:50.671374: Pseudo dice [0.9248, 0.9456, 0.9448]\n",
      "2024-01-02 00:59:50.676373: Epoch time: 125.65 s\n",
      "2024-01-02 00:59:52.090551: \n",
      "2024-01-02 00:59:52.096551: Epoch 889\n",
      "2024-01-02 00:59:52.102553: Current learning rate: 0.00138\n",
      "2024-01-02 01:01:57.807162: train_loss -0.9263\n",
      "2024-01-02 01:01:57.814154: val_loss -0.8266\n",
      "2024-01-02 01:01:57.822153: Pseudo dice [0.9243, 0.9448, 0.9442]\n",
      "2024-01-02 01:01:57.828153: Epoch time: 125.72 s\n",
      "2024-01-02 01:01:59.051900: \n",
      "2024-01-02 01:01:59.056951: Epoch 890\n",
      "2024-01-02 01:01:59.066034: Current learning rate: 0.00137\n",
      "2024-01-02 01:04:04.496724: train_loss -0.926\n",
      "2024-01-02 01:04:04.503725: val_loss -0.8267\n",
      "2024-01-02 01:04:04.508729: Pseudo dice [0.9236, 0.9429, 0.9416]\n",
      "2024-01-02 01:04:04.515728: Epoch time: 125.45 s\n",
      "2024-01-02 01:04:05.718864: \n",
      "2024-01-02 01:04:05.729437: Epoch 891\n",
      "2024-01-02 01:04:05.734496: Current learning rate: 0.00136\n",
      "2024-01-02 01:06:11.418159: train_loss -0.9255\n",
      "2024-01-02 01:06:11.426150: val_loss -0.8275\n",
      "2024-01-02 01:06:11.437150: Pseudo dice [0.9244, 0.9455, 0.9446]\n",
      "2024-01-02 01:06:11.442150: Epoch time: 125.7 s\n",
      "2024-01-02 01:06:12.530219: \n",
      "2024-01-02 01:06:12.536215: Epoch 892\n",
      "2024-01-02 01:06:12.543314: Current learning rate: 0.00135\n",
      "2024-01-02 01:08:18.044669: train_loss -0.925\n",
      "2024-01-02 01:08:18.052668: val_loss -0.8198\n",
      "2024-01-02 01:08:18.061670: Pseudo dice [0.9244, 0.9439, 0.9433]\n",
      "2024-01-02 01:08:18.067668: Epoch time: 125.52 s\n",
      "2024-01-02 01:08:19.180618: \n",
      "2024-01-02 01:08:19.186159: Epoch 893\n",
      "2024-01-02 01:08:19.191161: Current learning rate: 0.00134\n",
      "2024-01-02 01:10:25.021393: train_loss -0.923\n",
      "2024-01-02 01:10:25.029393: val_loss -0.8289\n",
      "2024-01-02 01:10:25.035394: Pseudo dice [0.9246, 0.9442, 0.9431]\n",
      "2024-01-02 01:10:25.041394: Epoch time: 125.84 s\n",
      "2024-01-02 01:10:26.163229: \n",
      "2024-01-02 01:10:26.170082: Epoch 894\n",
      "2024-01-02 01:10:26.174231: Current learning rate: 0.00133\n",
      "2024-01-02 01:12:31.664762: train_loss -0.9243\n",
      "2024-01-02 01:12:31.672763: val_loss -0.8274\n",
      "2024-01-02 01:12:31.679766: Pseudo dice [0.9264, 0.9447, 0.9432]\n",
      "2024-01-02 01:12:31.684764: Epoch time: 125.5 s\n",
      "2024-01-02 01:12:32.832970: \n",
      "2024-01-02 01:12:32.839232: Epoch 895\n",
      "2024-01-02 01:12:32.844314: Current learning rate: 0.00132\n",
      "2024-01-02 01:14:38.692677: train_loss -0.9263\n",
      "2024-01-02 01:14:38.700681: val_loss -0.8289\n",
      "2024-01-02 01:14:38.706681: Pseudo dice [0.9243, 0.9423, 0.9416]\n",
      "2024-01-02 01:14:38.711185: Epoch time: 125.86 s\n",
      "2024-01-02 01:14:39.837576: \n",
      "2024-01-02 01:14:39.842642: Epoch 896\n",
      "2024-01-02 01:14:39.847637: Current learning rate: 0.0013\n",
      "2024-01-02 01:16:45.365895: train_loss -0.9251\n",
      "2024-01-02 01:16:45.373898: val_loss -0.8288\n",
      "2024-01-02 01:16:45.379905: Pseudo dice [0.9241, 0.9445, 0.9436]\n",
      "2024-01-02 01:16:45.385423: Epoch time: 125.53 s\n",
      "2024-01-02 01:16:46.686812: \n",
      "2024-01-02 01:16:46.696811: Epoch 897\n",
      "2024-01-02 01:16:46.701812: Current learning rate: 0.00129\n",
      "2024-01-02 01:18:52.380082: train_loss -0.9247\n",
      "2024-01-02 01:18:52.389084: val_loss -0.8259\n",
      "2024-01-02 01:18:52.396602: Pseudo dice [0.9246, 0.944, 0.9432]\n",
      "2024-01-02 01:18:52.401618: Epoch time: 125.7 s\n",
      "2024-01-02 01:18:53.582299: \n",
      "2024-01-02 01:18:53.589349: Epoch 898\n",
      "2024-01-02 01:18:53.594402: Current learning rate: 0.00128\n",
      "2024-01-02 01:20:59.249097: train_loss -0.9247\n",
      "2024-01-02 01:20:59.256097: val_loss -0.8243\n",
      "2024-01-02 01:20:59.261096: Pseudo dice [0.9251, 0.9447, 0.9441]\n",
      "2024-01-02 01:20:59.266096: Epoch time: 125.67 s\n",
      "2024-01-02 01:21:00.366559: \n",
      "2024-01-02 01:21:00.373317: Epoch 899\n",
      "2024-01-02 01:21:00.378324: Current learning rate: 0.00127\n",
      "2024-01-02 01:23:06.252406: train_loss -0.9266\n",
      "2024-01-02 01:23:06.260405: val_loss -0.8263\n",
      "2024-01-02 01:23:06.267406: Pseudo dice [0.9246, 0.9446, 0.9438]\n",
      "2024-01-02 01:23:06.274405: Epoch time: 125.89 s\n",
      "2024-01-02 01:23:07.647407: \n",
      "2024-01-02 01:23:07.656967: Epoch 900\n",
      "2024-01-02 01:23:07.662026: Current learning rate: 0.00126\n",
      "2024-01-02 01:25:13.605323: train_loss -0.9219\n",
      "2024-01-02 01:25:13.613328: val_loss -0.8278\n",
      "2024-01-02 01:25:13.621325: Pseudo dice [0.9257, 0.9447, 0.9437]\n",
      "2024-01-02 01:25:13.629326: Epoch time: 125.96 s\n",
      "2024-01-02 01:25:14.867772: \n",
      "2024-01-02 01:25:14.881618: Epoch 901\n",
      "2024-01-02 01:25:14.885659: Current learning rate: 0.00125\n",
      "2024-01-02 01:27:20.897407: train_loss -0.9223\n",
      "2024-01-02 01:27:20.907846: val_loss -0.8252\n",
      "2024-01-02 01:27:20.915363: Pseudo dice [0.9262, 0.9455, 0.9444]\n",
      "2024-01-02 01:27:20.924363: Epoch time: 126.03 s\n",
      "2024-01-02 01:27:22.089737: \n",
      "2024-01-02 01:27:22.096507: Epoch 902\n",
      "2024-01-02 01:27:22.103524: Current learning rate: 0.00124\n",
      "2024-01-02 01:29:27.534682: train_loss -0.9271\n",
      "2024-01-02 01:29:27.542680: val_loss -0.8254\n",
      "2024-01-02 01:29:27.547683: Pseudo dice [0.9243, 0.9432, 0.9428]\n",
      "2024-01-02 01:29:27.553683: Epoch time: 125.45 s\n",
      "2024-01-02 01:29:28.669822: \n",
      "2024-01-02 01:29:28.676461: Epoch 903\n",
      "2024-01-02 01:29:28.683464: Current learning rate: 0.00122\n",
      "2024-01-02 01:31:34.106930: train_loss -0.9258\n",
      "2024-01-02 01:31:34.114930: val_loss -0.8338\n",
      "2024-01-02 01:31:34.120929: Pseudo dice [0.9254, 0.9443, 0.9435]\n",
      "2024-01-02 01:31:34.126930: Epoch time: 125.44 s\n",
      "2024-01-02 01:31:35.407251: \n",
      "2024-01-02 01:31:35.413735: Epoch 904\n",
      "2024-01-02 01:31:35.418220: Current learning rate: 0.00121\n",
      "2024-01-02 01:33:41.241575: train_loss -0.926\n",
      "2024-01-02 01:33:41.249577: val_loss -0.8225\n",
      "2024-01-02 01:33:41.257576: Pseudo dice [0.9241, 0.9441, 0.9436]\n",
      "2024-01-02 01:33:41.263577: Epoch time: 125.84 s\n",
      "2024-01-02 01:33:42.613269: \n",
      "2024-01-02 01:33:42.622436: Epoch 905\n",
      "2024-01-02 01:33:42.627451: Current learning rate: 0.0012\n",
      "2024-01-02 01:35:48.162021: train_loss -0.925\n",
      "2024-01-02 01:35:48.169021: val_loss -0.8269\n",
      "2024-01-02 01:35:48.178533: Pseudo dice [0.9249, 0.9438, 0.9434]\n",
      "2024-01-02 01:35:48.186532: Epoch time: 125.55 s\n",
      "2024-01-02 01:35:49.363020: \n",
      "2024-01-02 01:35:49.369027: Epoch 906\n",
      "2024-01-02 01:35:49.374691: Current learning rate: 0.00119\n",
      "2024-01-02 01:37:54.736069: train_loss -0.9263\n",
      "2024-01-02 01:37:54.746064: val_loss -0.8264\n",
      "2024-01-02 01:37:54.753068: Pseudo dice [0.9247, 0.9449, 0.9441]\n",
      "2024-01-02 01:37:54.759067: Epoch time: 125.37 s\n",
      "2024-01-02 01:37:55.874546: \n",
      "2024-01-02 01:37:55.881679: Epoch 907\n",
      "2024-01-02 01:37:55.887630: Current learning rate: 0.00118\n",
      "2024-01-02 01:40:01.409318: train_loss -0.9245\n",
      "2024-01-02 01:40:01.419324: val_loss -0.8333\n",
      "2024-01-02 01:40:01.424324: Pseudo dice [0.9262, 0.9451, 0.9445]\n",
      "2024-01-02 01:40:01.430324: Epoch time: 125.54 s\n",
      "2024-01-02 01:40:02.465639: \n",
      "2024-01-02 01:40:02.473679: Epoch 908\n",
      "2024-01-02 01:40:02.478716: Current learning rate: 0.00117\n",
      "2024-01-02 01:42:07.952267: train_loss -0.9255\n",
      "2024-01-02 01:42:07.960268: val_loss -0.8275\n",
      "2024-01-02 01:42:07.966267: Pseudo dice [0.9264, 0.9448, 0.9445]\n",
      "2024-01-02 01:42:07.971269: Epoch time: 125.49 s\n",
      "2024-01-02 01:42:09.094761: \n",
      "2024-01-02 01:42:09.101065: Epoch 909\n",
      "2024-01-02 01:42:09.109131: Current learning rate: 0.00116\n",
      "2024-01-02 01:44:14.754857: train_loss -0.9273\n",
      "2024-01-02 01:44:14.764858: val_loss -0.8235\n",
      "2024-01-02 01:44:14.770859: Pseudo dice [0.9256, 0.9439, 0.9438]\n",
      "2024-01-02 01:44:14.777862: Epoch time: 125.66 s\n",
      "2024-01-02 01:44:15.997564: \n",
      "2024-01-02 01:44:16.003655: Epoch 910\n",
      "2024-01-02 01:44:16.007623: Current learning rate: 0.00115\n",
      "2024-01-02 01:46:21.357013: train_loss -0.928\n",
      "2024-01-02 01:46:21.369017: val_loss -0.8319\n",
      "2024-01-02 01:46:21.377012: Pseudo dice [0.9255, 0.9449, 0.9445]\n",
      "2024-01-02 01:46:21.384012: Epoch time: 125.36 s\n",
      "2024-01-02 01:46:22.434441: \n",
      "2024-01-02 01:46:22.444326: Epoch 911\n",
      "2024-01-02 01:46:22.451267: Current learning rate: 0.00113\n",
      "2024-01-02 01:48:27.835295: train_loss -0.9283\n",
      "2024-01-02 01:48:27.845295: val_loss -0.8253\n",
      "2024-01-02 01:48:27.851295: Pseudo dice [0.9256, 0.944, 0.9439]\n",
      "2024-01-02 01:48:27.856294: Epoch time: 125.4 s\n",
      "2024-01-02 01:48:28.933346: \n",
      "2024-01-02 01:48:28.940710: Epoch 912\n",
      "2024-01-02 01:48:28.945787: Current learning rate: 0.00112\n",
      "2024-01-02 01:50:34.803839: train_loss -0.9248\n",
      "2024-01-02 01:50:34.810836: val_loss -0.8278\n",
      "2024-01-02 01:50:34.817835: Pseudo dice [0.9245, 0.9446, 0.9441]\n",
      "2024-01-02 01:50:34.824838: Epoch time: 125.87 s\n",
      "2024-01-02 01:50:36.172387: \n",
      "2024-01-02 01:50:36.181396: Epoch 913\n",
      "2024-01-02 01:50:36.185928: Current learning rate: 0.00111\n",
      "2024-01-02 01:52:41.557745: train_loss -0.9278\n",
      "2024-01-02 01:52:41.566746: val_loss -0.8297\n",
      "2024-01-02 01:52:41.571745: Pseudo dice [0.9264, 0.9447, 0.9437]\n",
      "2024-01-02 01:52:41.578783: Epoch time: 125.39 s\n",
      "2024-01-02 01:52:42.698567: \n",
      "2024-01-02 01:52:42.703935: Epoch 914\n",
      "2024-01-02 01:52:42.707928: Current learning rate: 0.0011\n",
      "2024-01-02 01:54:48.054796: train_loss -0.9281\n",
      "2024-01-02 01:54:48.063795: val_loss -0.8272\n",
      "2024-01-02 01:54:48.068795: Pseudo dice [0.9261, 0.9436, 0.9425]\n",
      "2024-01-02 01:54:48.075807: Epoch time: 125.36 s\n",
      "2024-01-02 01:54:49.237359: \n",
      "2024-01-02 01:54:49.245359: Epoch 915\n",
      "2024-01-02 01:54:49.256364: Current learning rate: 0.00109\n",
      "2024-01-02 01:56:54.929919: train_loss -0.9269\n",
      "2024-01-02 01:56:54.938924: val_loss -0.8266\n",
      "2024-01-02 01:56:54.944923: Pseudo dice [0.9255, 0.9438, 0.9428]\n",
      "2024-01-02 01:56:54.951933: Epoch time: 125.69 s\n",
      "2024-01-02 01:56:56.098011: \n",
      "2024-01-02 01:56:56.108219: Epoch 916\n",
      "2024-01-02 01:56:56.116142: Current learning rate: 0.00108\n",
      "2024-01-02 01:59:01.917113: train_loss -0.9255\n",
      "2024-01-02 01:59:01.926124: val_loss -0.8266\n",
      "2024-01-02 01:59:01.932119: Pseudo dice [0.9248, 0.9453, 0.9449]\n",
      "2024-01-02 01:59:01.939116: Epoch time: 125.82 s\n",
      "2024-01-02 01:59:03.074794: \n",
      "2024-01-02 01:59:03.080310: Epoch 917\n",
      "2024-01-02 01:59:03.085474: Current learning rate: 0.00106\n",
      "2024-01-02 02:01:08.507266: train_loss -0.9267\n",
      "2024-01-02 02:01:08.515266: val_loss -0.8284\n",
      "2024-01-02 02:01:08.523302: Pseudo dice [0.9251, 0.9445, 0.9437]\n",
      "2024-01-02 02:01:08.529272: Epoch time: 125.43 s\n",
      "2024-01-02 02:01:09.729628: \n",
      "2024-01-02 02:01:09.742453: Epoch 918\n",
      "2024-01-02 02:01:09.749243: Current learning rate: 0.00105\n",
      "2024-01-02 02:03:15.472440: train_loss -0.9256\n",
      "2024-01-02 02:03:15.479439: val_loss -0.8342\n",
      "2024-01-02 02:03:15.488435: Pseudo dice [0.9257, 0.9455, 0.9438]\n",
      "2024-01-02 02:03:15.495435: Epoch time: 125.74 s\n",
      "2024-01-02 02:03:16.703860: \n",
      "2024-01-02 02:03:16.714188: Epoch 919\n",
      "2024-01-02 02:03:16.721180: Current learning rate: 0.00104\n",
      "2024-01-02 02:05:22.157322: train_loss -0.9266\n",
      "2024-01-02 02:05:22.167327: val_loss -0.8256\n",
      "2024-01-02 02:05:22.175323: Pseudo dice [0.9254, 0.9444, 0.944]\n",
      "2024-01-02 02:05:22.182330: Epoch time: 125.45 s\n",
      "2024-01-02 02:05:23.356232: \n",
      "2024-01-02 02:05:23.363232: Epoch 920\n",
      "2024-01-02 02:05:23.373399: Current learning rate: 0.00103\n",
      "2024-01-02 02:07:29.425285: train_loss -0.9242\n",
      "2024-01-02 02:07:29.434783: val_loss -0.8199\n",
      "2024-01-02 02:07:29.443781: Pseudo dice [0.927, 0.9451, 0.9443]\n",
      "2024-01-02 02:07:29.453785: Epoch time: 126.07 s\n",
      "2024-01-02 02:07:30.856994: \n",
      "2024-01-02 02:07:30.861994: Epoch 921\n",
      "2024-01-02 02:07:30.866994: Current learning rate: 0.00102\n",
      "2024-01-02 02:09:36.656758: train_loss -0.9241\n",
      "2024-01-02 02:09:36.664831: val_loss -0.8252\n",
      "2024-01-02 02:09:36.670846: Pseudo dice [0.9251, 0.9441, 0.9444]\n",
      "2024-01-02 02:09:36.678845: Epoch time: 125.8 s\n",
      "2024-01-02 02:09:37.995025: \n",
      "2024-01-02 02:09:38.001102: Epoch 922\n",
      "2024-01-02 02:09:38.006034: Current learning rate: 0.00101\n",
      "2024-01-02 02:11:43.387377: train_loss -0.9268\n",
      "2024-01-02 02:11:43.397377: val_loss -0.8241\n",
      "2024-01-02 02:11:43.403377: Pseudo dice [0.9261, 0.9454, 0.9445]\n",
      "2024-01-02 02:11:43.411377: Epoch time: 125.39 s\n",
      "2024-01-02 02:11:44.502336: \n",
      "2024-01-02 02:11:44.511461: Epoch 923\n",
      "2024-01-02 02:11:44.518528: Current learning rate: 0.001\n",
      "2024-01-02 02:13:50.042455: train_loss -0.9269\n",
      "2024-01-02 02:13:50.051456: val_loss -0.8272\n",
      "2024-01-02 02:13:50.058455: Pseudo dice [0.9264, 0.9447, 0.9438]\n",
      "2024-01-02 02:13:50.064456: Epoch time: 125.54 s\n",
      "2024-01-02 02:13:51.277381: \n",
      "2024-01-02 02:13:51.283141: Epoch 924\n",
      "2024-01-02 02:13:51.287805: Current learning rate: 0.00098\n",
      "2024-01-02 02:15:57.435943: train_loss -0.9255\n",
      "2024-01-02 02:15:57.448949: val_loss -0.8256\n",
      "2024-01-02 02:15:57.456950: Pseudo dice [0.9267, 0.9459, 0.9446]\n",
      "2024-01-02 02:15:57.465948: Epoch time: 126.16 s\n",
      "2024-01-02 02:15:58.850934: \n",
      "2024-01-02 02:15:58.860934: Epoch 925\n",
      "2024-01-02 02:15:58.865933: Current learning rate: 0.00097\n",
      "2024-01-02 02:18:04.375179: train_loss -0.9233\n",
      "2024-01-02 02:18:04.386685: val_loss -0.8304\n",
      "2024-01-02 02:18:04.395689: Pseudo dice [0.9263, 0.945, 0.9437]\n",
      "2024-01-02 02:18:04.400685: Epoch time: 125.53 s\n",
      "2024-01-02 02:18:05.464764: \n",
      "2024-01-02 02:18:05.471703: Epoch 926\n",
      "2024-01-02 02:18:05.481722: Current learning rate: 0.00096\n",
      "2024-01-02 02:20:11.266781: train_loss -0.9261\n",
      "2024-01-02 02:20:11.274871: val_loss -0.828\n",
      "2024-01-02 02:20:11.279872: Pseudo dice [0.9264, 0.945, 0.9442]\n",
      "2024-01-02 02:20:11.284874: Epoch time: 125.8 s\n",
      "2024-01-02 02:20:12.428858: \n",
      "2024-01-02 02:20:12.438813: Epoch 927\n",
      "2024-01-02 02:20:12.445746: Current learning rate: 0.00095\n",
      "2024-01-02 02:22:17.891812: train_loss -0.9264\n",
      "2024-01-02 02:22:17.898813: val_loss -0.8256\n",
      "2024-01-02 02:22:17.904320: Pseudo dice [0.9251, 0.9444, 0.9435]\n",
      "2024-01-02 02:22:17.911321: Epoch time: 125.46 s\n",
      "2024-01-02 02:22:19.052934: \n",
      "2024-01-02 02:22:19.059437: Epoch 928\n",
      "2024-01-02 02:22:19.064478: Current learning rate: 0.00094\n",
      "2024-01-02 02:24:24.795968: train_loss -0.9252\n",
      "2024-01-02 02:24:24.806034: val_loss -0.8261\n",
      "2024-01-02 02:24:24.815007: Pseudo dice [0.9254, 0.9448, 0.9436]\n",
      "2024-01-02 02:24:24.822008: Epoch time: 125.74 s\n",
      "2024-01-02 02:24:26.225188: \n",
      "2024-01-02 02:24:26.232274: Epoch 929\n",
      "2024-01-02 02:24:26.237407: Current learning rate: 0.00092\n",
      "2024-01-02 02:26:32.136303: train_loss -0.9269\n",
      "2024-01-02 02:26:32.144304: val_loss -0.8287\n",
      "2024-01-02 02:26:32.149304: Pseudo dice [0.9253, 0.9451, 0.9444]\n",
      "2024-01-02 02:26:32.155304: Epoch time: 125.91 s\n",
      "2024-01-02 02:26:33.349227: \n",
      "2024-01-02 02:26:33.357682: Epoch 930\n",
      "2024-01-02 02:26:33.365677: Current learning rate: 0.00091\n",
      "2024-01-02 02:28:38.706801: train_loss -0.9264\n",
      "2024-01-02 02:28:38.714801: val_loss -0.8305\n",
      "2024-01-02 02:28:38.721802: Pseudo dice [0.925, 0.9434, 0.9423]\n",
      "2024-01-02 02:28:38.727800: Epoch time: 125.36 s\n",
      "2024-01-02 02:28:39.987926: \n",
      "2024-01-02 02:28:39.994735: Epoch 931\n",
      "2024-01-02 02:28:40.002802: Current learning rate: 0.0009\n",
      "2024-01-02 02:30:45.603429: train_loss -0.9282\n",
      "2024-01-02 02:30:45.610429: val_loss -0.8254\n",
      "2024-01-02 02:30:45.617541: Pseudo dice [0.9247, 0.9435, 0.9432]\n",
      "2024-01-02 02:30:45.622540: Epoch time: 125.62 s\n",
      "2024-01-02 02:30:46.911060: \n",
      "2024-01-02 02:30:46.917056: Epoch 932\n",
      "2024-01-02 02:30:46.923057: Current learning rate: 0.00089\n",
      "2024-01-02 02:32:52.365226: train_loss -0.9266\n",
      "2024-01-02 02:32:52.373225: val_loss -0.8293\n",
      "2024-01-02 02:32:52.379232: Pseudo dice [0.926, 0.9456, 0.9445]\n",
      "2024-01-02 02:32:52.387225: Epoch time: 125.46 s\n",
      "2024-01-02 02:32:53.557375: \n",
      "2024-01-02 02:32:53.563320: Epoch 933\n",
      "2024-01-02 02:32:53.571393: Current learning rate: 0.00088\n",
      "2024-01-02 02:34:59.001997: train_loss -0.9269\n",
      "2024-01-02 02:34:59.008997: val_loss -0.8279\n",
      "2024-01-02 02:34:59.015002: Pseudo dice [0.9261, 0.9453, 0.9446]\n",
      "2024-01-02 02:34:59.020999: Epoch time: 125.45 s\n",
      "2024-01-02 02:35:00.163936: \n",
      "2024-01-02 02:35:00.169469: Epoch 934\n",
      "2024-01-02 02:35:00.176469: Current learning rate: 0.00087\n",
      "2024-01-02 02:37:05.883431: train_loss -0.927\n",
      "2024-01-02 02:37:05.891432: val_loss -0.828\n",
      "2024-01-02 02:37:05.897949: Pseudo dice [0.9244, 0.9453, 0.9447]\n",
      "2024-01-02 02:37:05.902949: Epoch time: 125.72 s\n",
      "2024-01-02 02:37:07.137015: \n",
      "2024-01-02 02:37:07.143014: Epoch 935\n",
      "2024-01-02 02:37:07.149807: Current learning rate: 0.00085\n",
      "2024-01-02 02:39:13.718908: train_loss -0.9204\n",
      "2024-01-02 02:39:13.726907: val_loss -0.8234\n",
      "2024-01-02 02:39:13.733907: Pseudo dice [0.9255, 0.9447, 0.9441]\n",
      "2024-01-02 02:39:13.739907: Epoch time: 126.58 s\n",
      "2024-01-02 02:39:14.860510: \n",
      "2024-01-02 02:39:14.868577: Epoch 936\n",
      "2024-01-02 02:39:14.873637: Current learning rate: 0.00084\n",
      "2024-01-02 02:41:20.207920: train_loss -0.9282\n",
      "2024-01-02 02:41:20.215920: val_loss -0.8241\n",
      "2024-01-02 02:41:20.223919: Pseudo dice [0.9252, 0.9448, 0.9437]\n",
      "2024-01-02 02:41:20.230919: Epoch time: 125.35 s\n",
      "2024-01-02 02:41:21.564883: \n",
      "2024-01-02 02:41:21.571394: Epoch 937\n",
      "2024-01-02 02:41:21.576461: Current learning rate: 0.00083\n",
      "2024-01-02 02:43:27.065092: train_loss -0.9264\n",
      "2024-01-02 02:43:27.072094: val_loss -0.821\n",
      "2024-01-02 02:43:27.079094: Pseudo dice [0.9256, 0.9439, 0.9431]\n",
      "2024-01-02 02:43:27.084093: Epoch time: 125.5 s\n",
      "2024-01-02 02:43:28.313604: \n",
      "2024-01-02 02:43:28.320495: Epoch 938\n",
      "2024-01-02 02:43:28.328494: Current learning rate: 0.00082\n",
      "2024-01-02 02:45:34.004348: train_loss -0.9275\n",
      "2024-01-02 02:45:34.011354: val_loss -0.8307\n",
      "2024-01-02 02:45:34.018337: Pseudo dice [0.925, 0.9442, 0.9441]\n",
      "2024-01-02 02:45:34.024337: Epoch time: 125.69 s\n",
      "2024-01-02 02:45:35.283356: \n",
      "2024-01-02 02:45:35.291273: Epoch 939\n",
      "2024-01-02 02:45:35.298363: Current learning rate: 0.00081\n",
      "2024-01-02 02:47:40.975830: train_loss -0.9251\n",
      "2024-01-02 02:47:40.984579: val_loss -0.8262\n",
      "2024-01-02 02:47:40.993091: Pseudo dice [0.9241, 0.9439, 0.943]\n",
      "2024-01-02 02:47:41.000809: Epoch time: 125.69 s\n",
      "2024-01-02 02:47:42.169662: \n",
      "2024-01-02 02:47:42.181991: Epoch 940\n",
      "2024-01-02 02:47:42.187000: Current learning rate: 0.00079\n",
      "2024-01-02 02:49:47.949151: train_loss -0.926\n",
      "2024-01-02 02:49:47.958659: val_loss -0.8282\n",
      "2024-01-02 02:49:47.966661: Pseudo dice [0.9265, 0.9451, 0.944]\n",
      "2024-01-02 02:49:47.973660: Epoch time: 125.78 s\n",
      "2024-01-02 02:49:49.419213: \n",
      "2024-01-02 02:49:49.426212: Epoch 941\n",
      "2024-01-02 02:49:49.432213: Current learning rate: 0.00078\n",
      "2024-01-02 02:51:54.865987: train_loss -0.9268\n",
      "2024-01-02 02:51:54.874431: val_loss -0.8287\n",
      "2024-01-02 02:51:54.879430: Pseudo dice [0.9259, 0.9443, 0.9437]\n",
      "2024-01-02 02:51:54.885430: Epoch time: 125.45 s\n",
      "2024-01-02 02:51:55.951029: \n",
      "2024-01-02 02:51:55.960960: Epoch 942\n",
      "2024-01-02 02:51:55.966972: Current learning rate: 0.00077\n",
      "2024-01-02 02:54:01.444715: train_loss -0.9279\n",
      "2024-01-02 02:54:01.454716: val_loss -0.83\n",
      "2024-01-02 02:54:01.464715: Pseudo dice [0.9244, 0.944, 0.9431]\n",
      "2024-01-02 02:54:01.470720: Epoch time: 125.49 s\n",
      "2024-01-02 02:54:02.833871: \n",
      "2024-01-02 02:54:02.841880: Epoch 943\n",
      "2024-01-02 02:54:02.846593: Current learning rate: 0.00076\n",
      "2024-01-02 02:56:08.620379: train_loss -0.9248\n",
      "2024-01-02 02:56:08.627379: val_loss -0.8243\n",
      "2024-01-02 02:56:08.634380: Pseudo dice [0.925, 0.9439, 0.9432]\n",
      "2024-01-02 02:56:08.641379: Epoch time: 125.79 s\n",
      "2024-01-02 02:56:09.812507: \n",
      "2024-01-02 02:56:09.820948: Epoch 944\n",
      "2024-01-02 02:56:09.826019: Current learning rate: 0.00075\n",
      "2024-01-02 02:58:15.116402: train_loss -0.9283\n",
      "2024-01-02 02:58:15.123402: val_loss -0.8305\n",
      "2024-01-02 02:58:15.130402: Pseudo dice [0.9261, 0.9442, 0.9434]\n",
      "2024-01-02 02:58:15.136403: Epoch time: 125.3 s\n",
      "2024-01-02 02:58:16.539905: \n",
      "2024-01-02 02:58:16.546168: Epoch 945\n",
      "2024-01-02 02:58:16.550787: Current learning rate: 0.00074\n",
      "2024-01-02 03:00:22.059510: train_loss -0.9275\n",
      "2024-01-02 03:00:22.066510: val_loss -0.8281\n",
      "2024-01-02 03:00:22.071510: Pseudo dice [0.9254, 0.9449, 0.9446]\n",
      "2024-01-02 03:00:22.077509: Epoch time: 125.52 s\n",
      "2024-01-02 03:00:23.343375: \n",
      "2024-01-02 03:00:23.350355: Epoch 946\n",
      "2024-01-02 03:00:23.355737: Current learning rate: 0.00072\n",
      "2024-01-02 03:02:29.202781: train_loss -0.9253\n",
      "2024-01-02 03:02:29.214782: val_loss -0.8275\n",
      "2024-01-02 03:02:29.224783: Pseudo dice [0.9251, 0.9437, 0.9428]\n",
      "2024-01-02 03:02:29.231784: Epoch time: 125.86 s\n",
      "2024-01-02 03:02:30.558072: \n",
      "2024-01-02 03:02:30.568079: Epoch 947\n",
      "2024-01-02 03:02:30.575079: Current learning rate: 0.00071\n",
      "2024-01-02 03:04:36.204776: train_loss -0.9232\n",
      "2024-01-02 03:04:36.212784: val_loss -0.8279\n",
      "2024-01-02 03:04:36.218781: Pseudo dice [0.9254, 0.9443, 0.9438]\n",
      "2024-01-02 03:04:36.224778: Epoch time: 125.65 s\n",
      "2024-01-02 03:04:37.371480: \n",
      "2024-01-02 03:04:37.381567: Epoch 948\n",
      "2024-01-02 03:04:37.386550: Current learning rate: 0.0007\n",
      "2024-01-02 03:06:42.895854: train_loss -0.9271\n",
      "2024-01-02 03:06:42.905384: val_loss -0.8261\n",
      "2024-01-02 03:06:42.913376: Pseudo dice [0.9245, 0.9441, 0.943]\n",
      "2024-01-02 03:06:42.919378: Epoch time: 125.53 s\n",
      "2024-01-02 03:06:44.041010: \n",
      "2024-01-02 03:06:44.048937: Epoch 949\n",
      "2024-01-02 03:06:44.053531: Current learning rate: 0.00069\n",
      "2024-01-02 03:08:49.674012: train_loss -0.9265\n",
      "2024-01-02 03:08:49.681021: val_loss -0.8248\n",
      "2024-01-02 03:08:49.686021: Pseudo dice [0.9249, 0.9449, 0.9443]\n",
      "2024-01-02 03:08:49.692017: Epoch time: 125.64 s\n",
      "2024-01-02 03:08:51.138652: \n",
      "2024-01-02 03:08:51.148300: Epoch 950\n",
      "2024-01-02 03:08:51.153105: Current learning rate: 0.00067\n",
      "2024-01-02 03:10:56.522446: train_loss -0.9279\n",
      "2024-01-02 03:10:56.529446: val_loss -0.8258\n",
      "2024-01-02 03:10:56.536079: Pseudo dice [0.9248, 0.9443, 0.9436]\n",
      "2024-01-02 03:10:56.541079: Epoch time: 125.39 s\n",
      "2024-01-02 03:10:57.623811: \n",
      "2024-01-02 03:10:57.632848: Epoch 951\n",
      "2024-01-02 03:10:57.639640: Current learning rate: 0.00066\n",
      "2024-01-02 03:13:03.202606: train_loss -0.9296\n",
      "2024-01-02 03:13:03.208609: val_loss -0.8284\n",
      "2024-01-02 03:13:03.214612: Pseudo dice [0.9253, 0.9445, 0.9435]\n",
      "2024-01-02 03:13:03.220609: Epoch time: 125.58 s\n",
      "2024-01-02 03:13:04.461984: \n",
      "2024-01-02 03:13:04.473058: Epoch 952\n",
      "2024-01-02 03:13:04.478132: Current learning rate: 0.00065\n",
      "2024-01-02 03:15:09.881248: train_loss -0.931\n",
      "2024-01-02 03:15:09.892246: val_loss -0.8272\n",
      "2024-01-02 03:15:09.902249: Pseudo dice [0.9231, 0.9437, 0.9425]\n",
      "2024-01-02 03:15:09.907249: Epoch time: 125.42 s\n",
      "2024-01-02 03:15:11.203114: \n",
      "2024-01-02 03:15:11.209001: Epoch 953\n",
      "2024-01-02 03:15:11.214949: Current learning rate: 0.00064\n",
      "2024-01-02 03:17:16.755136: train_loss -0.9257\n",
      "2024-01-02 03:17:16.764127: val_loss -0.8266\n",
      "2024-01-02 03:17:16.774131: Pseudo dice [0.9248, 0.9435, 0.9426]\n",
      "2024-01-02 03:17:16.780127: Epoch time: 125.55 s\n",
      "2024-01-02 03:17:17.930081: \n",
      "2024-01-02 03:17:17.941680: Epoch 954\n",
      "2024-01-02 03:17:17.950710: Current learning rate: 0.00063\n",
      "2024-01-02 03:19:23.388379: train_loss -0.9287\n",
      "2024-01-02 03:19:23.396367: val_loss -0.8293\n",
      "2024-01-02 03:19:23.403367: Pseudo dice [0.9263, 0.9453, 0.9446]\n",
      "2024-01-02 03:19:23.412368: Epoch time: 125.46 s\n",
      "2024-01-02 03:19:24.547783: \n",
      "2024-01-02 03:19:24.554374: Epoch 955\n",
      "2024-01-02 03:19:24.560443: Current learning rate: 0.00061\n",
      "2024-01-02 03:21:29.700206: train_loss -0.932\n",
      "2024-01-02 03:21:29.707207: val_loss -0.8262\n",
      "2024-01-02 03:21:29.713207: Pseudo dice [0.9245, 0.9436, 0.9429]\n",
      "2024-01-02 03:21:29.718207: Epoch time: 125.15 s\n",
      "2024-01-02 03:21:30.853114: \n",
      "2024-01-02 03:21:30.860218: Epoch 956\n",
      "2024-01-02 03:21:30.868281: Current learning rate: 0.0006\n",
      "2024-01-02 03:23:36.322788: train_loss -0.9276\n",
      "2024-01-02 03:23:36.329782: val_loss -0.8245\n",
      "2024-01-02 03:23:36.337790: Pseudo dice [0.9249, 0.9444, 0.9439]\n",
      "2024-01-02 03:23:36.343791: Epoch time: 125.47 s\n",
      "2024-01-02 03:23:37.416284: \n",
      "2024-01-02 03:23:37.423354: Epoch 957\n",
      "2024-01-02 03:23:37.429387: Current learning rate: 0.00059\n",
      "2024-01-02 03:25:42.734623: train_loss -0.9315\n",
      "2024-01-02 03:25:42.741625: val_loss -0.8252\n",
      "2024-01-02 03:25:42.747624: Pseudo dice [0.9246, 0.9449, 0.944]\n",
      "2024-01-02 03:25:42.754625: Epoch time: 125.32 s\n",
      "2024-01-02 03:25:43.851777: \n",
      "2024-01-02 03:25:43.858859: Epoch 958\n",
      "2024-01-02 03:25:43.863849: Current learning rate: 0.00058\n",
      "2024-01-02 03:27:49.551180: train_loss -0.9251\n",
      "2024-01-02 03:27:49.562182: val_loss -0.8242\n",
      "2024-01-02 03:27:49.570181: Pseudo dice [0.9244, 0.9445, 0.9434]\n",
      "2024-01-02 03:27:49.578186: Epoch time: 125.7 s\n",
      "2024-01-02 03:27:50.805244: \n",
      "2024-01-02 03:27:50.811321: Epoch 959\n",
      "2024-01-02 03:27:50.818396: Current learning rate: 0.00056\n",
      "2024-01-02 03:29:56.753198: train_loss -0.9258\n",
      "2024-01-02 03:29:56.763198: val_loss -0.8297\n",
      "2024-01-02 03:29:56.772202: Pseudo dice [0.9252, 0.9446, 0.9436]\n",
      "2024-01-02 03:29:56.784216: Epoch time: 125.95 s\n",
      "2024-01-02 03:29:58.108390: \n",
      "2024-01-02 03:29:58.115333: Epoch 960\n",
      "2024-01-02 03:29:58.127330: Current learning rate: 0.00055\n",
      "2024-01-02 03:32:03.681333: train_loss -0.9272\n",
      "2024-01-02 03:32:03.687327: val_loss -0.8256\n",
      "2024-01-02 03:32:03.694328: Pseudo dice [0.9237, 0.9439, 0.9435]\n",
      "2024-01-02 03:32:03.700328: Epoch time: 125.57 s\n",
      "2024-01-02 03:32:05.041279: \n",
      "2024-01-02 03:32:05.047376: Epoch 961\n",
      "2024-01-02 03:32:05.054456: Current learning rate: 0.00054\n",
      "2024-01-02 03:34:10.708377: train_loss -0.9274\n",
      "2024-01-02 03:34:10.717376: val_loss -0.8307\n",
      "2024-01-02 03:34:10.725379: Pseudo dice [0.9263, 0.9439, 0.9432]\n",
      "2024-01-02 03:34:10.734380: Epoch time: 125.67 s\n",
      "2024-01-02 03:34:11.930527: \n",
      "2024-01-02 03:34:11.938200: Epoch 962\n",
      "2024-01-02 03:34:11.943182: Current learning rate: 0.00053\n",
      "2024-01-02 03:36:17.424355: train_loss -0.9278\n",
      "2024-01-02 03:36:17.431359: val_loss -0.8251\n",
      "2024-01-02 03:36:17.437360: Pseudo dice [0.9252, 0.9439, 0.9433]\n",
      "2024-01-02 03:36:17.441885: Epoch time: 125.49 s\n",
      "2024-01-02 03:36:18.592860: \n",
      "2024-01-02 03:36:18.599071: Epoch 963\n",
      "2024-01-02 03:36:18.605131: Current learning rate: 0.00051\n",
      "2024-01-02 03:38:24.178179: train_loss -0.926\n",
      "2024-01-02 03:38:24.187183: val_loss -0.8285\n",
      "2024-01-02 03:38:24.193183: Pseudo dice [0.9249, 0.9446, 0.9438]\n",
      "2024-01-02 03:38:24.197690: Epoch time: 125.59 s\n",
      "2024-01-02 03:38:25.369343: \n",
      "2024-01-02 03:38:25.378335: Epoch 964\n",
      "2024-01-02 03:38:25.383362: Current learning rate: 0.0005\n",
      "2024-01-02 03:40:31.374555: train_loss -0.9286\n",
      "2024-01-02 03:40:31.382557: val_loss -0.827\n",
      "2024-01-02 03:40:31.390549: Pseudo dice [0.9254, 0.9442, 0.9437]\n",
      "2024-01-02 03:40:31.396550: Epoch time: 126.01 s\n",
      "2024-01-02 03:40:32.712689: \n",
      "2024-01-02 03:40:32.722671: Epoch 965\n",
      "2024-01-02 03:40:32.727482: Current learning rate: 0.00049\n",
      "2024-01-02 03:42:38.325648: train_loss -0.9272\n",
      "2024-01-02 03:42:38.334649: val_loss -0.8302\n",
      "2024-01-02 03:42:38.344653: Pseudo dice [0.925, 0.9443, 0.9434]\n",
      "2024-01-02 03:42:38.351653: Epoch time: 125.62 s\n",
      "2024-01-02 03:42:39.586877: \n",
      "2024-01-02 03:42:39.592978: Epoch 966\n",
      "2024-01-02 03:42:39.598193: Current learning rate: 0.00048\n",
      "2024-01-02 03:44:45.183790: train_loss -0.9286\n",
      "2024-01-02 03:44:45.190301: val_loss -0.8288\n",
      "2024-01-02 03:44:45.197306: Pseudo dice [0.9253, 0.9442, 0.9436]\n",
      "2024-01-02 03:44:45.201299: Epoch time: 125.6 s\n",
      "2024-01-02 03:44:46.379094: \n",
      "2024-01-02 03:44:46.385158: Epoch 967\n",
      "2024-01-02 03:44:46.391796: Current learning rate: 0.00046\n",
      "2024-01-02 03:46:51.755646: train_loss -0.9284\n",
      "2024-01-02 03:46:51.765651: val_loss -0.8265\n",
      "2024-01-02 03:46:51.774164: Pseudo dice [0.9254, 0.9442, 0.9438]\n",
      "2024-01-02 03:46:51.780172: Epoch time: 125.38 s\n",
      "2024-01-02 03:46:52.913265: \n",
      "2024-01-02 03:46:52.922264: Epoch 968\n",
      "2024-01-02 03:46:52.928263: Current learning rate: 0.00045\n",
      "2024-01-02 03:48:58.583658: train_loss -0.927\n",
      "2024-01-02 03:48:58.591167: val_loss -0.8277\n",
      "2024-01-02 03:48:58.597169: Pseudo dice [0.9263, 0.945, 0.9448]\n",
      "2024-01-02 03:48:58.602170: Epoch time: 125.67 s\n",
      "2024-01-02 03:49:00.071531: \n",
      "2024-01-02 03:49:00.085543: Epoch 969\n",
      "2024-01-02 03:49:00.096072: Current learning rate: 0.00044\n",
      "2024-01-02 03:51:05.804980: train_loss -0.9281\n",
      "2024-01-02 03:51:05.813064: val_loss -0.8242\n",
      "2024-01-02 03:51:05.820067: Pseudo dice [0.9251, 0.9453, 0.9446]\n",
      "2024-01-02 03:51:05.826064: Epoch time: 125.73 s\n",
      "2024-01-02 03:51:06.945936: \n",
      "2024-01-02 03:51:06.953018: Epoch 970\n",
      "2024-01-02 03:51:06.957997: Current learning rate: 0.00043\n",
      "2024-01-02 03:53:12.547622: train_loss -0.9294\n",
      "2024-01-02 03:53:12.557722: val_loss -0.8275\n",
      "2024-01-02 03:53:12.563741: Pseudo dice [0.9259, 0.9446, 0.9437]\n",
      "2024-01-02 03:53:12.569743: Epoch time: 125.6 s\n",
      "2024-01-02 03:53:13.797399: \n",
      "2024-01-02 03:53:13.805953: Epoch 971\n",
      "2024-01-02 03:53:13.809957: Current learning rate: 0.00041\n",
      "2024-01-02 03:55:19.367523: train_loss -0.9253\n",
      "2024-01-02 03:55:19.373531: val_loss -0.8296\n",
      "2024-01-02 03:55:19.381040: Pseudo dice [0.9249, 0.9438, 0.9429]\n",
      "2024-01-02 03:55:19.387055: Epoch time: 125.57 s\n",
      "2024-01-02 03:55:20.568554: \n",
      "2024-01-02 03:55:20.576233: Epoch 972\n",
      "2024-01-02 03:55:20.581472: Current learning rate: 0.0004\n",
      "2024-01-02 03:57:26.227639: train_loss -0.9269\n",
      "2024-01-02 03:57:26.238637: val_loss -0.8271\n",
      "2024-01-02 03:57:26.244639: Pseudo dice [0.9254, 0.945, 0.9438]\n",
      "2024-01-02 03:57:26.251638: Epoch time: 125.66 s\n",
      "2024-01-02 03:57:27.487391: \n",
      "2024-01-02 03:57:27.496581: Epoch 973\n",
      "2024-01-02 03:57:27.501645: Current learning rate: 0.00039\n",
      "2024-01-02 03:59:33.042468: train_loss -0.9268\n",
      "2024-01-02 03:59:33.049468: val_loss -0.8262\n",
      "2024-01-02 03:59:33.056467: Pseudo dice [0.9248, 0.9449, 0.9442]\n",
      "2024-01-02 03:59:33.061987: Epoch time: 125.56 s\n",
      "2024-01-02 03:59:34.247038: \n",
      "2024-01-02 03:59:34.253225: Epoch 974\n",
      "2024-01-02 03:59:34.263161: Current learning rate: 0.00037\n",
      "2024-01-02 04:01:40.020806: train_loss -0.929\n",
      "2024-01-02 04:01:40.031806: val_loss -0.831\n",
      "2024-01-02 04:01:40.036804: Pseudo dice [0.9262, 0.9452, 0.944]\n",
      "2024-01-02 04:01:40.042804: Epoch time: 125.77 s\n",
      "2024-01-02 04:01:41.190913: \n",
      "2024-01-02 04:01:41.197378: Epoch 975\n",
      "2024-01-02 04:01:41.205439: Current learning rate: 0.00036\n",
      "2024-01-02 04:03:46.883105: train_loss -0.9276\n",
      "2024-01-02 04:03:46.891627: val_loss -0.8283\n",
      "2024-01-02 04:03:46.898626: Pseudo dice [0.9256, 0.944, 0.9432]\n",
      "2024-01-02 04:03:46.904629: Epoch time: 125.69 s\n",
      "2024-01-02 04:03:48.236373: \n",
      "2024-01-02 04:03:48.247358: Epoch 976\n",
      "2024-01-02 04:03:48.252901: Current learning rate: 0.00035\n",
      "2024-01-02 04:05:54.075713: train_loss -0.9261\n",
      "2024-01-02 04:05:54.084714: val_loss -0.8268\n",
      "2024-01-02 04:05:54.090713: Pseudo dice [0.9248, 0.9441, 0.9428]\n",
      "2024-01-02 04:05:54.096717: Epoch time: 125.84 s\n",
      "2024-01-02 04:05:55.325444: \n",
      "2024-01-02 04:05:55.333992: Epoch 977\n",
      "2024-01-02 04:05:55.341623: Current learning rate: 0.00034\n",
      "2024-01-02 04:08:00.709925: train_loss -0.9299\n",
      "2024-01-02 04:08:00.721925: val_loss -0.8256\n",
      "2024-01-02 04:08:00.732942: Pseudo dice [0.9251, 0.9448, 0.9438]\n",
      "2024-01-02 04:08:00.741931: Epoch time: 125.39 s\n",
      "2024-01-02 04:08:01.845914: \n",
      "2024-01-02 04:08:01.851624: Epoch 978\n",
      "2024-01-02 04:08:01.860618: Current learning rate: 0.00032\n",
      "2024-01-02 04:10:07.375577: train_loss -0.9265\n",
      "2024-01-02 04:10:07.383571: val_loss -0.8329\n",
      "2024-01-02 04:10:07.389570: Pseudo dice [0.9256, 0.9435, 0.9426]\n",
      "2024-01-02 04:10:07.395569: Epoch time: 125.53 s\n",
      "2024-01-02 04:10:08.541010: \n",
      "2024-01-02 04:10:08.548087: Epoch 979\n",
      "2024-01-02 04:10:08.553100: Current learning rate: 0.00031\n",
      "2024-01-02 04:12:14.168896: train_loss -0.9279\n",
      "2024-01-02 04:12:14.176893: val_loss -0.8257\n",
      "2024-01-02 04:12:14.182894: Pseudo dice [0.9243, 0.943, 0.9416]\n",
      "2024-01-02 04:12:14.187896: Epoch time: 125.63 s\n",
      "2024-01-02 04:12:15.412357: \n",
      "2024-01-02 04:12:15.418360: Epoch 980\n",
      "2024-01-02 04:12:15.426416: Current learning rate: 0.0003\n",
      "2024-01-02 04:14:20.915360: train_loss -0.9273\n",
      "2024-01-02 04:14:20.924362: val_loss -0.826\n",
      "2024-01-02 04:14:20.933384: Pseudo dice [0.9249, 0.9445, 0.9432]\n",
      "2024-01-02 04:14:20.939372: Epoch time: 125.51 s\n",
      "2024-01-02 04:14:22.238487: \n",
      "2024-01-02 04:14:22.242891: Epoch 981\n",
      "2024-01-02 04:14:22.249635: Current learning rate: 0.00028\n",
      "2024-01-02 04:16:27.749518: train_loss -0.9283\n",
      "2024-01-02 04:16:27.756518: val_loss -0.8253\n",
      "2024-01-02 04:16:27.761518: Pseudo dice [0.9243, 0.9435, 0.9428]\n",
      "2024-01-02 04:16:27.766518: Epoch time: 125.51 s\n",
      "2024-01-02 04:16:28.940924: \n",
      "2024-01-02 04:16:28.946867: Epoch 982\n",
      "2024-01-02 04:16:28.951877: Current learning rate: 0.00027\n",
      "2024-01-02 04:18:34.707140: train_loss -0.9254\n",
      "2024-01-02 04:18:34.716140: val_loss -0.8298\n",
      "2024-01-02 04:18:34.722145: Pseudo dice [0.9254, 0.9446, 0.9437]\n",
      "2024-01-02 04:18:34.730139: Epoch time: 125.77 s\n",
      "2024-01-02 04:18:35.843931: \n",
      "2024-01-02 04:18:35.856855: Epoch 983\n",
      "2024-01-02 04:18:35.860849: Current learning rate: 0.00026\n",
      "2024-01-02 04:20:41.437957: train_loss -0.9302\n",
      "2024-01-02 04:20:41.446957: val_loss -0.8261\n",
      "2024-01-02 04:20:41.457961: Pseudo dice [0.9262, 0.9446, 0.9436]\n",
      "2024-01-02 04:20:41.462961: Epoch time: 125.6 s\n",
      "2024-01-02 04:20:42.733869: \n",
      "2024-01-02 04:20:42.743589: Epoch 984\n",
      "2024-01-02 04:20:42.748586: Current learning rate: 0.00024\n",
      "2024-01-02 04:22:48.278512: train_loss -0.9286\n",
      "2024-01-02 04:22:48.285512: val_loss -0.8273\n",
      "2024-01-02 04:22:48.292131: Pseudo dice [0.9248, 0.9437, 0.942]\n",
      "2024-01-02 04:22:48.298104: Epoch time: 125.55 s\n",
      "2024-01-02 04:22:49.472583: \n",
      "2024-01-02 04:22:49.478561: Epoch 985\n",
      "2024-01-02 04:22:49.487632: Current learning rate: 0.00023\n",
      "2024-01-02 04:24:54.970021: train_loss -0.9281\n",
      "2024-01-02 04:24:54.981014: val_loss -0.8274\n",
      "2024-01-02 04:24:54.987013: Pseudo dice [0.9257, 0.944, 0.9434]\n",
      "2024-01-02 04:24:54.992012: Epoch time: 125.5 s\n",
      "2024-01-02 04:24:56.165670: \n",
      "2024-01-02 04:24:56.172621: Epoch 986\n",
      "2024-01-02 04:24:56.178619: Current learning rate: 0.00021\n",
      "2024-01-02 04:27:01.534637: train_loss -0.9299\n",
      "2024-01-02 04:27:01.541637: val_loss -0.8289\n",
      "2024-01-02 04:27:01.547641: Pseudo dice [0.9259, 0.9442, 0.9433]\n",
      "2024-01-02 04:27:01.554652: Epoch time: 125.37 s\n",
      "2024-01-02 04:27:02.693150: \n",
      "2024-01-02 04:27:02.703140: Epoch 987\n",
      "2024-01-02 04:27:02.708197: Current learning rate: 0.0002\n",
      "2024-01-02 04:29:08.284730: train_loss -0.9267\n",
      "2024-01-02 04:29:08.291726: val_loss -0.8293\n",
      "2024-01-02 04:29:08.297727: Pseudo dice [0.9253, 0.9441, 0.9428]\n",
      "2024-01-02 04:29:08.306726: Epoch time: 125.59 s\n",
      "2024-01-02 04:29:09.450410: \n",
      "2024-01-02 04:29:09.464432: Epoch 988\n",
      "2024-01-02 04:29:09.477440: Current learning rate: 0.00019\n",
      "2024-01-02 04:31:14.703464: train_loss -0.9311\n",
      "2024-01-02 04:31:14.710464: val_loss -0.8275\n",
      "2024-01-02 04:31:14.718463: Pseudo dice [0.925, 0.9447, 0.9439]\n",
      "2024-01-02 04:31:14.724464: Epoch time: 125.25 s\n",
      "2024-01-02 04:31:15.881231: \n",
      "2024-01-02 04:31:15.886696: Epoch 989\n",
      "2024-01-02 04:31:15.894767: Current learning rate: 0.00017\n",
      "2024-01-02 04:33:21.717078: train_loss -0.9298\n",
      "2024-01-02 04:33:21.727594: val_loss -0.8265\n",
      "2024-01-02 04:33:21.736595: Pseudo dice [0.9253, 0.9439, 0.943]\n",
      "2024-01-02 04:33:21.747598: Epoch time: 125.84 s\n",
      "2024-01-02 04:33:22.851409: \n",
      "2024-01-02 04:33:22.858354: Epoch 990\n",
      "2024-01-02 04:33:22.863711: Current learning rate: 0.00016\n",
      "2024-01-02 04:35:28.330213: train_loss -0.9286\n",
      "2024-01-02 04:35:28.340204: val_loss -0.827\n",
      "2024-01-02 04:35:28.346205: Pseudo dice [0.9254, 0.9448, 0.9441]\n",
      "2024-01-02 04:35:28.351204: Epoch time: 125.48 s\n",
      "2024-01-02 04:35:29.459348: \n",
      "2024-01-02 04:35:29.467408: Epoch 991\n",
      "2024-01-02 04:35:29.472419: Current learning rate: 0.00014\n",
      "2024-01-02 04:37:34.894715: train_loss -0.9268\n",
      "2024-01-02 04:37:34.902715: val_loss -0.826\n",
      "2024-01-02 04:37:34.912276: Pseudo dice [0.9252, 0.9443, 0.9431]\n",
      "2024-01-02 04:37:34.921264: Epoch time: 125.44 s\n",
      "2024-01-02 04:37:36.319782: \n",
      "2024-01-02 04:37:36.326632: Epoch 992\n",
      "2024-01-02 04:37:36.332637: Current learning rate: 0.00013\n",
      "2024-01-02 04:39:41.787547: train_loss -0.9303\n",
      "2024-01-02 04:39:41.794547: val_loss -0.8297\n",
      "2024-01-02 04:39:41.800547: Pseudo dice [0.9248, 0.9439, 0.9429]\n",
      "2024-01-02 04:39:41.806559: Epoch time: 125.47 s\n",
      "2024-01-02 04:39:42.991359: \n",
      "2024-01-02 04:39:42.999039: Epoch 993\n",
      "2024-01-02 04:39:43.006180: Current learning rate: 0.00011\n",
      "2024-01-02 04:41:48.744224: train_loss -0.9264\n",
      "2024-01-02 04:41:48.755226: val_loss -0.8312\n",
      "2024-01-02 04:41:48.760233: Pseudo dice [0.9255, 0.9451, 0.9442]\n",
      "2024-01-02 04:41:48.765232: Epoch time: 125.75 s\n",
      "2024-01-02 04:41:49.878668: \n",
      "2024-01-02 04:41:49.885024: Epoch 994\n",
      "2024-01-02 04:41:49.890100: Current learning rate: 0.0001\n",
      "2024-01-02 04:43:55.107987: train_loss -0.9297\n",
      "2024-01-02 04:43:55.116991: val_loss -0.8267\n",
      "2024-01-02 04:43:55.124987: Pseudo dice [0.9253, 0.9451, 0.9441]\n",
      "2024-01-02 04:43:55.128987: Epoch time: 125.23 s\n",
      "2024-01-02 04:43:56.227355: \n",
      "2024-01-02 04:43:56.232887: Epoch 995\n",
      "2024-01-02 04:43:56.238928: Current learning rate: 8e-05\n",
      "2024-01-02 04:46:02.008110: train_loss -0.9262\n",
      "2024-01-02 04:46:02.016114: val_loss -0.8287\n",
      "2024-01-02 04:46:02.021112: Pseudo dice [0.9259, 0.9442, 0.9434]\n",
      "2024-01-02 04:46:02.026112: Epoch time: 125.78 s\n",
      "2024-01-02 04:46:03.312125: \n",
      "2024-01-02 04:46:03.319109: Epoch 996\n",
      "2024-01-02 04:46:03.323165: Current learning rate: 7e-05\n",
      "2024-01-02 04:48:08.850218: train_loss -0.9289\n",
      "2024-01-02 04:48:08.862726: val_loss -0.8296\n",
      "2024-01-02 04:48:08.870240: Pseudo dice [0.9261, 0.9444, 0.9436]\n",
      "2024-01-02 04:48:08.879239: Epoch time: 125.54 s\n",
      "2024-01-02 04:48:10.088192: \n",
      "2024-01-02 04:48:10.094322: Epoch 997\n",
      "2024-01-02 04:48:10.099732: Current learning rate: 5e-05\n",
      "2024-01-02 04:50:15.505476: train_loss -0.928\n",
      "2024-01-02 04:50:15.514476: val_loss -0.8263\n",
      "2024-01-02 04:50:15.519479: Pseudo dice [0.9253, 0.9447, 0.9437]\n",
      "2024-01-02 04:50:15.524479: Epoch time: 125.42 s\n",
      "2024-01-02 04:50:16.708430: \n",
      "2024-01-02 04:50:16.713968: Epoch 998\n",
      "2024-01-02 04:50:16.720040: Current learning rate: 4e-05\n",
      "2024-01-02 04:52:22.054815: train_loss -0.9297\n",
      "2024-01-02 04:52:22.064814: val_loss -0.8312\n",
      "2024-01-02 04:52:22.072815: Pseudo dice [0.9262, 0.9443, 0.9436]\n",
      "2024-01-02 04:52:22.077822: Epoch time: 125.35 s\n",
      "2024-01-02 04:52:23.490477: \n",
      "2024-01-02 04:52:23.497577: Epoch 999\n",
      "2024-01-02 04:52:23.502018: Current learning rate: 2e-05\n",
      "2024-01-02 04:54:29.030643: train_loss -0.9282\n",
      "2024-01-02 04:54:29.037643: val_loss -0.8269\n",
      "2024-01-02 04:54:29.042645: Pseudo dice [0.9254, 0.9449, 0.9441]\n",
      "2024-01-02 04:54:29.048657: Epoch time: 125.54 s\n",
      "2024-01-02 04:54:30.916879: Training done.\n",
      "2024-01-02 04:54:30.958859: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-02 04:54:30.969939: The split file contains 5 splits.\n",
      "2024-01-02 04:54:30.976938: Desired fold for training: 0\n",
      "2024-01-02 04:54:30.982935: This split has 8 training and 2 validation cases.\n",
      "2024-01-02 04:54:30.989935: predicting case_0\n",
      "2024-01-02 04:54:34.759091: predicting case_7\n",
      "2024-01-02 04:54:45.585415: Validation complete\n",
      "2024-01-02 04:54:45.590423: Mean Validation Dice:  0.9384002863703483\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-02 04:54:46.379723: unpacking dataset...\n",
      "2024-01-02 04:54:46.633871: unpacking done...\n",
      "2024-01-02 04:54:46.639415: do_dummy_2d_data_aug: False\n",
      "2024-01-02 04:54:46.643414: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-02 04:54:46.647414: The split file contains 5 splits.\n",
      "2024-01-02 04:54:46.651419: Desired fold for training: 1\n",
      "2024-01-02 04:54:46.654701: This split has 8 training and 2 validation cases.\n",
      "2024-01-02 04:54:46.683253: Unable to plot network architecture:\n",
      "2024-01-02 04:54:46.687277: No module named 'hiddenlayer'\n",
      "2024-01-02 04:54:46.709376: \n",
      "2024-01-02 04:54:46.714298: Epoch 0\n",
      "2024-01-02 04:54:46.717804: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-02 04:57:05.225632: train_loss -0.398\n",
      "2024-01-02 04:57:05.231621: val_loss -0.6315\n",
      "2024-01-02 04:57:05.238622: Pseudo dice [0.8313, 0.8851, 0.865]\n",
      "2024-01-02 04:57:05.243630: Epoch time: 138.52 s\n",
      "2024-01-02 04:57:05.249623: Yayy! New best EMA pseudo Dice: 0.8604\n",
      "2024-01-02 04:57:06.419203: \n",
      "2024-01-02 04:57:06.427195: Epoch 1\n",
      "2024-01-02 04:57:06.431282: Current learning rate: 0.00999\n",
      "2024-01-02 04:59:11.125515: train_loss -0.6765\n",
      "2024-01-02 04:59:11.134530: val_loss -0.726\n",
      "2024-01-02 04:59:11.144515: Pseudo dice [0.8755, 0.912, 0.8955]\n",
      "2024-01-02 04:59:11.151525: Epoch time: 124.71 s\n",
      "2024-01-02 04:59:11.157526: Yayy! New best EMA pseudo Dice: 0.8638\n",
      "2024-01-02 04:59:12.526817: \n",
      "2024-01-02 04:59:12.533168: Epoch 2\n",
      "2024-01-02 04:59:12.538244: Current learning rate: 0.00998\n",
      "2024-01-02 05:01:17.354881: train_loss -0.7359\n",
      "2024-01-02 05:01:17.363392: val_loss -0.7719\n",
      "2024-01-02 05:01:17.367392: Pseudo dice [0.8809, 0.9278, 0.912]\n",
      "2024-01-02 05:01:17.372392: Epoch time: 124.83 s\n",
      "2024-01-02 05:01:17.376393: Yayy! New best EMA pseudo Dice: 0.8681\n",
      "2024-01-02 05:01:18.755066: \n",
      "2024-01-02 05:01:18.762714: Epoch 3\n",
      "2024-01-02 05:01:18.767796: Current learning rate: 0.00997\n",
      "2024-01-02 05:03:23.721980: train_loss -0.7696\n",
      "2024-01-02 05:03:23.729291: val_loss -0.7882\n",
      "2024-01-02 05:03:23.736815: Pseudo dice [0.8911, 0.9326, 0.9168]\n",
      "2024-01-02 05:03:23.740816: Epoch time: 124.97 s\n",
      "2024-01-02 05:03:23.745817: Yayy! New best EMA pseudo Dice: 0.8727\n",
      "2024-01-02 05:03:25.072436: \n",
      "2024-01-02 05:03:25.076868: Epoch 4\n",
      "2024-01-02 05:03:25.081899: Current learning rate: 0.00996\n",
      "2024-01-02 05:05:29.863001: train_loss -0.7857\n",
      "2024-01-02 05:05:29.869999: val_loss -0.7968\n",
      "2024-01-02 05:05:29.874999: Pseudo dice [0.8913, 0.9357, 0.9225]\n",
      "2024-01-02 05:05:29.878998: Epoch time: 124.79 s\n",
      "2024-01-02 05:05:29.883998: Yayy! New best EMA pseudo Dice: 0.8771\n",
      "2024-01-02 05:05:31.348451: \n",
      "2024-01-02 05:05:31.358575: Epoch 5\n",
      "2024-01-02 05:05:31.368653: Current learning rate: 0.00995\n",
      "2024-01-02 05:07:36.575835: train_loss -0.7966\n",
      "2024-01-02 05:07:36.582835: val_loss -0.8031\n",
      "2024-01-02 05:07:36.588835: Pseudo dice [0.8949, 0.9358, 0.9213]\n",
      "2024-01-02 05:07:36.596325: Epoch time: 125.23 s\n",
      "2024-01-02 05:07:36.602400: Yayy! New best EMA pseudo Dice: 0.8811\n",
      "2024-01-02 05:07:38.327427: \n",
      "2024-01-02 05:07:38.333281: Epoch 6\n",
      "2024-01-02 05:07:38.337542: Current learning rate: 0.00995\n",
      "2024-01-02 05:09:43.317770: train_loss -0.8057\n",
      "2024-01-02 05:09:43.323771: val_loss -0.8115\n",
      "2024-01-02 05:09:43.330770: Pseudo dice [0.8994, 0.9403, 0.9265]\n",
      "2024-01-02 05:09:43.336310: Epoch time: 124.99 s\n",
      "2024-01-02 05:09:43.341305: Yayy! New best EMA pseudo Dice: 0.8852\n",
      "2024-01-02 05:09:44.904945: \n",
      "2024-01-02 05:09:44.909884: Epoch 7\n",
      "2024-01-02 05:09:44.914934: Current learning rate: 0.00994\n",
      "2024-01-02 05:11:49.655724: train_loss -0.8088\n",
      "2024-01-02 05:11:49.665728: val_loss -0.8147\n",
      "2024-01-02 05:11:49.677653: Pseudo dice [0.9004, 0.9409, 0.9291]\n",
      "2024-01-02 05:11:49.683652: Epoch time: 124.75 s\n",
      "2024-01-02 05:11:49.689653: Yayy! New best EMA pseudo Dice: 0.889\n",
      "2024-01-02 05:11:51.116025: \n",
      "2024-01-02 05:11:51.121022: Epoch 8\n",
      "2024-01-02 05:11:51.128974: Current learning rate: 0.00993\n",
      "2024-01-02 05:13:55.943496: train_loss -0.8154\n",
      "2024-01-02 05:13:55.949491: val_loss -0.8166\n",
      "2024-01-02 05:13:55.954491: Pseudo dice [0.9033, 0.9414, 0.9265]\n",
      "2024-01-02 05:13:55.959520: Epoch time: 124.83 s\n",
      "2024-01-02 05:13:55.963537: Yayy! New best EMA pseudo Dice: 0.8925\n",
      "2024-01-02 05:13:57.439773: \n",
      "2024-01-02 05:13:57.444781: Epoch 9\n",
      "2024-01-02 05:13:57.449823: Current learning rate: 0.00992\n",
      "2024-01-02 05:16:02.680502: train_loss -0.8197\n",
      "2024-01-02 05:16:02.688503: val_loss -0.8163\n",
      "2024-01-02 05:16:02.696509: Pseudo dice [0.9012, 0.94, 0.9285]\n",
      "2024-01-02 05:16:02.703505: Epoch time: 125.24 s\n",
      "2024-01-02 05:16:02.709504: Yayy! New best EMA pseudo Dice: 0.8956\n",
      "2024-01-02 05:16:04.055000: \n",
      "2024-01-02 05:16:04.061982: Epoch 10\n",
      "2024-01-02 05:16:04.069004: Current learning rate: 0.00991\n",
      "2024-01-02 05:18:08.956811: train_loss -0.8221\n",
      "2024-01-02 05:18:08.964812: val_loss -0.8236\n",
      "2024-01-02 05:18:08.972811: Pseudo dice [0.9045, 0.9419, 0.9327]\n",
      "2024-01-02 05:18:08.977812: Epoch time: 124.9 s\n",
      "2024-01-02 05:18:08.981812: Yayy! New best EMA pseudo Dice: 0.8986\n",
      "2024-01-02 05:18:10.464827: \n",
      "2024-01-02 05:18:10.476615: Epoch 11\n",
      "2024-01-02 05:18:10.481657: Current learning rate: 0.0099\n",
      "2024-01-02 05:20:15.340722: train_loss -0.8273\n",
      "2024-01-02 05:20:15.347720: val_loss -0.8197\n",
      "2024-01-02 05:20:15.352722: Pseudo dice [0.8983, 0.9412, 0.9292]\n",
      "2024-01-02 05:20:15.357721: Epoch time: 124.88 s\n",
      "2024-01-02 05:20:15.362723: Yayy! New best EMA pseudo Dice: 0.9011\n",
      "2024-01-02 05:20:16.699471: \n",
      "2024-01-02 05:20:16.707471: Epoch 12\n",
      "2024-01-02 05:20:16.711725: Current learning rate: 0.00989\n",
      "2024-01-02 05:22:21.665272: train_loss -0.8296\n",
      "2024-01-02 05:22:21.671781: val_loss -0.8278\n",
      "2024-01-02 05:22:21.678780: Pseudo dice [0.9046, 0.9439, 0.933]\n",
      "2024-01-02 05:22:21.687779: Epoch time: 124.97 s\n",
      "2024-01-02 05:22:21.691779: Yayy! New best EMA pseudo Dice: 0.9037\n",
      "2024-01-02 05:22:23.019461: \n",
      "2024-01-02 05:22:23.030060: Epoch 13\n",
      "2024-01-02 05:22:23.035984: Current learning rate: 0.00988\n",
      "2024-01-02 05:24:28.164235: train_loss -0.8337\n",
      "2024-01-02 05:24:28.171236: val_loss -0.8252\n",
      "2024-01-02 05:24:28.178743: Pseudo dice [0.9019, 0.9441, 0.9332]\n",
      "2024-01-02 05:24:28.183750: Epoch time: 125.15 s\n",
      "2024-01-02 05:24:28.189747: Yayy! New best EMA pseudo Dice: 0.906\n",
      "2024-01-02 05:24:29.858333: \n",
      "2024-01-02 05:24:29.864008: Epoch 14\n",
      "2024-01-02 05:24:29.871011: Current learning rate: 0.00987\n",
      "2024-01-02 05:26:34.823506: train_loss -0.8328\n",
      "2024-01-02 05:26:34.831506: val_loss -0.8267\n",
      "2024-01-02 05:26:34.839022: Pseudo dice [0.9014, 0.9456, 0.9337]\n",
      "2024-01-02 05:26:34.844022: Epoch time: 124.97 s\n",
      "2024-01-02 05:26:34.848029: Yayy! New best EMA pseudo Dice: 0.9081\n",
      "2024-01-02 05:26:36.207738: \n",
      "2024-01-02 05:26:36.213754: Epoch 15\n",
      "2024-01-02 05:26:36.218503: Current learning rate: 0.00986\n",
      "2024-01-02 05:28:40.904842: train_loss -0.8392\n",
      "2024-01-02 05:28:40.913851: val_loss -0.8277\n",
      "2024-01-02 05:28:40.919851: Pseudo dice [0.8986, 0.9446, 0.9349]\n",
      "2024-01-02 05:28:40.926865: Epoch time: 124.7 s\n",
      "2024-01-02 05:28:40.931863: Yayy! New best EMA pseudo Dice: 0.9098\n",
      "2024-01-02 05:28:42.300137: \n",
      "2024-01-02 05:28:42.307186: Epoch 16\n",
      "2024-01-02 05:28:42.311444: Current learning rate: 0.00986\n",
      "2024-01-02 05:30:47.625450: train_loss -0.8378\n",
      "2024-01-02 05:30:47.635451: val_loss -0.8273\n",
      "2024-01-02 05:30:47.641455: Pseudo dice [0.9042, 0.9453, 0.933]\n",
      "2024-01-02 05:30:47.651451: Epoch time: 125.33 s\n",
      "2024-01-02 05:30:47.656451: Yayy! New best EMA pseudo Dice: 0.9116\n",
      "2024-01-02 05:30:49.098179: \n",
      "2024-01-02 05:30:49.104105: Epoch 17\n",
      "2024-01-02 05:30:49.112180: Current learning rate: 0.00985\n",
      "2024-01-02 05:32:54.032950: train_loss -0.8408\n",
      "2024-01-02 05:32:54.040951: val_loss -0.8291\n",
      "2024-01-02 05:32:54.045950: Pseudo dice [0.9043, 0.9435, 0.9356]\n",
      "2024-01-02 05:32:54.052949: Epoch time: 124.94 s\n",
      "2024-01-02 05:32:54.056950: Yayy! New best EMA pseudo Dice: 0.9132\n",
      "2024-01-02 05:32:55.613440: \n",
      "2024-01-02 05:32:55.620445: Epoch 18\n",
      "2024-01-02 05:32:55.630351: Current learning rate: 0.00984\n",
      "2024-01-02 05:35:00.885363: train_loss -0.841\n",
      "2024-01-02 05:35:00.892364: val_loss -0.8252\n",
      "2024-01-02 05:35:00.897366: Pseudo dice [0.9034, 0.9442, 0.9305]\n",
      "2024-01-02 05:35:00.901366: Epoch time: 125.27 s\n",
      "2024-01-02 05:35:00.905366: Yayy! New best EMA pseudo Dice: 0.9145\n",
      "2024-01-02 05:35:02.465881: \n",
      "2024-01-02 05:35:02.470968: Epoch 19\n",
      "2024-01-02 05:35:02.475524: Current learning rate: 0.00983\n",
      "2024-01-02 05:37:07.511351: train_loss -0.8447\n",
      "2024-01-02 05:37:07.519342: val_loss -0.8153\n",
      "2024-01-02 05:37:07.525342: Pseudo dice [0.8963, 0.9408, 0.9269]\n",
      "2024-01-02 05:37:07.529342: Epoch time: 125.05 s\n",
      "2024-01-02 05:37:07.534340: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2024-01-02 05:37:08.989172: \n",
      "2024-01-02 05:37:09.000978: Epoch 20\n",
      "2024-01-02 05:37:09.006039: Current learning rate: 0.00982\n",
      "2024-01-02 05:39:14.081493: train_loss -0.8461\n",
      "2024-01-02 05:39:14.088501: val_loss -0.8283\n",
      "2024-01-02 05:39:14.096494: Pseudo dice [0.9011, 0.9452, 0.9349]\n",
      "2024-01-02 05:39:14.102493: Epoch time: 125.09 s\n",
      "2024-01-02 05:39:14.108495: Yayy! New best EMA pseudo Dice: 0.9164\n",
      "2024-01-02 05:39:15.723885: \n",
      "2024-01-02 05:39:15.732940: Epoch 21\n",
      "2024-01-02 05:39:15.737060: Current learning rate: 0.00981\n",
      "2024-01-02 05:41:21.477115: train_loss -0.8459\n",
      "2024-01-02 05:41:21.484117: val_loss -0.8244\n",
      "2024-01-02 05:41:21.489116: Pseudo dice [0.9018, 0.942, 0.9338]\n",
      "2024-01-02 05:41:21.497115: Epoch time: 125.75 s\n",
      "2024-01-02 05:41:21.505118: Yayy! New best EMA pseudo Dice: 0.9173\n",
      "2024-01-02 05:41:23.037244: \n",
      "2024-01-02 05:41:23.043064: Epoch 22\n",
      "2024-01-02 05:41:23.048084: Current learning rate: 0.0098\n",
      "2024-01-02 05:43:28.331098: train_loss -0.8457\n",
      "2024-01-02 05:43:28.338097: val_loss -0.8237\n",
      "2024-01-02 05:43:28.344094: Pseudo dice [0.9008, 0.9427, 0.9351]\n",
      "2024-01-02 05:43:28.350094: Epoch time: 125.29 s\n",
      "2024-01-02 05:43:28.356092: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2024-01-02 05:43:29.937734: \n",
      "2024-01-02 05:43:29.945728: Epoch 23\n",
      "2024-01-02 05:43:29.949733: Current learning rate: 0.00979\n",
      "2024-01-02 05:45:35.046852: train_loss -0.8496\n",
      "2024-01-02 05:45:35.053854: val_loss -0.8304\n",
      "2024-01-02 05:45:35.062864: Pseudo dice [0.9066, 0.9444, 0.9366]\n",
      "2024-01-02 05:45:35.067867: Epoch time: 125.11 s\n",
      "2024-01-02 05:45:35.073861: Yayy! New best EMA pseudo Dice: 0.9193\n",
      "2024-01-02 05:45:36.519892: \n",
      "2024-01-02 05:45:36.526303: Epoch 24\n",
      "2024-01-02 05:45:36.530498: Current learning rate: 0.00978\n",
      "2024-01-02 05:47:41.605193: train_loss -0.8505\n",
      "2024-01-02 05:47:41.613193: val_loss -0.8318\n",
      "2024-01-02 05:47:41.618193: Pseudo dice [0.9018, 0.9463, 0.9348]\n",
      "2024-01-02 05:47:41.623193: Epoch time: 125.09 s\n",
      "2024-01-02 05:47:41.627198: Yayy! New best EMA pseudo Dice: 0.9201\n",
      "2024-01-02 05:47:42.956155: \n",
      "2024-01-02 05:47:42.967155: Epoch 25\n",
      "2024-01-02 05:47:42.972248: Current learning rate: 0.00977\n",
      "2024-01-02 05:49:48.250621: train_loss -0.8509\n",
      "2024-01-02 05:49:48.256621: val_loss -0.8256\n",
      "2024-01-02 05:49:48.263622: Pseudo dice [0.908, 0.9429, 0.934]\n",
      "2024-01-02 05:49:48.267620: Epoch time: 125.3 s\n",
      "2024-01-02 05:49:48.271620: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2024-01-02 05:49:49.606324: \n",
      "2024-01-02 05:49:49.613147: Epoch 26\n",
      "2024-01-02 05:49:49.620149: Current learning rate: 0.00977\n",
      "2024-01-02 05:51:54.841635: train_loss -0.8544\n",
      "2024-01-02 05:51:54.847637: val_loss -0.8303\n",
      "2024-01-02 05:51:54.852635: Pseudo dice [0.9056, 0.9438, 0.9345]\n",
      "2024-01-02 05:51:54.859710: Epoch time: 125.24 s\n",
      "2024-01-02 05:51:54.865666: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2024-01-02 05:51:56.222454: \n",
      "2024-01-02 05:51:56.229864: Epoch 27\n",
      "2024-01-02 05:51:56.233852: Current learning rate: 0.00976\n",
      "2024-01-02 05:54:01.206265: train_loss -0.8568\n",
      "2024-01-02 05:54:01.213265: val_loss -0.8341\n",
      "2024-01-02 05:54:01.219265: Pseudo dice [0.9073, 0.9467, 0.9384]\n",
      "2024-01-02 05:54:01.226266: Epoch time: 124.98 s\n",
      "2024-01-02 05:54:01.233274: Yayy! New best EMA pseudo Dice: 0.9226\n",
      "2024-01-02 05:54:02.563606: \n",
      "2024-01-02 05:54:02.568991: Epoch 28\n",
      "2024-01-02 05:54:02.578080: Current learning rate: 0.00975\n",
      "2024-01-02 05:56:07.851153: train_loss -0.8563\n",
      "2024-01-02 05:56:07.865153: val_loss -0.8319\n",
      "2024-01-02 05:56:07.879664: Pseudo dice [0.9073, 0.9468, 0.9362]\n",
      "2024-01-02 05:56:07.885680: Epoch time: 125.29 s\n",
      "2024-01-02 05:56:07.891675: Yayy! New best EMA pseudo Dice: 0.9233\n",
      "2024-01-02 05:56:09.436558: \n",
      "2024-01-02 05:56:09.442358: Epoch 29\n",
      "2024-01-02 05:56:09.451277: Current learning rate: 0.00974\n",
      "2024-01-02 05:58:14.612050: train_loss -0.8587\n",
      "2024-01-02 05:58:14.619050: val_loss -0.8303\n",
      "2024-01-02 05:58:14.625058: Pseudo dice [0.9068, 0.9454, 0.9347]\n",
      "2024-01-02 05:58:14.632052: Epoch time: 125.18 s\n",
      "2024-01-02 05:58:14.637057: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2024-01-02 05:58:16.033299: \n",
      "2024-01-02 05:58:16.040382: Epoch 30\n",
      "2024-01-02 05:58:16.047456: Current learning rate: 0.00973\n",
      "2024-01-02 06:00:21.106564: train_loss -0.858\n",
      "2024-01-02 06:00:21.118109: val_loss -0.8376\n",
      "2024-01-02 06:00:21.125116: Pseudo dice [0.9075, 0.9487, 0.9395]\n",
      "2024-01-02 06:00:21.132113: Epoch time: 125.07 s\n",
      "2024-01-02 06:00:21.141116: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2024-01-02 06:00:22.714362: \n",
      "2024-01-02 06:00:22.727640: Epoch 31\n",
      "2024-01-02 06:00:22.732700: Current learning rate: 0.00972\n",
      "2024-01-02 06:02:28.014157: train_loss -0.8576\n",
      "2024-01-02 06:02:28.023148: val_loss -0.8322\n",
      "2024-01-02 06:02:28.029147: Pseudo dice [0.9076, 0.947, 0.9368]\n",
      "2024-01-02 06:02:28.035151: Epoch time: 125.3 s\n",
      "2024-01-02 06:02:28.042147: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-02 06:02:29.528008: \n",
      "2024-01-02 06:02:29.536335: Epoch 32\n",
      "2024-01-02 06:02:29.541340: Current learning rate: 0.00971\n",
      "2024-01-02 06:04:34.654866: train_loss -0.8591\n",
      "2024-01-02 06:04:34.662872: val_loss -0.8351\n",
      "2024-01-02 06:04:34.669380: Pseudo dice [0.9075, 0.9481, 0.938]\n",
      "2024-01-02 06:04:34.676381: Epoch time: 125.13 s\n",
      "2024-01-02 06:04:34.681385: Yayy! New best EMA pseudo Dice: 0.9259\n",
      "2024-01-02 06:04:36.235394: \n",
      "2024-01-02 06:04:36.243637: Epoch 33\n",
      "2024-01-02 06:04:36.249645: Current learning rate: 0.0097\n",
      "2024-01-02 06:06:41.707183: train_loss -0.8599\n",
      "2024-01-02 06:06:41.714183: val_loss -0.8385\n",
      "2024-01-02 06:06:41.719694: Pseudo dice [0.9067, 0.9483, 0.9413]\n",
      "2024-01-02 06:06:41.725694: Epoch time: 125.47 s\n",
      "2024-01-02 06:06:41.730695: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2024-01-02 06:06:43.128263: \n",
      "2024-01-02 06:06:43.133861: Epoch 34\n",
      "2024-01-02 06:06:43.142995: Current learning rate: 0.00969\n",
      "2024-01-02 06:08:48.589035: train_loss -0.8632\n",
      "2024-01-02 06:08:48.598546: val_loss -0.8368\n",
      "2024-01-02 06:08:48.605553: Pseudo dice [0.9102, 0.9487, 0.9394]\n",
      "2024-01-02 06:08:48.615551: Epoch time: 125.46 s\n",
      "2024-01-02 06:08:48.623549: Yayy! New best EMA pseudo Dice: 0.9271\n",
      "2024-01-02 06:08:50.185396: \n",
      "2024-01-02 06:08:50.192459: Epoch 35\n",
      "2024-01-02 06:08:50.197917: Current learning rate: 0.00968\n",
      "2024-01-02 06:10:55.868182: train_loss -0.8659\n",
      "2024-01-02 06:10:55.877179: val_loss -0.8331\n",
      "2024-01-02 06:10:55.885186: Pseudo dice [0.9072, 0.9465, 0.9393]\n",
      "2024-01-02 06:10:55.892692: Epoch time: 125.68 s\n",
      "2024-01-02 06:10:55.899220: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2024-01-02 06:10:57.480243: \n",
      "2024-01-02 06:10:57.489245: Epoch 36\n",
      "2024-01-02 06:10:57.495243: Current learning rate: 0.00968\n",
      "2024-01-02 06:13:03.963075: train_loss -0.8661\n",
      "2024-01-02 06:13:03.974655: val_loss -0.8395\n",
      "2024-01-02 06:13:03.984751: Pseudo dice [0.9063, 0.95, 0.941]\n",
      "2024-01-02 06:13:03.994328: Epoch time: 126.48 s\n",
      "2024-01-02 06:13:04.004312: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2024-01-02 06:13:05.605296: \n",
      "2024-01-02 06:13:05.612307: Epoch 37\n",
      "2024-01-02 06:13:05.616302: Current learning rate: 0.00967\n",
      "2024-01-02 06:15:12.845197: train_loss -0.8655\n",
      "2024-01-02 06:15:12.859201: val_loss -0.8333\n",
      "2024-01-02 06:15:12.871711: Pseudo dice [0.9066, 0.9459, 0.9402]\n",
      "2024-01-02 06:15:12.883713: Epoch time: 127.24 s\n",
      "2024-01-02 06:15:12.892716: Yayy! New best EMA pseudo Dice: 0.9283\n",
      "2024-01-02 06:15:14.567480: \n",
      "2024-01-02 06:15:14.574016: Epoch 38\n",
      "2024-01-02 06:15:14.579553: Current learning rate: 0.00966\n",
      "2024-01-02 06:17:21.052111: train_loss -0.8672\n",
      "2024-01-02 06:17:21.064111: val_loss -0.8339\n",
      "2024-01-02 06:17:21.073111: Pseudo dice [0.9051, 0.9453, 0.9403]\n",
      "2024-01-02 06:17:21.083114: Epoch time: 126.49 s\n",
      "2024-01-02 06:17:21.092110: Yayy! New best EMA pseudo Dice: 0.9285\n",
      "2024-01-02 06:17:22.684941: \n",
      "2024-01-02 06:17:22.694324: Epoch 39\n",
      "2024-01-02 06:17:22.700418: Current learning rate: 0.00965\n",
      "2024-01-02 06:19:28.491678: train_loss -0.869\n",
      "2024-01-02 06:19:28.498677: val_loss -0.841\n",
      "2024-01-02 06:19:28.504676: Pseudo dice [0.9116, 0.9485, 0.9417]\n",
      "2024-01-02 06:19:28.511685: Epoch time: 125.81 s\n",
      "2024-01-02 06:19:28.517691: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2024-01-02 06:19:29.962432: \n",
      "2024-01-02 06:19:29.968403: Epoch 40\n",
      "2024-01-02 06:19:29.972489: Current learning rate: 0.00964\n",
      "2024-01-02 06:21:35.604124: train_loss -0.8696\n",
      "2024-01-02 06:21:35.613128: val_loss -0.8369\n",
      "2024-01-02 06:21:35.619128: Pseudo dice [0.909, 0.9479, 0.9407]\n",
      "2024-01-02 06:21:35.625130: Epoch time: 125.64 s\n",
      "2024-01-02 06:21:35.634150: Yayy! New best EMA pseudo Dice: 0.9294\n",
      "2024-01-02 06:21:37.140986: \n",
      "2024-01-02 06:21:37.147164: Epoch 41\n",
      "2024-01-02 06:21:37.151247: Current learning rate: 0.00963\n",
      "2024-01-02 06:23:42.722586: train_loss -0.8716\n",
      "2024-01-02 06:23:42.732587: val_loss -0.8415\n",
      "2024-01-02 06:23:42.741150: Pseudo dice [0.9087, 0.9517, 0.9424]\n",
      "2024-01-02 06:23:42.746150: Epoch time: 125.58 s\n",
      "2024-01-02 06:23:42.751150: Yayy! New best EMA pseudo Dice: 0.9299\n",
      "2024-01-02 06:23:44.282677: \n",
      "2024-01-02 06:23:44.290026: Epoch 42\n",
      "2024-01-02 06:23:44.298079: Current learning rate: 0.00962\n",
      "2024-01-02 06:25:50.150695: train_loss -0.8717\n",
      "2024-01-02 06:25:50.157695: val_loss -0.8412\n",
      "2024-01-02 06:25:50.167694: Pseudo dice [0.9119, 0.9505, 0.9401]\n",
      "2024-01-02 06:25:50.177692: Epoch time: 125.87 s\n",
      "2024-01-02 06:25:50.184694: Yayy! New best EMA pseudo Dice: 0.9303\n",
      "2024-01-02 06:25:51.746795: \n",
      "2024-01-02 06:25:51.752821: Epoch 43\n",
      "2024-01-02 06:25:51.758258: Current learning rate: 0.00961\n",
      "2024-01-02 06:27:57.495908: train_loss -0.8702\n",
      "2024-01-02 06:27:57.502910: val_loss -0.8404\n",
      "2024-01-02 06:27:57.512911: Pseudo dice [0.9115, 0.9507, 0.9409]\n",
      "2024-01-02 06:27:57.519910: Epoch time: 125.75 s\n",
      "2024-01-02 06:27:57.529939: Yayy! New best EMA pseudo Dice: 0.9307\n",
      "2024-01-02 06:27:59.202859: \n",
      "2024-01-02 06:27:59.210359: Epoch 44\n",
      "2024-01-02 06:27:59.216275: Current learning rate: 0.0096\n",
      "2024-01-02 06:30:04.892702: train_loss -0.8718\n",
      "2024-01-02 06:30:04.904817: val_loss -0.8408\n",
      "2024-01-02 06:30:04.914840: Pseudo dice [0.9091, 0.9499, 0.9423]\n",
      "2024-01-02 06:30:04.921847: Epoch time: 125.69 s\n",
      "2024-01-02 06:30:04.927854: Yayy! New best EMA pseudo Dice: 0.931\n",
      "2024-01-02 06:30:06.446667: \n",
      "2024-01-02 06:30:06.453674: Epoch 45\n",
      "2024-01-02 06:30:06.458683: Current learning rate: 0.00959\n",
      "2024-01-02 06:32:11.956774: train_loss -0.875\n",
      "2024-01-02 06:32:11.965773: val_loss -0.8403\n",
      "2024-01-02 06:32:11.971772: Pseudo dice [0.9115, 0.9499, 0.9405]\n",
      "2024-01-02 06:32:11.977774: Epoch time: 125.51 s\n",
      "2024-01-02 06:32:11.987773: Yayy! New best EMA pseudo Dice: 0.9313\n",
      "2024-01-02 06:32:13.435480: \n",
      "2024-01-02 06:32:13.446035: Epoch 46\n",
      "2024-01-02 06:32:13.454093: Current learning rate: 0.00959\n",
      "2024-01-02 06:34:19.777751: train_loss -0.8726\n",
      "2024-01-02 06:34:19.788439: val_loss -0.838\n",
      "2024-01-02 06:34:19.794437: Pseudo dice [0.9095, 0.9478, 0.9403]\n",
      "2024-01-02 06:34:19.800009: Epoch time: 126.35 s\n",
      "2024-01-02 06:34:19.806014: Yayy! New best EMA pseudo Dice: 0.9314\n",
      "2024-01-02 06:34:21.257972: \n",
      "2024-01-02 06:34:21.264977: Epoch 47\n",
      "2024-01-02 06:34:21.271972: Current learning rate: 0.00958\n",
      "2024-01-02 06:36:27.234979: train_loss -0.8741\n",
      "2024-01-02 06:36:27.243978: val_loss -0.8428\n",
      "2024-01-02 06:36:27.248977: Pseudo dice [0.912, 0.949, 0.9419]\n",
      "2024-01-02 06:36:27.254979: Epoch time: 125.98 s\n",
      "2024-01-02 06:36:27.262978: Yayy! New best EMA pseudo Dice: 0.9317\n",
      "2024-01-02 06:36:28.658214: \n",
      "2024-01-02 06:36:28.665217: Epoch 48\n",
      "2024-01-02 06:36:28.670282: Current learning rate: 0.00957\n",
      "2024-01-02 06:38:34.861396: train_loss -0.8754\n",
      "2024-01-02 06:38:34.870403: val_loss -0.836\n",
      "2024-01-02 06:38:34.879425: Pseudo dice [0.9083, 0.9487, 0.94]\n",
      "2024-01-02 06:38:34.888428: Epoch time: 126.2 s\n",
      "2024-01-02 06:38:34.895966: Yayy! New best EMA pseudo Dice: 0.9318\n",
      "2024-01-02 06:38:36.392605: \n",
      "2024-01-02 06:38:36.403231: Epoch 49\n",
      "2024-01-02 06:38:36.408229: Current learning rate: 0.00956\n",
      "2024-01-02 06:40:41.960127: train_loss -0.8751\n",
      "2024-01-02 06:40:41.967137: val_loss -0.8417\n",
      "2024-01-02 06:40:41.973120: Pseudo dice [0.9112, 0.9504, 0.9425]\n",
      "2024-01-02 06:40:41.977132: Epoch time: 125.57 s\n",
      "2024-01-02 06:40:42.279439: Yayy! New best EMA pseudo Dice: 0.9321\n",
      "2024-01-02 06:40:43.557667: \n",
      "2024-01-02 06:40:43.563812: Epoch 50\n",
      "2024-01-02 06:40:43.568834: Current learning rate: 0.00955\n",
      "2024-01-02 06:42:48.937194: train_loss -0.875\n",
      "2024-01-02 06:42:48.944195: val_loss -0.8401\n",
      "2024-01-02 06:42:48.950193: Pseudo dice [0.9108, 0.9496, 0.9411]\n",
      "2024-01-02 06:42:48.958197: Epoch time: 125.38 s\n",
      "2024-01-02 06:42:48.963206: Yayy! New best EMA pseudo Dice: 0.9323\n",
      "2024-01-02 06:42:50.561138: \n",
      "2024-01-02 06:42:50.570198: Epoch 51\n",
      "2024-01-02 06:42:50.574847: Current learning rate: 0.00954\n",
      "2024-01-02 06:44:57.113200: train_loss -0.8746\n",
      "2024-01-02 06:44:57.121876: val_loss -0.8397\n",
      "2024-01-02 06:44:57.128876: Pseudo dice [0.9105, 0.9477, 0.9411]\n",
      "2024-01-02 06:44:57.137880: Epoch time: 126.55 s\n",
      "2024-01-02 06:44:57.143878: Yayy! New best EMA pseudo Dice: 0.9323\n",
      "2024-01-02 06:44:58.607175: \n",
      "2024-01-02 06:44:58.613161: Epoch 52\n",
      "2024-01-02 06:44:58.618165: Current learning rate: 0.00953\n",
      "2024-01-02 06:47:06.527821: train_loss -0.8756\n",
      "2024-01-02 06:47:06.541823: val_loss -0.8435\n",
      "2024-01-02 06:47:06.549822: Pseudo dice [0.9095, 0.951, 0.942]\n",
      "2024-01-02 06:47:06.556821: Epoch time: 127.92 s\n",
      "2024-01-02 06:47:06.564821: Yayy! New best EMA pseudo Dice: 0.9325\n",
      "2024-01-02 06:47:08.373174: \n",
      "2024-01-02 06:47:08.379179: Epoch 53\n",
      "2024-01-02 06:47:08.384177: Current learning rate: 0.00952\n",
      "2024-01-02 06:49:15.405431: train_loss -0.8774\n",
      "2024-01-02 06:49:15.413430: val_loss -0.8444\n",
      "2024-01-02 06:49:15.422429: Pseudo dice [0.9111, 0.952, 0.944]\n",
      "2024-01-02 06:49:15.429431: Epoch time: 127.03 s\n",
      "2024-01-02 06:49:15.444845: Yayy! New best EMA pseudo Dice: 0.9328\n",
      "2024-01-02 06:49:17.117334: \n",
      "2024-01-02 06:49:17.124336: Epoch 54\n",
      "2024-01-02 06:49:17.129339: Current learning rate: 0.00951\n",
      "2024-01-02 06:51:23.803132: train_loss -0.8775\n",
      "2024-01-02 06:51:23.811131: val_loss -0.8411\n",
      "2024-01-02 06:51:23.818134: Pseudo dice [0.9152, 0.9509, 0.941]\n",
      "2024-01-02 06:51:23.825134: Epoch time: 126.69 s\n",
      "2024-01-02 06:51:23.832132: Yayy! New best EMA pseudo Dice: 0.9331\n",
      "2024-01-02 06:51:25.387334: \n",
      "2024-01-02 06:51:25.393329: Epoch 55\n",
      "2024-01-02 06:51:25.399202: Current learning rate: 0.0095\n",
      "2024-01-02 06:53:32.072846: train_loss -0.8748\n",
      "2024-01-02 06:53:32.083999: val_loss -0.843\n",
      "2024-01-02 06:53:32.091998: Pseudo dice [0.9137, 0.9501, 0.9411]\n",
      "2024-01-02 06:53:32.103999: Epoch time: 126.69 s\n",
      "2024-01-02 06:53:32.112999: Yayy! New best EMA pseudo Dice: 0.9333\n",
      "2024-01-02 06:53:33.796150: \n",
      "2024-01-02 06:53:33.805151: Epoch 56\n",
      "2024-01-02 06:53:33.815151: Current learning rate: 0.00949\n",
      "2024-01-02 06:55:39.739222: train_loss -0.8778\n",
      "2024-01-02 06:55:39.748222: val_loss -0.838\n",
      "2024-01-02 06:55:39.756224: Pseudo dice [0.9078, 0.9479, 0.9415]\n",
      "2024-01-02 06:55:39.762227: Epoch time: 125.94 s\n",
      "2024-01-02 06:55:40.909929: \n",
      "2024-01-02 06:55:40.918201: Epoch 57\n",
      "2024-01-02 06:55:40.923282: Current learning rate: 0.00949\n",
      "2024-01-02 06:57:46.199522: train_loss -0.8769\n",
      "2024-01-02 06:57:46.209522: val_loss -0.8388\n",
      "2024-01-02 06:57:46.215474: Pseudo dice [0.911, 0.9488, 0.9411]\n",
      "2024-01-02 06:57:46.221474: Epoch time: 125.29 s\n",
      "2024-01-02 06:57:47.349278: \n",
      "2024-01-02 06:57:47.356285: Epoch 58\n",
      "2024-01-02 06:57:47.363322: Current learning rate: 0.00948\n",
      "2024-01-02 06:59:52.771077: train_loss -0.8788\n",
      "2024-01-02 06:59:52.780587: val_loss -0.8409\n",
      "2024-01-02 06:59:52.788102: Pseudo dice [0.9134, 0.9511, 0.9417]\n",
      "2024-01-02 06:59:52.795100: Epoch time: 125.42 s\n",
      "2024-01-02 06:59:52.802102: Yayy! New best EMA pseudo Dice: 0.9335\n",
      "2024-01-02 06:59:54.529948: \n",
      "2024-01-02 06:59:54.536013: Epoch 59\n",
      "2024-01-02 06:59:54.541012: Current learning rate: 0.00947\n",
      "2024-01-02 07:01:59.909042: train_loss -0.8796\n",
      "2024-01-02 07:01:59.921275: val_loss -0.8424\n",
      "2024-01-02 07:01:59.927277: Pseudo dice [0.9126, 0.9498, 0.9434]\n",
      "2024-01-02 07:01:59.934277: Epoch time: 125.38 s\n",
      "2024-01-02 07:01:59.941278: Yayy! New best EMA pseudo Dice: 0.9336\n",
      "2024-01-02 07:02:01.436935: \n",
      "2024-01-02 07:02:01.444876: Epoch 60\n",
      "2024-01-02 07:02:01.449876: Current learning rate: 0.00946\n",
      "2024-01-02 07:04:06.642830: train_loss -0.8787\n",
      "2024-01-02 07:04:06.651164: val_loss -0.8407\n",
      "2024-01-02 07:04:06.659164: Pseudo dice [0.9092, 0.9502, 0.9423]\n",
      "2024-01-02 07:04:06.668171: Epoch time: 125.21 s\n",
      "2024-01-02 07:04:06.677192: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2024-01-02 07:04:08.044075: \n",
      "2024-01-02 07:04:08.055226: Epoch 61\n",
      "2024-01-02 07:04:08.063290: Current learning rate: 0.00945\n",
      "2024-01-02 07:06:13.409424: train_loss -0.8827\n",
      "2024-01-02 07:06:13.421520: val_loss -0.838\n",
      "2024-01-02 07:06:13.430518: Pseudo dice [0.9107, 0.9497, 0.943]\n",
      "2024-01-02 07:06:13.436519: Epoch time: 125.37 s\n",
      "2024-01-02 07:06:13.445525: Yayy! New best EMA pseudo Dice: 0.9338\n",
      "2024-01-02 07:06:14.869702: \n",
      "2024-01-02 07:06:14.875264: Epoch 62\n",
      "2024-01-02 07:06:14.883298: Current learning rate: 0.00944\n",
      "2024-01-02 07:08:20.418865: train_loss -0.8813\n",
      "2024-01-02 07:08:20.427876: val_loss -0.8405\n",
      "2024-01-02 07:08:20.434873: Pseudo dice [0.9126, 0.95, 0.9402]\n",
      "2024-01-02 07:08:20.440865: Epoch time: 125.55 s\n",
      "2024-01-02 07:08:20.447877: Yayy! New best EMA pseudo Dice: 0.9338\n",
      "2024-01-02 07:08:21.821988: \n",
      "2024-01-02 07:08:21.827566: Epoch 63\n",
      "2024-01-02 07:08:21.831552: Current learning rate: 0.00943\n",
      "2024-01-02 07:10:26.942761: train_loss -0.8811\n",
      "2024-01-02 07:10:26.948762: val_loss -0.8428\n",
      "2024-01-02 07:10:26.954762: Pseudo dice [0.9114, 0.95, 0.9426]\n",
      "2024-01-02 07:10:26.960761: Epoch time: 125.12 s\n",
      "2024-01-02 07:10:26.965761: Yayy! New best EMA pseudo Dice: 0.9339\n",
      "2024-01-02 07:10:28.333151: \n",
      "2024-01-02 07:10:28.338081: Epoch 64\n",
      "2024-01-02 07:10:28.343385: Current learning rate: 0.00942\n",
      "2024-01-02 07:12:33.503214: train_loss -0.8829\n",
      "2024-01-02 07:12:33.512206: val_loss -0.8419\n",
      "2024-01-02 07:12:33.517210: Pseudo dice [0.9111, 0.9492, 0.9438]\n",
      "2024-01-02 07:12:33.522210: Epoch time: 125.17 s\n",
      "2024-01-02 07:12:33.527290: Yayy! New best EMA pseudo Dice: 0.934\n",
      "2024-01-02 07:12:34.900277: \n",
      "2024-01-02 07:12:34.906277: Epoch 65\n",
      "2024-01-02 07:12:34.911076: Current learning rate: 0.00941\n",
      "2024-01-02 07:14:40.678786: train_loss -0.8814\n",
      "2024-01-02 07:14:40.685800: val_loss -0.8441\n",
      "2024-01-02 07:14:40.691791: Pseudo dice [0.9115, 0.9511, 0.9448]\n",
      "2024-01-02 07:14:40.699787: Epoch time: 125.78 s\n",
      "2024-01-02 07:14:40.703787: Yayy! New best EMA pseudo Dice: 0.9342\n",
      "2024-01-02 07:14:42.125616: \n",
      "2024-01-02 07:14:42.131424: Epoch 66\n",
      "2024-01-02 07:14:42.136428: Current learning rate: 0.0094\n",
      "2024-01-02 07:16:47.774413: train_loss -0.8794\n",
      "2024-01-02 07:16:47.781417: val_loss -0.8471\n",
      "2024-01-02 07:16:47.788119: Pseudo dice [0.9138, 0.9517, 0.9428]\n",
      "2024-01-02 07:16:47.796118: Epoch time: 125.65 s\n",
      "2024-01-02 07:16:47.803117: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2024-01-02 07:16:49.496239: \n",
      "2024-01-02 07:16:49.504990: Epoch 67\n",
      "2024-01-02 07:16:49.510546: Current learning rate: 0.00939\n",
      "2024-01-02 07:18:54.873220: train_loss -0.8798\n",
      "2024-01-02 07:18:54.880209: val_loss -0.8405\n",
      "2024-01-02 07:18:54.885208: Pseudo dice [0.9116, 0.9499, 0.9429]\n",
      "2024-01-02 07:18:54.899253: Epoch time: 125.38 s\n",
      "2024-01-02 07:18:54.904255: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2024-01-02 07:18:56.371345: \n",
      "2024-01-02 07:18:56.377342: Epoch 68\n",
      "2024-01-02 07:18:56.382343: Current learning rate: 0.00939\n",
      "2024-01-02 07:21:02.090861: train_loss -0.8843\n",
      "2024-01-02 07:21:02.097849: val_loss -0.8454\n",
      "2024-01-02 07:21:02.103848: Pseudo dice [0.9121, 0.9516, 0.9414]\n",
      "2024-01-02 07:21:02.108358: Epoch time: 125.72 s\n",
      "2024-01-02 07:21:02.113871: Yayy! New best EMA pseudo Dice: 0.9345\n",
      "2024-01-02 07:21:03.733609: \n",
      "2024-01-02 07:21:03.741649: Epoch 69\n",
      "2024-01-02 07:21:03.745665: Current learning rate: 0.00938\n",
      "2024-01-02 07:23:08.867674: train_loss -0.8815\n",
      "2024-01-02 07:23:08.877262: val_loss -0.8442\n",
      "2024-01-02 07:23:08.884268: Pseudo dice [0.9144, 0.9516, 0.942]\n",
      "2024-01-02 07:23:08.889270: Epoch time: 125.13 s\n",
      "2024-01-02 07:23:08.894269: Yayy! New best EMA pseudo Dice: 0.9346\n",
      "2024-01-02 07:23:10.333071: \n",
      "2024-01-02 07:23:10.339174: Epoch 70\n",
      "2024-01-02 07:23:10.343149: Current learning rate: 0.00937\n",
      "2024-01-02 07:25:15.546700: train_loss -0.8826\n",
      "2024-01-02 07:25:15.552708: val_loss -0.8441\n",
      "2024-01-02 07:25:15.557705: Pseudo dice [0.9137, 0.9507, 0.9425]\n",
      "2024-01-02 07:25:15.561704: Epoch time: 125.21 s\n",
      "2024-01-02 07:25:15.566703: Yayy! New best EMA pseudo Dice: 0.9347\n",
      "2024-01-02 07:25:17.158712: \n",
      "2024-01-02 07:25:17.164514: Epoch 71\n",
      "2024-01-02 07:25:17.168514: Current learning rate: 0.00936\n",
      "2024-01-02 07:27:22.555875: train_loss -0.883\n",
      "2024-01-02 07:27:22.563877: val_loss -0.8486\n",
      "2024-01-02 07:27:22.570881: Pseudo dice [0.914, 0.9513, 0.9448]\n",
      "2024-01-02 07:27:22.576878: Epoch time: 125.4 s\n",
      "2024-01-02 07:27:22.581877: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2024-01-02 07:27:24.157997: \n",
      "2024-01-02 07:27:24.163049: Epoch 72\n",
      "2024-01-02 07:27:24.168035: Current learning rate: 0.00935\n",
      "2024-01-02 07:29:29.317323: train_loss -0.8863\n",
      "2024-01-02 07:29:29.323324: val_loss -0.8427\n",
      "2024-01-02 07:29:29.328845: Pseudo dice [0.9137, 0.9514, 0.9405]\n",
      "2024-01-02 07:29:29.333843: Epoch time: 125.16 s\n",
      "2024-01-02 07:29:29.338844: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2024-01-02 07:29:30.840200: \n",
      "2024-01-02 07:29:30.846291: Epoch 73\n",
      "2024-01-02 07:29:30.850347: Current learning rate: 0.00934\n",
      "2024-01-02 07:31:36.338560: train_loss -0.8837\n",
      "2024-01-02 07:31:36.345557: val_loss -0.8456\n",
      "2024-01-02 07:31:36.350220: Pseudo dice [0.9131, 0.9508, 0.9428]\n",
      "2024-01-02 07:31:36.355294: Epoch time: 125.5 s\n",
      "2024-01-02 07:31:36.359300: Yayy! New best EMA pseudo Dice: 0.935\n",
      "2024-01-02 07:31:38.011472: \n",
      "2024-01-02 07:31:38.022172: Epoch 74\n",
      "2024-01-02 07:31:38.028171: Current learning rate: 0.00933\n",
      "2024-01-02 07:33:43.267630: train_loss -0.8857\n",
      "2024-01-02 07:33:43.275631: val_loss -0.8404\n",
      "2024-01-02 07:33:43.280641: Pseudo dice [0.9123, 0.9507, 0.941]\n",
      "2024-01-02 07:33:43.284641: Epoch time: 125.26 s\n",
      "2024-01-02 07:33:44.505627: \n",
      "2024-01-02 07:33:44.510695: Epoch 75\n",
      "2024-01-02 07:33:44.514689: Current learning rate: 0.00932\n",
      "2024-01-02 07:35:49.569124: train_loss -0.8829\n",
      "2024-01-02 07:35:49.576132: val_loss -0.8463\n",
      "2024-01-02 07:35:49.581125: Pseudo dice [0.9126, 0.953, 0.9444]\n",
      "2024-01-02 07:35:49.590657: Epoch time: 125.06 s\n",
      "2024-01-02 07:35:49.594655: Yayy! New best EMA pseudo Dice: 0.9351\n",
      "2024-01-02 07:35:51.146566: \n",
      "2024-01-02 07:35:51.152575: Epoch 76\n",
      "2024-01-02 07:35:51.157516: Current learning rate: 0.00931\n",
      "2024-01-02 07:37:56.615350: train_loss -0.8823\n",
      "2024-01-02 07:37:56.622349: val_loss -0.8409\n",
      "2024-01-02 07:37:56.630194: Pseudo dice [0.9128, 0.949, 0.9419]\n",
      "2024-01-02 07:37:56.638185: Epoch time: 125.47 s\n",
      "2024-01-02 07:37:57.844975: \n",
      "2024-01-02 07:37:57.851978: Epoch 77\n",
      "2024-01-02 07:37:57.857040: Current learning rate: 0.0093\n",
      "2024-01-02 07:40:03.255592: train_loss -0.8837\n",
      "2024-01-02 07:40:03.261590: val_loss -0.843\n",
      "2024-01-02 07:40:03.266591: Pseudo dice [0.911, 0.951, 0.9443]\n",
      "2024-01-02 07:40:03.270588: Epoch time: 125.41 s\n",
      "2024-01-02 07:40:04.434110: \n",
      "2024-01-02 07:40:04.443107: Epoch 78\n",
      "2024-01-02 07:40:04.447119: Current learning rate: 0.0093\n",
      "2024-01-02 07:42:10.042698: train_loss -0.8843\n",
      "2024-01-02 07:42:10.050689: val_loss -0.8417\n",
      "2024-01-02 07:42:10.055689: Pseudo dice [0.9157, 0.951, 0.9421]\n",
      "2024-01-02 07:42:10.060690: Epoch time: 125.61 s\n",
      "2024-01-02 07:42:10.065691: Yayy! New best EMA pseudo Dice: 0.9352\n",
      "2024-01-02 07:42:11.443695: \n",
      "2024-01-02 07:42:11.448971: Epoch 79\n",
      "2024-01-02 07:42:11.453027: Current learning rate: 0.00929\n",
      "2024-01-02 07:44:17.176512: train_loss -0.883\n",
      "2024-01-02 07:44:17.184028: val_loss -0.8427\n",
      "2024-01-02 07:44:17.191043: Pseudo dice [0.9111, 0.9521, 0.9419]\n",
      "2024-01-02 07:44:17.197028: Epoch time: 125.73 s\n",
      "2024-01-02 07:44:18.480862: \n",
      "2024-01-02 07:44:18.486608: Epoch 80\n",
      "2024-01-02 07:44:18.490683: Current learning rate: 0.00928\n",
      "2024-01-02 07:46:23.715141: train_loss -0.8842\n",
      "2024-01-02 07:46:23.724142: val_loss -0.8427\n",
      "2024-01-02 07:46:23.729141: Pseudo dice [0.9104, 0.9504, 0.9406]\n",
      "2024-01-02 07:46:23.734149: Epoch time: 125.24 s\n",
      "2024-01-02 07:46:24.996462: \n",
      "2024-01-02 07:46:25.006398: Epoch 81\n",
      "2024-01-02 07:46:25.011472: Current learning rate: 0.00927\n",
      "2024-01-02 07:48:30.721446: train_loss -0.8843\n",
      "2024-01-02 07:48:30.728447: val_loss -0.843\n",
      "2024-01-02 07:48:30.733447: Pseudo dice [0.9148, 0.9506, 0.9422]\n",
      "2024-01-02 07:48:30.738456: Epoch time: 125.73 s\n",
      "2024-01-02 07:48:32.128244: \n",
      "2024-01-02 07:48:32.135238: Epoch 82\n",
      "2024-01-02 07:48:32.140179: Current learning rate: 0.00926\n",
      "2024-01-02 07:50:37.508512: train_loss -0.8839\n",
      "2024-01-02 07:50:37.516513: val_loss -0.8447\n",
      "2024-01-02 07:50:37.524026: Pseudo dice [0.9114, 0.9517, 0.9431]\n",
      "2024-01-02 07:50:37.529027: Epoch time: 125.38 s\n",
      "2024-01-02 07:50:38.653304: \n",
      "2024-01-02 07:50:38.661307: Epoch 83\n",
      "2024-01-02 07:50:38.666235: Current learning rate: 0.00925\n",
      "2024-01-02 07:52:44.275380: train_loss -0.8864\n",
      "2024-01-02 07:52:44.282384: val_loss -0.8467\n",
      "2024-01-02 07:52:44.287387: Pseudo dice [0.9134, 0.951, 0.943]\n",
      "2024-01-02 07:52:44.292382: Epoch time: 125.62 s\n",
      "2024-01-02 07:52:44.301382: Yayy! New best EMA pseudo Dice: 0.9352\n",
      "2024-01-02 07:52:45.930263: \n",
      "2024-01-02 07:52:45.935268: Epoch 84\n",
      "2024-01-02 07:52:45.940226: Current learning rate: 0.00924\n",
      "2024-01-02 07:54:51.334176: train_loss -0.8835\n",
      "2024-01-02 07:54:51.343169: val_loss -0.8477\n",
      "2024-01-02 07:54:51.352177: Pseudo dice [0.9145, 0.9519, 0.9463]\n",
      "2024-01-02 07:54:51.358175: Epoch time: 125.41 s\n",
      "2024-01-02 07:54:51.366179: Yayy! New best EMA pseudo Dice: 0.9355\n",
      "2024-01-02 07:54:52.716365: \n",
      "2024-01-02 07:54:52.722010: Epoch 85\n",
      "2024-01-02 07:54:52.726003: Current learning rate: 0.00923\n",
      "2024-01-02 07:56:58.090727: train_loss -0.8861\n",
      "2024-01-02 07:56:58.099727: val_loss -0.841\n",
      "2024-01-02 07:56:58.104727: Pseudo dice [0.9121, 0.9499, 0.9431]\n",
      "2024-01-02 07:56:58.109727: Epoch time: 125.38 s\n",
      "2024-01-02 07:56:59.224386: \n",
      "2024-01-02 07:56:59.233385: Epoch 86\n",
      "2024-01-02 07:56:59.237385: Current learning rate: 0.00922\n",
      "2024-01-02 07:59:04.668030: train_loss -0.8842\n",
      "2024-01-02 07:59:04.676109: val_loss -0.8393\n",
      "2024-01-02 07:59:04.683095: Pseudo dice [0.91, 0.9502, 0.9434]\n",
      "2024-01-02 07:59:04.691106: Epoch time: 125.44 s\n",
      "2024-01-02 07:59:05.821476: \n",
      "2024-01-02 07:59:05.827469: Epoch 87\n",
      "2024-01-02 07:59:05.835402: Current learning rate: 0.00921\n",
      "2024-01-02 08:01:11.318736: train_loss -0.8859\n",
      "2024-01-02 08:01:11.325744: val_loss -0.8428\n",
      "2024-01-02 08:01:11.332244: Pseudo dice [0.9116, 0.9513, 0.9414]\n",
      "2024-01-02 08:01:11.337246: Epoch time: 125.5 s\n",
      "2024-01-02 08:01:12.436309: \n",
      "2024-01-02 08:01:12.443929: Epoch 88\n",
      "2024-01-02 08:01:12.448005: Current learning rate: 0.0092\n",
      "2024-01-02 08:03:17.751369: train_loss -0.8865\n",
      "2024-01-02 08:03:17.761370: val_loss -0.8402\n",
      "2024-01-02 08:03:17.768369: Pseudo dice [0.9118, 0.9502, 0.9433]\n",
      "2024-01-02 08:03:17.774371: Epoch time: 125.32 s\n",
      "2024-01-02 08:03:18.972883: \n",
      "2024-01-02 08:03:18.983904: Epoch 89\n",
      "2024-01-02 08:03:18.995885: Current learning rate: 0.0092\n",
      "2024-01-02 08:05:24.419442: train_loss -0.8872\n",
      "2024-01-02 08:05:24.427440: val_loss -0.8438\n",
      "2024-01-02 08:05:24.432440: Pseudo dice [0.9125, 0.9518, 0.9438]\n",
      "2024-01-02 08:05:24.438518: Epoch time: 125.45 s\n",
      "2024-01-02 08:05:25.514939: \n",
      "2024-01-02 08:05:25.520595: Epoch 90\n",
      "2024-01-02 08:05:25.526896: Current learning rate: 0.00919\n",
      "2024-01-02 08:07:30.827785: train_loss -0.8871\n",
      "2024-01-02 08:07:30.833788: val_loss -0.844\n",
      "2024-01-02 08:07:30.838786: Pseudo dice [0.9108, 0.9522, 0.9444]\n",
      "2024-01-02 08:07:30.844784: Epoch time: 125.31 s\n",
      "2024-01-02 08:07:32.052216: \n",
      "2024-01-02 08:07:32.057289: Epoch 91\n",
      "2024-01-02 08:07:32.062277: Current learning rate: 0.00918\n",
      "2024-01-02 08:09:37.451204: train_loss -0.8863\n",
      "2024-01-02 08:09:37.458198: val_loss -0.835\n",
      "2024-01-02 08:09:37.463210: Pseudo dice [0.9121, 0.949, 0.9415]\n",
      "2024-01-02 08:09:37.468210: Epoch time: 125.4 s\n",
      "2024-01-02 08:09:38.533834: \n",
      "2024-01-02 08:09:38.543647: Epoch 92\n",
      "2024-01-02 08:09:38.550730: Current learning rate: 0.00917\n",
      "2024-01-02 08:11:43.984142: train_loss -0.8896\n",
      "2024-01-02 08:11:43.991141: val_loss -0.8447\n",
      "2024-01-02 08:11:43.996140: Pseudo dice [0.913, 0.9508, 0.9437]\n",
      "2024-01-02 08:11:44.000140: Epoch time: 125.45 s\n",
      "2024-01-02 08:11:45.151158: \n",
      "2024-01-02 08:11:45.156156: Epoch 93\n",
      "2024-01-02 08:11:45.160827: Current learning rate: 0.00916\n",
      "2024-01-02 08:13:50.947246: train_loss -0.889\n",
      "2024-01-02 08:13:50.955256: val_loss -0.8468\n",
      "2024-01-02 08:13:50.963246: Pseudo dice [0.9153, 0.9509, 0.9439]\n",
      "2024-01-02 08:13:50.968760: Epoch time: 125.8 s\n",
      "2024-01-02 08:13:52.258013: \n",
      "2024-01-02 08:13:52.265014: Epoch 94\n",
      "2024-01-02 08:13:52.274199: Current learning rate: 0.00915\n",
      "2024-01-02 08:15:57.757644: train_loss -0.8888\n",
      "2024-01-02 08:15:57.764643: val_loss -0.8405\n",
      "2024-01-02 08:15:57.770153: Pseudo dice [0.9107, 0.9493, 0.943]\n",
      "2024-01-02 08:15:57.775153: Epoch time: 125.5 s\n",
      "2024-01-02 08:15:59.031126: \n",
      "2024-01-02 08:15:59.036189: Epoch 95\n",
      "2024-01-02 08:15:59.041197: Current learning rate: 0.00914\n",
      "2024-01-02 08:18:04.241461: train_loss -0.8898\n",
      "2024-01-02 08:18:04.249463: val_loss -0.8389\n",
      "2024-01-02 08:18:04.255462: Pseudo dice [0.9126, 0.9493, 0.9432]\n",
      "2024-01-02 08:18:04.260465: Epoch time: 125.21 s\n",
      "2024-01-02 08:18:05.646015: \n",
      "2024-01-02 08:18:05.655954: Epoch 96\n",
      "2024-01-02 08:18:05.664953: Current learning rate: 0.00913\n",
      "2024-01-02 08:20:11.061003: train_loss -0.8885\n",
      "2024-01-02 08:20:11.070992: val_loss -0.8443\n",
      "2024-01-02 08:20:11.077510: Pseudo dice [0.9149, 0.9504, 0.9437]\n",
      "2024-01-02 08:20:11.082511: Epoch time: 125.42 s\n",
      "2024-01-02 08:20:12.185562: \n",
      "2024-01-02 08:20:12.192562: Epoch 97\n",
      "2024-01-02 08:20:12.196553: Current learning rate: 0.00912\n",
      "2024-01-02 08:22:17.514335: train_loss -0.8887\n",
      "2024-01-02 08:22:17.521335: val_loss -0.8429\n",
      "2024-01-02 08:22:17.527342: Pseudo dice [0.9146, 0.9503, 0.9424]\n",
      "2024-01-02 08:22:17.532884: Epoch time: 125.33 s\n",
      "2024-01-02 08:22:18.640948: \n",
      "2024-01-02 08:22:18.646472: Epoch 98\n",
      "2024-01-02 08:22:18.650539: Current learning rate: 0.00911\n",
      "2024-01-02 08:24:23.943664: train_loss -0.8891\n",
      "2024-01-02 08:24:23.950662: val_loss -0.843\n",
      "2024-01-02 08:24:23.957662: Pseudo dice [0.9132, 0.9505, 0.9442]\n",
      "2024-01-02 08:24:23.963677: Epoch time: 125.3 s\n",
      "2024-01-02 08:24:23.970676: Yayy! New best EMA pseudo Dice: 0.9355\n",
      "2024-01-02 08:24:25.518715: \n",
      "2024-01-02 08:24:25.527212: Epoch 99\n",
      "2024-01-02 08:24:25.531769: Current learning rate: 0.0091\n",
      "2024-01-02 08:26:31.006942: train_loss -0.8885\n",
      "2024-01-02 08:26:31.013941: val_loss -0.847\n",
      "2024-01-02 08:26:31.021940: Pseudo dice [0.9149, 0.9521, 0.9439]\n",
      "2024-01-02 08:26:31.028940: Epoch time: 125.49 s\n",
      "2024-01-02 08:26:31.405487: Yayy! New best EMA pseudo Dice: 0.9356\n",
      "2024-01-02 08:26:32.734274: \n",
      "2024-01-02 08:26:32.739263: Epoch 100\n",
      "2024-01-02 08:26:32.744258: Current learning rate: 0.0091\n",
      "2024-01-02 08:28:37.846082: train_loss -0.8901\n",
      "2024-01-02 08:28:37.853083: val_loss -0.8348\n",
      "2024-01-02 08:28:37.860080: Pseudo dice [0.9109, 0.9501, 0.9427]\n",
      "2024-01-02 08:28:37.867087: Epoch time: 125.11 s\n",
      "2024-01-02 08:28:38.955875: \n",
      "2024-01-02 08:28:38.965744: Epoch 101\n",
      "2024-01-02 08:28:38.975745: Current learning rate: 0.00909\n",
      "2024-01-02 08:30:44.386520: train_loss -0.887\n",
      "2024-01-02 08:30:44.393521: val_loss -0.8441\n",
      "2024-01-02 08:30:44.399511: Pseudo dice [0.9116, 0.9512, 0.9442]\n",
      "2024-01-02 08:30:44.405521: Epoch time: 125.43 s\n",
      "2024-01-02 08:30:45.647620: \n",
      "2024-01-02 08:30:45.652600: Epoch 102\n",
      "2024-01-02 08:30:45.657672: Current learning rate: 0.00908\n",
      "2024-01-02 08:32:51.149516: train_loss -0.8856\n",
      "2024-01-02 08:32:51.157519: val_loss -0.8433\n",
      "2024-01-02 08:32:51.167516: Pseudo dice [0.914, 0.9515, 0.9416]\n",
      "2024-01-02 08:32:51.172522: Epoch time: 125.5 s\n",
      "2024-01-02 08:32:52.361976: \n",
      "2024-01-02 08:32:52.367042: Epoch 103\n",
      "2024-01-02 08:32:52.371975: Current learning rate: 0.00907\n",
      "2024-01-02 08:34:57.272387: train_loss -0.8875\n",
      "2024-01-02 08:34:57.280904: val_loss -0.8422\n",
      "2024-01-02 08:34:57.287904: Pseudo dice [0.9115, 0.9515, 0.9435]\n",
      "2024-01-02 08:34:57.294895: Epoch time: 124.91 s\n",
      "2024-01-02 08:34:58.526129: \n",
      "2024-01-02 08:34:58.532091: Epoch 104\n",
      "2024-01-02 08:34:58.536635: Current learning rate: 0.00906\n",
      "2024-01-02 08:37:03.759748: train_loss -0.8906\n",
      "2024-01-02 08:37:03.766259: val_loss -0.841\n",
      "2024-01-02 08:37:03.773261: Pseudo dice [0.9144, 0.9504, 0.9446]\n",
      "2024-01-02 08:37:03.780335: Epoch time: 125.23 s\n",
      "2024-01-02 08:37:03.788263: Yayy! New best EMA pseudo Dice: 0.9357\n",
      "2024-01-02 08:37:05.312114: \n",
      "2024-01-02 08:37:05.317114: Epoch 105\n",
      "2024-01-02 08:37:05.322114: Current learning rate: 0.00905\n",
      "2024-01-02 08:39:10.796275: train_loss -0.8882\n",
      "2024-01-02 08:39:10.805276: val_loss -0.8429\n",
      "2024-01-02 08:39:10.812275: Pseudo dice [0.9116, 0.9519, 0.9446]\n",
      "2024-01-02 08:39:10.818278: Epoch time: 125.49 s\n",
      "2024-01-02 08:39:10.826275: Yayy! New best EMA pseudo Dice: 0.9357\n",
      "2024-01-02 08:39:12.357749: \n",
      "2024-01-02 08:39:12.366929: Epoch 106\n",
      "2024-01-02 08:39:12.371673: Current learning rate: 0.00904\n",
      "2024-01-02 08:41:17.545683: train_loss -0.8868\n",
      "2024-01-02 08:41:17.551679: val_loss -0.8433\n",
      "2024-01-02 08:41:17.557677: Pseudo dice [0.9137, 0.951, 0.9428]\n",
      "2024-01-02 08:41:17.563679: Epoch time: 125.19 s\n",
      "2024-01-02 08:41:17.568678: Yayy! New best EMA pseudo Dice: 0.9357\n",
      "2024-01-02 08:41:18.942718: \n",
      "2024-01-02 08:41:18.948719: Epoch 107\n",
      "2024-01-02 08:41:18.955793: Current learning rate: 0.00903\n",
      "2024-01-02 08:43:23.803463: train_loss -0.8911\n",
      "2024-01-02 08:43:23.811464: val_loss -0.8409\n",
      "2024-01-02 08:43:23.817469: Pseudo dice [0.9116, 0.9515, 0.943]\n",
      "2024-01-02 08:43:23.822468: Epoch time: 124.86 s\n",
      "2024-01-02 08:43:24.951611: \n",
      "2024-01-02 08:43:24.960277: Epoch 108\n",
      "2024-01-02 08:43:24.966265: Current learning rate: 0.00902\n",
      "2024-01-02 08:45:30.601103: train_loss -0.8896\n",
      "2024-01-02 08:45:30.611095: val_loss -0.8455\n",
      "2024-01-02 08:45:30.618606: Pseudo dice [0.9129, 0.9516, 0.9451]\n",
      "2024-01-02 08:45:30.624604: Epoch time: 125.65 s\n",
      "2024-01-02 08:45:30.629605: Yayy! New best EMA pseudo Dice: 0.9358\n",
      "2024-01-02 08:45:32.138453: \n",
      "2024-01-02 08:45:32.145478: Epoch 109\n",
      "2024-01-02 08:45:32.150455: Current learning rate: 0.00901\n",
      "2024-01-02 08:47:37.708132: train_loss -0.8872\n",
      "2024-01-02 08:47:37.717133: val_loss -0.8387\n",
      "2024-01-02 08:47:37.722133: Pseudo dice [0.9104, 0.9497, 0.9423]\n",
      "2024-01-02 08:47:37.729133: Epoch time: 125.57 s\n",
      "2024-01-02 08:47:38.936082: \n",
      "2024-01-02 08:47:38.944153: Epoch 110\n",
      "2024-01-02 08:47:38.948145: Current learning rate: 0.009\n",
      "2024-01-02 08:49:44.436097: train_loss -0.89\n",
      "2024-01-02 08:49:44.443611: val_loss -0.8427\n",
      "2024-01-02 08:49:44.449611: Pseudo dice [0.911, 0.9515, 0.9454]\n",
      "2024-01-02 08:49:44.457428: Epoch time: 125.5 s\n",
      "2024-01-02 08:49:45.588550: \n",
      "2024-01-02 08:49:45.594950: Epoch 111\n",
      "2024-01-02 08:49:45.599888: Current learning rate: 0.009\n",
      "2024-01-02 08:51:51.185653: train_loss -0.8896\n",
      "2024-01-02 08:51:51.196654: val_loss -0.8425\n",
      "2024-01-02 08:51:51.206653: Pseudo dice [0.9136, 0.9527, 0.9419]\n",
      "2024-01-02 08:51:51.216653: Epoch time: 125.6 s\n",
      "2024-01-02 08:51:52.655326: \n",
      "2024-01-02 08:51:52.662416: Epoch 112\n",
      "2024-01-02 08:51:52.667349: Current learning rate: 0.00899\n",
      "2024-01-02 08:53:57.973192: train_loss -0.8903\n",
      "2024-01-02 08:53:57.981191: val_loss -0.8417\n",
      "2024-01-02 08:53:57.985698: Pseudo dice [0.9137, 0.9501, 0.944]\n",
      "2024-01-02 08:53:57.993699: Epoch time: 125.32 s\n",
      "2024-01-02 08:53:59.080702: \n",
      "2024-01-02 08:53:59.089358: Epoch 113\n",
      "2024-01-02 08:53:59.094349: Current learning rate: 0.00898\n",
      "2024-01-02 08:56:04.379708: train_loss -0.8922\n",
      "2024-01-02 08:56:04.389708: val_loss -0.8401\n",
      "2024-01-02 08:56:04.396709: Pseudo dice [0.9104, 0.9498, 0.9436]\n",
      "2024-01-02 08:56:04.403710: Epoch time: 125.3 s\n",
      "2024-01-02 08:56:05.528093: \n",
      "2024-01-02 08:56:05.534455: Epoch 114\n",
      "2024-01-02 08:56:05.540494: Current learning rate: 0.00897\n",
      "2024-01-02 08:58:10.963607: train_loss -0.8927\n",
      "2024-01-02 08:58:10.971609: val_loss -0.8476\n",
      "2024-01-02 08:58:10.977609: Pseudo dice [0.9133, 0.9532, 0.9448]\n",
      "2024-01-02 08:58:10.982610: Epoch time: 125.44 s\n",
      "2024-01-02 08:58:12.271809: \n",
      "2024-01-02 08:58:12.280944: Epoch 115\n",
      "2024-01-02 08:58:12.291031: Current learning rate: 0.00896\n",
      "2024-01-02 09:00:17.667659: train_loss -0.8914\n",
      "2024-01-02 09:00:17.676657: val_loss -0.8405\n",
      "2024-01-02 09:00:17.681657: Pseudo dice [0.9138, 0.95, 0.943]\n",
      "2024-01-02 09:00:17.686657: Epoch time: 125.4 s\n",
      "2024-01-02 09:00:18.839620: \n",
      "2024-01-02 09:00:18.845377: Epoch 116\n",
      "2024-01-02 09:00:18.849377: Current learning rate: 0.00895\n",
      "2024-01-02 09:02:24.199734: train_loss -0.8917\n",
      "2024-01-02 09:02:24.206732: val_loss -0.8448\n",
      "2024-01-02 09:02:24.211731: Pseudo dice [0.9167, 0.9518, 0.9437]\n",
      "2024-01-02 09:02:24.217734: Epoch time: 125.36 s\n",
      "2024-01-02 09:02:24.222739: Yayy! New best EMA pseudo Dice: 0.9359\n",
      "2024-01-02 09:02:25.670336: \n",
      "2024-01-02 09:02:25.676192: Epoch 117\n",
      "2024-01-02 09:02:25.680267: Current learning rate: 0.00894\n",
      "2024-01-02 09:04:31.220934: train_loss -0.8909\n",
      "2024-01-02 09:04:31.227934: val_loss -0.8443\n",
      "2024-01-02 09:04:31.232934: Pseudo dice [0.9128, 0.9515, 0.9441]\n",
      "2024-01-02 09:04:31.236945: Epoch time: 125.55 s\n",
      "2024-01-02 09:04:31.242934: Yayy! New best EMA pseudo Dice: 0.9359\n",
      "2024-01-02 09:04:32.619543: \n",
      "2024-01-02 09:04:32.625532: Epoch 118\n",
      "2024-01-02 09:04:32.629539: Current learning rate: 0.00893\n",
      "2024-01-02 09:06:38.031263: train_loss -0.8935\n",
      "2024-01-02 09:06:38.042264: val_loss -0.8407\n",
      "2024-01-02 09:06:38.047263: Pseudo dice [0.9127, 0.9519, 0.9445]\n",
      "2024-01-02 09:06:38.053262: Epoch time: 125.41 s\n",
      "2024-01-02 09:06:38.058263: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2024-01-02 09:06:39.626372: \n",
      "2024-01-02 09:06:39.633466: Epoch 119\n",
      "2024-01-02 09:06:39.639468: Current learning rate: 0.00892\n",
      "2024-01-02 09:08:44.945846: train_loss -0.8908\n",
      "2024-01-02 09:08:44.953371: val_loss -0.8456\n",
      "2024-01-02 09:08:44.960371: Pseudo dice [0.9124, 0.9525, 0.9453]\n",
      "2024-01-02 09:08:44.966359: Epoch time: 125.32 s\n",
      "2024-01-02 09:08:44.972373: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2024-01-02 09:08:46.602053: \n",
      "2024-01-02 09:08:46.607138: Epoch 120\n",
      "2024-01-02 09:08:46.611140: Current learning rate: 0.00891\n",
      "2024-01-02 09:10:51.855346: train_loss -0.8934\n",
      "2024-01-02 09:10:51.862346: val_loss -0.8423\n",
      "2024-01-02 09:10:51.869349: Pseudo dice [0.9139, 0.9507, 0.9446]\n",
      "2024-01-02 09:10:51.877347: Epoch time: 125.26 s\n",
      "2024-01-02 09:10:51.886347: Yayy! New best EMA pseudo Dice: 0.9361\n",
      "2024-01-02 09:10:53.338904: \n",
      "2024-01-02 09:10:53.345273: Epoch 121\n",
      "2024-01-02 09:10:53.351943: Current learning rate: 0.0089\n",
      "2024-01-02 09:12:58.852548: train_loss -0.8924\n",
      "2024-01-02 09:12:58.863549: val_loss -0.8417\n",
      "2024-01-02 09:12:58.872024: Pseudo dice [0.9132, 0.951, 0.9432]\n",
      "2024-01-02 09:12:58.879022: Epoch time: 125.51 s\n",
      "2024-01-02 09:13:00.035858: \n",
      "2024-01-02 09:13:00.041857: Epoch 122\n",
      "2024-01-02 09:13:00.047861: Current learning rate: 0.00889\n",
      "2024-01-02 09:15:05.431655: train_loss -0.8922\n",
      "2024-01-02 09:15:05.437655: val_loss -0.8354\n",
      "2024-01-02 09:15:05.443659: Pseudo dice [0.9091, 0.9488, 0.9436]\n",
      "2024-01-02 09:15:05.449659: Epoch time: 125.4 s\n",
      "2024-01-02 09:15:06.602348: \n",
      "2024-01-02 09:15:06.608353: Epoch 123\n",
      "2024-01-02 09:15:06.612424: Current learning rate: 0.00889\n",
      "2024-01-02 09:17:11.927734: train_loss -0.8922\n",
      "2024-01-02 09:17:11.933735: val_loss -0.8433\n",
      "2024-01-02 09:17:11.939736: Pseudo dice [0.9149, 0.9526, 0.9435]\n",
      "2024-01-02 09:17:11.944742: Epoch time: 125.33 s\n",
      "2024-01-02 09:17:13.048900: \n",
      "2024-01-02 09:17:13.054384: Epoch 124\n",
      "2024-01-02 09:17:13.060311: Current learning rate: 0.00888\n",
      "2024-01-02 09:19:18.597664: train_loss -0.8947\n",
      "2024-01-02 09:19:18.604664: val_loss -0.8448\n",
      "2024-01-02 09:19:18.610663: Pseudo dice [0.9141, 0.9531, 0.9445]\n",
      "2024-01-02 09:19:18.615663: Epoch time: 125.55 s\n",
      "2024-01-02 09:19:19.769153: \n",
      "2024-01-02 09:19:19.775151: Epoch 125\n",
      "2024-01-02 09:19:19.779152: Current learning rate: 0.00887\n",
      "2024-01-02 09:21:25.119265: train_loss -0.8932\n",
      "2024-01-02 09:21:25.125270: val_loss -0.8481\n",
      "2024-01-02 09:21:25.130271: Pseudo dice [0.9153, 0.9539, 0.9448]\n",
      "2024-01-02 09:21:25.135782: Epoch time: 125.35 s\n",
      "2024-01-02 09:21:25.140795: Yayy! New best EMA pseudo Dice: 0.9363\n",
      "2024-01-02 09:21:26.459765: \n",
      "2024-01-02 09:21:26.465951: Epoch 126\n",
      "2024-01-02 09:21:26.470026: Current learning rate: 0.00886\n",
      "2024-01-02 09:23:31.743550: train_loss -0.8954\n",
      "2024-01-02 09:23:31.752550: val_loss -0.8442\n",
      "2024-01-02 09:23:31.759551: Pseudo dice [0.9169, 0.9533, 0.9434]\n",
      "2024-01-02 09:23:31.766549: Epoch time: 125.28 s\n",
      "2024-01-02 09:23:31.771549: Yayy! New best EMA pseudo Dice: 0.9364\n",
      "2024-01-02 09:23:33.400492: \n",
      "2024-01-02 09:23:33.410313: Epoch 127\n",
      "2024-01-02 09:23:33.416322: Current learning rate: 0.00885\n",
      "2024-01-02 09:25:38.767218: train_loss -0.8926\n",
      "2024-01-02 09:25:38.774225: val_loss -0.8368\n",
      "2024-01-02 09:25:38.780225: Pseudo dice [0.9125, 0.9485, 0.9434]\n",
      "2024-01-02 09:25:38.784224: Epoch time: 125.37 s\n",
      "2024-01-02 09:25:39.950304: \n",
      "2024-01-02 09:25:39.956390: Epoch 128\n",
      "2024-01-02 09:25:39.964396: Current learning rate: 0.00884\n",
      "2024-01-02 09:27:45.620455: train_loss -0.8903\n",
      "2024-01-02 09:27:45.627456: val_loss -0.8455\n",
      "2024-01-02 09:27:45.637453: Pseudo dice [0.9151, 0.9534, 0.9446]\n",
      "2024-01-02 09:27:45.643459: Epoch time: 125.67 s\n",
      "2024-01-02 09:27:46.932145: \n",
      "2024-01-02 09:27:46.939148: Epoch 129\n",
      "2024-01-02 09:27:46.944146: Current learning rate: 0.00883\n",
      "2024-01-02 09:29:52.421996: train_loss -0.8927\n",
      "2024-01-02 09:29:52.428998: val_loss -0.8433\n",
      "2024-01-02 09:29:52.436996: Pseudo dice [0.914, 0.9522, 0.9439]\n",
      "2024-01-02 09:29:52.443999: Epoch time: 125.49 s\n",
      "2024-01-02 09:29:52.449996: Yayy! New best EMA pseudo Dice: 0.9364\n",
      "2024-01-02 09:29:54.114629: \n",
      "2024-01-02 09:29:54.121629: Epoch 130\n",
      "2024-01-02 09:29:54.127623: Current learning rate: 0.00882\n",
      "2024-01-02 09:31:59.351233: train_loss -0.8943\n",
      "2024-01-02 09:31:59.358232: val_loss -0.8384\n",
      "2024-01-02 09:31:59.363230: Pseudo dice [0.9132, 0.9511, 0.9445]\n",
      "2024-01-02 09:31:59.368230: Epoch time: 125.24 s\n",
      "2024-01-02 09:32:00.572523: \n",
      "2024-01-02 09:32:00.582589: Epoch 131\n",
      "2024-01-02 09:32:00.586591: Current learning rate: 0.00881\n",
      "2024-01-02 09:34:06.300572: train_loss -0.8938\n",
      "2024-01-02 09:34:06.306575: val_loss -0.8397\n",
      "2024-01-02 09:34:06.312573: Pseudo dice [0.9117, 0.9518, 0.9428]\n",
      "2024-01-02 09:34:06.318573: Epoch time: 125.73 s\n",
      "2024-01-02 09:34:07.682549: \n",
      "2024-01-02 09:34:07.687735: Epoch 132\n",
      "2024-01-02 09:34:07.692735: Current learning rate: 0.0088\n",
      "2024-01-02 09:36:13.120384: train_loss -0.8936\n",
      "2024-01-02 09:36:13.129383: val_loss -0.837\n",
      "2024-01-02 09:36:13.135387: Pseudo dice [0.914, 0.9492, 0.9429]\n",
      "2024-01-02 09:36:13.141383: Epoch time: 125.44 s\n",
      "2024-01-02 09:36:14.297152: \n",
      "2024-01-02 09:36:14.302739: Epoch 133\n",
      "2024-01-02 09:36:14.307739: Current learning rate: 0.00879\n",
      "2024-01-02 09:38:19.736369: train_loss -0.8888\n",
      "2024-01-02 09:38:19.744371: val_loss -0.8405\n",
      "2024-01-02 09:38:19.752376: Pseudo dice [0.9131, 0.9513, 0.9414]\n",
      "2024-01-02 09:38:19.759391: Epoch time: 125.44 s\n",
      "2024-01-02 09:38:21.010578: \n",
      "2024-01-02 09:38:21.018588: Epoch 134\n",
      "2024-01-02 09:38:21.023526: Current learning rate: 0.00879\n",
      "2024-01-02 09:40:26.259026: train_loss -0.8932\n",
      "2024-01-02 09:40:26.266026: val_loss -0.8412\n",
      "2024-01-02 09:40:26.272026: Pseudo dice [0.9122, 0.951, 0.9443]\n",
      "2024-01-02 09:40:26.277027: Epoch time: 125.25 s\n",
      "2024-01-02 09:40:27.622170: \n",
      "2024-01-02 09:40:27.630016: Epoch 135\n",
      "2024-01-02 09:40:27.640030: Current learning rate: 0.00878\n",
      "2024-01-02 09:42:32.628177: train_loss -0.8986\n",
      "2024-01-02 09:42:32.635170: val_loss -0.8428\n",
      "2024-01-02 09:42:32.639169: Pseudo dice [0.9145, 0.9516, 0.9429]\n",
      "2024-01-02 09:42:32.646164: Epoch time: 125.01 s\n",
      "2024-01-02 09:42:33.750468: \n",
      "2024-01-02 09:42:33.756484: Epoch 136\n",
      "2024-01-02 09:42:33.761504: Current learning rate: 0.00877\n",
      "2024-01-02 09:44:38.924101: train_loss -0.8964\n",
      "2024-01-02 09:44:38.931101: val_loss -0.8408\n",
      "2024-01-02 09:44:38.937122: Pseudo dice [0.9116, 0.9514, 0.9448]\n",
      "2024-01-02 09:44:38.945112: Epoch time: 125.17 s\n",
      "2024-01-02 09:44:40.114595: \n",
      "2024-01-02 09:44:40.124935: Epoch 137\n",
      "2024-01-02 09:44:40.129924: Current learning rate: 0.00876\n",
      "2024-01-02 09:46:45.597360: train_loss -0.894\n",
      "2024-01-02 09:46:45.603359: val_loss -0.8408\n",
      "2024-01-02 09:46:45.608363: Pseudo dice [0.9128, 0.9505, 0.9426]\n",
      "2024-01-02 09:46:45.613368: Epoch time: 125.48 s\n",
      "2024-01-02 09:46:46.818551: \n",
      "2024-01-02 09:46:46.824288: Epoch 138\n",
      "2024-01-02 09:46:46.840679: Current learning rate: 0.00875\n",
      "2024-01-02 09:48:52.030347: train_loss -0.8978\n",
      "2024-01-02 09:48:52.036348: val_loss -0.8336\n",
      "2024-01-02 09:48:52.043348: Pseudo dice [0.9108, 0.9496, 0.9434]\n",
      "2024-01-02 09:48:52.048348: Epoch time: 125.21 s\n",
      "2024-01-02 09:48:53.308985: \n",
      "2024-01-02 09:48:53.317994: Epoch 139\n",
      "2024-01-02 09:48:53.322706: Current learning rate: 0.00874\n",
      "2024-01-02 09:50:58.752213: train_loss -0.8973\n",
      "2024-01-02 09:50:58.760714: val_loss -0.8343\n",
      "2024-01-02 09:50:58.765732: Pseudo dice [0.9134, 0.9499, 0.9439]\n",
      "2024-01-02 09:50:58.771714: Epoch time: 125.44 s\n",
      "2024-01-02 09:50:59.928361: \n",
      "2024-01-02 09:50:59.933982: Epoch 140\n",
      "2024-01-02 09:50:59.942051: Current learning rate: 0.00873\n",
      "2024-01-02 09:53:05.246837: train_loss -0.8967\n",
      "2024-01-02 09:53:05.254845: val_loss -0.8408\n",
      "2024-01-02 09:53:05.261840: Pseudo dice [0.9123, 0.9506, 0.945]\n",
      "2024-01-02 09:53:05.266919: Epoch time: 125.32 s\n",
      "2024-01-02 09:53:06.650610: \n",
      "2024-01-02 09:53:06.656558: Epoch 141\n",
      "2024-01-02 09:53:06.660556: Current learning rate: 0.00872\n",
      "2024-01-02 09:55:12.089178: train_loss -0.8958\n",
      "2024-01-02 09:55:12.098195: val_loss -0.8364\n",
      "2024-01-02 09:55:12.105192: Pseudo dice [0.9132, 0.9496, 0.9429]\n",
      "2024-01-02 09:55:12.111708: Epoch time: 125.44 s\n",
      "2024-01-02 09:55:13.631187: \n",
      "2024-01-02 09:55:13.639534: Epoch 142\n",
      "2024-01-02 09:55:13.646540: Current learning rate: 0.00871\n",
      "2024-01-02 09:57:18.679920: train_loss -0.8986\n",
      "2024-01-02 09:57:18.690921: val_loss -0.8395\n",
      "2024-01-02 09:57:18.698460: Pseudo dice [0.9124, 0.951, 0.9441]\n",
      "2024-01-02 09:57:18.706470: Epoch time: 125.05 s\n",
      "2024-01-02 09:57:19.866080: \n",
      "2024-01-02 09:57:19.874247: Epoch 143\n",
      "2024-01-02 09:57:19.879248: Current learning rate: 0.0087\n",
      "2024-01-02 09:59:25.259981: train_loss -0.9005\n",
      "2024-01-02 09:59:25.267973: val_loss -0.8386\n",
      "2024-01-02 09:59:25.272977: Pseudo dice [0.9102, 0.9517, 0.9444]\n",
      "2024-01-02 09:59:25.279978: Epoch time: 125.39 s\n",
      "2024-01-02 09:59:26.345217: \n",
      "2024-01-02 09:59:26.351056: Epoch 144\n",
      "2024-01-02 09:59:26.355131: Current learning rate: 0.00869\n",
      "2024-01-02 10:01:32.097050: train_loss -0.8949\n",
      "2024-01-02 10:01:32.104564: val_loss -0.8397\n",
      "2024-01-02 10:01:32.109573: Pseudo dice [0.9121, 0.9518, 0.9416]\n",
      "2024-01-02 10:01:32.115565: Epoch time: 125.75 s\n",
      "2024-01-02 10:01:33.341191: \n",
      "2024-01-02 10:01:33.346634: Epoch 145\n",
      "2024-01-02 10:01:33.351693: Current learning rate: 0.00868\n",
      "2024-01-02 10:03:38.875285: train_loss -0.8936\n",
      "2024-01-02 10:03:38.881793: val_loss -0.8446\n",
      "2024-01-02 10:03:38.888792: Pseudo dice [0.9155, 0.9522, 0.9447]\n",
      "2024-01-02 10:03:38.894792: Epoch time: 125.54 s\n",
      "2024-01-02 10:03:40.124163: \n",
      "2024-01-02 10:03:40.132161: Epoch 146\n",
      "2024-01-02 10:03:40.136172: Current learning rate: 0.00868\n",
      "2024-01-02 10:05:45.918124: train_loss -0.8935\n",
      "2024-01-02 10:05:45.925127: val_loss -0.8432\n",
      "2024-01-02 10:05:45.933202: Pseudo dice [0.9145, 0.9528, 0.9422]\n",
      "2024-01-02 10:05:45.940290: Epoch time: 125.79 s\n",
      "2024-01-02 10:05:47.156591: \n",
      "2024-01-02 10:05:47.162641: Epoch 147\n",
      "2024-01-02 10:05:47.169640: Current learning rate: 0.00867\n",
      "2024-01-02 10:07:52.199939: train_loss -0.8945\n",
      "2024-01-02 10:07:52.206940: val_loss -0.8424\n",
      "2024-01-02 10:07:52.215962: Pseudo dice [0.9115, 0.9515, 0.9435]\n",
      "2024-01-02 10:07:52.222975: Epoch time: 125.04 s\n",
      "2024-01-02 10:07:53.422588: \n",
      "2024-01-02 10:07:53.429797: Epoch 148\n",
      "2024-01-02 10:07:53.437504: Current learning rate: 0.00866\n",
      "2024-01-02 10:09:59.320774: train_loss -0.8949\n",
      "2024-01-02 10:09:59.328775: val_loss -0.8345\n",
      "2024-01-02 10:09:59.333775: Pseudo dice [0.9092, 0.9495, 0.9448]\n",
      "2024-01-02 10:09:59.338776: Epoch time: 125.9 s\n",
      "2024-01-02 10:10:00.690937: \n",
      "2024-01-02 10:10:00.700312: Epoch 149\n",
      "2024-01-02 10:10:00.705320: Current learning rate: 0.00865\n",
      "2024-01-02 10:12:06.318145: train_loss -0.8916\n",
      "2024-01-02 10:12:06.324329: val_loss -0.8373\n",
      "2024-01-02 10:12:06.329330: Pseudo dice [0.9144, 0.9508, 0.9434]\n",
      "2024-01-02 10:12:06.335840: Epoch time: 125.63 s\n",
      "2024-01-02 10:12:07.847763: \n",
      "2024-01-02 10:12:07.854763: Epoch 150\n",
      "2024-01-02 10:12:07.858825: Current learning rate: 0.00864\n",
      "2024-01-02 10:14:13.488252: train_loss -0.8961\n",
      "2024-01-02 10:14:13.494252: val_loss -0.8402\n",
      "2024-01-02 10:14:13.499253: Pseudo dice [0.9124, 0.9515, 0.9442]\n",
      "2024-01-02 10:14:13.505265: Epoch time: 125.64 s\n",
      "2024-01-02 10:14:14.695440: \n",
      "2024-01-02 10:14:14.700477: Epoch 151\n",
      "2024-01-02 10:14:14.705405: Current learning rate: 0.00863\n",
      "2024-01-02 10:16:19.884434: train_loss -0.8976\n",
      "2024-01-02 10:16:19.893434: val_loss -0.835\n",
      "2024-01-02 10:16:19.900435: Pseudo dice [0.9123, 0.9489, 0.9431]\n",
      "2024-01-02 10:16:19.908839: Epoch time: 125.19 s\n",
      "2024-01-02 10:16:21.124875: \n",
      "2024-01-02 10:16:21.129875: Epoch 152\n",
      "2024-01-02 10:16:21.134919: Current learning rate: 0.00862\n",
      "2024-01-02 10:18:26.420591: train_loss -0.8953\n",
      "2024-01-02 10:18:26.427605: val_loss -0.8392\n",
      "2024-01-02 10:18:26.434606: Pseudo dice [0.912, 0.9494, 0.9448]\n",
      "2024-01-02 10:18:26.440605: Epoch time: 125.3 s\n",
      "2024-01-02 10:18:27.629391: \n",
      "2024-01-02 10:18:27.635545: Epoch 153\n",
      "2024-01-02 10:18:27.643545: Current learning rate: 0.00861\n",
      "2024-01-02 10:20:33.126713: train_loss -0.8927\n",
      "2024-01-02 10:20:33.132716: val_loss -0.8438\n",
      "2024-01-02 10:20:33.139714: Pseudo dice [0.9147, 0.9528, 0.943]\n",
      "2024-01-02 10:20:33.143722: Epoch time: 125.5 s\n",
      "2024-01-02 10:20:34.336783: \n",
      "2024-01-02 10:20:34.347777: Epoch 154\n",
      "2024-01-02 10:20:34.355728: Current learning rate: 0.0086\n",
      "2024-01-02 10:22:39.549335: train_loss -0.8961\n",
      "2024-01-02 10:22:39.559333: val_loss -0.8377\n",
      "2024-01-02 10:22:39.565341: Pseudo dice [0.9109, 0.951, 0.9427]\n",
      "2024-01-02 10:22:39.571341: Epoch time: 125.21 s\n",
      "2024-01-02 10:22:40.803853: \n",
      "2024-01-02 10:22:40.809911: Epoch 155\n",
      "2024-01-02 10:22:40.817897: Current learning rate: 0.00859\n",
      "2024-01-02 10:24:46.238873: train_loss -0.8958\n",
      "2024-01-02 10:24:46.247399: val_loss -0.8416\n",
      "2024-01-02 10:24:46.252408: Pseudo dice [0.9144, 0.9513, 0.9445]\n",
      "2024-01-02 10:24:46.259931: Epoch time: 125.44 s\n",
      "2024-01-02 10:24:47.435216: \n",
      "2024-01-02 10:24:47.439294: Epoch 156\n",
      "2024-01-02 10:24:47.443282: Current learning rate: 0.00858\n",
      "2024-01-02 10:26:53.392125: train_loss -0.8955\n",
      "2024-01-02 10:26:53.401125: val_loss -0.8311\n",
      "2024-01-02 10:26:53.407656: Pseudo dice [0.9123, 0.9483, 0.9437]\n",
      "2024-01-02 10:26:53.413643: Epoch time: 125.96 s\n",
      "2024-01-02 10:26:54.888119: \n",
      "2024-01-02 10:26:54.893328: Epoch 157\n",
      "2024-01-02 10:26:54.898342: Current learning rate: 0.00858\n",
      "2024-01-02 10:29:00.230212: train_loss -0.8975\n",
      "2024-01-02 10:29:00.237212: val_loss -0.8402\n",
      "2024-01-02 10:29:00.253539: Pseudo dice [0.9138, 0.9513, 0.9458]\n",
      "2024-01-02 10:29:00.259531: Epoch time: 125.34 s\n",
      "2024-01-02 10:29:01.624708: \n",
      "2024-01-02 10:29:01.629789: Epoch 158\n",
      "2024-01-02 10:29:01.634787: Current learning rate: 0.00857\n",
      "2024-01-02 10:31:07.412023: train_loss -0.8939\n",
      "2024-01-02 10:31:07.418024: val_loss -0.8375\n",
      "2024-01-02 10:31:07.426025: Pseudo dice [0.9152, 0.9517, 0.9448]\n",
      "2024-01-02 10:31:07.432024: Epoch time: 125.79 s\n",
      "2024-01-02 10:31:08.548892: \n",
      "2024-01-02 10:31:08.554528: Epoch 159\n",
      "2024-01-02 10:31:08.559461: Current learning rate: 0.00856\n",
      "2024-01-02 10:33:14.240024: train_loss -0.8944\n",
      "2024-01-02 10:33:14.249025: val_loss -0.8395\n",
      "2024-01-02 10:33:14.254030: Pseudo dice [0.9118, 0.9508, 0.9436]\n",
      "2024-01-02 10:33:14.260024: Epoch time: 125.69 s\n",
      "2024-01-02 10:33:15.548074: \n",
      "2024-01-02 10:33:15.554521: Epoch 160\n",
      "2024-01-02 10:33:15.560568: Current learning rate: 0.00855\n",
      "2024-01-02 10:35:21.193697: train_loss -0.8962\n",
      "2024-01-02 10:35:21.202703: val_loss -0.8409\n",
      "2024-01-02 10:35:21.208701: Pseudo dice [0.9143, 0.951, 0.9441]\n",
      "2024-01-02 10:35:21.213213: Epoch time: 125.65 s\n",
      "2024-01-02 10:35:22.512433: \n",
      "2024-01-02 10:35:22.517424: Epoch 161\n",
      "2024-01-02 10:35:22.524491: Current learning rate: 0.00854\n",
      "2024-01-02 10:37:27.993115: train_loss -0.8999\n",
      "2024-01-02 10:37:28.000115: val_loss -0.8403\n",
      "2024-01-02 10:37:28.005115: Pseudo dice [0.9142, 0.9525, 0.9447]\n",
      "2024-01-02 10:37:28.010118: Epoch time: 125.48 s\n",
      "2024-01-02 10:37:29.245984: \n",
      "2024-01-02 10:37:29.252911: Epoch 162\n",
      "2024-01-02 10:37:29.256918: Current learning rate: 0.00853\n",
      "2024-01-02 10:39:34.769928: train_loss -0.8972\n",
      "2024-01-02 10:39:34.780437: val_loss -0.8406\n",
      "2024-01-02 10:39:34.788442: Pseudo dice [0.9118, 0.9523, 0.9437]\n",
      "2024-01-02 10:39:34.794443: Epoch time: 125.52 s\n",
      "2024-01-02 10:39:35.965041: \n",
      "2024-01-02 10:39:35.970145: Epoch 163\n",
      "2024-01-02 10:39:35.978177: Current learning rate: 0.00852\n",
      "2024-01-02 10:41:41.697484: train_loss -0.8962\n",
      "2024-01-02 10:41:41.703483: val_loss -0.8348\n",
      "2024-01-02 10:41:41.709560: Pseudo dice [0.9108, 0.9494, 0.9455]\n",
      "2024-01-02 10:41:41.713569: Epoch time: 125.73 s\n",
      "2024-01-02 10:41:43.054131: \n",
      "2024-01-02 10:41:43.061138: Epoch 164\n",
      "2024-01-02 10:41:43.068198: Current learning rate: 0.00851\n",
      "2024-01-02 10:43:48.505436: train_loss -0.8992\n",
      "2024-01-02 10:43:48.514442: val_loss -0.8361\n",
      "2024-01-02 10:43:48.523444: Pseudo dice [0.9123, 0.9501, 0.9455]\n",
      "2024-01-02 10:43:48.530961: Epoch time: 125.45 s\n",
      "2024-01-02 10:43:49.636845: \n",
      "2024-01-02 10:43:49.646774: Epoch 165\n",
      "2024-01-02 10:43:49.659838: Current learning rate: 0.0085\n",
      "2024-01-02 10:45:55.173966: train_loss -0.9\n",
      "2024-01-02 10:45:55.183966: val_loss -0.8364\n",
      "2024-01-02 10:45:55.191965: Pseudo dice [0.9109, 0.9496, 0.9443]\n",
      "2024-01-02 10:45:55.196967: Epoch time: 125.54 s\n",
      "2024-01-02 10:45:56.377864: \n",
      "2024-01-02 10:45:56.384472: Epoch 166\n",
      "2024-01-02 10:45:56.398866: Current learning rate: 0.00849\n",
      "2024-01-02 10:48:01.938558: train_loss -0.8989\n",
      "2024-01-02 10:48:01.945639: val_loss -0.8428\n",
      "2024-01-02 10:48:01.952559: Pseudo dice [0.9143, 0.9523, 0.9444]\n",
      "2024-01-02 10:48:01.957561: Epoch time: 125.56 s\n",
      "2024-01-02 10:48:03.065640: \n",
      "2024-01-02 10:48:03.071708: Epoch 167\n",
      "2024-01-02 10:48:03.079710: Current learning rate: 0.00848\n",
      "2024-01-02 10:50:08.861583: train_loss -0.8954\n",
      "2024-01-02 10:50:08.870087: val_loss -0.8396\n",
      "2024-01-02 10:50:08.877089: Pseudo dice [0.9128, 0.9513, 0.9441]\n",
      "2024-01-02 10:50:08.884089: Epoch time: 125.8 s\n",
      "2024-01-02 10:50:10.053066: \n",
      "2024-01-02 10:50:10.058978: Epoch 168\n",
      "2024-01-02 10:50:10.062967: Current learning rate: 0.00847\n",
      "2024-01-02 10:52:15.926000: train_loss -0.8969\n",
      "2024-01-02 10:52:15.931993: val_loss -0.8369\n",
      "2024-01-02 10:52:15.937992: Pseudo dice [0.9129, 0.9503, 0.9443]\n",
      "2024-01-02 10:52:15.942992: Epoch time: 125.87 s\n",
      "2024-01-02 10:52:17.188141: \n",
      "2024-01-02 10:52:17.195140: Epoch 169\n",
      "2024-01-02 10:52:17.203229: Current learning rate: 0.00847\n",
      "2024-01-02 10:54:22.865041: train_loss -0.8987\n",
      "2024-01-02 10:54:22.872042: val_loss -0.8371\n",
      "2024-01-02 10:54:22.879043: Pseudo dice [0.9132, 0.9512, 0.9442]\n",
      "2024-01-02 10:54:22.884042: Epoch time: 125.68 s\n",
      "2024-01-02 10:54:24.102394: \n",
      "2024-01-02 10:54:24.109394: Epoch 170\n",
      "2024-01-02 10:54:24.115472: Current learning rate: 0.00846\n",
      "2024-01-02 10:56:29.623054: train_loss -0.9003\n",
      "2024-01-02 10:56:29.629057: val_loss -0.8359\n",
      "2024-01-02 10:56:29.635055: Pseudo dice [0.9115, 0.9508, 0.9452]\n",
      "2024-01-02 10:56:29.640054: Epoch time: 125.52 s\n",
      "2024-01-02 10:56:30.827681: \n",
      "2024-01-02 10:56:30.835850: Epoch 171\n",
      "2024-01-02 10:56:30.842842: Current learning rate: 0.00845\n",
      "2024-01-02 10:58:36.389112: train_loss -0.9001\n",
      "2024-01-02 10:58:36.396112: val_loss -0.8346\n",
      "2024-01-02 10:58:36.404114: Pseudo dice [0.9127, 0.9501, 0.9441]\n",
      "2024-01-02 10:58:36.410113: Epoch time: 125.56 s\n",
      "2024-01-02 10:58:37.996260: \n",
      "2024-01-02 10:58:38.002262: Epoch 172\n",
      "2024-01-02 10:58:38.006266: Current learning rate: 0.00844\n",
      "2024-01-02 11:00:43.684437: train_loss -0.8978\n",
      "2024-01-02 11:00:43.697440: val_loss -0.8459\n",
      "2024-01-02 11:00:43.706442: Pseudo dice [0.916, 0.9519, 0.9448]\n",
      "2024-01-02 11:00:43.713953: Epoch time: 125.69 s\n",
      "2024-01-02 11:00:45.081767: \n",
      "2024-01-02 11:00:45.087650: Epoch 173\n",
      "2024-01-02 11:00:45.092774: Current learning rate: 0.00843\n",
      "2024-01-02 11:02:50.641796: train_loss -0.9\n",
      "2024-01-02 11:02:50.653797: val_loss -0.8369\n",
      "2024-01-02 11:02:50.660797: Pseudo dice [0.9106, 0.9508, 0.9426]\n",
      "2024-01-02 11:02:50.666797: Epoch time: 125.56 s\n",
      "2024-01-02 11:02:51.930703: \n",
      "2024-01-02 11:02:51.938241: Epoch 174\n",
      "2024-01-02 11:02:51.944860: Current learning rate: 0.00842\n",
      "2024-01-02 11:04:57.572775: train_loss -0.9031\n",
      "2024-01-02 11:04:57.579775: val_loss -0.8435\n",
      "2024-01-02 11:04:57.586844: Pseudo dice [0.9142, 0.9517, 0.9434]\n",
      "2024-01-02 11:04:57.591855: Epoch time: 125.64 s\n",
      "2024-01-02 11:04:58.758374: \n",
      "2024-01-02 11:04:58.764321: Epoch 175\n",
      "2024-01-02 11:04:58.769957: Current learning rate: 0.00841\n",
      "2024-01-02 11:07:04.340169: train_loss -0.9002\n",
      "2024-01-02 11:07:04.346169: val_loss -0.8413\n",
      "2024-01-02 11:07:04.351169: Pseudo dice [0.9134, 0.9526, 0.9447]\n",
      "2024-01-02 11:07:04.356168: Epoch time: 125.58 s\n",
      "2024-01-02 11:07:05.464716: \n",
      "2024-01-02 11:07:05.470792: Epoch 176\n",
      "2024-01-02 11:07:05.475802: Current learning rate: 0.0084\n",
      "2024-01-02 11:09:10.775006: train_loss -0.9013\n",
      "2024-01-02 11:09:10.781515: val_loss -0.8354\n",
      "2024-01-02 11:09:10.791519: Pseudo dice [0.9138, 0.9506, 0.9437]\n",
      "2024-01-02 11:09:10.796521: Epoch time: 125.31 s\n",
      "2024-01-02 11:09:12.042373: \n",
      "2024-01-02 11:09:12.047801: Epoch 177\n",
      "2024-01-02 11:09:12.054816: Current learning rate: 0.00839\n",
      "2024-01-02 11:11:18.029561: train_loss -0.8977\n",
      "2024-01-02 11:11:18.036564: val_loss -0.8408\n",
      "2024-01-02 11:11:18.043577: Pseudo dice [0.9121, 0.9523, 0.9459]\n",
      "2024-01-02 11:11:18.048578: Epoch time: 125.99 s\n",
      "2024-01-02 11:11:19.322881: \n",
      "2024-01-02 11:11:19.327882: Epoch 178\n",
      "2024-01-02 11:11:19.331882: Current learning rate: 0.00838\n",
      "2024-01-02 11:13:25.026226: train_loss -0.899\n",
      "2024-01-02 11:13:25.033231: val_loss -0.8436\n",
      "2024-01-02 11:13:25.041226: Pseudo dice [0.9148, 0.9529, 0.9441]\n",
      "2024-01-02 11:13:25.047229: Epoch time: 125.7 s\n",
      "2024-01-02 11:13:26.492661: \n",
      "2024-01-02 11:13:26.498596: Epoch 179\n",
      "2024-01-02 11:13:26.502695: Current learning rate: 0.00837\n",
      "2024-01-02 11:15:32.163342: train_loss -0.8993\n",
      "2024-01-02 11:15:32.171330: val_loss -0.8398\n",
      "2024-01-02 11:15:32.176342: Pseudo dice [0.915, 0.9515, 0.9445]\n",
      "2024-01-02 11:15:32.182336: Epoch time: 125.67 s\n",
      "2024-01-02 11:15:33.475736: \n",
      "2024-01-02 11:15:33.486811: Epoch 180\n",
      "2024-01-02 11:15:33.496735: Current learning rate: 0.00836\n",
      "2024-01-02 11:17:38.901291: train_loss -0.8992\n",
      "2024-01-02 11:17:38.908808: val_loss -0.8357\n",
      "2024-01-02 11:17:38.914806: Pseudo dice [0.9105, 0.9501, 0.9445]\n",
      "2024-01-02 11:17:38.920816: Epoch time: 125.43 s\n",
      "2024-01-02 11:17:40.145925: \n",
      "2024-01-02 11:17:40.153970: Epoch 181\n",
      "2024-01-02 11:17:40.164109: Current learning rate: 0.00836\n",
      "2024-01-02 11:19:46.077464: train_loss -0.9012\n",
      "2024-01-02 11:19:46.083464: val_loss -0.8417\n",
      "2024-01-02 11:19:46.088464: Pseudo dice [0.9127, 0.9531, 0.9439]\n",
      "2024-01-02 11:19:46.093465: Epoch time: 125.93 s\n",
      "2024-01-02 11:19:47.315613: \n",
      "2024-01-02 11:19:47.321547: Epoch 182\n",
      "2024-01-02 11:19:47.326539: Current learning rate: 0.00835\n",
      "2024-01-02 11:21:52.755932: train_loss -0.8937\n",
      "2024-01-02 11:21:52.761931: val_loss -0.8367\n",
      "2024-01-02 11:21:52.766930: Pseudo dice [0.9096, 0.951, 0.9428]\n",
      "2024-01-02 11:21:52.771932: Epoch time: 125.44 s\n",
      "2024-01-02 11:21:53.907410: \n",
      "2024-01-02 11:21:53.912410: Epoch 183\n",
      "2024-01-02 11:21:53.919427: Current learning rate: 0.00834\n",
      "2024-01-02 11:23:59.416470: train_loss -0.8969\n",
      "2024-01-02 11:23:59.426013: val_loss -0.83\n",
      "2024-01-02 11:23:59.433004: Pseudo dice [0.9078, 0.9488, 0.9453]\n",
      "2024-01-02 11:23:59.439006: Epoch time: 125.51 s\n",
      "2024-01-02 11:24:00.546565: \n",
      "2024-01-02 11:24:00.552557: Epoch 184\n",
      "2024-01-02 11:24:00.556560: Current learning rate: 0.00833\n",
      "2024-01-02 11:26:06.104829: train_loss -0.8878\n",
      "2024-01-02 11:26:06.113831: val_loss -0.8346\n",
      "2024-01-02 11:26:06.118830: Pseudo dice [0.9106, 0.9492, 0.9425]\n",
      "2024-01-02 11:26:06.122830: Epoch time: 125.56 s\n",
      "2024-01-02 11:26:07.321996: \n",
      "2024-01-02 11:26:07.326998: Epoch 185\n",
      "2024-01-02 11:26:07.334994: Current learning rate: 0.00832\n",
      "2024-01-02 11:28:12.776219: train_loss -0.8926\n",
      "2024-01-02 11:28:12.783220: val_loss -0.8383\n",
      "2024-01-02 11:28:12.787234: Pseudo dice [0.9137, 0.9504, 0.9439]\n",
      "2024-01-02 11:28:12.793226: Epoch time: 125.46 s\n",
      "2024-01-02 11:28:14.110991: \n",
      "2024-01-02 11:28:14.116992: Epoch 186\n",
      "2024-01-02 11:28:14.121988: Current learning rate: 0.00831\n",
      "2024-01-02 11:30:19.631813: train_loss -0.8993\n",
      "2024-01-02 11:30:19.638818: val_loss -0.8406\n",
      "2024-01-02 11:30:19.644816: Pseudo dice [0.9146, 0.9517, 0.9446]\n",
      "2024-01-02 11:30:19.649320: Epoch time: 125.52 s\n",
      "2024-01-02 11:30:20.911205: \n",
      "2024-01-02 11:30:20.917517: Epoch 187\n",
      "2024-01-02 11:30:20.922557: Current learning rate: 0.0083\n",
      "2024-01-02 11:32:26.402565: train_loss -0.8981\n",
      "2024-01-02 11:32:26.410074: val_loss -0.8386\n",
      "2024-01-02 11:32:26.416074: Pseudo dice [0.9105, 0.9502, 0.9444]\n",
      "2024-01-02 11:32:26.421074: Epoch time: 125.49 s\n",
      "2024-01-02 11:32:27.622378: \n",
      "2024-01-02 11:32:27.628447: Epoch 188\n",
      "2024-01-02 11:32:27.633417: Current learning rate: 0.00829\n",
      "2024-01-02 11:34:33.657162: train_loss -0.8967\n",
      "2024-01-02 11:34:33.668166: val_loss -0.8361\n",
      "2024-01-02 11:34:33.674678: Pseudo dice [0.9125, 0.9495, 0.9438]\n",
      "2024-01-02 11:34:33.680679: Epoch time: 126.04 s\n",
      "2024-01-02 11:34:35.144747: \n",
      "2024-01-02 11:34:35.149742: Epoch 189\n",
      "2024-01-02 11:34:35.153742: Current learning rate: 0.00828\n",
      "2024-01-02 11:36:40.433184: train_loss -0.8984\n",
      "2024-01-02 11:36:40.441204: val_loss -0.8356\n",
      "2024-01-02 11:36:40.446187: Pseudo dice [0.9128, 0.951, 0.9441]\n",
      "2024-01-02 11:36:40.452195: Epoch time: 125.29 s\n",
      "2024-01-02 11:36:41.651747: \n",
      "2024-01-02 11:36:41.656513: Epoch 190\n",
      "2024-01-02 11:36:41.661522: Current learning rate: 0.00827\n",
      "2024-01-02 11:38:46.949754: train_loss -0.8995\n",
      "2024-01-02 11:38:46.958745: val_loss -0.8352\n",
      "2024-01-02 11:38:46.964749: Pseudo dice [0.9132, 0.9511, 0.9432]\n",
      "2024-01-02 11:38:46.970747: Epoch time: 125.3 s\n",
      "2024-01-02 11:38:48.171244: \n",
      "2024-01-02 11:38:48.177462: Epoch 191\n",
      "2024-01-02 11:38:48.182528: Current learning rate: 0.00826\n",
      "2024-01-02 11:40:53.735311: train_loss -0.8991\n",
      "2024-01-02 11:40:53.741331: val_loss -0.8315\n",
      "2024-01-02 11:40:53.747312: Pseudo dice [0.9109, 0.9494, 0.9435]\n",
      "2024-01-02 11:40:53.751319: Epoch time: 125.56 s\n",
      "2024-01-02 11:40:54.905050: \n",
      "2024-01-02 11:40:54.910047: Epoch 192\n",
      "2024-01-02 11:40:54.914047: Current learning rate: 0.00825\n",
      "2024-01-02 11:43:00.824602: train_loss -0.8955\n",
      "2024-01-02 11:43:00.830604: val_loss -0.8355\n",
      "2024-01-02 11:43:00.836863: Pseudo dice [0.9104, 0.9495, 0.9432]\n",
      "2024-01-02 11:43:00.841862: Epoch time: 125.92 s\n",
      "2024-01-02 11:43:02.261826: \n",
      "2024-01-02 11:43:02.270356: Epoch 193\n",
      "2024-01-02 11:43:02.275350: Current learning rate: 0.00824\n",
      "2024-01-02 11:45:07.893351: train_loss -0.8966\n",
      "2024-01-02 11:45:07.900347: val_loss -0.8361\n",
      "2024-01-02 11:45:07.906350: Pseudo dice [0.9124, 0.9502, 0.9438]\n",
      "2024-01-02 11:45:07.911344: Epoch time: 125.63 s\n",
      "2024-01-02 11:45:09.114812: \n",
      "2024-01-02 11:45:09.119779: Epoch 194\n",
      "2024-01-02 11:45:09.124721: Current learning rate: 0.00824\n",
      "2024-01-02 11:47:14.366554: train_loss -0.9011\n",
      "2024-01-02 11:47:14.373559: val_loss -0.8404\n",
      "2024-01-02 11:47:14.378554: Pseudo dice [0.9172, 0.9516, 0.9451]\n",
      "2024-01-02 11:47:14.386557: Epoch time: 125.25 s\n",
      "2024-01-02 11:47:15.721743: \n",
      "2024-01-02 11:47:15.727801: Epoch 195\n",
      "2024-01-02 11:47:15.735801: Current learning rate: 0.00823\n",
      "2024-01-02 11:49:21.632374: train_loss -0.9026\n",
      "2024-01-02 11:49:21.638373: val_loss -0.8398\n",
      "2024-01-02 11:49:21.646372: Pseudo dice [0.9143, 0.952, 0.9434]\n",
      "2024-01-02 11:49:21.653372: Epoch time: 125.91 s\n",
      "2024-01-02 11:49:22.974729: \n",
      "2024-01-02 11:49:22.980676: Epoch 196\n",
      "2024-01-02 11:49:22.984676: Current learning rate: 0.00822\n",
      "2024-01-02 11:51:28.308848: train_loss -0.8992\n",
      "2024-01-02 11:51:28.315847: val_loss -0.8356\n",
      "2024-01-02 11:51:28.320370: Pseudo dice [0.9128, 0.9524, 0.9434]\n",
      "2024-01-02 11:51:28.325357: Epoch time: 125.33 s\n",
      "2024-01-02 11:51:29.485812: \n",
      "2024-01-02 11:51:29.490880: Epoch 197\n",
      "2024-01-02 11:51:29.496826: Current learning rate: 0.00821\n",
      "2024-01-02 11:53:34.909417: train_loss -0.9016\n",
      "2024-01-02 11:53:34.916409: val_loss -0.831\n",
      "2024-01-02 11:53:34.921417: Pseudo dice [0.9146, 0.9482, 0.943]\n",
      "2024-01-02 11:53:34.927410: Epoch time: 125.43 s\n",
      "2024-01-02 11:53:36.067309: \n",
      "2024-01-02 11:53:36.073402: Epoch 198\n",
      "2024-01-02 11:53:36.080477: Current learning rate: 0.0082\n",
      "2024-01-02 11:55:41.707612: train_loss -0.8998\n",
      "2024-01-02 11:55:41.715613: val_loss -0.8436\n",
      "2024-01-02 11:55:41.722612: Pseudo dice [0.9156, 0.9524, 0.9438]\n",
      "2024-01-02 11:55:41.729612: Epoch time: 125.64 s\n",
      "2024-01-02 11:55:42.838915: \n",
      "2024-01-02 11:55:42.847023: Epoch 199\n",
      "2024-01-02 11:55:42.852124: Current learning rate: 0.00819\n",
      "2024-01-02 11:57:48.098338: train_loss -0.9028\n",
      "2024-01-02 11:57:48.104339: val_loss -0.8393\n",
      "2024-01-02 11:57:48.110347: Pseudo dice [0.9132, 0.9513, 0.9451]\n",
      "2024-01-02 11:57:48.115411: Epoch time: 125.26 s\n",
      "2024-01-02 11:57:49.651164: \n",
      "2024-01-02 11:57:49.657165: Epoch 200\n",
      "2024-01-02 11:57:49.663683: Current learning rate: 0.00818\n",
      "2024-01-02 11:59:55.122878: train_loss -0.9026\n",
      "2024-01-02 11:59:55.130874: val_loss -0.8352\n",
      "2024-01-02 11:59:55.136873: Pseudo dice [0.9133, 0.9511, 0.9446]\n",
      "2024-01-02 11:59:55.142883: Epoch time: 125.47 s\n",
      "2024-01-02 11:59:56.544656: \n",
      "2024-01-02 11:59:56.550726: Epoch 201\n",
      "2024-01-02 11:59:56.566032: Current learning rate: 0.00817\n",
      "2024-01-02 12:02:02.276834: train_loss -0.9044\n",
      "2024-01-02 12:02:02.286836: val_loss -0.8353\n",
      "2024-01-02 12:02:02.296836: Pseudo dice [0.9136, 0.9502, 0.9444]\n",
      "2024-01-02 12:02:02.301836: Epoch time: 125.73 s\n",
      "2024-01-02 12:02:03.434346: \n",
      "2024-01-02 12:02:03.441917: Epoch 202\n",
      "2024-01-02 12:02:03.447998: Current learning rate: 0.00816\n",
      "2024-01-02 12:04:09.036123: train_loss -0.9021\n",
      "2024-01-02 12:04:09.049123: val_loss -0.8378\n",
      "2024-01-02 12:04:09.057132: Pseudo dice [0.9137, 0.9511, 0.9432]\n",
      "2024-01-02 12:04:09.062132: Epoch time: 125.6 s\n",
      "2024-01-02 12:04:10.310230: \n",
      "2024-01-02 12:04:10.316235: Epoch 203\n",
      "2024-01-02 12:04:10.320751: Current learning rate: 0.00815\n",
      "2024-01-02 12:06:15.856678: train_loss -0.9003\n",
      "2024-01-02 12:06:15.864686: val_loss -0.8387\n",
      "2024-01-02 12:06:15.869677: Pseudo dice [0.9143, 0.9511, 0.9429]\n",
      "2024-01-02 12:06:15.879189: Epoch time: 125.55 s\n",
      "2024-01-02 12:06:17.168741: \n",
      "2024-01-02 12:06:17.178497: Epoch 204\n",
      "2024-01-02 12:06:17.184457: Current learning rate: 0.00814\n",
      "2024-01-02 12:08:22.647827: train_loss -0.902\n",
      "2024-01-02 12:08:22.653846: val_loss -0.8338\n",
      "2024-01-02 12:08:22.660891: Pseudo dice [0.9125, 0.9496, 0.9422]\n",
      "2024-01-02 12:08:22.665905: Epoch time: 125.48 s\n",
      "2024-01-02 12:08:23.890630: \n",
      "2024-01-02 12:08:23.898671: Epoch 205\n",
      "2024-01-02 12:08:23.903705: Current learning rate: 0.00813\n",
      "2024-01-02 12:10:29.512445: train_loss -0.9014\n",
      "2024-01-02 12:10:29.521446: val_loss -0.8345\n",
      "2024-01-02 12:10:29.528449: Pseudo dice [0.913, 0.9516, 0.9435]\n",
      "2024-01-02 12:10:29.538039: Epoch time: 125.62 s\n",
      "2024-01-02 12:10:30.756618: \n",
      "2024-01-02 12:10:30.762910: Epoch 206\n",
      "2024-01-02 12:10:30.767911: Current learning rate: 0.00813\n",
      "2024-01-02 12:12:36.308215: train_loss -0.9005\n",
      "2024-01-02 12:12:36.316207: val_loss -0.8411\n",
      "2024-01-02 12:12:36.321207: Pseudo dice [0.9125, 0.9523, 0.9465]\n",
      "2024-01-02 12:12:36.325215: Epoch time: 125.55 s\n",
      "2024-01-02 12:12:37.426271: \n",
      "2024-01-02 12:12:37.439427: Epoch 207\n",
      "2024-01-02 12:12:37.445433: Current learning rate: 0.00812\n",
      "2024-01-02 12:14:43.118585: train_loss -0.9046\n",
      "2024-01-02 12:14:43.124593: val_loss -0.8386\n",
      "2024-01-02 12:14:43.131586: Pseudo dice [0.9131, 0.9519, 0.9433]\n",
      "2024-01-02 12:14:43.136586: Epoch time: 125.69 s\n",
      "2024-01-02 12:14:44.298654: \n",
      "2024-01-02 12:14:44.305108: Epoch 208\n",
      "2024-01-02 12:14:44.309070: Current learning rate: 0.00811\n",
      "2024-01-02 12:16:49.501405: train_loss -0.9055\n",
      "2024-01-02 12:16:49.508429: val_loss -0.8377\n",
      "2024-01-02 12:16:49.514428: Pseudo dice [0.9156, 0.9521, 0.9439]\n",
      "2024-01-02 12:16:49.519937: Epoch time: 125.2 s\n",
      "2024-01-02 12:16:50.791927: \n",
      "2024-01-02 12:16:50.799991: Epoch 209\n",
      "2024-01-02 12:16:50.805073: Current learning rate: 0.0081\n",
      "2024-01-02 12:18:56.508148: train_loss -0.9039\n",
      "2024-01-02 12:18:56.515149: val_loss -0.8343\n",
      "2024-01-02 12:18:56.520153: Pseudo dice [0.9124, 0.9505, 0.9432]\n",
      "2024-01-02 12:18:56.524149: Epoch time: 125.72 s\n",
      "2024-01-02 12:18:57.564279: \n",
      "2024-01-02 12:18:57.572875: Epoch 210\n",
      "2024-01-02 12:18:57.577535: Current learning rate: 0.00809\n",
      "2024-01-02 12:21:03.070299: train_loss -0.9034\n",
      "2024-01-02 12:21:03.076298: val_loss -0.8342\n",
      "2024-01-02 12:21:03.082298: Pseudo dice [0.9108, 0.9511, 0.944]\n",
      "2024-01-02 12:21:03.091298: Epoch time: 125.51 s\n",
      "2024-01-02 12:21:04.304560: \n",
      "2024-01-02 12:21:04.310557: Epoch 211\n",
      "2024-01-02 12:21:04.315548: Current learning rate: 0.00808\n",
      "2024-01-02 12:23:10.183117: train_loss -0.9019\n",
      "2024-01-02 12:23:10.192130: val_loss -0.8359\n",
      "2024-01-02 12:23:10.199140: Pseudo dice [0.9126, 0.9518, 0.9446]\n",
      "2024-01-02 12:23:10.206119: Epoch time: 125.88 s\n",
      "2024-01-02 12:23:11.413723: \n",
      "2024-01-02 12:23:11.418724: Epoch 212\n",
      "2024-01-02 12:23:11.422723: Current learning rate: 0.00807\n",
      "2024-01-02 12:25:16.915856: train_loss -0.8992\n",
      "2024-01-02 12:25:16.921858: val_loss -0.841\n",
      "2024-01-02 12:25:16.926856: Pseudo dice [0.9128, 0.9519, 0.9435]\n",
      "2024-01-02 12:25:16.932370: Epoch time: 125.5 s\n",
      "2024-01-02 12:25:18.114607: \n",
      "2024-01-02 12:25:18.120683: Epoch 213\n",
      "2024-01-02 12:25:18.125741: Current learning rate: 0.00806\n",
      "2024-01-02 12:27:23.758959: train_loss -0.9036\n",
      "2024-01-02 12:27:23.765960: val_loss -0.8344\n",
      "2024-01-02 12:27:23.770960: Pseudo dice [0.9105, 0.9507, 0.9442]\n",
      "2024-01-02 12:27:23.781966: Epoch time: 125.65 s\n",
      "2024-01-02 12:27:25.082335: \n",
      "2024-01-02 12:27:25.090335: Epoch 214\n",
      "2024-01-02 12:27:25.095336: Current learning rate: 0.00805\n",
      "2024-01-02 12:29:30.641260: train_loss -0.9032\n",
      "2024-01-02 12:29:30.649261: val_loss -0.8348\n",
      "2024-01-02 12:29:30.655262: Pseudo dice [0.9136, 0.9512, 0.9432]\n",
      "2024-01-02 12:29:30.660262: Epoch time: 125.56 s\n",
      "2024-01-02 12:29:31.804943: \n",
      "2024-01-02 12:29:31.810631: Epoch 215\n",
      "2024-01-02 12:29:31.815603: Current learning rate: 0.00804\n",
      "2024-01-02 12:31:37.382411: train_loss -0.9019\n",
      "2024-01-02 12:31:37.391407: val_loss -0.8336\n",
      "2024-01-02 12:31:37.397408: Pseudo dice [0.9139, 0.95, 0.9457]\n",
      "2024-01-02 12:31:37.402418: Epoch time: 125.58 s\n",
      "2024-01-02 12:31:38.762610: \n",
      "2024-01-02 12:31:38.767611: Epoch 216\n",
      "2024-01-02 12:31:38.772611: Current learning rate: 0.00803\n",
      "2024-01-02 12:33:44.291012: train_loss -0.9031\n",
      "2024-01-02 12:33:44.298582: val_loss -0.8334\n",
      "2024-01-02 12:33:44.306589: Pseudo dice [0.9119, 0.9506, 0.944]\n",
      "2024-01-02 12:33:44.312590: Epoch time: 125.53 s\n",
      "2024-01-02 12:33:45.497080: \n",
      "2024-01-02 12:33:45.505085: Epoch 217\n",
      "2024-01-02 12:33:45.510100: Current learning rate: 0.00802\n",
      "2024-01-02 12:35:51.035269: train_loss -0.9047\n",
      "2024-01-02 12:35:51.041268: val_loss -0.8375\n",
      "2024-01-02 12:35:51.047268: Pseudo dice [0.9133, 0.9512, 0.9436]\n",
      "2024-01-02 12:35:51.052268: Epoch time: 125.54 s\n",
      "2024-01-02 12:35:52.123739: \n",
      "2024-01-02 12:35:52.129275: Epoch 218\n",
      "2024-01-02 12:35:52.134266: Current learning rate: 0.00801\n",
      "2024-01-02 12:37:57.758396: train_loss -0.904\n",
      "2024-01-02 12:37:57.766403: val_loss -0.8365\n",
      "2024-01-02 12:37:57.772403: Pseudo dice [0.9141, 0.9504, 0.945]\n",
      "2024-01-02 12:37:57.780404: Epoch time: 125.64 s\n",
      "2024-01-02 12:37:59.032891: \n",
      "2024-01-02 12:37:59.037972: Epoch 219\n",
      "2024-01-02 12:37:59.042893: Current learning rate: 0.00801\n",
      "2024-01-02 12:40:04.614025: train_loss -0.9012\n",
      "2024-01-02 12:40:04.620033: val_loss -0.836\n",
      "2024-01-02 12:40:04.625559: Pseudo dice [0.9122, 0.9507, 0.9438]\n",
      "2024-01-02 12:40:04.630557: Epoch time: 125.58 s\n",
      "2024-01-02 12:40:05.718524: \n",
      "2024-01-02 12:40:05.724053: Epoch 220\n",
      "2024-01-02 12:40:05.735143: Current learning rate: 0.008\n",
      "2024-01-02 12:42:11.693911: train_loss -0.9017\n",
      "2024-01-02 12:42:11.702920: val_loss -0.837\n",
      "2024-01-02 12:42:11.711918: Pseudo dice [0.9133, 0.9523, 0.944]\n",
      "2024-01-02 12:42:11.719922: Epoch time: 125.98 s\n",
      "2024-01-02 12:42:13.011393: \n",
      "2024-01-02 12:42:13.018447: Epoch 221\n",
      "2024-01-02 12:42:13.024098: Current learning rate: 0.00799\n",
      "2024-01-02 12:44:18.667003: train_loss -0.9042\n",
      "2024-01-02 12:44:18.673184: val_loss -0.8292\n",
      "2024-01-02 12:44:18.678172: Pseudo dice [0.9134, 0.9492, 0.9428]\n",
      "2024-01-02 12:44:18.683172: Epoch time: 125.66 s\n",
      "2024-01-02 12:44:19.953555: \n",
      "2024-01-02 12:44:19.961561: Epoch 222\n",
      "2024-01-02 12:44:19.972558: Current learning rate: 0.00798\n",
      "2024-01-02 12:46:25.377084: train_loss -0.9056\n",
      "2024-01-02 12:46:25.385080: val_loss -0.8348\n",
      "2024-01-02 12:46:25.392090: Pseudo dice [0.9125, 0.9503, 0.9453]\n",
      "2024-01-02 12:46:25.398082: Epoch time: 125.43 s\n",
      "2024-01-02 12:46:26.468840: \n",
      "2024-01-02 12:46:26.477781: Epoch 223\n",
      "2024-01-02 12:46:26.484799: Current learning rate: 0.00797\n",
      "2024-01-02 12:48:32.237502: train_loss -0.9035\n",
      "2024-01-02 12:48:32.244503: val_loss -0.8358\n",
      "2024-01-02 12:48:32.249502: Pseudo dice [0.9155, 0.9514, 0.9456]\n",
      "2024-01-02 12:48:32.256507: Epoch time: 125.77 s\n",
      "2024-01-02 12:48:33.508502: \n",
      "2024-01-02 12:48:33.514596: Epoch 224\n",
      "2024-01-02 12:48:33.521601: Current learning rate: 0.00796\n",
      "2024-01-02 12:50:39.174280: train_loss -0.9046\n",
      "2024-01-02 12:50:39.182285: val_loss -0.8347\n",
      "2024-01-02 12:50:39.190290: Pseudo dice [0.9127, 0.9511, 0.9438]\n",
      "2024-01-02 12:50:39.199280: Epoch time: 125.67 s\n",
      "2024-01-02 12:50:40.405089: \n",
      "2024-01-02 12:50:40.410085: Epoch 225\n",
      "2024-01-02 12:50:40.414093: Current learning rate: 0.00795\n",
      "2024-01-02 12:52:45.949104: train_loss -0.9046\n",
      "2024-01-02 12:52:45.955103: val_loss -0.8261\n",
      "2024-01-02 12:52:45.960103: Pseudo dice [0.9105, 0.9478, 0.944]\n",
      "2024-01-02 12:52:45.965103: Epoch time: 125.54 s\n",
      "2024-01-02 12:52:47.096912: \n",
      "2024-01-02 12:52:47.106913: Epoch 226\n",
      "2024-01-02 12:52:47.111976: Current learning rate: 0.00794\n",
      "2024-01-02 12:54:52.684717: train_loss -0.9019\n",
      "2024-01-02 12:54:52.690717: val_loss -0.834\n",
      "2024-01-02 12:54:52.697716: Pseudo dice [0.9137, 0.9516, 0.9435]\n",
      "2024-01-02 12:54:52.702716: Epoch time: 125.59 s\n",
      "2024-01-02 12:54:53.856545: \n",
      "2024-01-02 12:54:53.865635: Epoch 227\n",
      "2024-01-02 12:54:53.869597: Current learning rate: 0.00793\n",
      "2024-01-02 12:56:59.487226: train_loss -0.9036\n",
      "2024-01-02 12:56:59.495221: val_loss -0.8342\n",
      "2024-01-02 12:56:59.502234: Pseudo dice [0.9117, 0.9497, 0.944]\n",
      "2024-01-02 12:56:59.507224: Epoch time: 125.63 s\n",
      "2024-01-02 12:57:00.777680: \n",
      "2024-01-02 12:57:00.783672: Epoch 228\n",
      "2024-01-02 12:57:00.789301: Current learning rate: 0.00792\n",
      "2024-01-02 12:59:06.566257: train_loss -0.9022\n",
      "2024-01-02 12:59:06.575274: val_loss -0.8314\n",
      "2024-01-02 12:59:06.583266: Pseudo dice [0.9132, 0.9504, 0.9441]\n",
      "2024-01-02 12:59:06.589776: Epoch time: 125.79 s\n",
      "2024-01-02 12:59:07.824998: \n",
      "2024-01-02 12:59:07.835000: Epoch 229\n",
      "2024-01-02 12:59:07.844000: Current learning rate: 0.00791\n",
      "2024-01-02 13:01:13.437515: train_loss -0.9036\n",
      "2024-01-02 13:01:13.445515: val_loss -0.8353\n",
      "2024-01-02 13:01:13.452024: Pseudo dice [0.9113, 0.9521, 0.9444]\n",
      "2024-01-02 13:01:13.456023: Epoch time: 125.61 s\n",
      "2024-01-02 13:01:14.560519: \n",
      "2024-01-02 13:01:14.569468: Epoch 230\n",
      "2024-01-02 13:01:14.574339: Current learning rate: 0.0079\n",
      "2024-01-02 13:03:20.266335: train_loss -0.903\n",
      "2024-01-02 13:03:20.272337: val_loss -0.8347\n",
      "2024-01-02 13:03:20.279336: Pseudo dice [0.9141, 0.9505, 0.945]\n",
      "2024-01-02 13:03:20.285343: Epoch time: 125.71 s\n",
      "2024-01-02 13:03:21.463799: \n",
      "2024-01-02 13:03:21.469799: Epoch 231\n",
      "2024-01-02 13:03:21.476799: Current learning rate: 0.00789\n",
      "2024-01-02 13:05:27.119020: train_loss -0.8963\n",
      "2024-01-02 13:05:27.130024: val_loss -0.834\n",
      "2024-01-02 13:05:27.138527: Pseudo dice [0.9114, 0.9512, 0.9421]\n",
      "2024-01-02 13:05:27.145532: Epoch time: 125.66 s\n",
      "2024-01-02 13:05:28.644341: \n",
      "2024-01-02 13:05:28.649348: Epoch 232\n",
      "2024-01-02 13:05:28.657356: Current learning rate: 0.00789\n",
      "2024-01-02 13:07:35.220417: train_loss -0.898\n",
      "2024-01-02 13:07:35.227419: val_loss -0.8366\n",
      "2024-01-02 13:07:35.236419: Pseudo dice [0.9142, 0.9506, 0.9443]\n",
      "2024-01-02 13:07:35.244419: Epoch time: 126.58 s\n",
      "2024-01-02 13:07:36.624939: \n",
      "2024-01-02 13:07:36.633878: Epoch 233\n",
      "2024-01-02 13:07:36.638873: Current learning rate: 0.00788\n",
      "2024-01-02 13:09:42.483898: train_loss -0.8963\n",
      "2024-01-02 13:09:42.490898: val_loss -0.8357\n",
      "2024-01-02 13:09:42.495899: Pseudo dice [0.9131, 0.9508, 0.9431]\n",
      "2024-01-02 13:09:42.500898: Epoch time: 125.86 s\n",
      "2024-01-02 13:09:43.567043: \n",
      "2024-01-02 13:09:43.577990: Epoch 234\n",
      "2024-01-02 13:09:43.588073: Current learning rate: 0.00787\n",
      "2024-01-02 13:11:49.408332: train_loss -0.8996\n",
      "2024-01-02 13:11:49.415390: val_loss -0.8339\n",
      "2024-01-02 13:11:49.422344: Pseudo dice [0.9092, 0.95, 0.9435]\n",
      "2024-01-02 13:11:49.429850: Epoch time: 125.84 s\n",
      "2024-01-02 13:11:50.623106: \n",
      "2024-01-02 13:11:50.631334: Epoch 235\n",
      "2024-01-02 13:11:50.639412: Current learning rate: 0.00786\n",
      "2024-01-02 13:13:56.151091: train_loss -0.9038\n",
      "2024-01-02 13:13:56.160167: val_loss -0.8351\n",
      "2024-01-02 13:13:56.167173: Pseudo dice [0.9128, 0.951, 0.9448]\n",
      "2024-01-02 13:13:56.174087: Epoch time: 125.53 s\n",
      "2024-01-02 13:13:57.285291: \n",
      "2024-01-02 13:13:57.291330: Epoch 236\n",
      "2024-01-02 13:13:57.300332: Current learning rate: 0.00785\n",
      "2024-01-02 13:16:02.946322: train_loss -0.9029\n",
      "2024-01-02 13:16:02.953332: val_loss -0.8393\n",
      "2024-01-02 13:16:02.961329: Pseudo dice [0.914, 0.9515, 0.9444]\n",
      "2024-01-02 13:16:02.969321: Epoch time: 125.66 s\n",
      "2024-01-02 13:16:04.030854: \n",
      "2024-01-02 13:16:04.039720: Epoch 237\n",
      "2024-01-02 13:16:04.044109: Current learning rate: 0.00784\n",
      "2024-01-02 13:18:10.097785: train_loss -0.9031\n",
      "2024-01-02 13:18:10.107303: val_loss -0.8357\n",
      "2024-01-02 13:18:10.113818: Pseudo dice [0.9126, 0.951, 0.9432]\n",
      "2024-01-02 13:18:10.118819: Epoch time: 126.07 s\n",
      "2024-01-02 13:18:11.483007: \n",
      "2024-01-02 13:18:11.492004: Epoch 238\n",
      "2024-01-02 13:18:11.496996: Current learning rate: 0.00783\n",
      "2024-01-02 13:20:17.093342: train_loss -0.9027\n",
      "2024-01-02 13:20:17.103853: val_loss -0.8327\n",
      "2024-01-02 13:20:17.111858: Pseudo dice [0.9128, 0.9518, 0.9438]\n",
      "2024-01-02 13:20:17.122365: Epoch time: 125.61 s\n",
      "2024-01-02 13:20:18.351490: \n",
      "2024-01-02 13:20:18.358483: Epoch 239\n",
      "2024-01-02 13:20:18.367480: Current learning rate: 0.00782\n",
      "2024-01-02 13:22:24.081035: train_loss -0.902\n",
      "2024-01-02 13:22:24.089034: val_loss -0.836\n",
      "2024-01-02 13:22:24.093035: Pseudo dice [0.9142, 0.9508, 0.9448]\n",
      "2024-01-02 13:22:24.098035: Epoch time: 125.73 s\n",
      "2024-01-02 13:22:25.380220: \n",
      "2024-01-02 13:22:25.386348: Epoch 240\n",
      "2024-01-02 13:22:25.390350: Current learning rate: 0.00781\n",
      "2024-01-02 13:24:31.120914: train_loss -0.9031\n",
      "2024-01-02 13:24:31.128914: val_loss -0.8344\n",
      "2024-01-02 13:24:31.135914: Pseudo dice [0.911, 0.9512, 0.9433]\n",
      "2024-01-02 13:24:31.142913: Epoch time: 125.74 s\n",
      "2024-01-02 13:24:32.254822: \n",
      "2024-01-02 13:24:32.259378: Epoch 241\n",
      "2024-01-02 13:24:32.265011: Current learning rate: 0.0078\n",
      "2024-01-02 13:26:38.315302: train_loss -0.9025\n",
      "2024-01-02 13:26:38.323307: val_loss -0.8332\n",
      "2024-01-02 13:26:38.329307: Pseudo dice [0.9113, 0.9511, 0.9441]\n",
      "2024-01-02 13:26:38.335307: Epoch time: 126.06 s\n",
      "2024-01-02 13:26:39.631746: \n",
      "2024-01-02 13:26:39.636331: Epoch 242\n",
      "2024-01-02 13:26:39.642567: Current learning rate: 0.00779\n",
      "2024-01-02 13:28:45.155805: train_loss -0.9027\n",
      "2024-01-02 13:28:45.162808: val_loss -0.8258\n",
      "2024-01-02 13:28:45.168825: Pseudo dice [0.9117, 0.9471, 0.9433]\n",
      "2024-01-02 13:28:45.173823: Epoch time: 125.53 s\n",
      "2024-01-02 13:28:46.317276: \n",
      "2024-01-02 13:28:46.323575: Epoch 243\n",
      "2024-01-02 13:28:46.328573: Current learning rate: 0.00778\n",
      "2024-01-02 13:30:51.880313: train_loss -0.9032\n",
      "2024-01-02 13:30:51.887312: val_loss -0.8296\n",
      "2024-01-02 13:30:51.894313: Pseudo dice [0.9127, 0.9482, 0.9417]\n",
      "2024-01-02 13:30:51.901313: Epoch time: 125.56 s\n",
      "2024-01-02 13:30:53.122937: \n",
      "2024-01-02 13:30:53.136904: Epoch 244\n",
      "2024-01-02 13:30:53.141907: Current learning rate: 0.00777\n",
      "2024-01-02 13:32:58.478866: train_loss -0.9046\n",
      "2024-01-02 13:32:58.485865: val_loss -0.8393\n",
      "2024-01-02 13:32:58.490866: Pseudo dice [0.9124, 0.9517, 0.9455]\n",
      "2024-01-02 13:32:58.495866: Epoch time: 125.36 s\n",
      "2024-01-02 13:32:59.573938: \n",
      "2024-01-02 13:32:59.579476: Epoch 245\n",
      "2024-01-02 13:32:59.585550: Current learning rate: 0.00777\n",
      "2024-01-02 13:35:05.172532: train_loss -0.9041\n",
      "2024-01-02 13:35:05.178536: val_loss -0.8299\n",
      "2024-01-02 13:35:05.183536: Pseudo dice [0.9128, 0.9503, 0.9448]\n",
      "2024-01-02 13:35:05.188038: Epoch time: 125.6 s\n",
      "2024-01-02 13:35:06.362349: \n",
      "2024-01-02 13:35:06.367344: Epoch 246\n",
      "2024-01-02 13:35:06.372330: Current learning rate: 0.00776\n",
      "2024-01-02 13:37:12.084589: train_loss -0.9047\n",
      "2024-01-02 13:37:12.091591: val_loss -0.8337\n",
      "2024-01-02 13:37:12.096589: Pseudo dice [0.914, 0.9518, 0.9438]\n",
      "2024-01-02 13:37:12.101589: Epoch time: 125.72 s\n",
      "2024-01-02 13:37:13.309539: \n",
      "2024-01-02 13:37:13.318542: Epoch 247\n",
      "2024-01-02 13:37:13.323559: Current learning rate: 0.00775\n",
      "2024-01-02 13:39:19.040169: train_loss -0.9033\n",
      "2024-01-02 13:39:19.047177: val_loss -0.8327\n",
      "2024-01-02 13:39:19.053178: Pseudo dice [0.9127, 0.9515, 0.9453]\n",
      "2024-01-02 13:39:19.059177: Epoch time: 125.73 s\n",
      "2024-01-02 13:39:20.216651: \n",
      "2024-01-02 13:39:20.222708: Epoch 248\n",
      "2024-01-02 13:39:20.227741: Current learning rate: 0.00774\n",
      "2024-01-02 13:41:25.830535: train_loss -0.9035\n",
      "2024-01-02 13:41:25.838535: val_loss -0.8318\n",
      "2024-01-02 13:41:25.844537: Pseudo dice [0.9105, 0.9495, 0.9443]\n",
      "2024-01-02 13:41:25.849540: Epoch time: 125.61 s\n",
      "2024-01-02 13:41:27.003981: \n",
      "2024-01-02 13:41:27.010051: Epoch 249\n",
      "2024-01-02 13:41:27.017100: Current learning rate: 0.00773\n",
      "2024-01-02 13:43:32.786564: train_loss -0.905\n",
      "2024-01-02 13:43:32.793562: val_loss -0.8355\n",
      "2024-01-02 13:43:32.800564: Pseudo dice [0.9133, 0.9511, 0.9429]\n",
      "2024-01-02 13:43:32.805565: Epoch time: 125.78 s\n",
      "2024-01-02 13:43:34.221661: \n",
      "2024-01-02 13:43:34.227254: Epoch 250\n",
      "2024-01-02 13:43:34.234259: Current learning rate: 0.00772\n",
      "2024-01-02 13:45:39.425017: train_loss -0.9047\n",
      "2024-01-02 13:45:39.431017: val_loss -0.8343\n",
      "2024-01-02 13:45:39.437019: Pseudo dice [0.9139, 0.9504, 0.9448]\n",
      "2024-01-02 13:45:39.444025: Epoch time: 125.21 s\n",
      "2024-01-02 13:45:40.525422: \n",
      "2024-01-02 13:45:40.531401: Epoch 251\n",
      "2024-01-02 13:45:40.536342: Current learning rate: 0.00771\n",
      "2024-01-02 13:47:46.094737: train_loss -0.9066\n",
      "2024-01-02 13:47:46.102744: val_loss -0.8333\n",
      "2024-01-02 13:47:46.109734: Pseudo dice [0.9133, 0.95, 0.9439]\n",
      "2024-01-02 13:47:46.117733: Epoch time: 125.57 s\n",
      "2024-01-02 13:47:47.297265: \n",
      "2024-01-02 13:47:47.303715: Epoch 252\n",
      "2024-01-02 13:47:47.308684: Current learning rate: 0.0077\n",
      "2024-01-02 13:49:52.649457: train_loss -0.9059\n",
      "2024-01-02 13:49:52.655463: val_loss -0.833\n",
      "2024-01-02 13:49:52.660463: Pseudo dice [0.9126, 0.9508, 0.9443]\n",
      "2024-01-02 13:49:52.664967: Epoch time: 125.35 s\n",
      "2024-01-02 13:49:53.810530: \n",
      "2024-01-02 13:49:53.816276: Epoch 253\n",
      "2024-01-02 13:49:53.820278: Current learning rate: 0.00769\n",
      "2024-01-02 13:51:59.813476: train_loss -0.9076\n",
      "2024-01-02 13:51:59.824989: val_loss -0.8351\n",
      "2024-01-02 13:51:59.832987: Pseudo dice [0.9123, 0.9513, 0.9448]\n",
      "2024-01-02 13:51:59.839996: Epoch time: 126.0 s\n",
      "2024-01-02 13:52:01.231137: \n",
      "2024-01-02 13:52:01.237127: Epoch 254\n",
      "2024-01-02 13:52:01.241135: Current learning rate: 0.00768\n",
      "2024-01-02 13:54:06.996753: train_loss -0.9061\n",
      "2024-01-02 13:54:07.004753: val_loss -0.8315\n",
      "2024-01-02 13:54:07.011755: Pseudo dice [0.911, 0.9495, 0.9446]\n",
      "2024-01-02 13:54:07.015754: Epoch time: 125.77 s\n",
      "2024-01-02 13:54:08.446968: \n",
      "2024-01-02 13:54:08.453001: Epoch 255\n",
      "2024-01-02 13:54:08.457073: Current learning rate: 0.00767\n",
      "2024-01-02 13:56:13.811589: train_loss -0.9038\n",
      "2024-01-02 13:56:13.820589: val_loss -0.8381\n",
      "2024-01-02 13:56:13.825588: Pseudo dice [0.9136, 0.9511, 0.9451]\n",
      "2024-01-02 13:56:13.831590: Epoch time: 125.37 s\n",
      "2024-01-02 13:56:14.988380: \n",
      "2024-01-02 13:56:14.995437: Epoch 256\n",
      "2024-01-02 13:56:15.000418: Current learning rate: 0.00766\n",
      "2024-01-02 13:58:20.631871: train_loss -0.9058\n",
      "2024-01-02 13:58:20.638871: val_loss -0.8391\n",
      "2024-01-02 13:58:20.644872: Pseudo dice [0.9128, 0.9521, 0.9465]\n",
      "2024-01-02 13:58:20.649871: Epoch time: 125.65 s\n",
      "2024-01-02 13:58:21.884223: \n",
      "2024-01-02 13:58:21.890850: Epoch 257\n",
      "2024-01-02 13:58:21.896950: Current learning rate: 0.00765\n",
      "2024-01-02 14:00:27.446613: train_loss -0.9032\n",
      "2024-01-02 14:00:27.455615: val_loss -0.8325\n",
      "2024-01-02 14:00:27.460615: Pseudo dice [0.9103, 0.9481, 0.9437]\n",
      "2024-01-02 14:00:27.465611: Epoch time: 125.56 s\n",
      "2024-01-02 14:00:28.598482: \n",
      "2024-01-02 14:00:28.606821: Epoch 258\n",
      "2024-01-02 14:00:28.610883: Current learning rate: 0.00764\n",
      "2024-01-02 14:02:34.369289: train_loss -0.9017\n",
      "2024-01-02 14:02:34.376295: val_loss -0.8367\n",
      "2024-01-02 14:02:34.381294: Pseudo dice [0.9104, 0.9522, 0.9435]\n",
      "2024-01-02 14:02:34.388294: Epoch time: 125.77 s\n",
      "2024-01-02 14:02:35.543972: \n",
      "2024-01-02 14:02:35.551325: Epoch 259\n",
      "2024-01-02 14:02:35.555842: Current learning rate: 0.00764\n",
      "2024-01-02 14:04:41.139399: train_loss -0.9079\n",
      "2024-01-02 14:04:41.147411: val_loss -0.8363\n",
      "2024-01-02 14:04:41.154411: Pseudo dice [0.9094, 0.9514, 0.9442]\n",
      "2024-01-02 14:04:41.160399: Epoch time: 125.6 s\n",
      "2024-01-02 14:04:42.297683: \n",
      "2024-01-02 14:04:42.302978: Epoch 260\n",
      "2024-01-02 14:04:42.308004: Current learning rate: 0.00763\n",
      "2024-01-02 14:06:47.991017: train_loss -0.906\n",
      "2024-01-02 14:06:47.998016: val_loss -0.8321\n",
      "2024-01-02 14:06:48.003015: Pseudo dice [0.9122, 0.9501, 0.944]\n",
      "2024-01-02 14:06:48.008015: Epoch time: 125.7 s\n",
      "2024-01-02 14:06:49.164226: \n",
      "2024-01-02 14:06:49.176623: Epoch 261\n",
      "2024-01-02 14:06:49.184541: Current learning rate: 0.00762\n",
      "2024-01-02 14:08:54.580824: train_loss -0.9078\n",
      "2024-01-02 14:08:54.586824: val_loss -0.8316\n",
      "2024-01-02 14:08:54.591368: Pseudo dice [0.9146, 0.9498, 0.9442]\n",
      "2024-01-02 14:08:54.598354: Epoch time: 125.42 s\n",
      "2024-01-02 14:08:55.796685: \n",
      "2024-01-02 14:08:55.810378: Epoch 262\n",
      "2024-01-02 14:08:55.818371: Current learning rate: 0.00761\n",
      "2024-01-02 14:11:01.361524: train_loss -0.907\n",
      "2024-01-02 14:11:01.370525: val_loss -0.8366\n",
      "2024-01-02 14:11:01.375523: Pseudo dice [0.9118, 0.951, 0.944]\n",
      "2024-01-02 14:11:01.380524: Epoch time: 125.57 s\n",
      "2024-01-02 14:11:02.674532: \n",
      "2024-01-02 14:11:02.681118: Epoch 263\n",
      "2024-01-02 14:11:02.688708: Current learning rate: 0.0076\n",
      "2024-01-02 14:13:08.182721: train_loss -0.9066\n",
      "2024-01-02 14:13:08.189729: val_loss -0.8324\n",
      "2024-01-02 14:13:08.195719: Pseudo dice [0.911, 0.9508, 0.9449]\n",
      "2024-01-02 14:13:08.200719: Epoch time: 125.51 s\n",
      "2024-01-02 14:13:09.281024: \n",
      "2024-01-02 14:13:09.289968: Epoch 264\n",
      "2024-01-02 14:13:09.294039: Current learning rate: 0.00759\n",
      "2024-01-02 14:15:14.708215: train_loss -0.9072\n",
      "2024-01-02 14:15:14.714218: val_loss -0.8376\n",
      "2024-01-02 14:15:14.721216: Pseudo dice [0.9134, 0.9507, 0.9432]\n",
      "2024-01-02 14:15:14.727222: Epoch time: 125.43 s\n",
      "2024-01-02 14:15:15.812603: \n",
      "2024-01-02 14:15:15.819675: Epoch 265\n",
      "2024-01-02 14:15:15.823661: Current learning rate: 0.00758\n",
      "2024-01-02 14:17:21.705967: train_loss -0.9063\n",
      "2024-01-02 14:17:21.714973: val_loss -0.8347\n",
      "2024-01-02 14:17:21.720973: Pseudo dice [0.9125, 0.9518, 0.9416]\n",
      "2024-01-02 14:17:21.726974: Epoch time: 125.89 s\n",
      "2024-01-02 14:17:22.925672: \n",
      "2024-01-02 14:17:22.933672: Epoch 266\n",
      "2024-01-02 14:17:22.938672: Current learning rate: 0.00757\n",
      "2024-01-02 14:19:28.451748: train_loss -0.9055\n",
      "2024-01-02 14:19:28.457748: val_loss -0.8348\n",
      "2024-01-02 14:19:28.462748: Pseudo dice [0.9129, 0.9522, 0.9445]\n",
      "2024-01-02 14:19:28.467824: Epoch time: 125.53 s\n",
      "2024-01-02 14:19:29.676133: \n",
      "2024-01-02 14:19:29.685297: Epoch 267\n",
      "2024-01-02 14:19:29.690237: Current learning rate: 0.00756\n",
      "2024-01-02 14:21:35.007066: train_loss -0.9097\n",
      "2024-01-02 14:21:35.014066: val_loss -0.8395\n",
      "2024-01-02 14:21:35.019065: Pseudo dice [0.9149, 0.9523, 0.945]\n",
      "2024-01-02 14:21:35.024065: Epoch time: 125.33 s\n",
      "2024-01-02 14:21:36.114717: \n",
      "2024-01-02 14:21:36.120719: Epoch 268\n",
      "2024-01-02 14:21:36.126118: Current learning rate: 0.00755\n",
      "2024-01-02 14:23:41.948776: train_loss -0.9064\n",
      "2024-01-02 14:23:41.959783: val_loss -0.835\n",
      "2024-01-02 14:23:41.967777: Pseudo dice [0.9159, 0.9503, 0.9426]\n",
      "2024-01-02 14:23:41.972777: Epoch time: 125.84 s\n",
      "2024-01-02 14:23:43.193604: \n",
      "2024-01-02 14:23:43.204383: Epoch 269\n",
      "2024-01-02 14:23:43.216379: Current learning rate: 0.00754\n",
      "2024-01-02 14:25:48.750782: train_loss -0.906\n",
      "2024-01-02 14:25:48.758784: val_loss -0.8294\n",
      "2024-01-02 14:25:48.768284: Pseudo dice [0.9107, 0.9509, 0.9455]\n",
      "2024-01-02 14:25:48.773289: Epoch time: 125.56 s\n",
      "2024-01-02 14:25:50.291711: \n",
      "2024-01-02 14:25:50.298289: Epoch 270\n",
      "2024-01-02 14:25:50.303344: Current learning rate: 0.00753\n",
      "2024-01-02 14:27:56.146476: train_loss -0.9048\n",
      "2024-01-02 14:27:56.151476: val_loss -0.835\n",
      "2024-01-02 14:27:56.157986: Pseudo dice [0.9118, 0.9502, 0.9437]\n",
      "2024-01-02 14:27:56.163998: Epoch time: 125.86 s\n",
      "2024-01-02 14:27:57.473102: \n",
      "2024-01-02 14:27:57.485017: Epoch 271\n",
      "2024-01-02 14:27:57.489061: Current learning rate: 0.00752\n",
      "2024-01-02 14:30:03.093052: train_loss -0.9061\n",
      "2024-01-02 14:30:03.102054: val_loss -0.8338\n",
      "2024-01-02 14:30:03.108054: Pseudo dice [0.9138, 0.9503, 0.9453]\n",
      "2024-01-02 14:30:03.113051: Epoch time: 125.62 s\n",
      "2024-01-02 14:30:04.260995: \n",
      "2024-01-02 14:30:04.266525: Epoch 272\n",
      "2024-01-02 14:30:04.281587: Current learning rate: 0.00751\n",
      "2024-01-02 14:32:09.839488: train_loss -0.9054\n",
      "2024-01-02 14:32:09.847996: val_loss -0.8353\n",
      "2024-01-02 14:32:09.853997: Pseudo dice [0.916, 0.9518, 0.9439]\n",
      "2024-01-02 14:32:09.859998: Epoch time: 125.58 s\n",
      "2024-01-02 14:32:10.970211: \n",
      "2024-01-02 14:32:10.975224: Epoch 273\n",
      "2024-01-02 14:32:10.980220: Current learning rate: 0.00751\n",
      "2024-01-02 14:34:16.553907: train_loss -0.9066\n",
      "2024-01-02 14:34:16.561908: val_loss -0.8363\n",
      "2024-01-02 14:34:16.569908: Pseudo dice [0.914, 0.9519, 0.9426]\n",
      "2024-01-02 14:34:16.578909: Epoch time: 125.59 s\n",
      "2024-01-02 14:34:17.828295: \n",
      "2024-01-02 14:34:17.835295: Epoch 274\n",
      "2024-01-02 14:34:17.840146: Current learning rate: 0.0075\n",
      "2024-01-02 14:36:23.569330: train_loss -0.9079\n",
      "2024-01-02 14:36:23.576333: val_loss -0.8311\n",
      "2024-01-02 14:36:23.581335: Pseudo dice [0.912, 0.9505, 0.943]\n",
      "2024-01-02 14:36:23.586332: Epoch time: 125.74 s\n",
      "2024-01-02 14:36:24.717742: \n",
      "2024-01-02 14:36:24.724768: Epoch 275\n",
      "2024-01-02 14:36:24.729763: Current learning rate: 0.00749\n",
      "2024-01-02 14:38:30.285737: train_loss -0.9056\n",
      "2024-01-02 14:38:30.291733: val_loss -0.8319\n",
      "2024-01-02 14:38:30.299720: Pseudo dice [0.913, 0.9507, 0.9454]\n",
      "2024-01-02 14:38:30.306721: Epoch time: 125.57 s\n",
      "2024-01-02 14:38:31.468440: \n",
      "2024-01-02 14:38:31.476441: Epoch 276\n",
      "2024-01-02 14:38:31.481514: Current learning rate: 0.00748\n",
      "2024-01-02 14:40:37.016645: train_loss -0.9057\n",
      "2024-01-02 14:40:37.023649: val_loss -0.8287\n",
      "2024-01-02 14:40:37.029649: Pseudo dice [0.9123, 0.9502, 0.945]\n",
      "2024-01-02 14:40:37.037157: Epoch time: 125.55 s\n",
      "2024-01-02 14:40:38.227665: \n",
      "2024-01-02 14:40:38.233723: Epoch 277\n",
      "2024-01-02 14:40:38.240792: Current learning rate: 0.00747\n",
      "2024-01-02 14:42:44.172862: train_loss -0.9097\n",
      "2024-01-02 14:42:44.178863: val_loss -0.8351\n",
      "2024-01-02 14:42:44.185871: Pseudo dice [0.9133, 0.9509, 0.9419]\n",
      "2024-01-02 14:42:44.190868: Epoch time: 125.95 s\n",
      "2024-01-02 14:42:45.750780: \n",
      "2024-01-02 14:42:45.756780: Epoch 278\n",
      "2024-01-02 14:42:45.760780: Current learning rate: 0.00746\n",
      "2024-01-02 14:44:51.435078: train_loss -0.9073\n",
      "2024-01-02 14:44:51.442076: val_loss -0.8338\n",
      "2024-01-02 14:44:51.448087: Pseudo dice [0.9134, 0.9517, 0.9447]\n",
      "2024-01-02 14:44:51.454077: Epoch time: 125.69 s\n",
      "2024-01-02 14:44:52.595852: \n",
      "2024-01-02 14:44:52.604639: Epoch 279\n",
      "2024-01-02 14:44:52.612576: Current learning rate: 0.00745\n",
      "2024-01-02 14:46:58.530245: train_loss -0.9081\n",
      "2024-01-02 14:46:58.539245: val_loss -0.8279\n",
      "2024-01-02 14:46:58.547246: Pseudo dice [0.9101, 0.9491, 0.9446]\n",
      "2024-01-02 14:46:58.554248: Epoch time: 125.94 s\n",
      "2024-01-02 14:46:59.783717: \n",
      "2024-01-02 14:46:59.789716: Epoch 280\n",
      "2024-01-02 14:46:59.793716: Current learning rate: 0.00744\n",
      "2024-01-02 14:49:05.537704: train_loss -0.9069\n",
      "2024-01-02 14:49:05.544709: val_loss -0.8396\n",
      "2024-01-02 14:49:05.550709: Pseudo dice [0.912, 0.9522, 0.9439]\n",
      "2024-01-02 14:49:05.555709: Epoch time: 125.75 s\n",
      "2024-01-02 14:49:06.739989: \n",
      "2024-01-02 14:49:06.745759: Epoch 281\n",
      "2024-01-02 14:49:06.750802: Current learning rate: 0.00743\n",
      "2024-01-02 14:51:12.190524: train_loss -0.9075\n",
      "2024-01-02 14:51:12.198524: val_loss -0.8309\n",
      "2024-01-02 14:51:12.204526: Pseudo dice [0.9144, 0.9505, 0.9441]\n",
      "2024-01-02 14:51:12.212536: Epoch time: 125.45 s\n",
      "2024-01-02 14:51:13.412694: \n",
      "2024-01-02 14:51:13.418207: Epoch 282\n",
      "2024-01-02 14:51:13.422584: Current learning rate: 0.00742\n",
      "2024-01-02 14:53:18.806522: train_loss -0.9106\n",
      "2024-01-02 14:53:18.813521: val_loss -0.8288\n",
      "2024-01-02 14:53:18.819028: Pseudo dice [0.9129, 0.9512, 0.9431]\n",
      "2024-01-02 14:53:18.824029: Epoch time: 125.39 s\n",
      "2024-01-02 14:53:19.928117: \n",
      "2024-01-02 14:53:19.935558: Epoch 283\n",
      "2024-01-02 14:53:19.939553: Current learning rate: 0.00741\n",
      "2024-01-02 14:55:25.722378: train_loss -0.9079\n",
      "2024-01-02 14:55:25.730377: val_loss -0.8327\n",
      "2024-01-02 14:55:25.736377: Pseudo dice [0.9113, 0.9506, 0.9445]\n",
      "2024-01-02 14:55:25.741377: Epoch time: 125.8 s\n",
      "2024-01-02 14:55:26.949431: \n",
      "2024-01-02 14:55:26.959234: Epoch 284\n",
      "2024-01-02 14:55:26.967291: Current learning rate: 0.0074\n",
      "2024-01-02 14:57:32.595070: train_loss -0.9055\n",
      "2024-01-02 14:57:32.603070: val_loss -0.8327\n",
      "2024-01-02 14:57:32.608069: Pseudo dice [0.912, 0.9514, 0.9441]\n",
      "2024-01-02 14:57:32.613071: Epoch time: 125.65 s\n",
      "2024-01-02 14:57:33.791823: \n",
      "2024-01-02 14:57:33.803289: Epoch 285\n",
      "2024-01-02 14:57:33.807218: Current learning rate: 0.00739\n",
      "2024-01-02 14:59:39.153169: train_loss -0.9089\n",
      "2024-01-02 14:59:39.159255: val_loss -0.8337\n",
      "2024-01-02 14:59:39.164257: Pseudo dice [0.9114, 0.9506, 0.945]\n",
      "2024-01-02 14:59:39.169173: Epoch time: 125.36 s\n",
      "2024-01-02 14:59:40.460262: \n",
      "2024-01-02 14:59:40.471584: Epoch 286\n",
      "2024-01-02 14:59:40.475634: Current learning rate: 0.00738\n",
      "2024-01-02 15:01:45.739163: train_loss -0.9114\n",
      "2024-01-02 15:01:45.745153: val_loss -0.8345\n",
      "2024-01-02 15:01:45.751688: Pseudo dice [0.9099, 0.9512, 0.9439]\n",
      "2024-01-02 15:01:45.756682: Epoch time: 125.28 s\n",
      "2024-01-02 15:01:46.927721: \n",
      "2024-01-02 15:01:46.933725: Epoch 287\n",
      "2024-01-02 15:01:46.938726: Current learning rate: 0.00738\n",
      "2024-01-02 15:03:52.560816: train_loss -0.9071\n",
      "2024-01-02 15:03:52.567818: val_loss -0.8287\n",
      "2024-01-02 15:03:52.576820: Pseudo dice [0.9117, 0.9502, 0.9431]\n",
      "2024-01-02 15:03:52.583820: Epoch time: 125.63 s\n",
      "2024-01-02 15:03:53.780254: \n",
      "2024-01-02 15:03:53.786461: Epoch 288\n",
      "2024-01-02 15:03:53.790533: Current learning rate: 0.00737\n",
      "2024-01-02 15:05:59.481797: train_loss -0.9077\n",
      "2024-01-02 15:05:59.489797: val_loss -0.8371\n",
      "2024-01-02 15:05:59.496796: Pseudo dice [0.9134, 0.9513, 0.9448]\n",
      "2024-01-02 15:05:59.503797: Epoch time: 125.7 s\n",
      "2024-01-02 15:06:00.722173: \n",
      "2024-01-02 15:06:00.732114: Epoch 289\n",
      "2024-01-02 15:06:00.737181: Current learning rate: 0.00736\n",
      "2024-01-02 15:08:06.305552: train_loss -0.9059\n",
      "2024-01-02 15:08:06.313552: val_loss -0.8322\n",
      "2024-01-02 15:08:06.317555: Pseudo dice [0.9131, 0.9508, 0.9442]\n",
      "2024-01-02 15:08:06.322555: Epoch time: 125.58 s\n",
      "2024-01-02 15:08:07.413908: \n",
      "2024-01-02 15:08:07.423386: Epoch 290\n",
      "2024-01-02 15:08:07.431454: Current learning rate: 0.00735\n",
      "2024-01-02 15:10:12.998846: train_loss -0.908\n",
      "2024-01-02 15:10:13.007365: val_loss -0.8361\n",
      "2024-01-02 15:10:13.015354: Pseudo dice [0.9142, 0.9513, 0.9422]\n",
      "2024-01-02 15:10:13.021868: Epoch time: 125.59 s\n",
      "2024-01-02 15:10:14.158652: \n",
      "2024-01-02 15:10:14.171427: Epoch 291\n",
      "2024-01-02 15:10:14.179501: Current learning rate: 0.00734\n",
      "2024-01-02 15:12:20.180859: train_loss -0.9065\n",
      "2024-01-02 15:12:20.188859: val_loss -0.8352\n",
      "2024-01-02 15:12:20.193859: Pseudo dice [0.915, 0.9515, 0.9449]\n",
      "2024-01-02 15:12:20.199858: Epoch time: 126.03 s\n",
      "2024-01-02 15:12:21.402689: \n",
      "2024-01-02 15:12:21.410691: Epoch 292\n",
      "2024-01-02 15:12:21.418689: Current learning rate: 0.00733\n",
      "2024-01-02 15:14:27.009164: train_loss -0.9064\n",
      "2024-01-02 15:14:27.019164: val_loss -0.8337\n",
      "2024-01-02 15:14:27.025673: Pseudo dice [0.9115, 0.9503, 0.9439]\n",
      "2024-01-02 15:14:27.031674: Epoch time: 125.61 s\n",
      "2024-01-02 15:14:28.258892: \n",
      "2024-01-02 15:14:28.270441: Epoch 293\n",
      "2024-01-02 15:14:28.274514: Current learning rate: 0.00732\n",
      "2024-01-02 15:16:34.169169: train_loss -0.9062\n",
      "2024-01-02 15:16:34.176677: val_loss -0.832\n",
      "2024-01-02 15:16:34.183683: Pseudo dice [0.9114, 0.9512, 0.9436]\n",
      "2024-01-02 15:16:34.189692: Epoch time: 125.91 s\n",
      "2024-01-02 15:16:35.450526: \n",
      "2024-01-02 15:16:35.456888: Epoch 294\n",
      "2024-01-02 15:16:35.461465: Current learning rate: 0.00731\n",
      "2024-01-02 15:18:40.746880: train_loss -0.9069\n",
      "2024-01-02 15:18:40.753879: val_loss -0.8328\n",
      "2024-01-02 15:18:40.759879: Pseudo dice [0.9126, 0.9507, 0.9435]\n",
      "2024-01-02 15:18:40.764879: Epoch time: 125.3 s\n",
      "2024-01-02 15:18:41.862356: \n",
      "2024-01-02 15:18:41.869635: Epoch 295\n",
      "2024-01-02 15:18:41.874671: Current learning rate: 0.0073\n",
      "2024-01-02 15:20:47.511275: train_loss -0.9084\n",
      "2024-01-02 15:20:47.521277: val_loss -0.8252\n",
      "2024-01-02 15:20:47.529275: Pseudo dice [0.9101, 0.9489, 0.9443]\n",
      "2024-01-02 15:20:47.535280: Epoch time: 125.65 s\n",
      "2024-01-02 15:20:48.840399: \n",
      "2024-01-02 15:20:48.848469: Epoch 296\n",
      "2024-01-02 15:20:48.853462: Current learning rate: 0.00729\n",
      "2024-01-02 15:22:54.248940: train_loss -0.9091\n",
      "2024-01-02 15:22:54.258940: val_loss -0.8288\n",
      "2024-01-02 15:22:54.265945: Pseudo dice [0.9111, 0.9515, 0.9451]\n",
      "2024-01-02 15:22:54.273942: Epoch time: 125.41 s\n",
      "2024-01-02 15:22:55.426176: \n",
      "2024-01-02 15:22:55.432335: Epoch 297\n",
      "2024-01-02 15:22:55.440337: Current learning rate: 0.00728\n",
      "2024-01-02 15:25:01.081288: train_loss -0.908\n",
      "2024-01-02 15:25:01.087798: val_loss -0.838\n",
      "2024-01-02 15:25:01.095798: Pseudo dice [0.9126, 0.9516, 0.9445]\n",
      "2024-01-02 15:25:01.100804: Epoch time: 125.66 s\n",
      "2024-01-02 15:25:02.270442: \n",
      "2024-01-02 15:25:02.276859: Epoch 298\n",
      "2024-01-02 15:25:02.280861: Current learning rate: 0.00727\n",
      "2024-01-02 15:27:07.920621: train_loss -0.9065\n",
      "2024-01-02 15:27:07.928628: val_loss -0.8278\n",
      "2024-01-02 15:27:07.936627: Pseudo dice [0.9101, 0.9495, 0.9461]\n",
      "2024-01-02 15:27:07.943136: Epoch time: 125.65 s\n",
      "2024-01-02 15:27:09.147597: \n",
      "2024-01-02 15:27:09.153714: Epoch 299\n",
      "2024-01-02 15:27:09.163713: Current learning rate: 0.00726\n",
      "2024-01-02 15:29:14.986683: train_loss -0.9068\n",
      "2024-01-02 15:29:14.994688: val_loss -0.8353\n",
      "2024-01-02 15:29:15.001688: Pseudo dice [0.9148, 0.9517, 0.9447]\n",
      "2024-01-02 15:29:15.010216: Epoch time: 125.84 s\n",
      "2024-01-02 15:29:16.586358: \n",
      "2024-01-02 15:29:16.592605: Epoch 300\n",
      "2024-01-02 15:29:16.598681: Current learning rate: 0.00725\n",
      "2024-01-02 15:31:22.097880: train_loss -0.9064\n",
      "2024-01-02 15:31:22.104872: val_loss -0.8323\n",
      "2024-01-02 15:31:22.111875: Pseudo dice [0.9136, 0.9509, 0.9456]\n",
      "2024-01-02 15:31:22.116872: Epoch time: 125.51 s\n",
      "2024-01-02 15:31:23.277441: \n",
      "2024-01-02 15:31:23.284442: Epoch 301\n",
      "2024-01-02 15:31:23.288440: Current learning rate: 0.00724\n",
      "2024-01-02 15:33:29.120303: train_loss -0.9109\n",
      "2024-01-02 15:33:29.129307: val_loss -0.8364\n",
      "2024-01-02 15:33:29.136307: Pseudo dice [0.9117, 0.9511, 0.9448]\n",
      "2024-01-02 15:33:29.141816: Epoch time: 125.85 s\n",
      "2024-01-02 15:33:30.600301: \n",
      "2024-01-02 15:33:30.605831: Epoch 302\n",
      "2024-01-02 15:33:30.613662: Current learning rate: 0.00724\n",
      "2024-01-02 15:35:36.062536: train_loss -0.9069\n",
      "2024-01-02 15:35:36.068536: val_loss -0.8311\n",
      "2024-01-02 15:35:36.074538: Pseudo dice [0.9143, 0.9516, 0.9433]\n",
      "2024-01-02 15:35:36.080536: Epoch time: 125.46 s\n",
      "2024-01-02 15:35:37.188497: \n",
      "2024-01-02 15:35:37.194063: Epoch 303\n",
      "2024-01-02 15:35:37.201429: Current learning rate: 0.00723\n",
      "2024-01-02 15:37:42.677332: train_loss -0.9077\n",
      "2024-01-02 15:37:42.684341: val_loss -0.8354\n",
      "2024-01-02 15:37:42.692342: Pseudo dice [0.9135, 0.9509, 0.9451]\n",
      "2024-01-02 15:37:42.701339: Epoch time: 125.49 s\n",
      "2024-01-02 15:37:43.779762: \n",
      "2024-01-02 15:37:43.785762: Epoch 304\n",
      "2024-01-02 15:37:43.789762: Current learning rate: 0.00722\n",
      "2024-01-02 15:39:49.358796: train_loss -0.9075\n",
      "2024-01-02 15:39:49.368796: val_loss -0.8361\n",
      "2024-01-02 15:39:49.373791: Pseudo dice [0.9121, 0.9525, 0.9424]\n",
      "2024-01-02 15:39:49.378869: Epoch time: 125.58 s\n",
      "2024-01-02 15:39:50.494181: \n",
      "2024-01-02 15:39:50.502787: Epoch 305\n",
      "2024-01-02 15:39:50.507853: Current learning rate: 0.00721\n",
      "2024-01-02 15:41:55.895917: train_loss -0.9097\n",
      "2024-01-02 15:41:55.901913: val_loss -0.8356\n",
      "2024-01-02 15:41:55.906930: Pseudo dice [0.9129, 0.951, 0.9409]\n",
      "2024-01-02 15:41:55.910928: Epoch time: 125.4 s\n",
      "2024-01-02 15:41:56.962989: \n",
      "2024-01-02 15:41:56.968930: Epoch 306\n",
      "2024-01-02 15:41:56.973928: Current learning rate: 0.0072\n",
      "2024-01-02 15:44:02.485013: train_loss -0.9098\n",
      "2024-01-02 15:44:02.497021: val_loss -0.8349\n",
      "2024-01-02 15:44:02.505546: Pseudo dice [0.9112, 0.9505, 0.9458]\n",
      "2024-01-02 15:44:02.511053: Epoch time: 125.52 s\n",
      "2024-01-02 15:44:03.732698: \n",
      "2024-01-02 15:44:03.742954: Epoch 307\n",
      "2024-01-02 15:44:03.750012: Current learning rate: 0.00719\n",
      "2024-01-02 15:46:09.266294: train_loss -0.9082\n",
      "2024-01-02 15:46:09.273294: val_loss -0.8233\n",
      "2024-01-02 15:46:09.280294: Pseudo dice [0.9093, 0.948, 0.9422]\n",
      "2024-01-02 15:46:09.287295: Epoch time: 125.53 s\n",
      "2024-01-02 15:46:10.387366: \n",
      "2024-01-02 15:46:10.393730: Epoch 308\n",
      "2024-01-02 15:46:10.398746: Current learning rate: 0.00718\n",
      "2024-01-02 15:48:16.433644: train_loss -0.9083\n",
      "2024-01-02 15:48:16.442645: val_loss -0.8356\n",
      "2024-01-02 15:48:16.449646: Pseudo dice [0.9127, 0.9521, 0.9454]\n",
      "2024-01-02 15:48:16.456646: Epoch time: 126.05 s\n",
      "2024-01-02 15:48:17.901740: \n",
      "2024-01-02 15:48:17.906809: Epoch 309\n",
      "2024-01-02 15:48:17.914376: Current learning rate: 0.00717\n",
      "2024-01-02 15:50:23.372180: train_loss -0.9087\n",
      "2024-01-02 15:50:23.380181: val_loss -0.8325\n",
      "2024-01-02 15:50:23.387181: Pseudo dice [0.9109, 0.9509, 0.9449]\n",
      "2024-01-02 15:50:23.393180: Epoch time: 125.47 s\n",
      "2024-01-02 15:50:24.530032: \n",
      "2024-01-02 15:50:24.536216: Epoch 310\n",
      "2024-01-02 15:50:24.541132: Current learning rate: 0.00716\n",
      "2024-01-02 15:52:29.968743: train_loss -0.9084\n",
      "2024-01-02 15:52:29.978137: val_loss -0.8292\n",
      "2024-01-02 15:52:29.986434: Pseudo dice [0.9115, 0.9499, 0.9453]\n",
      "2024-01-02 15:52:29.990449: Epoch time: 125.44 s\n",
      "2024-01-02 15:52:31.173944: \n",
      "2024-01-02 15:52:31.179880: Epoch 311\n",
      "2024-01-02 15:52:31.186880: Current learning rate: 0.00715\n",
      "2024-01-02 15:54:36.625027: train_loss -0.9065\n",
      "2024-01-02 15:54:36.634027: val_loss -0.8324\n",
      "2024-01-02 15:54:36.639031: Pseudo dice [0.9102, 0.9504, 0.9428]\n",
      "2024-01-02 15:54:36.644031: Epoch time: 125.45 s\n",
      "2024-01-02 15:54:37.871578: \n",
      "2024-01-02 15:54:37.878578: Epoch 312\n",
      "2024-01-02 15:54:37.883569: Current learning rate: 0.00714\n",
      "2024-01-02 15:56:43.448688: train_loss -0.9089\n",
      "2024-01-02 15:56:43.455690: val_loss -0.8297\n",
      "2024-01-02 15:56:43.462688: Pseudo dice [0.9101, 0.9502, 0.9445]\n",
      "2024-01-02 15:56:43.468692: Epoch time: 125.58 s\n",
      "2024-01-02 15:56:44.607560: \n",
      "2024-01-02 15:56:44.613039: Epoch 313\n",
      "2024-01-02 15:56:44.617004: Current learning rate: 0.00713\n",
      "2024-01-02 15:58:50.246312: train_loss -0.9095\n",
      "2024-01-02 15:58:50.254309: val_loss -0.8297\n",
      "2024-01-02 15:58:50.259308: Pseudo dice [0.9112, 0.9508, 0.9448]\n",
      "2024-01-02 15:58:50.264312: Epoch time: 125.64 s\n",
      "2024-01-02 15:58:51.458827: \n",
      "2024-01-02 15:58:51.469847: Epoch 314\n",
      "2024-01-02 15:58:51.473926: Current learning rate: 0.00712\n",
      "2024-01-02 16:00:57.385244: train_loss -0.9054\n",
      "2024-01-02 16:00:57.393243: val_loss -0.8313\n",
      "2024-01-02 16:00:57.402761: Pseudo dice [0.9088, 0.9504, 0.9434]\n",
      "2024-01-02 16:00:57.409759: Epoch time: 125.93 s\n",
      "2024-01-02 16:00:58.723423: \n",
      "2024-01-02 16:00:58.731411: Epoch 315\n",
      "2024-01-02 16:00:58.736217: Current learning rate: 0.00711\n",
      "2024-01-02 16:03:04.377112: train_loss -0.906\n",
      "2024-01-02 16:03:04.385236: val_loss -0.8334\n",
      "2024-01-02 16:03:04.390236: Pseudo dice [0.9127, 0.9511, 0.9441]\n",
      "2024-01-02 16:03:04.394237: Epoch time: 125.65 s\n",
      "2024-01-02 16:03:05.566002: \n",
      "2024-01-02 16:03:05.571650: Epoch 316\n",
      "2024-01-02 16:03:05.576650: Current learning rate: 0.0071\n",
      "2024-01-02 16:05:11.034620: train_loss -0.9115\n",
      "2024-01-02 16:05:11.040620: val_loss -0.8339\n",
      "2024-01-02 16:05:11.048620: Pseudo dice [0.9122, 0.9501, 0.9449]\n",
      "2024-01-02 16:05:11.054624: Epoch time: 125.47 s\n",
      "2024-01-02 16:05:12.573517: \n",
      "2024-01-02 16:05:12.584394: Epoch 317\n",
      "2024-01-02 16:05:12.589407: Current learning rate: 0.0071\n",
      "2024-01-02 16:07:18.032011: train_loss -0.9075\n",
      "2024-01-02 16:07:18.041517: val_loss -0.8289\n",
      "2024-01-02 16:07:18.048025: Pseudo dice [0.912, 0.9497, 0.9437]\n",
      "2024-01-02 16:07:18.054025: Epoch time: 125.46 s\n",
      "2024-01-02 16:07:19.251899: \n",
      "2024-01-02 16:07:19.257330: Epoch 318\n",
      "2024-01-02 16:07:19.265390: Current learning rate: 0.00709\n",
      "2024-01-02 16:09:24.911600: train_loss -0.905\n",
      "2024-01-02 16:09:24.918599: val_loss -0.8278\n",
      "2024-01-02 16:09:24.923599: Pseudo dice [0.912, 0.9485, 0.9406]\n",
      "2024-01-02 16:09:24.930598: Epoch time: 125.66 s\n",
      "2024-01-02 16:09:26.048359: \n",
      "2024-01-02 16:09:26.055138: Epoch 319\n",
      "2024-01-02 16:09:26.064137: Current learning rate: 0.00708\n",
      "2024-01-02 16:11:31.701928: train_loss -0.9097\n",
      "2024-01-02 16:11:31.710927: val_loss -0.8362\n",
      "2024-01-02 16:11:31.717927: Pseudo dice [0.9138, 0.9524, 0.9432]\n",
      "2024-01-02 16:11:31.724926: Epoch time: 125.65 s\n",
      "2024-01-02 16:11:33.069583: \n",
      "2024-01-02 16:11:33.075504: Epoch 320\n",
      "2024-01-02 16:11:33.079512: Current learning rate: 0.00707\n",
      "2024-01-02 16:13:38.611506: train_loss -0.9098\n",
      "2024-01-02 16:13:38.619504: val_loss -0.8308\n",
      "2024-01-02 16:13:38.625511: Pseudo dice [0.9117, 0.9499, 0.9439]\n",
      "2024-01-02 16:13:38.633507: Epoch time: 125.54 s\n",
      "2024-01-02 16:13:40.025797: \n",
      "2024-01-02 16:13:40.034796: Epoch 321\n",
      "2024-01-02 16:13:40.039340: Current learning rate: 0.00706\n",
      "2024-01-02 16:15:45.736031: train_loss -0.9082\n",
      "2024-01-02 16:15:45.744549: val_loss -0.8372\n",
      "2024-01-02 16:15:45.752537: Pseudo dice [0.9148, 0.9508, 0.9435]\n",
      "2024-01-02 16:15:45.758537: Epoch time: 125.71 s\n",
      "2024-01-02 16:15:46.975280: \n",
      "2024-01-02 16:15:46.982322: Epoch 322\n",
      "2024-01-02 16:15:46.986340: Current learning rate: 0.00705\n",
      "2024-01-02 16:17:52.368707: train_loss -0.9103\n",
      "2024-01-02 16:17:52.375709: val_loss -0.8362\n",
      "2024-01-02 16:17:52.381707: Pseudo dice [0.9131, 0.9519, 0.9463]\n",
      "2024-01-02 16:17:52.386706: Epoch time: 125.39 s\n",
      "2024-01-02 16:17:53.580304: \n",
      "2024-01-02 16:17:53.585915: Epoch 323\n",
      "2024-01-02 16:17:53.591932: Current learning rate: 0.00704\n",
      "2024-01-02 16:19:59.183580: train_loss -0.9085\n",
      "2024-01-02 16:19:59.192581: val_loss -0.8313\n",
      "2024-01-02 16:19:59.197580: Pseudo dice [0.9125, 0.9508, 0.9445]\n",
      "2024-01-02 16:19:59.203580: Epoch time: 125.6 s\n",
      "2024-01-02 16:20:00.411765: \n",
      "2024-01-02 16:20:00.417156: Epoch 324\n",
      "2024-01-02 16:20:00.421783: Current learning rate: 0.00703\n",
      "2024-01-02 16:22:05.860842: train_loss -0.9095\n",
      "2024-01-02 16:22:05.868840: val_loss -0.8346\n",
      "2024-01-02 16:22:05.876841: Pseudo dice [0.9119, 0.9517, 0.9439]\n",
      "2024-01-02 16:22:05.881841: Epoch time: 125.45 s\n",
      "2024-01-02 16:22:06.978365: \n",
      "2024-01-02 16:22:06.986440: Epoch 325\n",
      "2024-01-02 16:22:06.992373: Current learning rate: 0.00702\n",
      "2024-01-02 16:24:12.285763: train_loss -0.9125\n",
      "2024-01-02 16:24:12.296763: val_loss -0.8367\n",
      "2024-01-02 16:24:12.306762: Pseudo dice [0.9155, 0.9527, 0.9436]\n",
      "2024-01-02 16:24:12.311763: Epoch time: 125.31 s\n",
      "2024-01-02 16:24:13.468499: \n",
      "2024-01-02 16:24:13.475021: Epoch 326\n",
      "2024-01-02 16:24:13.480029: Current learning rate: 0.00701\n",
      "2024-01-02 16:26:19.127421: train_loss -0.9094\n",
      "2024-01-02 16:26:19.133928: val_loss -0.8294\n",
      "2024-01-02 16:26:19.141933: Pseudo dice [0.9126, 0.9506, 0.9439]\n",
      "2024-01-02 16:26:19.149938: Epoch time: 125.66 s\n",
      "2024-01-02 16:26:20.404854: \n",
      "2024-01-02 16:26:20.413234: Epoch 327\n",
      "2024-01-02 16:26:20.418274: Current learning rate: 0.007\n",
      "2024-01-02 16:28:26.037471: train_loss -0.9113\n",
      "2024-01-02 16:28:26.048983: val_loss -0.8358\n",
      "2024-01-02 16:28:26.058977: Pseudo dice [0.9127, 0.9524, 0.9459]\n",
      "2024-01-02 16:28:26.067977: Epoch time: 125.63 s\n",
      "2024-01-02 16:28:27.455136: \n",
      "2024-01-02 16:28:27.461830: Epoch 328\n",
      "2024-01-02 16:28:27.466846: Current learning rate: 0.00699\n",
      "2024-01-02 16:30:33.161694: train_loss -0.9102\n",
      "2024-01-02 16:30:33.169695: val_loss -0.8312\n",
      "2024-01-02 16:30:33.174696: Pseudo dice [0.9128, 0.9503, 0.9436]\n",
      "2024-01-02 16:30:33.180696: Epoch time: 125.71 s\n",
      "2024-01-02 16:30:34.419091: \n",
      "2024-01-02 16:30:34.424091: Epoch 329\n",
      "2024-01-02 16:30:34.429619: Current learning rate: 0.00698\n",
      "2024-01-02 16:32:39.946135: train_loss -0.9104\n",
      "2024-01-02 16:32:39.953136: val_loss -0.8284\n",
      "2024-01-02 16:32:39.958136: Pseudo dice [0.9136, 0.9505, 0.9428]\n",
      "2024-01-02 16:32:39.964139: Epoch time: 125.53 s\n",
      "2024-01-02 16:32:41.056556: \n",
      "2024-01-02 16:32:41.062830: Epoch 330\n",
      "2024-01-02 16:32:41.070889: Current learning rate: 0.00697\n",
      "2024-01-02 16:34:46.407281: train_loss -0.9107\n",
      "2024-01-02 16:34:46.413281: val_loss -0.8383\n",
      "2024-01-02 16:34:46.418281: Pseudo dice [0.9146, 0.9523, 0.9448]\n",
      "2024-01-02 16:34:46.433910: Epoch time: 125.35 s\n",
      "2024-01-02 16:34:47.598874: \n",
      "2024-01-02 16:34:47.604907: Epoch 331\n",
      "2024-01-02 16:34:47.612628: Current learning rate: 0.00696\n",
      "2024-01-02 16:36:53.043810: train_loss -0.9112\n",
      "2024-01-02 16:36:53.051313: val_loss -0.8307\n",
      "2024-01-02 16:36:53.057318: Pseudo dice [0.915, 0.9505, 0.945]\n",
      "2024-01-02 16:36:53.062319: Epoch time: 125.45 s\n",
      "2024-01-02 16:36:54.531054: \n",
      "2024-01-02 16:36:54.536121: Epoch 332\n",
      "2024-01-02 16:36:54.541122: Current learning rate: 0.00696\n",
      "2024-01-02 16:39:00.193280: train_loss -0.9079\n",
      "2024-01-02 16:39:00.201281: val_loss -0.8361\n",
      "2024-01-02 16:39:00.206793: Pseudo dice [0.9134, 0.9523, 0.9444]\n",
      "2024-01-02 16:39:00.211792: Epoch time: 125.66 s\n",
      "2024-01-02 16:39:01.473963: \n",
      "2024-01-02 16:39:01.480572: Epoch 333\n",
      "2024-01-02 16:39:01.485535: Current learning rate: 0.00695\n",
      "2024-01-02 16:41:07.034681: train_loss -0.9097\n",
      "2024-01-02 16:41:07.041681: val_loss -0.8314\n",
      "2024-01-02 16:41:07.046680: Pseudo dice [0.9092, 0.9502, 0.9435]\n",
      "2024-01-02 16:41:07.051679: Epoch time: 125.56 s\n",
      "2024-01-02 16:41:08.277869: \n",
      "2024-01-02 16:41:08.283497: Epoch 334\n",
      "2024-01-02 16:41:08.290561: Current learning rate: 0.00694\n",
      "2024-01-02 16:43:14.066109: train_loss -0.9087\n",
      "2024-01-02 16:43:14.073119: val_loss -0.8301\n",
      "2024-01-02 16:43:14.079114: Pseudo dice [0.9096, 0.9492, 0.9433]\n",
      "2024-01-02 16:43:14.084113: Epoch time: 125.79 s\n",
      "2024-01-02 16:43:15.309301: \n",
      "2024-01-02 16:43:15.316134: Epoch 335\n",
      "2024-01-02 16:43:15.321176: Current learning rate: 0.00693\n",
      "2024-01-02 16:45:20.540316: train_loss -0.9119\n",
      "2024-01-02 16:45:20.547325: val_loss -0.8324\n",
      "2024-01-02 16:45:20.552332: Pseudo dice [0.9133, 0.9504, 0.9454]\n",
      "2024-01-02 16:45:20.556325: Epoch time: 125.23 s\n",
      "2024-01-02 16:45:21.722807: \n",
      "2024-01-02 16:45:21.739626: Epoch 336\n",
      "2024-01-02 16:45:21.745635: Current learning rate: 0.00692\n",
      "2024-01-02 16:47:27.398001: train_loss -0.9113\n",
      "2024-01-02 16:47:27.406153: val_loss -0.8318\n",
      "2024-01-02 16:47:27.413158: Pseudo dice [0.9116, 0.9504, 0.9448]\n",
      "2024-01-02 16:47:27.419667: Epoch time: 125.68 s\n",
      "2024-01-02 16:47:28.878650: \n",
      "2024-01-02 16:47:28.884640: Epoch 337\n",
      "2024-01-02 16:47:28.888641: Current learning rate: 0.00691\n",
      "2024-01-02 16:49:34.388373: train_loss -0.9079\n",
      "2024-01-02 16:49:34.399374: val_loss -0.8339\n",
      "2024-01-02 16:49:34.405375: Pseudo dice [0.9119, 0.9514, 0.9428]\n",
      "2024-01-02 16:49:34.409390: Epoch time: 125.51 s\n",
      "2024-01-02 16:49:35.640148: \n",
      "2024-01-02 16:49:35.646141: Epoch 338\n",
      "2024-01-02 16:49:35.650149: Current learning rate: 0.0069\n",
      "2024-01-02 16:51:41.193815: train_loss -0.9106\n",
      "2024-01-02 16:51:41.201792: val_loss -0.8371\n",
      "2024-01-02 16:51:41.206856: Pseudo dice [0.9118, 0.9525, 0.9445]\n",
      "2024-01-02 16:51:41.211855: Epoch time: 125.56 s\n",
      "2024-01-02 16:51:42.413736: \n",
      "2024-01-02 16:51:42.424855: Epoch 339\n",
      "2024-01-02 16:51:42.429928: Current learning rate: 0.00689\n",
      "2024-01-02 16:53:47.854486: train_loss -0.9103\n",
      "2024-01-02 16:53:47.865492: val_loss -0.8336\n",
      "2024-01-02 16:53:47.869996: Pseudo dice [0.9114, 0.9518, 0.9428]\n",
      "2024-01-02 16:53:47.876000: Epoch time: 125.44 s\n",
      "2024-01-02 16:53:49.205105: \n",
      "2024-01-02 16:53:49.210053: Epoch 340\n",
      "2024-01-02 16:53:49.215142: Current learning rate: 0.00688\n",
      "2024-01-02 16:55:54.696934: train_loss -0.9076\n",
      "2024-01-02 16:55:54.703937: val_loss -0.8356\n",
      "2024-01-02 16:55:54.708937: Pseudo dice [0.9134, 0.9515, 0.9449]\n",
      "2024-01-02 16:55:54.714440: Epoch time: 125.49 s\n",
      "2024-01-02 16:55:56.042106: \n",
      "2024-01-02 16:55:56.048099: Epoch 341\n",
      "2024-01-02 16:55:56.055104: Current learning rate: 0.00687\n",
      "2024-01-02 16:58:01.296511: train_loss -0.9129\n",
      "2024-01-02 16:58:01.305513: val_loss -0.8316\n",
      "2024-01-02 16:58:01.313590: Pseudo dice [0.9133, 0.9512, 0.9449]\n",
      "2024-01-02 16:58:01.319657: Epoch time: 125.26 s\n",
      "2024-01-02 16:58:02.538519: \n",
      "2024-01-02 16:58:02.544509: Epoch 342\n",
      "2024-01-02 16:58:02.548517: Current learning rate: 0.00686\n",
      "2024-01-02 17:00:08.013351: train_loss -0.9093\n",
      "2024-01-02 17:00:08.022351: val_loss -0.8362\n",
      "2024-01-02 17:00:08.028351: Pseudo dice [0.9139, 0.9526, 0.9443]\n",
      "2024-01-02 17:00:08.033351: Epoch time: 125.48 s\n",
      "2024-01-02 17:00:09.183182: \n",
      "2024-01-02 17:00:09.191183: Epoch 343\n",
      "2024-01-02 17:00:09.197832: Current learning rate: 0.00685\n",
      "2024-01-02 17:02:14.613284: train_loss -0.9099\n",
      "2024-01-02 17:02:14.620284: val_loss -0.8377\n",
      "2024-01-02 17:02:14.626285: Pseudo dice [0.9117, 0.9535, 0.9449]\n",
      "2024-01-02 17:02:14.631287: Epoch time: 125.43 s\n",
      "2024-01-02 17:02:15.847460: \n",
      "2024-01-02 17:02:15.858452: Epoch 344\n",
      "2024-01-02 17:02:15.863460: Current learning rate: 0.00684\n",
      "2024-01-02 17:04:21.373639: train_loss -0.9102\n",
      "2024-01-02 17:04:21.381639: val_loss -0.8296\n",
      "2024-01-02 17:04:21.388644: Pseudo dice [0.9121, 0.951, 0.942]\n",
      "2024-01-02 17:04:21.395648: Epoch time: 125.53 s\n",
      "2024-01-02 17:04:22.613979: \n",
      "2024-01-02 17:04:22.619964: Epoch 345\n",
      "2024-01-02 17:04:22.626909: Current learning rate: 0.00683\n",
      "2024-01-02 17:06:27.929961: train_loss -0.911\n",
      "2024-01-02 17:06:27.936961: val_loss -0.8305\n",
      "2024-01-02 17:06:27.943964: Pseudo dice [0.9123, 0.9514, 0.9428]\n",
      "2024-01-02 17:06:27.951968: Epoch time: 125.32 s\n",
      "2024-01-02 17:06:29.114853: \n",
      "2024-01-02 17:06:29.122924: Epoch 346\n",
      "2024-01-02 17:06:29.127923: Current learning rate: 0.00682\n",
      "2024-01-02 17:08:35.197036: train_loss -0.9049\n",
      "2024-01-02 17:08:35.205034: val_loss -0.8316\n",
      "2024-01-02 17:08:35.211041: Pseudo dice [0.9116, 0.9513, 0.9421]\n",
      "2024-01-02 17:08:35.218036: Epoch time: 126.08 s\n",
      "2024-01-02 17:08:36.710089: \n",
      "2024-01-02 17:08:36.716090: Epoch 347\n",
      "2024-01-02 17:08:36.721159: Current learning rate: 0.00681\n",
      "2024-01-02 17:10:42.227727: train_loss -0.9062\n",
      "2024-01-02 17:10:42.234730: val_loss -0.8279\n",
      "2024-01-02 17:10:42.239733: Pseudo dice [0.9133, 0.9491, 0.9453]\n",
      "2024-01-02 17:10:42.244241: Epoch time: 125.52 s\n",
      "2024-01-02 17:10:43.501097: \n",
      "2024-01-02 17:10:43.509106: Epoch 348\n",
      "2024-01-02 17:10:43.515112: Current learning rate: 0.0068\n",
      "2024-01-02 17:12:49.302881: train_loss -0.9053\n",
      "2024-01-02 17:12:49.313868: val_loss -0.8307\n",
      "2024-01-02 17:12:49.322867: Pseudo dice [0.9115, 0.9501, 0.944]\n",
      "2024-01-02 17:12:49.329872: Epoch time: 125.8 s\n",
      "2024-01-02 17:12:50.645881: \n",
      "2024-01-02 17:12:50.654154: Epoch 349\n",
      "2024-01-02 17:12:50.661246: Current learning rate: 0.0068\n",
      "2024-01-02 17:14:56.329425: train_loss -0.9093\n",
      "2024-01-02 17:14:56.337423: val_loss -0.8324\n",
      "2024-01-02 17:14:56.343423: Pseudo dice [0.9142, 0.9523, 0.9438]\n",
      "2024-01-02 17:14:56.348426: Epoch time: 125.68 s\n",
      "2024-01-02 17:14:57.773879: \n",
      "2024-01-02 17:14:57.779840: Epoch 350\n",
      "2024-01-02 17:14:57.784785: Current learning rate: 0.00679\n",
      "2024-01-02 17:17:03.155478: train_loss -0.9132\n",
      "2024-01-02 17:17:03.167478: val_loss -0.8264\n",
      "2024-01-02 17:17:03.172477: Pseudo dice [0.913, 0.9499, 0.9434]\n",
      "2024-01-02 17:17:03.177483: Epoch time: 125.38 s\n",
      "2024-01-02 17:17:04.321124: \n",
      "2024-01-02 17:17:04.327136: Epoch 351\n",
      "2024-01-02 17:17:04.334068: Current learning rate: 0.00678\n",
      "2024-01-02 17:19:09.690585: train_loss -0.911\n",
      "2024-01-02 17:19:09.696589: val_loss -0.8126\n",
      "2024-01-02 17:19:09.703588: Pseudo dice [0.9024, 0.9467, 0.9377]\n",
      "2024-01-02 17:19:09.708587: Epoch time: 125.37 s\n",
      "2024-01-02 17:19:10.942211: \n",
      "2024-01-02 17:19:10.948228: Epoch 352\n",
      "2024-01-02 17:19:10.952026: Current learning rate: 0.00677\n",
      "2024-01-02 17:21:16.378021: train_loss -0.905\n",
      "2024-01-02 17:21:16.388021: val_loss -0.8246\n",
      "2024-01-02 17:21:16.396032: Pseudo dice [0.9082, 0.9504, 0.9417]\n",
      "2024-01-02 17:21:16.401027: Epoch time: 125.44 s\n",
      "2024-01-02 17:21:17.724212: \n",
      "2024-01-02 17:21:17.733145: Epoch 353\n",
      "2024-01-02 17:21:17.737231: Current learning rate: 0.00676\n",
      "2024-01-02 17:23:24.019325: train_loss -0.8927\n",
      "2024-01-02 17:23:24.027324: val_loss -0.8338\n",
      "2024-01-02 17:23:24.037323: Pseudo dice [0.9108, 0.9496, 0.9435]\n",
      "2024-01-02 17:23:24.043326: Epoch time: 126.3 s\n",
      "2024-01-02 17:23:25.436742: \n",
      "2024-01-02 17:23:25.445798: Epoch 354\n",
      "2024-01-02 17:23:25.450835: Current learning rate: 0.00675\n",
      "2024-01-02 17:25:30.867863: train_loss -0.8954\n",
      "2024-01-02 17:25:30.877864: val_loss -0.8318\n",
      "2024-01-02 17:25:30.888864: Pseudo dice [0.91, 0.9512, 0.9447]\n",
      "2024-01-02 17:25:30.897865: Epoch time: 125.43 s\n",
      "2024-01-02 17:25:32.148143: \n",
      "2024-01-02 17:25:32.161903: Epoch 355\n",
      "2024-01-02 17:25:32.170908: Current learning rate: 0.00674\n",
      "2024-01-02 17:27:37.345376: train_loss -0.8995\n",
      "2024-01-02 17:27:37.352378: val_loss -0.8361\n",
      "2024-01-02 17:27:37.361378: Pseudo dice [0.9127, 0.9515, 0.9454]\n",
      "2024-01-02 17:27:37.367378: Epoch time: 125.2 s\n",
      "2024-01-02 17:27:38.518067: \n",
      "2024-01-02 17:27:38.524058: Epoch 356\n",
      "2024-01-02 17:27:38.528057: Current learning rate: 0.00673\n",
      "2024-01-02 17:29:44.034609: train_loss -0.9063\n",
      "2024-01-02 17:29:44.040614: val_loss -0.8414\n",
      "2024-01-02 17:29:44.045609: Pseudo dice [0.9145, 0.9528, 0.9449]\n",
      "2024-01-02 17:29:44.050609: Epoch time: 125.52 s\n",
      "2024-01-02 17:29:45.251715: \n",
      "2024-01-02 17:29:45.257719: Epoch 357\n",
      "2024-01-02 17:29:45.262788: Current learning rate: 0.00672\n",
      "2024-01-02 17:31:50.600561: train_loss -0.9093\n",
      "2024-01-02 17:31:50.606558: val_loss -0.8342\n",
      "2024-01-02 17:31:50.612579: Pseudo dice [0.9109, 0.952, 0.9451]\n",
      "2024-01-02 17:31:50.622557: Epoch time: 125.35 s\n",
      "2024-01-02 17:31:51.943632: \n",
      "2024-01-02 17:31:51.949203: Epoch 358\n",
      "2024-01-02 17:31:51.954267: Current learning rate: 0.00671\n",
      "2024-01-02 17:33:57.388089: train_loss -0.9081\n",
      "2024-01-02 17:33:57.394601: val_loss -0.8355\n",
      "2024-01-02 17:33:57.400600: Pseudo dice [0.9088, 0.9511, 0.9436]\n",
      "2024-01-02 17:33:57.405600: Epoch time: 125.45 s\n",
      "2024-01-02 17:33:58.629682: \n",
      "2024-01-02 17:33:58.642152: Epoch 359\n",
      "2024-01-02 17:33:58.647077: Current learning rate: 0.0067\n",
      "2024-01-02 17:36:03.969998: train_loss -0.9083\n",
      "2024-01-02 17:36:03.977998: val_loss -0.8314\n",
      "2024-01-02 17:36:03.985015: Pseudo dice [0.9105, 0.95, 0.942]\n",
      "2024-01-02 17:36:03.993506: Epoch time: 125.34 s\n",
      "2024-01-02 17:36:05.130888: \n",
      "2024-01-02 17:36:05.135889: Epoch 360\n",
      "2024-01-02 17:36:05.140888: Current learning rate: 0.00669\n",
      "2024-01-02 17:38:10.462550: train_loss -0.9091\n",
      "2024-01-02 17:38:10.469554: val_loss -0.8338\n",
      "2024-01-02 17:38:10.475554: Pseudo dice [0.9124, 0.951, 0.9452]\n",
      "2024-01-02 17:38:10.480554: Epoch time: 125.33 s\n",
      "2024-01-02 17:38:11.740834: \n",
      "2024-01-02 17:38:11.752750: Epoch 361\n",
      "2024-01-02 17:38:11.757670: Current learning rate: 0.00668\n",
      "2024-01-02 17:40:17.363718: train_loss -0.9087\n",
      "2024-01-02 17:40:17.369718: val_loss -0.8258\n",
      "2024-01-02 17:40:17.377719: Pseudo dice [0.9136, 0.9498, 0.9457]\n",
      "2024-01-02 17:40:17.383717: Epoch time: 125.62 s\n",
      "2024-01-02 17:40:18.778457: \n",
      "2024-01-02 17:40:18.788977: Epoch 362\n",
      "2024-01-02 17:40:18.796982: Current learning rate: 0.00667\n",
      "2024-01-02 17:42:23.943150: train_loss -0.9116\n",
      "2024-01-02 17:42:23.952148: val_loss -0.8263\n",
      "2024-01-02 17:42:23.958158: Pseudo dice [0.9115, 0.9493, 0.9435]\n",
      "2024-01-02 17:42:23.963155: Epoch time: 125.17 s\n",
      "2024-01-02 17:42:25.217863: \n",
      "2024-01-02 17:42:25.224872: Epoch 363\n",
      "2024-01-02 17:42:25.234377: Current learning rate: 0.00666\n",
      "2024-01-02 17:44:30.748232: train_loss -0.9095\n",
      "2024-01-02 17:44:30.754233: val_loss -0.8325\n",
      "2024-01-02 17:44:30.759232: Pseudo dice [0.9119, 0.9519, 0.9452]\n",
      "2024-01-02 17:44:30.764231: Epoch time: 125.53 s\n",
      "2024-01-02 17:44:31.925821: \n",
      "2024-01-02 17:44:31.932885: Epoch 364\n",
      "2024-01-02 17:44:31.937542: Current learning rate: 0.00665\n",
      "2024-01-02 17:46:38.090151: train_loss -0.9082\n",
      "2024-01-02 17:46:38.100662: val_loss -0.8349\n",
      "2024-01-02 17:46:38.108662: Pseudo dice [0.9137, 0.9513, 0.9442]\n",
      "2024-01-02 17:46:38.114662: Epoch time: 126.17 s\n",
      "2024-01-02 17:46:39.352561: \n",
      "2024-01-02 17:46:39.357497: Epoch 365\n",
      "2024-01-02 17:46:39.365556: Current learning rate: 0.00665\n",
      "2024-01-02 17:48:44.856038: train_loss -0.9089\n",
      "2024-01-02 17:48:44.864038: val_loss -0.8373\n",
      "2024-01-02 17:48:44.871229: Pseudo dice [0.9124, 0.9521, 0.9452]\n",
      "2024-01-02 17:48:44.878227: Epoch time: 125.51 s\n",
      "2024-01-02 17:48:46.128445: \n",
      "2024-01-02 17:48:46.137779: Epoch 366\n",
      "2024-01-02 17:48:46.143770: Current learning rate: 0.00664\n",
      "2024-01-02 17:50:57.466906: train_loss -0.9119\n",
      "2024-01-02 17:50:57.484652: val_loss -0.8346\n",
      "2024-01-02 17:50:57.496311: Pseudo dice [0.9128, 0.9531, 0.9447]\n",
      "2024-01-02 17:50:57.505434: Epoch time: 131.34 s\n",
      "2024-01-02 17:50:59.389819: \n",
      "2024-01-02 17:50:59.396819: Epoch 367\n",
      "2024-01-02 17:50:59.401814: Current learning rate: 0.00663\n",
      "2024-01-02 17:53:05.686376: train_loss -0.9126\n",
      "2024-01-02 17:53:05.693381: val_loss -0.8323\n",
      "2024-01-02 17:53:05.700628: Pseudo dice [0.9124, 0.9507, 0.9446]\n",
      "2024-01-02 17:53:05.705634: Epoch time: 126.3 s\n",
      "2024-01-02 17:53:06.910958: \n",
      "2024-01-02 17:53:06.917117: Epoch 368\n",
      "2024-01-02 17:53:06.922038: Current learning rate: 0.00662\n",
      "2024-01-02 17:55:13.018671: train_loss -0.9126\n",
      "2024-01-02 17:55:13.027671: val_loss -0.8312\n",
      "2024-01-02 17:55:13.035670: Pseudo dice [0.9119, 0.9514, 0.9452]\n",
      "2024-01-02 17:55:13.041670: Epoch time: 126.11 s\n",
      "2024-01-02 17:55:14.580472: \n",
      "2024-01-02 17:55:14.586471: Epoch 369\n",
      "2024-01-02 17:55:14.591463: Current learning rate: 0.00661\n",
      "2024-01-02 17:57:20.348362: train_loss -0.9145\n",
      "2024-01-02 17:57:20.355582: val_loss -0.824\n",
      "2024-01-02 17:57:20.363590: Pseudo dice [0.9105, 0.949, 0.9418]\n",
      "2024-01-02 17:57:20.370584: Epoch time: 125.77 s\n",
      "2024-01-02 17:57:21.717450: \n",
      "2024-01-02 17:57:21.723008: Epoch 370\n",
      "2024-01-02 17:57:21.727079: Current learning rate: 0.0066\n",
      "2024-01-02 17:59:27.465339: train_loss -0.9132\n",
      "2024-01-02 17:59:27.474849: val_loss -0.83\n",
      "2024-01-02 17:59:27.480849: Pseudo dice [0.9129, 0.9516, 0.9419]\n",
      "2024-01-02 17:59:27.485847: Epoch time: 125.75 s\n",
      "2024-01-02 17:59:28.684422: \n",
      "2024-01-02 17:59:28.692409: Epoch 371\n",
      "2024-01-02 17:59:28.697412: Current learning rate: 0.00659\n",
      "2024-01-02 18:01:34.834663: train_loss -0.9126\n",
      "2024-01-02 18:01:34.841664: val_loss -0.8282\n",
      "2024-01-02 18:01:34.849663: Pseudo dice [0.9129, 0.9502, 0.9464]\n",
      "2024-01-02 18:01:34.855664: Epoch time: 126.15 s\n",
      "2024-01-02 18:01:36.206408: \n",
      "2024-01-02 18:01:36.213356: Epoch 372\n",
      "2024-01-02 18:01:36.220071: Current learning rate: 0.00658\n",
      "2024-01-02 18:03:42.574557: train_loss -0.9078\n",
      "2024-01-02 18:03:42.580557: val_loss -0.8294\n",
      "2024-01-02 18:03:42.585558: Pseudo dice [0.9107, 0.9511, 0.9449]\n",
      "2024-01-02 18:03:42.591067: Epoch time: 126.37 s\n",
      "2024-01-02 18:03:43.935652: \n",
      "2024-01-02 18:03:43.941747: Epoch 373\n",
      "2024-01-02 18:03:43.946147: Current learning rate: 0.00657\n",
      "2024-01-02 18:05:49.464997: train_loss -0.9116\n",
      "2024-01-02 18:05:49.473616: val_loss -0.8314\n",
      "2024-01-02 18:05:49.481600: Pseudo dice [0.9115, 0.9517, 0.945]\n",
      "2024-01-02 18:05:49.488104: Epoch time: 125.53 s\n",
      "2024-01-02 18:05:50.572645: \n",
      "2024-01-02 18:05:50.580964: Epoch 374\n",
      "2024-01-02 18:05:50.588050: Current learning rate: 0.00656\n",
      "2024-01-02 18:07:56.313442: train_loss -0.9086\n",
      "2024-01-02 18:07:56.320949: val_loss -0.8342\n",
      "2024-01-02 18:07:56.328950: Pseudo dice [0.9109, 0.9516, 0.9433]\n",
      "2024-01-02 18:07:56.333952: Epoch time: 125.74 s\n",
      "2024-01-02 18:07:57.536874: \n",
      "2024-01-02 18:07:57.545503: Epoch 375\n",
      "2024-01-02 18:07:57.549499: Current learning rate: 0.00655\n",
      "2024-01-02 18:10:03.071088: train_loss -0.9124\n",
      "2024-01-02 18:10:03.079097: val_loss -0.8319\n",
      "2024-01-02 18:10:03.084087: Pseudo dice [0.9097, 0.9516, 0.9444]\n",
      "2024-01-02 18:10:03.089087: Epoch time: 125.54 s\n",
      "2024-01-02 18:10:04.321691: \n",
      "2024-01-02 18:10:04.327758: Epoch 376\n",
      "2024-01-02 18:10:04.332902: Current learning rate: 0.00654\n",
      "2024-01-02 18:12:10.214406: train_loss -0.9111\n",
      "2024-01-02 18:12:10.221410: val_loss -0.8347\n",
      "2024-01-02 18:12:10.226406: Pseudo dice [0.9117, 0.9518, 0.9447]\n",
      "2024-01-02 18:12:10.230406: Epoch time: 125.89 s\n",
      "2024-01-02 18:12:11.528847: \n",
      "2024-01-02 18:12:11.543005: Epoch 377\n",
      "2024-01-02 18:12:11.549611: Current learning rate: 0.00653\n",
      "2024-01-02 18:14:16.970614: train_loss -0.9115\n",
      "2024-01-02 18:14:16.979615: val_loss -0.8311\n",
      "2024-01-02 18:14:16.986618: Pseudo dice [0.9088, 0.9509, 0.9435]\n",
      "2024-01-02 18:14:16.991619: Epoch time: 125.44 s\n",
      "2024-01-02 18:14:18.291631: \n",
      "2024-01-02 18:14:18.311483: Epoch 378\n",
      "2024-01-02 18:14:18.320489: Current learning rate: 0.00652\n",
      "2024-01-02 18:16:23.973863: train_loss -0.9112\n",
      "2024-01-02 18:16:23.980377: val_loss -0.8329\n",
      "2024-01-02 18:16:23.986375: Pseudo dice [0.9138, 0.9515, 0.9419]\n",
      "2024-01-02 18:16:23.992883: Epoch time: 125.68 s\n",
      "2024-01-02 18:16:25.253026: \n",
      "2024-01-02 18:16:25.259734: Epoch 379\n",
      "2024-01-02 18:16:25.264730: Current learning rate: 0.00651\n",
      "2024-01-02 18:18:30.697515: train_loss -0.914\n",
      "2024-01-02 18:18:30.703516: val_loss -0.8364\n",
      "2024-01-02 18:18:30.708523: Pseudo dice [0.9105, 0.953, 0.9431]\n",
      "2024-01-02 18:18:30.713509: Epoch time: 125.45 s\n",
      "2024-01-02 18:18:31.955595: \n",
      "2024-01-02 18:18:31.961859: Epoch 380\n",
      "2024-01-02 18:18:31.966939: Current learning rate: 0.0065\n",
      "2024-01-02 18:20:37.521156: train_loss -0.9122\n",
      "2024-01-02 18:20:37.530145: val_loss -0.8288\n",
      "2024-01-02 18:20:37.536657: Pseudo dice [0.9134, 0.951, 0.9456]\n",
      "2024-01-02 18:20:37.541656: Epoch time: 125.57 s\n",
      "2024-01-02 18:20:38.843523: \n",
      "2024-01-02 18:20:38.849520: Epoch 381\n",
      "2024-01-02 18:20:38.854459: Current learning rate: 0.00649\n",
      "2024-01-02 18:22:44.540732: train_loss -0.9101\n",
      "2024-01-02 18:22:44.547748: val_loss -0.8332\n",
      "2024-01-02 18:22:44.553753: Pseudo dice [0.9123, 0.953, 0.9445]\n",
      "2024-01-02 18:22:44.561744: Epoch time: 125.7 s\n",
      "2024-01-02 18:22:45.945306: \n",
      "2024-01-02 18:22:45.953376: Epoch 382\n",
      "2024-01-02 18:22:45.958383: Current learning rate: 0.00648\n",
      "2024-01-02 18:24:51.937472: train_loss -0.909\n",
      "2024-01-02 18:24:51.946188: val_loss -0.8347\n",
      "2024-01-02 18:24:51.951188: Pseudo dice [0.913, 0.9539, 0.9462]\n",
      "2024-01-02 18:24:51.957188: Epoch time: 125.99 s\n",
      "2024-01-02 18:24:53.095264: \n",
      "2024-01-02 18:24:53.103331: Epoch 383\n",
      "2024-01-02 18:24:53.108319: Current learning rate: 0.00648\n",
      "2024-01-02 18:26:58.518975: train_loss -0.9125\n",
      "2024-01-02 18:26:58.528534: val_loss -0.8316\n",
      "2024-01-02 18:26:58.539512: Pseudo dice [0.9134, 0.9516, 0.9432]\n",
      "2024-01-02 18:26:58.544504: Epoch time: 125.42 s\n",
      "2024-01-02 18:26:59.858897: \n",
      "2024-01-02 18:26:59.870139: Epoch 384\n",
      "2024-01-02 18:26:59.878220: Current learning rate: 0.00647\n",
      "2024-01-02 18:29:05.487288: train_loss -0.9098\n",
      "2024-01-02 18:29:05.494293: val_loss -0.8274\n",
      "2024-01-02 18:29:05.501292: Pseudo dice [0.9089, 0.9516, 0.9435]\n",
      "2024-01-02 18:29:05.505293: Epoch time: 125.63 s\n",
      "2024-01-02 18:29:06.686386: \n",
      "2024-01-02 18:29:06.692385: Epoch 385\n",
      "2024-01-02 18:29:06.696386: Current learning rate: 0.00646\n",
      "2024-01-02 18:31:12.639263: train_loss -0.9117\n",
      "2024-01-02 18:31:12.647261: val_loss -0.8314\n",
      "2024-01-02 18:31:12.653261: Pseudo dice [0.9128, 0.951, 0.9453]\n",
      "2024-01-02 18:31:12.661261: Epoch time: 125.95 s\n",
      "2024-01-02 18:31:13.756156: \n",
      "2024-01-02 18:31:13.766663: Epoch 386\n",
      "2024-01-02 18:31:13.771713: Current learning rate: 0.00645\n",
      "2024-01-02 18:33:19.608705: train_loss -0.91\n",
      "2024-01-02 18:33:19.619214: val_loss -0.8314\n",
      "2024-01-02 18:33:19.625213: Pseudo dice [0.9066, 0.953, 0.9438]\n",
      "2024-01-02 18:33:19.634213: Epoch time: 125.85 s\n",
      "2024-01-02 18:33:20.911354: \n",
      "2024-01-02 18:33:20.917400: Epoch 387\n",
      "2024-01-02 18:33:20.921997: Current learning rate: 0.00644\n",
      "2024-01-02 18:35:26.868613: train_loss -0.9099\n",
      "2024-01-02 18:35:26.878624: val_loss -0.8353\n",
      "2024-01-02 18:35:26.885624: Pseudo dice [0.911, 0.9517, 0.9448]\n",
      "2024-01-02 18:35:26.890624: Epoch time: 125.96 s\n",
      "2024-01-02 18:35:28.054611: \n",
      "2024-01-02 18:35:28.065231: Epoch 388\n",
      "2024-01-02 18:35:28.070790: Current learning rate: 0.00643\n",
      "2024-01-02 18:37:33.897805: train_loss -0.9072\n",
      "2024-01-02 18:37:33.906879: val_loss -0.8278\n",
      "2024-01-02 18:37:33.911893: Pseudo dice [0.9124, 0.9497, 0.9447]\n",
      "2024-01-02 18:37:33.915965: Epoch time: 125.84 s\n",
      "2024-01-02 18:37:35.124889: \n",
      "2024-01-02 18:37:35.131523: Epoch 389\n",
      "2024-01-02 18:37:35.137523: Current learning rate: 0.00642\n",
      "2024-01-02 18:39:40.461904: train_loss -0.9104\n",
      "2024-01-02 18:39:40.468905: val_loss -0.8358\n",
      "2024-01-02 18:39:40.475421: Pseudo dice [0.9079, 0.9526, 0.9432]\n",
      "2024-01-02 18:39:40.481421: Epoch time: 125.34 s\n",
      "2024-01-02 18:39:41.767002: \n",
      "2024-01-02 18:39:41.777510: Epoch 390\n",
      "2024-01-02 18:39:41.782509: Current learning rate: 0.00641\n",
      "2024-01-02 18:41:47.850210: train_loss -0.9057\n",
      "2024-01-02 18:41:47.858211: val_loss -0.8337\n",
      "2024-01-02 18:41:47.867213: Pseudo dice [0.9117, 0.9525, 0.9438]\n",
      "2024-01-02 18:41:47.873217: Epoch time: 126.09 s\n",
      "2024-01-02 18:41:49.400186: \n",
      "2024-01-02 18:41:49.405632: Epoch 391\n",
      "2024-01-02 18:41:49.411634: Current learning rate: 0.0064\n",
      "2024-01-02 18:43:54.826837: train_loss -0.9094\n",
      "2024-01-02 18:43:54.832837: val_loss -0.8364\n",
      "2024-01-02 18:43:54.837837: Pseudo dice [0.9112, 0.9519, 0.9458]\n",
      "2024-01-02 18:43:54.842836: Epoch time: 125.43 s\n",
      "2024-01-02 18:43:56.018408: \n",
      "2024-01-02 18:43:56.025554: Epoch 392\n",
      "2024-01-02 18:43:56.030547: Current learning rate: 0.00639\n",
      "2024-01-02 18:46:01.622971: train_loss -0.9095\n",
      "2024-01-02 18:46:01.628976: val_loss -0.8317\n",
      "2024-01-02 18:46:01.633972: Pseudo dice [0.9113, 0.9522, 0.9433]\n",
      "2024-01-02 18:46:01.640980: Epoch time: 125.61 s\n",
      "2024-01-02 18:46:02.936496: \n",
      "2024-01-02 18:46:02.944805: Epoch 393\n",
      "2024-01-02 18:46:02.948781: Current learning rate: 0.00638\n",
      "2024-01-02 18:48:08.528152: train_loss -0.9109\n",
      "2024-01-02 18:48:08.535145: val_loss -0.8342\n",
      "2024-01-02 18:48:08.542145: Pseudo dice [0.9088, 0.9525, 0.9458]\n",
      "2024-01-02 18:48:08.547145: Epoch time: 125.59 s\n",
      "2024-01-02 18:48:09.784728: \n",
      "2024-01-02 18:48:09.792147: Epoch 394\n",
      "2024-01-02 18:48:09.797205: Current learning rate: 0.00637\n",
      "2024-01-02 18:50:15.489432: train_loss -0.9126\n",
      "2024-01-02 18:50:15.496432: val_loss -0.8227\n",
      "2024-01-02 18:50:15.501432: Pseudo dice [0.911, 0.95, 0.9444]\n",
      "2024-01-02 18:50:15.506432: Epoch time: 125.71 s\n",
      "2024-01-02 18:50:16.747416: \n",
      "2024-01-02 18:50:16.753410: Epoch 395\n",
      "2024-01-02 18:50:16.758049: Current learning rate: 0.00636\n",
      "2024-01-02 18:52:22.707911: train_loss -0.9071\n",
      "2024-01-02 18:52:22.714908: val_loss -0.8368\n",
      "2024-01-02 18:52:22.722913: Pseudo dice [0.9147, 0.9532, 0.9452]\n",
      "2024-01-02 18:52:22.727911: Epoch time: 125.96 s\n",
      "2024-01-02 18:52:23.960152: \n",
      "2024-01-02 18:52:23.967153: Epoch 396\n",
      "2024-01-02 18:52:23.975365: Current learning rate: 0.00635\n",
      "2024-01-02 18:54:29.552671: train_loss -0.912\n",
      "2024-01-02 18:54:29.559672: val_loss -0.8378\n",
      "2024-01-02 18:54:29.567671: Pseudo dice [0.9108, 0.953, 0.946]\n",
      "2024-01-02 18:54:29.573578: Epoch time: 125.59 s\n",
      "2024-01-02 18:54:30.817630: \n",
      "2024-01-02 18:54:30.823623: Epoch 397\n",
      "2024-01-02 18:54:30.828630: Current learning rate: 0.00634\n",
      "2024-01-02 18:56:36.610135: train_loss -0.912\n",
      "2024-01-02 18:56:36.617136: val_loss -0.8349\n",
      "2024-01-02 18:56:36.624138: Pseudo dice [0.9109, 0.9522, 0.945]\n",
      "2024-01-02 18:56:36.630137: Epoch time: 125.79 s\n",
      "2024-01-02 18:56:37.851484: \n",
      "2024-01-02 18:56:37.862452: Epoch 398\n",
      "2024-01-02 18:56:37.866532: Current learning rate: 0.00633\n",
      "2024-01-02 18:58:43.395694: train_loss -0.9124\n",
      "2024-01-02 18:58:43.402697: val_loss -0.83\n",
      "2024-01-02 18:58:43.407696: Pseudo dice [0.9125, 0.9513, 0.9454]\n",
      "2024-01-02 18:58:43.414710: Epoch time: 125.55 s\n",
      "2024-01-02 18:58:44.814000: \n",
      "2024-01-02 18:58:44.820008: Epoch 399\n",
      "2024-01-02 18:58:44.824015: Current learning rate: 0.00632\n",
      "2024-01-02 19:00:50.696576: train_loss -0.9128\n",
      "2024-01-02 19:00:50.703578: val_loss -0.8334\n",
      "2024-01-02 19:00:50.711551: Pseudo dice [0.9109, 0.9518, 0.9458]\n",
      "2024-01-02 19:00:50.718551: Epoch time: 125.88 s\n",
      "2024-01-02 19:00:52.313517: \n",
      "2024-01-02 19:00:52.318451: Epoch 400\n",
      "2024-01-02 19:00:52.322451: Current learning rate: 0.00631\n",
      "2024-01-02 19:02:57.832804: train_loss -0.9128\n",
      "2024-01-02 19:02:57.839805: val_loss -0.8304\n",
      "2024-01-02 19:02:57.844806: Pseudo dice [0.9127, 0.9513, 0.9458]\n",
      "2024-01-02 19:02:57.849818: Epoch time: 125.52 s\n",
      "2024-01-02 19:02:58.997548: \n",
      "2024-01-02 19:02:59.003542: Epoch 401\n",
      "2024-01-02 19:02:59.007564: Current learning rate: 0.0063\n",
      "2024-01-02 19:05:04.636340: train_loss -0.913\n",
      "2024-01-02 19:05:04.645516: val_loss -0.8314\n",
      "2024-01-02 19:05:04.650519: Pseudo dice [0.9107, 0.9511, 0.9451]\n",
      "2024-01-02 19:05:04.659526: Epoch time: 125.64 s\n",
      "2024-01-02 19:05:06.013794: \n",
      "2024-01-02 19:05:06.018786: Epoch 402\n",
      "2024-01-02 19:05:06.022776: Current learning rate: 0.0063\n",
      "2024-01-02 19:07:11.605317: train_loss -0.9124\n",
      "2024-01-02 19:07:11.612317: val_loss -0.828\n",
      "2024-01-02 19:07:11.619318: Pseudo dice [0.91, 0.9507, 0.943]\n",
      "2024-01-02 19:07:11.627317: Epoch time: 125.59 s\n",
      "2024-01-02 19:07:12.793488: \n",
      "2024-01-02 19:07:12.804388: Epoch 403\n",
      "2024-01-02 19:07:12.812383: Current learning rate: 0.00629\n",
      "2024-01-02 19:09:18.600803: train_loss -0.9136\n",
      "2024-01-02 19:09:18.607788: val_loss -0.83\n",
      "2024-01-02 19:09:18.613789: Pseudo dice [0.9101, 0.9504, 0.9449]\n",
      "2024-01-02 19:09:18.618789: Epoch time: 125.81 s\n",
      "2024-01-02 19:09:19.834985: \n",
      "2024-01-02 19:09:19.844103: Epoch 404\n",
      "2024-01-02 19:09:19.849173: Current learning rate: 0.00628\n",
      "2024-01-02 19:11:25.306062: train_loss -0.9131\n",
      "2024-01-02 19:11:25.316060: val_loss -0.8323\n",
      "2024-01-02 19:11:25.323061: Pseudo dice [0.912, 0.9526, 0.9464]\n",
      "2024-01-02 19:11:25.329060: Epoch time: 125.47 s\n",
      "2024-01-02 19:11:26.627535: \n",
      "2024-01-02 19:11:26.632535: Epoch 405\n",
      "2024-01-02 19:11:26.637172: Current learning rate: 0.00627\n",
      "2024-01-02 19:13:32.265335: train_loss -0.9115\n",
      "2024-01-02 19:13:32.273335: val_loss -0.8367\n",
      "2024-01-02 19:13:32.279339: Pseudo dice [0.9097, 0.9526, 0.946]\n",
      "2024-01-02 19:13:32.284338: Epoch time: 125.64 s\n",
      "2024-01-02 19:13:33.659840: \n",
      "2024-01-02 19:13:33.665842: Epoch 406\n",
      "2024-01-02 19:13:33.672894: Current learning rate: 0.00626\n",
      "2024-01-02 19:15:39.325768: train_loss -0.9125\n",
      "2024-01-02 19:15:39.332758: val_loss -0.812\n",
      "2024-01-02 19:15:39.337270: Pseudo dice [0.9025, 0.9451, 0.9365]\n",
      "2024-01-02 19:15:39.342276: Epoch time: 125.67 s\n",
      "2024-01-02 19:15:40.582165: \n",
      "2024-01-02 19:15:40.595115: Epoch 407\n",
      "2024-01-02 19:15:40.601115: Current learning rate: 0.00625\n",
      "2024-01-02 19:17:46.289222: train_loss -0.9042\n",
      "2024-01-02 19:17:46.300228: val_loss -0.8301\n",
      "2024-01-02 19:17:46.306232: Pseudo dice [0.912, 0.9488, 0.9444]\n",
      "2024-01-02 19:17:46.357400: Epoch time: 125.71 s\n",
      "2024-01-02 19:17:47.503793: \n",
      "2024-01-02 19:17:47.512732: Epoch 408\n",
      "2024-01-02 19:17:47.516795: Current learning rate: 0.00624\n",
      "2024-01-02 19:19:53.003264: train_loss -0.901\n",
      "2024-01-02 19:19:53.011265: val_loss -0.8351\n",
      "2024-01-02 19:19:53.015269: Pseudo dice [0.9131, 0.9506, 0.9411]\n",
      "2024-01-02 19:19:53.021264: Epoch time: 125.5 s\n",
      "2024-01-02 19:19:54.349322: \n",
      "2024-01-02 19:19:54.356426: Epoch 409\n",
      "2024-01-02 19:19:54.364427: Current learning rate: 0.00623\n",
      "2024-01-02 19:21:59.923621: train_loss -0.9077\n",
      "2024-01-02 19:21:59.930623: val_loss -0.8321\n",
      "2024-01-02 19:21:59.935621: Pseudo dice [0.9121, 0.9514, 0.9456]\n",
      "2024-01-02 19:21:59.941622: Epoch time: 125.58 s\n",
      "2024-01-02 19:22:01.204686: \n",
      "2024-01-02 19:22:01.208678: Epoch 410\n",
      "2024-01-02 19:22:01.212682: Current learning rate: 0.00622\n",
      "2024-01-02 19:24:06.841600: train_loss -0.9078\n",
      "2024-01-02 19:24:06.851656: val_loss -0.8353\n",
      "2024-01-02 19:24:06.859656: Pseudo dice [0.912, 0.9522, 0.9448]\n",
      "2024-01-02 19:24:06.867657: Epoch time: 125.64 s\n",
      "2024-01-02 19:24:08.008287: \n",
      "2024-01-02 19:24:08.050517: Epoch 411\n",
      "2024-01-02 19:24:08.059546: Current learning rate: 0.00621\n",
      "2024-01-02 19:26:13.826635: train_loss -0.9078\n",
      "2024-01-02 19:26:13.832635: val_loss -0.8274\n",
      "2024-01-02 19:26:13.839640: Pseudo dice [0.909, 0.9506, 0.9456]\n",
      "2024-01-02 19:26:13.844645: Epoch time: 125.82 s\n",
      "2024-01-02 19:26:15.103271: \n",
      "2024-01-02 19:26:15.108270: Epoch 412\n",
      "2024-01-02 19:26:15.113307: Current learning rate: 0.0062\n",
      "2024-01-02 19:28:20.510682: train_loss -0.9143\n",
      "2024-01-02 19:28:20.518586: val_loss -0.8351\n",
      "2024-01-02 19:28:20.527577: Pseudo dice [0.9121, 0.9515, 0.9431]\n",
      "2024-01-02 19:28:20.532577: Epoch time: 125.41 s\n",
      "2024-01-02 19:28:21.856261: \n",
      "2024-01-02 19:28:21.862218: Epoch 413\n",
      "2024-01-02 19:28:21.866314: Current learning rate: 0.00619\n",
      "2024-01-02 19:30:27.182730: train_loss -0.9117\n",
      "2024-01-02 19:30:27.190731: val_loss -0.8071\n",
      "2024-01-02 19:30:27.199733: Pseudo dice [0.8965, 0.9438, 0.9357]\n",
      "2024-01-02 19:30:27.205730: Epoch time: 125.33 s\n",
      "2024-01-02 19:30:28.458232: \n",
      "2024-01-02 19:30:28.466870: Epoch 414\n",
      "2024-01-02 19:30:28.471874: Current learning rate: 0.00618\n",
      "2024-01-02 19:32:34.025868: train_loss -0.9014\n",
      "2024-01-02 19:32:34.035868: val_loss -0.8361\n",
      "2024-01-02 19:32:34.040868: Pseudo dice [0.9116, 0.9509, 0.9459]\n",
      "2024-01-02 19:32:34.045870: Epoch time: 125.57 s\n",
      "2024-01-02 19:32:35.168733: \n",
      "2024-01-02 19:32:35.179881: Epoch 415\n",
      "2024-01-02 19:32:35.189893: Current learning rate: 0.00617\n",
      "2024-01-02 19:34:41.618017: train_loss -0.9048\n",
      "2024-01-02 19:34:41.628016: val_loss -0.8274\n",
      "2024-01-02 19:34:41.637021: Pseudo dice [0.9106, 0.9495, 0.9426]\n",
      "2024-01-02 19:34:41.644020: Epoch time: 126.45 s\n",
      "2024-01-02 19:34:42.944049: \n",
      "2024-01-02 19:34:42.954016: Epoch 416\n",
      "2024-01-02 19:34:42.966089: Current learning rate: 0.00616\n",
      "2024-01-02 19:36:48.236681: train_loss -0.9113\n",
      "2024-01-02 19:36:48.245217: val_loss -0.827\n",
      "2024-01-02 19:36:48.250292: Pseudo dice [0.9109, 0.9496, 0.9448]\n",
      "2024-01-02 19:36:48.255233: Epoch time: 125.29 s\n",
      "2024-01-02 19:36:49.418261: \n",
      "2024-01-02 19:36:49.424017: Epoch 417\n",
      "2024-01-02 19:36:49.429116: Current learning rate: 0.00615\n",
      "2024-01-02 19:38:55.058340: train_loss -0.9117\n",
      "2024-01-02 19:38:55.065340: val_loss -0.8304\n",
      "2024-01-02 19:38:55.070340: Pseudo dice [0.9111, 0.9514, 0.9453]\n",
      "2024-01-02 19:38:55.075340: Epoch time: 125.64 s\n",
      "2024-01-02 19:38:56.272979: \n",
      "2024-01-02 19:38:56.278917: Epoch 418\n",
      "2024-01-02 19:38:56.283909: Current learning rate: 0.00614\n",
      "2024-01-02 19:41:02.148813: train_loss -0.9111\n",
      "2024-01-02 19:41:02.156815: val_loss -0.8404\n",
      "2024-01-02 19:41:02.163819: Pseudo dice [0.9119, 0.9533, 0.9466]\n",
      "2024-01-02 19:41:02.171819: Epoch time: 125.88 s\n",
      "2024-01-02 19:41:03.344053: \n",
      "2024-01-02 19:41:03.349787: Epoch 419\n",
      "2024-01-02 19:41:03.354780: Current learning rate: 0.00613\n",
      "2024-01-02 19:43:08.894832: train_loss -0.9132\n",
      "2024-01-02 19:43:08.901824: val_loss -0.8306\n",
      "2024-01-02 19:43:08.909827: Pseudo dice [0.9115, 0.9515, 0.9449]\n",
      "2024-01-02 19:43:08.916818: Epoch time: 125.55 s\n",
      "2024-01-02 19:43:10.170579: \n",
      "2024-01-02 19:43:10.177650: Epoch 420\n",
      "2024-01-02 19:43:10.181647: Current learning rate: 0.00612\n",
      "2024-01-02 19:45:15.747768: train_loss -0.9132\n",
      "2024-01-02 19:45:15.756768: val_loss -0.8253\n",
      "2024-01-02 19:45:15.763773: Pseudo dice [0.9089, 0.9494, 0.9455]\n",
      "2024-01-02 19:45:15.770768: Epoch time: 125.58 s\n",
      "2024-01-02 19:45:17.200095: \n",
      "2024-01-02 19:45:17.206134: Epoch 421\n",
      "2024-01-02 19:45:17.211211: Current learning rate: 0.00612\n",
      "2024-01-02 19:47:22.626896: train_loss -0.9115\n",
      "2024-01-02 19:47:22.639904: val_loss -0.8188\n",
      "2024-01-02 19:47:22.646904: Pseudo dice [0.9045, 0.9502, 0.9398]\n",
      "2024-01-02 19:47:22.656415: Epoch time: 125.43 s\n",
      "2024-01-02 19:47:23.827774: \n",
      "2024-01-02 19:47:23.833771: Epoch 422\n",
      "2024-01-02 19:47:23.837794: Current learning rate: 0.00611\n",
      "2024-01-02 19:49:29.624949: train_loss -0.9057\n",
      "2024-01-02 19:49:29.632950: val_loss -0.835\n",
      "2024-01-02 19:49:29.638949: Pseudo dice [0.9103, 0.9523, 0.9465]\n",
      "2024-01-02 19:49:29.644949: Epoch time: 125.8 s\n",
      "2024-01-02 19:49:30.811703: \n",
      "2024-01-02 19:49:30.818216: Epoch 423\n",
      "2024-01-02 19:49:30.822275: Current learning rate: 0.0061\n",
      "2024-01-02 19:51:36.536819: train_loss -0.9118\n",
      "2024-01-02 19:51:36.543819: val_loss -0.8317\n",
      "2024-01-02 19:51:36.550819: Pseudo dice [0.9125, 0.9515, 0.9447]\n",
      "2024-01-02 19:51:36.554819: Epoch time: 125.73 s\n",
      "2024-01-02 19:51:37.814105: \n",
      "2024-01-02 19:51:37.827109: Epoch 424\n",
      "2024-01-02 19:51:37.834212: Current learning rate: 0.00609\n",
      "2024-01-02 19:53:43.552067: train_loss -0.9101\n",
      "2024-01-02 19:53:43.561066: val_loss -0.8187\n",
      "2024-01-02 19:53:43.567067: Pseudo dice [0.9043, 0.9489, 0.941]\n",
      "2024-01-02 19:53:43.575066: Epoch time: 125.74 s\n",
      "2024-01-02 19:53:44.753242: \n",
      "2024-01-02 19:53:44.762162: Epoch 425\n",
      "2024-01-02 19:53:44.767882: Current learning rate: 0.00608\n",
      "2024-01-02 19:55:50.246420: train_loss -0.9105\n",
      "2024-01-02 19:55:50.252418: val_loss -0.822\n",
      "2024-01-02 19:55:50.265052: Pseudo dice [0.9073, 0.9491, 0.9423]\n",
      "2024-01-02 19:55:50.271053: Epoch time: 125.49 s\n",
      "2024-01-02 19:55:51.465508: \n",
      "2024-01-02 19:55:51.469509: Epoch 426\n",
      "2024-01-02 19:55:51.473500: Current learning rate: 0.00607\n",
      "2024-01-02 19:57:56.866747: train_loss -0.9092\n",
      "2024-01-02 19:57:56.872757: val_loss -0.8343\n",
      "2024-01-02 19:57:56.880574: Pseudo dice [0.91, 0.9523, 0.9456]\n",
      "2024-01-02 19:57:56.886579: Epoch time: 125.4 s\n",
      "2024-01-02 19:57:58.161564: \n",
      "2024-01-02 19:57:58.172759: Epoch 427\n",
      "2024-01-02 19:57:58.181348: Current learning rate: 0.00606\n",
      "2024-01-02 20:00:03.907558: train_loss -0.9112\n",
      "2024-01-02 20:00:03.916559: val_loss -0.8327\n",
      "2024-01-02 20:00:03.927557: Pseudo dice [0.9118, 0.9524, 0.9439]\n",
      "2024-01-02 20:00:03.932557: Epoch time: 125.75 s\n",
      "2024-01-02 20:00:04.947914: \n",
      "2024-01-02 20:00:04.953561: Epoch 428\n",
      "2024-01-02 20:00:04.958505: Current learning rate: 0.00605\n",
      "2024-01-02 20:02:10.821503: train_loss -0.9137\n",
      "2024-01-02 20:02:10.828502: val_loss -0.8337\n",
      "2024-01-02 20:02:10.833014: Pseudo dice [0.9106, 0.9512, 0.9452]\n",
      "2024-01-02 20:02:10.838027: Epoch time: 125.87 s\n",
      "2024-01-02 20:02:12.124393: \n",
      "2024-01-02 20:02:12.130823: Epoch 429\n",
      "2024-01-02 20:02:12.135548: Current learning rate: 0.00604\n",
      "2024-01-02 20:04:17.722262: train_loss -0.9143\n",
      "2024-01-02 20:04:17.728260: val_loss -0.8317\n",
      "2024-01-02 20:04:17.734257: Pseudo dice [0.9104, 0.9509, 0.9451]\n",
      "2024-01-02 20:04:17.739260: Epoch time: 125.6 s\n",
      "2024-01-02 20:04:18.940570: \n",
      "2024-01-02 20:04:18.946588: Epoch 430\n",
      "2024-01-02 20:04:18.952642: Current learning rate: 0.00603\n",
      "2024-01-02 20:06:24.696254: train_loss -0.9118\n",
      "2024-01-02 20:06:24.703263: val_loss -0.8331\n",
      "2024-01-02 20:06:24.709263: Pseudo dice [0.9137, 0.9525, 0.9451]\n",
      "2024-01-02 20:06:24.715776: Epoch time: 125.76 s\n",
      "2024-01-02 20:06:25.924211: \n",
      "2024-01-02 20:06:25.944060: Epoch 431\n",
      "2024-01-02 20:06:25.955675: Current learning rate: 0.00602\n",
      "2024-01-02 20:08:31.550250: train_loss -0.9127\n",
      "2024-01-02 20:08:31.559249: val_loss -0.8306\n",
      "2024-01-02 20:08:31.565259: Pseudo dice [0.912, 0.9513, 0.9467]\n",
      "2024-01-02 20:08:31.570252: Epoch time: 125.63 s\n",
      "2024-01-02 20:08:32.924915: \n",
      "2024-01-02 20:08:32.929975: Epoch 432\n",
      "2024-01-02 20:08:32.934470: Current learning rate: 0.00601\n",
      "2024-01-02 20:10:39.202943: train_loss -0.9104\n",
      "2024-01-02 20:10:39.214934: val_loss -0.8286\n",
      "2024-01-02 20:10:39.224934: Pseudo dice [0.9113, 0.951, 0.9449]\n",
      "2024-01-02 20:10:39.230940: Epoch time: 126.28 s\n",
      "2024-01-02 20:10:40.312011: \n",
      "2024-01-02 20:10:40.320208: Epoch 433\n",
      "2024-01-02 20:10:40.327276: Current learning rate: 0.006\n",
      "2024-01-02 20:12:46.619077: train_loss -0.9094\n",
      "2024-01-02 20:12:46.629068: val_loss -0.8297\n",
      "2024-01-02 20:12:46.635068: Pseudo dice [0.9099, 0.9496, 0.9428]\n",
      "2024-01-02 20:12:46.640068: Epoch time: 126.31 s\n",
      "2024-01-02 20:12:47.769753: \n",
      "2024-01-02 20:12:47.774747: Epoch 434\n",
      "2024-01-02 20:12:47.779261: Current learning rate: 0.00599\n",
      "2024-01-02 20:14:52.930608: train_loss -0.9152\n",
      "2024-01-02 20:14:52.938052: val_loss -0.8264\n",
      "2024-01-02 20:14:52.942563: Pseudo dice [0.9118, 0.9496, 0.9458]\n",
      "2024-01-02 20:14:52.947562: Epoch time: 125.16 s\n",
      "2024-01-02 20:14:54.136434: \n",
      "2024-01-02 20:14:54.142061: Epoch 435\n",
      "2024-01-02 20:14:54.147130: Current learning rate: 0.00598\n",
      "2024-01-02 20:16:59.715323: train_loss -0.9149\n",
      "2024-01-02 20:16:59.724334: val_loss -0.8378\n",
      "2024-01-02 20:16:59.729321: Pseudo dice [0.9125, 0.953, 0.9462]\n",
      "2024-01-02 20:16:59.736321: Epoch time: 125.58 s\n",
      "2024-01-02 20:17:00.942829: \n",
      "2024-01-02 20:17:00.948821: Epoch 436\n",
      "2024-01-02 20:17:00.953285: Current learning rate: 0.00597\n",
      "2024-01-02 20:19:06.648022: train_loss -0.9127\n",
      "2024-01-02 20:19:06.655020: val_loss -0.8264\n",
      "2024-01-02 20:19:06.661026: Pseudo dice [0.9131, 0.9512, 0.9452]\n",
      "2024-01-02 20:19:06.667026: Epoch time: 125.71 s\n",
      "2024-01-02 20:19:08.011992: \n",
      "2024-01-02 20:19:08.016989: Epoch 437\n",
      "2024-01-02 20:19:08.022038: Current learning rate: 0.00596\n",
      "2024-01-02 20:21:13.414258: train_loss -0.915\n",
      "2024-01-02 20:21:13.420259: val_loss -0.8265\n",
      "2024-01-02 20:21:13.427259: Pseudo dice [0.9085, 0.9511, 0.9437]\n",
      "2024-01-02 20:21:13.434288: Epoch time: 125.4 s\n",
      "2024-01-02 20:21:14.770133: \n",
      "2024-01-02 20:21:14.775129: Epoch 438\n",
      "2024-01-02 20:21:14.780129: Current learning rate: 0.00595\n",
      "2024-01-02 20:23:20.358137: train_loss -0.9144\n",
      "2024-01-02 20:23:20.364136: val_loss -0.831\n",
      "2024-01-02 20:23:20.369136: Pseudo dice [0.9103, 0.9505, 0.9458]\n",
      "2024-01-02 20:23:20.373136: Epoch time: 125.59 s\n",
      "2024-01-02 20:23:21.501197: \n",
      "2024-01-02 20:23:21.508393: Epoch 439\n",
      "2024-01-02 20:23:21.515730: Current learning rate: 0.00594\n",
      "2024-01-02 20:25:27.241254: train_loss -0.9135\n",
      "2024-01-02 20:25:27.249254: val_loss -0.8318\n",
      "2024-01-02 20:25:27.255258: Pseudo dice [0.913, 0.9514, 0.9457]\n",
      "2024-01-02 20:25:27.264258: Epoch time: 125.74 s\n",
      "2024-01-02 20:25:28.535986: \n",
      "2024-01-02 20:25:28.542019: Epoch 440\n",
      "2024-01-02 20:25:28.546048: Current learning rate: 0.00593\n",
      "2024-01-02 20:27:34.132954: train_loss -0.9128\n",
      "2024-01-02 20:27:34.140955: val_loss -0.8286\n",
      "2024-01-02 20:27:34.146465: Pseudo dice [0.9128, 0.9521, 0.9459]\n",
      "2024-01-02 20:27:34.157463: Epoch time: 125.6 s\n",
      "2024-01-02 20:27:35.357237: \n",
      "2024-01-02 20:27:35.363325: Epoch 441\n",
      "2024-01-02 20:27:35.366948: Current learning rate: 0.00592\n",
      "2024-01-02 20:29:41.183376: train_loss -0.9139\n",
      "2024-01-02 20:29:41.192382: val_loss -0.8232\n",
      "2024-01-02 20:29:41.199382: Pseudo dice [0.9077, 0.9488, 0.9452]\n",
      "2024-01-02 20:29:41.206899: Epoch time: 125.83 s\n",
      "2024-01-02 20:29:42.391993: \n",
      "2024-01-02 20:29:42.398988: Epoch 442\n",
      "2024-01-02 20:29:42.403057: Current learning rate: 0.00592\n",
      "2024-01-02 20:31:47.962597: train_loss -0.915\n",
      "2024-01-02 20:31:47.971596: val_loss -0.8328\n",
      "2024-01-02 20:31:47.977590: Pseudo dice [0.912, 0.9522, 0.9453]\n",
      "2024-01-02 20:31:47.982108: Epoch time: 125.57 s\n",
      "2024-01-02 20:31:49.099799: \n",
      "2024-01-02 20:31:49.105357: Epoch 443\n",
      "2024-01-02 20:31:49.110334: Current learning rate: 0.00591\n",
      "2024-01-02 20:33:54.686830: train_loss -0.9139\n",
      "2024-01-02 20:33:54.694827: val_loss -0.824\n",
      "2024-01-02 20:33:54.702828: Pseudo dice [0.9121, 0.95, 0.946]\n",
      "2024-01-02 20:33:54.711827: Epoch time: 125.59 s\n",
      "2024-01-02 20:33:56.005534: \n",
      "2024-01-02 20:33:56.011466: Epoch 444\n",
      "2024-01-02 20:33:56.020479: Current learning rate: 0.0059\n",
      "2024-01-02 20:36:01.729411: train_loss -0.9146\n",
      "2024-01-02 20:36:01.738410: val_loss -0.8338\n",
      "2024-01-02 20:36:01.744411: Pseudo dice [0.9123, 0.9521, 0.9455]\n",
      "2024-01-02 20:36:01.751411: Epoch time: 125.72 s\n",
      "2024-01-02 20:36:02.907233: \n",
      "2024-01-02 20:36:02.913368: Epoch 445\n",
      "2024-01-02 20:36:02.922027: Current learning rate: 0.00589\n",
      "2024-01-02 20:38:08.595360: train_loss -0.9115\n",
      "2024-01-02 20:38:08.604358: val_loss -0.8303\n",
      "2024-01-02 20:38:08.612354: Pseudo dice [0.9102, 0.9514, 0.9451]\n",
      "2024-01-02 20:38:08.619352: Epoch time: 125.69 s\n",
      "2024-01-02 20:38:09.729754: \n",
      "2024-01-02 20:38:09.735768: Epoch 446\n",
      "2024-01-02 20:38:09.742361: Current learning rate: 0.00588\n",
      "2024-01-02 20:40:15.508867: train_loss -0.9137\n",
      "2024-01-02 20:40:15.520867: val_loss -0.8303\n",
      "2024-01-02 20:40:15.527867: Pseudo dice [0.9126, 0.9516, 0.9464]\n",
      "2024-01-02 20:40:15.532867: Epoch time: 125.78 s\n",
      "2024-01-02 20:40:16.644570: \n",
      "2024-01-02 20:40:16.653628: Epoch 447\n",
      "2024-01-02 20:40:16.659369: Current learning rate: 0.00587\n",
      "2024-01-02 20:42:22.231790: train_loss -0.9109\n",
      "2024-01-02 20:42:22.238793: val_loss -0.8302\n",
      "2024-01-02 20:42:22.247789: Pseudo dice [0.9132, 0.9525, 0.9443]\n",
      "2024-01-02 20:42:22.252789: Epoch time: 125.59 s\n",
      "2024-01-02 20:42:23.492026: \n",
      "2024-01-02 20:42:23.498108: Epoch 448\n",
      "2024-01-02 20:42:23.502178: Current learning rate: 0.00586\n",
      "2024-01-02 20:44:29.221040: train_loss -0.9116\n",
      "2024-01-02 20:44:29.227462: val_loss -0.8244\n",
      "2024-01-02 20:44:29.233462: Pseudo dice [0.9086, 0.9502, 0.944]\n",
      "2024-01-02 20:44:29.239463: Epoch time: 125.73 s\n",
      "2024-01-02 20:44:30.403172: \n",
      "2024-01-02 20:44:30.409220: Epoch 449\n",
      "2024-01-02 20:44:30.413238: Current learning rate: 0.00585\n",
      "2024-01-02 20:46:36.231119: train_loss -0.9109\n",
      "2024-01-02 20:46:36.238119: val_loss -0.8273\n",
      "2024-01-02 20:46:36.243119: Pseudo dice [0.9129, 0.9502, 0.9437]\n",
      "2024-01-02 20:46:36.248139: Epoch time: 125.83 s\n",
      "2024-01-02 20:46:37.641349: \n",
      "2024-01-02 20:46:37.646305: Epoch 450\n",
      "2024-01-02 20:46:37.651295: Current learning rate: 0.00584\n",
      "2024-01-02 20:48:42.964965: train_loss -0.9174\n",
      "2024-01-02 20:48:42.970964: val_loss -0.8325\n",
      "2024-01-02 20:48:42.977963: Pseudo dice [0.9129, 0.9519, 0.9454]\n",
      "2024-01-02 20:48:42.981973: Epoch time: 125.32 s\n",
      "2024-01-02 20:48:44.069898: \n",
      "2024-01-02 20:48:44.076962: Epoch 451\n",
      "2024-01-02 20:48:44.083589: Current learning rate: 0.00583\n",
      "2024-01-02 20:50:49.774198: train_loss -0.9156\n",
      "2024-01-02 20:50:49.781196: val_loss -0.8266\n",
      "2024-01-02 20:50:49.788198: Pseudo dice [0.9105, 0.9524, 0.946]\n",
      "2024-01-02 20:50:49.794197: Epoch time: 125.71 s\n",
      "2024-01-02 20:50:51.240251: \n",
      "2024-01-02 20:50:51.249220: Epoch 452\n",
      "2024-01-02 20:50:51.257289: Current learning rate: 0.00582\n",
      "2024-01-02 20:52:56.666331: train_loss -0.9149\n",
      "2024-01-02 20:52:56.675324: val_loss -0.8231\n",
      "2024-01-02 20:52:56.682323: Pseudo dice [0.9101, 0.9508, 0.9455]\n",
      "2024-01-02 20:52:56.689841: Epoch time: 125.43 s\n",
      "2024-01-02 20:52:57.853707: \n",
      "2024-01-02 20:52:57.860633: Epoch 453\n",
      "2024-01-02 20:52:57.865630: Current learning rate: 0.00581\n",
      "2024-01-02 20:55:03.410885: train_loss -0.9135\n",
      "2024-01-02 20:55:03.417886: val_loss -0.8344\n",
      "2024-01-02 20:55:03.422886: Pseudo dice [0.9093, 0.9526, 0.9448]\n",
      "2024-01-02 20:55:03.428886: Epoch time: 125.56 s\n",
      "2024-01-02 20:55:04.762762: \n",
      "2024-01-02 20:55:04.771762: Epoch 454\n",
      "2024-01-02 20:55:04.778762: Current learning rate: 0.0058\n",
      "2024-01-02 20:57:10.329733: train_loss -0.9147\n",
      "2024-01-02 20:57:10.336733: val_loss -0.8307\n",
      "2024-01-02 20:57:10.344733: Pseudo dice [0.9094, 0.952, 0.9466]\n",
      "2024-01-02 20:57:10.351732: Epoch time: 125.57 s\n",
      "2024-01-02 20:57:11.455585: \n",
      "2024-01-02 20:57:11.463643: Epoch 455\n",
      "2024-01-02 20:57:11.469643: Current learning rate: 0.00579\n",
      "2024-01-02 20:59:16.845237: train_loss -0.9158\n",
      "2024-01-02 20:59:16.851231: val_loss -0.8318\n",
      "2024-01-02 20:59:16.857270: Pseudo dice [0.9119, 0.9515, 0.9454]\n",
      "2024-01-02 20:59:16.863233: Epoch time: 125.39 s\n",
      "2024-01-02 20:59:17.960770: \n",
      "2024-01-02 20:59:17.965403: Epoch 456\n",
      "2024-01-02 20:59:17.972478: Current learning rate: 0.00578\n",
      "2024-01-02 21:01:23.507941: train_loss -0.9138\n",
      "2024-01-02 21:01:23.513925: val_loss -0.8297\n",
      "2024-01-02 21:01:23.519928: Pseudo dice [0.9129, 0.9522, 0.9455]\n",
      "2024-01-02 21:01:23.525938: Epoch time: 125.55 s\n",
      "2024-01-02 21:01:24.632743: \n",
      "2024-01-02 21:01:24.638239: Epoch 457\n",
      "2024-01-02 21:01:24.645309: Current learning rate: 0.00577\n",
      "2024-01-02 21:03:30.278822: train_loss -0.9163\n",
      "2024-01-02 21:03:30.284822: val_loss -0.8304\n",
      "2024-01-02 21:03:30.289823: Pseudo dice [0.912, 0.9514, 0.9444]\n",
      "2024-01-02 21:03:30.294822: Epoch time: 125.65 s\n",
      "2024-01-02 21:03:31.367958: \n",
      "2024-01-02 21:03:31.373903: Epoch 458\n",
      "2024-01-02 21:03:31.377903: Current learning rate: 0.00576\n",
      "2024-01-02 21:05:37.065607: train_loss -0.9139\n",
      "2024-01-02 21:05:37.072608: val_loss -0.8262\n",
      "2024-01-02 21:05:37.077607: Pseudo dice [0.9092, 0.9502, 0.9463]\n",
      "2024-01-02 21:05:37.081607: Epoch time: 125.7 s\n",
      "2024-01-02 21:05:38.346949: \n",
      "2024-01-02 21:05:38.352948: Epoch 459\n",
      "2024-01-02 21:05:38.358940: Current learning rate: 0.00575\n",
      "2024-01-02 21:07:44.055187: train_loss -0.9142\n",
      "2024-01-02 21:07:44.062186: val_loss -0.8292\n",
      "2024-01-02 21:07:44.070173: Pseudo dice [0.9127, 0.9517, 0.9439]\n",
      "2024-01-02 21:07:44.078173: Epoch time: 125.71 s\n",
      "2024-01-02 21:07:45.315345: \n",
      "2024-01-02 21:07:45.321349: Epoch 460\n",
      "2024-01-02 21:07:45.325341: Current learning rate: 0.00574\n",
      "2024-01-02 21:09:50.761369: train_loss -0.9132\n",
      "2024-01-02 21:09:50.770364: val_loss -0.8303\n",
      "2024-01-02 21:09:50.776397: Pseudo dice [0.9134, 0.9513, 0.9445]\n",
      "2024-01-02 21:09:50.781367: Epoch time: 125.45 s\n",
      "2024-01-02 21:09:51.998938: \n",
      "2024-01-02 21:09:52.006886: Epoch 461\n",
      "2024-01-02 21:09:52.011929: Current learning rate: 0.00573\n",
      "2024-01-02 21:11:57.647096: train_loss -0.9141\n",
      "2024-01-02 21:11:57.657096: val_loss -0.8303\n",
      "2024-01-02 21:11:57.662096: Pseudo dice [0.9112, 0.9515, 0.9458]\n",
      "2024-01-02 21:11:57.667096: Epoch time: 125.65 s\n",
      "2024-01-02 21:11:58.778087: \n",
      "2024-01-02 21:11:58.783409: Epoch 462\n",
      "2024-01-02 21:11:58.788470: Current learning rate: 0.00572\n",
      "2024-01-02 21:14:04.635206: train_loss -0.9163\n",
      "2024-01-02 21:14:04.642205: val_loss -0.824\n",
      "2024-01-02 21:14:04.650204: Pseudo dice [0.9107, 0.9505, 0.9445]\n",
      "2024-01-02 21:14:04.658205: Epoch time: 125.86 s\n",
      "2024-01-02 21:14:05.838773: \n",
      "2024-01-02 21:14:05.847774: Epoch 463\n",
      "2024-01-02 21:14:05.852721: Current learning rate: 0.00571\n",
      "2024-01-02 21:16:11.599943: train_loss -0.9101\n",
      "2024-01-02 21:16:11.606937: val_loss -0.8368\n",
      "2024-01-02 21:16:11.611941: Pseudo dice [0.9117, 0.9524, 0.9453]\n",
      "2024-01-02 21:16:11.615937: Epoch time: 125.76 s\n",
      "2024-01-02 21:16:12.665339: \n",
      "2024-01-02 21:16:12.671346: Epoch 464\n",
      "2024-01-02 21:16:12.679353: Current learning rate: 0.0057\n",
      "2024-01-02 21:18:18.134480: train_loss -0.916\n",
      "2024-01-02 21:18:18.144486: val_loss -0.8327\n",
      "2024-01-02 21:18:18.150468: Pseudo dice [0.9119, 0.951, 0.9459]\n",
      "2024-01-02 21:18:18.155468: Epoch time: 125.47 s\n",
      "2024-01-02 21:18:19.359689: \n",
      "2024-01-02 21:18:19.365678: Epoch 465\n",
      "2024-01-02 21:18:19.370668: Current learning rate: 0.0057\n",
      "2024-01-02 21:20:25.002449: train_loss -0.9151\n",
      "2024-01-02 21:20:25.012449: val_loss -0.8342\n",
      "2024-01-02 21:20:25.021450: Pseudo dice [0.9124, 0.9522, 0.9462]\n",
      "2024-01-02 21:20:25.027452: Epoch time: 125.64 s\n",
      "2024-01-02 21:20:26.196415: \n",
      "2024-01-02 21:20:26.207347: Epoch 466\n",
      "2024-01-02 21:20:26.213365: Current learning rate: 0.00569\n",
      "2024-01-02 21:22:32.009260: train_loss -0.9152\n",
      "2024-01-02 21:22:32.016260: val_loss -0.8239\n",
      "2024-01-02 21:22:32.023771: Pseudo dice [0.9084, 0.9497, 0.9445]\n",
      "2024-01-02 21:22:32.028771: Epoch time: 125.81 s\n",
      "2024-01-02 21:22:33.284446: \n",
      "2024-01-02 21:22:33.290581: Epoch 467\n",
      "2024-01-02 21:22:33.294588: Current learning rate: 0.00568\n",
      "2024-01-02 21:24:42.658203: train_loss -0.9146\n",
      "2024-01-02 21:24:42.670208: val_loss -0.825\n",
      "2024-01-02 21:24:42.678211: Pseudo dice [0.912, 0.9501, 0.9435]\n",
      "2024-01-02 21:24:42.687209: Epoch time: 129.37 s\n",
      "2024-01-02 21:24:44.268346: \n",
      "2024-01-02 21:24:44.277349: Epoch 468\n",
      "2024-01-02 21:24:44.282344: Current learning rate: 0.00567\n",
      "2024-01-02 21:26:50.566805: train_loss -0.9142\n",
      "2024-01-02 21:26:50.575806: val_loss -0.8294\n",
      "2024-01-02 21:26:50.590522: Pseudo dice [0.9116, 0.9515, 0.9453]\n",
      "2024-01-02 21:26:50.595522: Epoch time: 126.3 s\n",
      "2024-01-02 21:26:51.859123: \n",
      "2024-01-02 21:26:51.864128: Epoch 469\n",
      "2024-01-02 21:26:51.869120: Current learning rate: 0.00566\n",
      "2024-01-02 21:28:58.208652: train_loss -0.9156\n",
      "2024-01-02 21:28:58.217660: val_loss -0.8339\n",
      "2024-01-02 21:28:58.224657: Pseudo dice [0.9122, 0.9528, 0.9467]\n",
      "2024-01-02 21:28:58.234178: Epoch time: 126.35 s\n",
      "2024-01-02 21:28:59.638053: \n",
      "2024-01-02 21:28:59.645054: Epoch 470\n",
      "2024-01-02 21:28:59.652054: Current learning rate: 0.00565\n",
      "2024-01-02 21:31:05.550396: train_loss -0.9128\n",
      "2024-01-02 21:31:05.556396: val_loss -0.8248\n",
      "2024-01-02 21:31:05.561396: Pseudo dice [0.91, 0.9491, 0.9451]\n",
      "2024-01-02 21:31:05.568394: Epoch time: 125.91 s\n",
      "2024-01-02 21:31:06.779625: \n",
      "2024-01-02 21:31:06.789240: Epoch 471\n",
      "2024-01-02 21:31:06.794260: Current learning rate: 0.00564\n",
      "2024-01-02 21:33:12.470517: train_loss -0.9151\n",
      "2024-01-02 21:33:12.477517: val_loss -0.8311\n",
      "2024-01-02 21:33:12.482581: Pseudo dice [0.9128, 0.9515, 0.9449]\n",
      "2024-01-02 21:33:12.491598: Epoch time: 125.69 s\n",
      "2024-01-02 21:33:13.692709: \n",
      "2024-01-02 21:33:13.697701: Epoch 472\n",
      "2024-01-02 21:33:13.703715: Current learning rate: 0.00563\n",
      "2024-01-02 21:35:20.082743: train_loss -0.9135\n",
      "2024-01-02 21:35:20.092743: val_loss -0.8286\n",
      "2024-01-02 21:35:20.100742: Pseudo dice [0.912, 0.9518, 0.9451]\n",
      "2024-01-02 21:35:20.107743: Epoch time: 126.39 s\n",
      "2024-01-02 21:35:21.475137: \n",
      "2024-01-02 21:35:21.484120: Epoch 473\n",
      "2024-01-02 21:35:21.489057: Current learning rate: 0.00562\n",
      "2024-01-02 21:37:27.194676: train_loss -0.9121\n",
      "2024-01-02 21:37:27.201675: val_loss -0.8346\n",
      "2024-01-02 21:37:27.206185: Pseudo dice [0.9136, 0.952, 0.9453]\n",
      "2024-01-02 21:37:27.211185: Epoch time: 125.72 s\n",
      "2024-01-02 21:37:28.278050: \n",
      "2024-01-02 21:37:28.286052: Epoch 474\n",
      "2024-01-02 21:37:28.296136: Current learning rate: 0.00561\n",
      "2024-01-02 21:39:34.165871: train_loss -0.9124\n",
      "2024-01-02 21:39:34.176872: val_loss -0.8263\n",
      "2024-01-02 21:39:34.185871: Pseudo dice [0.9111, 0.952, 0.9445]\n",
      "2024-01-02 21:39:34.193873: Epoch time: 125.89 s\n",
      "2024-01-02 21:39:35.416175: \n",
      "2024-01-02 21:39:35.422279: Epoch 475\n",
      "2024-01-02 21:39:35.427333: Current learning rate: 0.0056\n",
      "2024-01-02 21:41:40.975429: train_loss -0.9157\n",
      "2024-01-02 21:41:40.981415: val_loss -0.835\n",
      "2024-01-02 21:41:40.988421: Pseudo dice [0.9146, 0.9514, 0.9469]\n",
      "2024-01-02 21:41:40.993414: Epoch time: 125.56 s\n",
      "2024-01-02 21:41:42.278628: \n",
      "2024-01-02 21:41:42.289126: Epoch 476\n",
      "2024-01-02 21:41:42.300095: Current learning rate: 0.00559\n",
      "2024-01-02 21:43:48.085438: train_loss -0.9136\n",
      "2024-01-02 21:43:48.095953: val_loss -0.8272\n",
      "2024-01-02 21:43:48.102960: Pseudo dice [0.91, 0.9512, 0.9455]\n",
      "2024-01-02 21:43:48.108959: Epoch time: 125.81 s\n",
      "2024-01-02 21:43:49.202962: \n",
      "2024-01-02 21:43:49.212023: Epoch 477\n",
      "2024-01-02 21:43:49.219038: Current learning rate: 0.00558\n",
      "2024-01-02 21:45:54.872781: train_loss -0.9139\n",
      "2024-01-02 21:45:54.879287: val_loss -0.8273\n",
      "2024-01-02 21:45:54.884289: Pseudo dice [0.912, 0.9502, 0.9456]\n",
      "2024-01-02 21:45:54.889293: Epoch time: 125.67 s\n",
      "2024-01-02 21:45:56.069475: \n",
      "2024-01-02 21:45:56.080163: Epoch 478\n",
      "2024-01-02 21:45:56.085094: Current learning rate: 0.00557\n",
      "2024-01-02 21:48:01.744313: train_loss -0.9126\n",
      "2024-01-02 21:48:01.751320: val_loss -0.8253\n",
      "2024-01-02 21:48:01.757331: Pseudo dice [0.9106, 0.9502, 0.9458]\n",
      "2024-01-02 21:48:01.764850: Epoch time: 125.68 s\n",
      "2024-01-02 21:48:02.858218: \n",
      "2024-01-02 21:48:02.864823: Epoch 479\n",
      "2024-01-02 21:48:02.869819: Current learning rate: 0.00556\n",
      "2024-01-02 21:50:08.573791: train_loss -0.9143\n",
      "2024-01-02 21:50:08.579790: val_loss -0.8289\n",
      "2024-01-02 21:50:08.584788: Pseudo dice [0.9132, 0.9514, 0.9451]\n",
      "2024-01-02 21:50:08.589787: Epoch time: 125.72 s\n",
      "2024-01-02 21:50:09.714587: \n",
      "2024-01-02 21:50:09.720594: Epoch 480\n",
      "2024-01-02 21:50:09.724603: Current learning rate: 0.00555\n",
      "2024-01-02 21:52:15.187833: train_loss -0.915\n",
      "2024-01-02 21:52:15.194340: val_loss -0.8281\n",
      "2024-01-02 21:52:15.200340: Pseudo dice [0.912, 0.9502, 0.9439]\n",
      "2024-01-02 21:52:15.208341: Epoch time: 125.47 s\n",
      "2024-01-02 21:52:16.329572: \n",
      "2024-01-02 21:52:16.336717: Epoch 481\n",
      "2024-01-02 21:52:16.340760: Current learning rate: 0.00554\n",
      "2024-01-02 21:54:22.311717: train_loss -0.9153\n",
      "2024-01-02 21:54:22.318717: val_loss -0.8284\n",
      "2024-01-02 21:54:22.327716: Pseudo dice [0.9121, 0.9503, 0.944]\n",
      "2024-01-02 21:54:22.334717: Epoch time: 125.98 s\n",
      "2024-01-02 21:54:23.534093: \n",
      "2024-01-02 21:54:23.542626: Epoch 482\n",
      "2024-01-02 21:54:23.547623: Current learning rate: 0.00553\n",
      "2024-01-02 21:56:29.148035: train_loss -0.9134\n",
      "2024-01-02 21:56:29.154035: val_loss -0.8226\n",
      "2024-01-02 21:56:29.161036: Pseudo dice [0.9122, 0.9514, 0.9446]\n",
      "2024-01-02 21:56:29.166035: Epoch time: 125.62 s\n",
      "2024-01-02 21:56:30.310292: \n",
      "2024-01-02 21:56:30.318365: Epoch 483\n",
      "2024-01-02 21:56:30.323354: Current learning rate: 0.00552\n",
      "2024-01-02 21:58:36.080084: train_loss -0.9125\n",
      "2024-01-02 21:58:36.089083: val_loss -0.8333\n",
      "2024-01-02 21:58:36.094591: Pseudo dice [0.9151, 0.9519, 0.9453]\n",
      "2024-01-02 21:58:36.100592: Epoch time: 125.77 s\n",
      "2024-01-02 21:58:37.501641: \n",
      "2024-01-02 21:58:37.509706: Epoch 484\n",
      "2024-01-02 21:58:37.514645: Current learning rate: 0.00551\n",
      "2024-01-02 22:00:43.181894: train_loss -0.9146\n",
      "2024-01-02 22:00:43.188895: val_loss -0.8335\n",
      "2024-01-02 22:00:43.193894: Pseudo dice [0.9112, 0.9528, 0.9445]\n",
      "2024-01-02 22:00:43.197897: Epoch time: 125.68 s\n",
      "2024-01-02 22:00:44.297850: \n",
      "2024-01-02 22:00:44.303793: Epoch 485\n",
      "2024-01-02 22:00:44.308785: Current learning rate: 0.0055\n",
      "2024-01-02 22:02:50.064646: train_loss -0.9143\n",
      "2024-01-02 22:02:50.075647: val_loss -0.8277\n",
      "2024-01-02 22:02:50.081645: Pseudo dice [0.9114, 0.9493, 0.9444]\n",
      "2024-01-02 22:02:50.088647: Epoch time: 125.77 s\n",
      "2024-01-02 22:02:51.270185: \n",
      "2024-01-02 22:02:51.275553: Epoch 486\n",
      "2024-01-02 22:02:51.280622: Current learning rate: 0.00549\n",
      "2024-01-02 22:04:57.095138: train_loss -0.915\n",
      "2024-01-02 22:04:57.104139: val_loss -0.8329\n",
      "2024-01-02 22:04:57.110140: Pseudo dice [0.9091, 0.9534, 0.9459]\n",
      "2024-01-02 22:04:57.118139: Epoch time: 125.83 s\n",
      "2024-01-02 22:04:58.260961: \n",
      "2024-01-02 22:04:58.266967: Epoch 487\n",
      "2024-01-02 22:04:58.271976: Current learning rate: 0.00548\n",
      "2024-01-02 22:07:03.964940: train_loss -0.9115\n",
      "2024-01-02 22:07:03.971453: val_loss -0.8299\n",
      "2024-01-02 22:07:03.977457: Pseudo dice [0.9121, 0.9511, 0.9433]\n",
      "2024-01-02 22:07:03.981527: Epoch time: 125.7 s\n",
      "2024-01-02 22:07:05.076899: \n",
      "2024-01-02 22:07:05.082683: Epoch 488\n",
      "2024-01-02 22:07:05.087679: Current learning rate: 0.00547\n",
      "2024-01-02 22:09:10.761430: train_loss -0.9132\n",
      "2024-01-02 22:09:10.776423: val_loss -0.8307\n",
      "2024-01-02 22:09:10.786423: Pseudo dice [0.9122, 0.9523, 0.9447]\n",
      "2024-01-02 22:09:10.794423: Epoch time: 125.69 s\n",
      "2024-01-02 22:09:11.966260: \n",
      "2024-01-02 22:09:11.972605: Epoch 489\n",
      "2024-01-02 22:09:11.978662: Current learning rate: 0.00546\n",
      "2024-01-02 22:11:18.157259: train_loss -0.9129\n",
      "2024-01-02 22:11:18.165260: val_loss -0.8297\n",
      "2024-01-02 22:11:18.170775: Pseudo dice [0.9103, 0.9516, 0.9454]\n",
      "2024-01-02 22:11:18.180789: Epoch time: 126.19 s\n",
      "2024-01-02 22:11:19.410695: \n",
      "2024-01-02 22:11:19.416686: Epoch 490\n",
      "2024-01-02 22:11:19.424690: Current learning rate: 0.00546\n",
      "2024-01-02 22:13:25.310860: train_loss -0.9114\n",
      "2024-01-02 22:13:25.319860: val_loss -0.827\n",
      "2024-01-02 22:13:25.327860: Pseudo dice [0.9115, 0.9506, 0.946]\n",
      "2024-01-02 22:13:25.333377: Epoch time: 125.9 s\n",
      "2024-01-02 22:13:26.434423: \n",
      "2024-01-02 22:13:26.440606: Epoch 491\n",
      "2024-01-02 22:13:26.447258: Current learning rate: 0.00545\n",
      "2024-01-02 22:15:33.229265: train_loss -0.913\n",
      "2024-01-02 22:15:33.240371: val_loss -0.8311\n",
      "2024-01-02 22:15:33.248371: Pseudo dice [0.9139, 0.952, 0.9446]\n",
      "2024-01-02 22:15:33.255371: Epoch time: 126.8 s\n",
      "2024-01-02 22:15:34.942726: \n",
      "2024-01-02 22:15:34.950732: Epoch 492\n",
      "2024-01-02 22:15:34.955731: Current learning rate: 0.00544\n",
      "2024-01-02 22:17:40.666818: train_loss -0.9135\n",
      "2024-01-02 22:17:40.673818: val_loss -0.8265\n",
      "2024-01-02 22:17:40.680827: Pseudo dice [0.9127, 0.9516, 0.9464]\n",
      "2024-01-02 22:17:40.687819: Epoch time: 125.73 s\n",
      "2024-01-02 22:17:41.818467: \n",
      "2024-01-02 22:17:41.831698: Epoch 493\n",
      "2024-01-02 22:17:41.836780: Current learning rate: 0.00543\n",
      "2024-01-02 22:19:47.520922: train_loss -0.9152\n",
      "2024-01-02 22:19:47.529934: val_loss -0.8284\n",
      "2024-01-02 22:19:47.535932: Pseudo dice [0.9134, 0.9516, 0.9434]\n",
      "2024-01-02 22:19:47.540927: Epoch time: 125.7 s\n",
      "2024-01-02 22:19:48.698537: \n",
      "2024-01-02 22:19:48.703984: Epoch 494\n",
      "2024-01-02 22:19:48.712126: Current learning rate: 0.00542\n",
      "2024-01-02 22:21:54.395770: train_loss -0.9137\n",
      "2024-01-02 22:21:54.405771: val_loss -0.8339\n",
      "2024-01-02 22:21:54.410769: Pseudo dice [0.9142, 0.9519, 0.9454]\n",
      "2024-01-02 22:21:54.417772: Epoch time: 125.7 s\n",
      "2024-01-02 22:21:55.499660: \n",
      "2024-01-02 22:21:55.507272: Epoch 495\n",
      "2024-01-02 22:21:55.515242: Current learning rate: 0.00541\n",
      "2024-01-02 22:24:01.383655: train_loss -0.9126\n",
      "2024-01-02 22:24:01.390665: val_loss -0.8289\n",
      "2024-01-02 22:24:01.397665: Pseudo dice [0.9123, 0.9521, 0.946]\n",
      "2024-01-02 22:24:01.405656: Epoch time: 125.88 s\n",
      "2024-01-02 22:24:02.533905: \n",
      "2024-01-02 22:24:02.541972: Epoch 496\n",
      "2024-01-02 22:24:02.545968: Current learning rate: 0.0054\n",
      "2024-01-02 22:26:08.297822: train_loss -0.9135\n",
      "2024-01-02 22:26:08.304827: val_loss -0.8276\n",
      "2024-01-02 22:26:08.312836: Pseudo dice [0.911, 0.951, 0.9457]\n",
      "2024-01-02 22:26:08.317837: Epoch time: 125.77 s\n",
      "2024-01-02 22:26:09.505621: \n",
      "2024-01-02 22:26:09.512626: Epoch 497\n",
      "2024-01-02 22:26:09.516626: Current learning rate: 0.00539\n",
      "2024-01-02 22:28:15.345463: train_loss -0.9127\n",
      "2024-01-02 22:28:15.354476: val_loss -0.8286\n",
      "2024-01-02 22:28:15.360466: Pseudo dice [0.9114, 0.9509, 0.944]\n",
      "2024-01-02 22:28:15.365463: Epoch time: 125.84 s\n",
      "2024-01-02 22:28:16.490705: \n",
      "2024-01-02 22:28:16.500268: Epoch 498\n",
      "2024-01-02 22:28:16.511314: Current learning rate: 0.00538\n",
      "2024-01-02 22:30:22.153949: train_loss -0.9154\n",
      "2024-01-02 22:30:22.160949: val_loss -0.8205\n",
      "2024-01-02 22:30:22.165951: Pseudo dice [0.9097, 0.9494, 0.9457]\n",
      "2024-01-02 22:30:22.170951: Epoch time: 125.66 s\n",
      "2024-01-02 22:30:23.465707: \n",
      "2024-01-02 22:30:23.470706: Epoch 499\n",
      "2024-01-02 22:30:23.475704: Current learning rate: 0.00537\n",
      "2024-01-02 22:32:29.090318: train_loss -0.9172\n",
      "2024-01-02 22:32:29.097322: val_loss -0.8251\n",
      "2024-01-02 22:32:29.103318: Pseudo dice [0.912, 0.9504, 0.9448]\n",
      "2024-01-02 22:32:29.107322: Epoch time: 125.63 s\n",
      "2024-01-02 22:32:30.646873: \n",
      "2024-01-02 22:32:30.651887: Epoch 500\n",
      "2024-01-02 22:32:30.656577: Current learning rate: 0.00536\n",
      "2024-01-02 22:34:36.383373: train_loss -0.9164\n",
      "2024-01-02 22:34:36.390370: val_loss -0.8311\n",
      "2024-01-02 22:34:36.397372: Pseudo dice [0.9121, 0.9519, 0.9445]\n",
      "2024-01-02 22:34:36.402368: Epoch time: 125.74 s\n",
      "2024-01-02 22:34:37.530297: \n",
      "2024-01-02 22:34:37.540152: Epoch 501\n",
      "2024-01-02 22:34:37.545815: Current learning rate: 0.00535\n",
      "2024-01-02 22:36:43.442347: train_loss -0.914\n",
      "2024-01-02 22:36:43.451346: val_loss -0.8275\n",
      "2024-01-02 22:36:43.456346: Pseudo dice [0.9131, 0.9507, 0.9459]\n",
      "2024-01-02 22:36:43.463349: Epoch time: 125.92 s\n",
      "2024-01-02 22:36:44.852215: \n",
      "2024-01-02 22:36:44.858228: Epoch 502\n",
      "2024-01-02 22:36:44.862221: Current learning rate: 0.00534\n",
      "2024-01-02 22:38:50.415629: train_loss -0.9174\n",
      "2024-01-02 22:38:50.429629: val_loss -0.8281\n",
      "2024-01-02 22:38:50.439629: Pseudo dice [0.9097, 0.9509, 0.9442]\n",
      "2024-01-02 22:38:50.445630: Epoch time: 125.57 s\n",
      "2024-01-02 22:38:51.612438: \n",
      "2024-01-02 22:38:51.621380: Epoch 503\n",
      "2024-01-02 22:38:51.625624: Current learning rate: 0.00533\n",
      "2024-01-02 22:40:57.299375: train_loss -0.917\n",
      "2024-01-02 22:40:57.305376: val_loss -0.8307\n",
      "2024-01-02 22:40:57.310376: Pseudo dice [0.9125, 0.9509, 0.9452]\n",
      "2024-01-02 22:40:57.316378: Epoch time: 125.69 s\n",
      "2024-01-02 22:40:58.541703: \n",
      "2024-01-02 22:40:58.550332: Epoch 504\n",
      "2024-01-02 22:40:58.555325: Current learning rate: 0.00532\n",
      "2024-01-02 22:43:04.367974: train_loss -0.9153\n",
      "2024-01-02 22:43:04.373975: val_loss -0.8322\n",
      "2024-01-02 22:43:04.379972: Pseudo dice [0.9117, 0.9516, 0.944]\n",
      "2024-01-02 22:43:04.384971: Epoch time: 125.83 s\n",
      "2024-01-02 22:43:05.531755: \n",
      "2024-01-02 22:43:05.538822: Epoch 505\n",
      "2024-01-02 22:43:05.543821: Current learning rate: 0.00531\n",
      "2024-01-02 22:45:11.410984: train_loss -0.9128\n",
      "2024-01-02 22:45:11.419984: val_loss -0.8325\n",
      "2024-01-02 22:45:11.426980: Pseudo dice [0.9128, 0.9519, 0.9468]\n",
      "2024-01-02 22:45:11.432977: Epoch time: 125.88 s\n",
      "2024-01-02 22:45:12.516032: \n",
      "2024-01-02 22:45:12.522032: Epoch 506\n",
      "2024-01-02 22:45:12.526542: Current learning rate: 0.0053\n",
      "2024-01-02 22:47:18.192501: train_loss -0.9154\n",
      "2024-01-02 22:47:18.199008: val_loss -0.8274\n",
      "2024-01-02 22:47:18.204008: Pseudo dice [0.9137, 0.9507, 0.9458]\n",
      "2024-01-02 22:47:18.209008: Epoch time: 125.68 s\n",
      "2024-01-02 22:47:19.562593: \n",
      "2024-01-02 22:47:19.568660: Epoch 507\n",
      "2024-01-02 22:47:19.573677: Current learning rate: 0.00529\n",
      "2024-01-02 22:49:25.245721: train_loss -0.9178\n",
      "2024-01-02 22:49:25.252710: val_loss -0.8309\n",
      "2024-01-02 22:49:25.260225: Pseudo dice [0.9123, 0.9509, 0.9449]\n",
      "2024-01-02 22:49:25.265222: Epoch time: 125.69 s\n",
      "2024-01-02 22:49:26.576809: \n",
      "2024-01-02 22:49:26.581809: Epoch 508\n",
      "2024-01-02 22:49:26.586809: Current learning rate: 0.00528\n",
      "2024-01-02 22:51:32.283693: train_loss -0.9168\n",
      "2024-01-02 22:51:32.292693: val_loss -0.8256\n",
      "2024-01-02 22:51:32.299694: Pseudo dice [0.9094, 0.951, 0.9438]\n",
      "2024-01-02 22:51:32.304695: Epoch time: 125.71 s\n",
      "2024-01-02 22:51:33.460294: \n",
      "2024-01-02 22:51:33.472955: Epoch 509\n",
      "2024-01-02 22:51:33.481946: Current learning rate: 0.00527\n",
      "2024-01-02 22:53:39.253739: train_loss -0.9134\n",
      "2024-01-02 22:53:39.261746: val_loss -0.8315\n",
      "2024-01-02 22:53:39.269747: Pseudo dice [0.9097, 0.9525, 0.9429]\n",
      "2024-01-02 22:53:39.278085: Epoch time: 125.8 s\n",
      "2024-01-02 22:53:40.406945: \n",
      "2024-01-02 22:53:40.414253: Epoch 510\n",
      "2024-01-02 22:53:40.419342: Current learning rate: 0.00526\n",
      "2024-01-02 22:55:46.074291: train_loss -0.9129\n",
      "2024-01-02 22:55:46.082294: val_loss -0.826\n",
      "2024-01-02 22:55:46.089290: Pseudo dice [0.9097, 0.9518, 0.9467]\n",
      "2024-01-02 22:55:46.094306: Epoch time: 125.67 s\n",
      "2024-01-02 22:55:47.247240: \n",
      "2024-01-02 22:55:47.254313: Epoch 511\n",
      "2024-01-02 22:55:47.262326: Current learning rate: 0.00525\n",
      "2024-01-02 22:57:53.267362: train_loss -0.9149\n",
      "2024-01-02 22:57:53.275362: val_loss -0.8288\n",
      "2024-01-02 22:57:53.280362: Pseudo dice [0.911, 0.951, 0.9448]\n",
      "2024-01-02 22:57:53.289364: Epoch time: 126.02 s\n",
      "2024-01-02 22:57:54.730079: \n",
      "2024-01-02 22:57:54.739119: Epoch 512\n",
      "2024-01-02 22:57:54.744043: Current learning rate: 0.00524\n",
      "2024-01-02 23:00:00.520190: train_loss -0.9148\n",
      "2024-01-02 23:00:00.528190: val_loss -0.8241\n",
      "2024-01-02 23:00:00.533189: Pseudo dice [0.9094, 0.9495, 0.943]\n",
      "2024-01-02 23:00:00.538189: Epoch time: 125.79 s\n",
      "2024-01-02 23:00:01.669620: \n",
      "2024-01-02 23:00:01.681062: Epoch 513\n",
      "2024-01-02 23:00:01.687140: Current learning rate: 0.00523\n",
      "2024-01-02 23:02:07.479821: train_loss -0.9107\n",
      "2024-01-02 23:02:07.486334: val_loss -0.8309\n",
      "2024-01-02 23:02:07.491349: Pseudo dice [0.909, 0.9509, 0.945]\n",
      "2024-01-02 23:02:07.497341: Epoch time: 125.81 s\n",
      "2024-01-02 23:02:08.627218: \n",
      "2024-01-02 23:02:08.633297: Epoch 514\n",
      "2024-01-02 23:02:08.638277: Current learning rate: 0.00522\n",
      "2024-01-02 23:04:14.168780: train_loss -0.8778\n",
      "2024-01-02 23:04:14.175789: val_loss -0.844\n",
      "2024-01-02 23:04:14.181787: Pseudo dice [0.9106, 0.9517, 0.9448]\n",
      "2024-01-02 23:04:14.188783: Epoch time: 125.54 s\n",
      "2024-01-02 23:04:15.594181: \n",
      "2024-01-02 23:04:15.599711: Epoch 515\n",
      "2024-01-02 23:04:15.604771: Current learning rate: 0.00521\n",
      "2024-01-02 23:06:21.313776: train_loss -0.8949\n",
      "2024-01-02 23:06:21.321787: val_loss -0.8401\n",
      "2024-01-02 23:06:21.329775: Pseudo dice [0.9128, 0.9518, 0.9449]\n",
      "2024-01-02 23:06:21.337776: Epoch time: 125.72 s\n",
      "2024-01-02 23:06:22.779181: \n",
      "2024-01-02 23:06:22.785187: Epoch 516\n",
      "2024-01-02 23:06:22.791257: Current learning rate: 0.0052\n",
      "2024-01-02 23:08:28.342033: train_loss -0.9012\n",
      "2024-01-02 23:08:28.351540: val_loss -0.8343\n",
      "2024-01-02 23:08:28.357540: Pseudo dice [0.9122, 0.9503, 0.9446]\n",
      "2024-01-02 23:08:28.361540: Epoch time: 125.56 s\n",
      "2024-01-02 23:08:29.506792: \n",
      "2024-01-02 23:08:29.511734: Epoch 517\n",
      "2024-01-02 23:08:29.515734: Current learning rate: 0.00519\n",
      "2024-01-02 23:10:35.281651: train_loss -0.9041\n",
      "2024-01-02 23:10:35.288650: val_loss -0.832\n",
      "2024-01-02 23:10:35.294649: Pseudo dice [0.9102, 0.9497, 0.9439]\n",
      "2024-01-02 23:10:35.300674: Epoch time: 125.78 s\n",
      "2024-01-02 23:10:36.438504: \n",
      "2024-01-02 23:10:36.446569: Epoch 518\n",
      "2024-01-02 23:10:36.455610: Current learning rate: 0.00518\n",
      "2024-01-02 23:12:42.124921: train_loss -0.9058\n",
      "2024-01-02 23:12:42.133913: val_loss -0.8269\n",
      "2024-01-02 23:12:42.139921: Pseudo dice [0.9075, 0.9496, 0.9455]\n",
      "2024-01-02 23:12:42.146417: Epoch time: 125.69 s\n",
      "2024-01-02 23:12:43.478568: \n",
      "2024-01-02 23:12:43.483554: Epoch 519\n",
      "2024-01-02 23:12:43.491633: Current learning rate: 0.00518\n",
      "2024-01-02 23:14:49.007701: train_loss -0.9077\n",
      "2024-01-02 23:14:49.017701: val_loss -0.8366\n",
      "2024-01-02 23:14:49.023701: Pseudo dice [0.9111, 0.9531, 0.9462]\n",
      "2024-01-02 23:14:49.029702: Epoch time: 125.53 s\n",
      "2024-01-02 23:14:50.262255: \n",
      "2024-01-02 23:14:50.267407: Epoch 520\n",
      "2024-01-02 23:14:50.271409: Current learning rate: 0.00517\n",
      "2024-01-02 23:16:55.882294: train_loss -0.9114\n",
      "2024-01-02 23:16:55.892292: val_loss -0.8282\n",
      "2024-01-02 23:16:55.899292: Pseudo dice [0.9125, 0.9489, 0.9459]\n",
      "2024-01-02 23:16:55.908291: Epoch time: 125.62 s\n",
      "2024-01-02 23:16:57.118429: \n",
      "2024-01-02 23:16:57.125490: Epoch 521\n",
      "2024-01-02 23:16:57.130360: Current learning rate: 0.00516\n",
      "2024-01-02 23:19:02.514765: train_loss -0.913\n",
      "2024-01-02 23:19:02.521293: val_loss -0.8354\n",
      "2024-01-02 23:19:02.529291: Pseudo dice [0.9143, 0.9508, 0.9449]\n",
      "2024-01-02 23:19:02.537294: Epoch time: 125.4 s\n",
      "2024-01-02 23:19:03.755911: \n",
      "2024-01-02 23:19:03.762124: Epoch 522\n",
      "2024-01-02 23:19:03.769063: Current learning rate: 0.00515\n",
      "2024-01-02 23:21:10.207457: train_loss -0.9105\n",
      "2024-01-02 23:21:10.216016: val_loss -0.8361\n",
      "2024-01-02 23:21:10.222016: Pseudo dice [0.9128, 0.9525, 0.9459]\n",
      "2024-01-02 23:21:10.227016: Epoch time: 126.45 s\n",
      "2024-01-02 23:21:11.741821: \n",
      "2024-01-02 23:21:11.746816: Epoch 523\n",
      "2024-01-02 23:21:11.751828: Current learning rate: 0.00514\n",
      "2024-01-02 23:23:17.383164: train_loss -0.9119\n",
      "2024-01-02 23:23:17.392829: val_loss -0.8322\n",
      "2024-01-02 23:23:17.397822: Pseudo dice [0.9088, 0.9514, 0.9468]\n",
      "2024-01-02 23:23:17.403882: Epoch time: 125.64 s\n",
      "2024-01-02 23:23:18.480358: \n",
      "2024-01-02 23:23:18.486359: Epoch 524\n",
      "2024-01-02 23:23:18.491385: Current learning rate: 0.00513\n",
      "2024-01-02 23:25:24.406900: train_loss -0.9111\n",
      "2024-01-02 23:25:24.413899: val_loss -0.8295\n",
      "2024-01-02 23:25:24.420907: Pseudo dice [0.9099, 0.9501, 0.9442]\n",
      "2024-01-02 23:25:24.427908: Epoch time: 125.93 s\n",
      "2024-01-02 23:25:25.758337: \n",
      "2024-01-02 23:25:25.763414: Epoch 525\n",
      "2024-01-02 23:25:25.768412: Current learning rate: 0.00512\n",
      "2024-01-02 23:27:31.545877: train_loss -0.9094\n",
      "2024-01-02 23:27:31.556892: val_loss -0.8342\n",
      "2024-01-02 23:27:31.564891: Pseudo dice [0.9129, 0.9518, 0.9439]\n",
      "2024-01-02 23:27:31.572878: Epoch time: 125.79 s\n",
      "2024-01-02 23:27:32.678662: \n",
      "2024-01-02 23:27:32.683786: Epoch 526\n",
      "2024-01-02 23:27:32.689441: Current learning rate: 0.00511\n",
      "2024-01-02 23:29:38.052566: train_loss -0.9165\n",
      "2024-01-02 23:29:38.062570: val_loss -0.83\n",
      "2024-01-02 23:29:38.067570: Pseudo dice [0.9121, 0.9513, 0.9451]\n",
      "2024-01-02 23:29:38.073570: Epoch time: 125.37 s\n",
      "2024-01-02 23:29:39.177122: \n",
      "2024-01-02 23:29:39.183705: Epoch 527\n",
      "2024-01-02 23:29:39.188648: Current learning rate: 0.0051\n",
      "2024-01-02 23:31:44.738974: train_loss -0.9168\n",
      "2024-01-02 23:31:44.745977: val_loss -0.8254\n",
      "2024-01-02 23:31:44.751977: Pseudo dice [0.9096, 0.9513, 0.9437]\n",
      "2024-01-02 23:31:44.756977: Epoch time: 125.56 s\n",
      "2024-01-02 23:31:45.877260: \n",
      "2024-01-02 23:31:45.883321: Epoch 528\n",
      "2024-01-02 23:31:45.888328: Current learning rate: 0.00509\n",
      "2024-01-02 23:33:51.482724: train_loss -0.9117\n",
      "2024-01-02 23:33:51.493726: val_loss -0.8303\n",
      "2024-01-02 23:33:51.500741: Pseudo dice [0.9132, 0.9517, 0.9453]\n",
      "2024-01-02 23:33:51.505741: Epoch time: 125.61 s\n",
      "2024-01-02 23:33:52.709473: \n",
      "2024-01-02 23:33:52.715391: Epoch 529\n",
      "2024-01-02 23:33:52.724216: Current learning rate: 0.00508\n",
      "2024-01-02 23:35:58.156445: train_loss -0.9146\n",
      "2024-01-02 23:35:58.165457: val_loss -0.8317\n",
      "2024-01-02 23:35:58.173458: Pseudo dice [0.9131, 0.951, 0.9459]\n",
      "2024-01-02 23:35:58.179443: Epoch time: 125.45 s\n",
      "2024-01-02 23:35:59.400187: \n",
      "2024-01-02 23:35:59.408251: Epoch 530\n",
      "2024-01-02 23:35:59.416256: Current learning rate: 0.00507\n",
      "2024-01-02 23:38:05.317236: train_loss -0.9133\n",
      "2024-01-02 23:38:05.327237: val_loss -0.8359\n",
      "2024-01-02 23:38:05.335237: Pseudo dice [0.9118, 0.953, 0.9445]\n",
      "2024-01-02 23:38:05.341242: Epoch time: 125.92 s\n",
      "2024-01-02 23:38:06.624084: \n",
      "2024-01-02 23:38:06.633058: Epoch 531\n",
      "2024-01-02 23:38:06.642025: Current learning rate: 0.00506\n",
      "2024-01-02 23:40:12.404724: train_loss -0.915\n",
      "2024-01-02 23:40:12.412725: val_loss -0.8398\n",
      "2024-01-02 23:40:12.417725: Pseudo dice [0.9143, 0.9534, 0.946]\n",
      "2024-01-02 23:40:12.422726: Epoch time: 125.78 s\n",
      "2024-01-02 23:40:13.703919: \n",
      "2024-01-02 23:40:13.710919: Epoch 532\n",
      "2024-01-02 23:40:13.715918: Current learning rate: 0.00505\n",
      "2024-01-02 23:42:19.063168: train_loss -0.9179\n",
      "2024-01-02 23:42:19.070174: val_loss -0.8358\n",
      "2024-01-02 23:42:19.074168: Pseudo dice [0.9117, 0.953, 0.9463]\n",
      "2024-01-02 23:42:19.079962: Epoch time: 125.36 s\n",
      "2024-01-02 23:42:20.195359: \n",
      "2024-01-02 23:42:20.201367: Epoch 533\n",
      "2024-01-02 23:42:20.206377: Current learning rate: 0.00504\n",
      "2024-01-02 23:44:25.854775: train_loss -0.9116\n",
      "2024-01-02 23:44:25.862775: val_loss -0.8312\n",
      "2024-01-02 23:44:25.867775: Pseudo dice [0.9122, 0.9523, 0.9439]\n",
      "2024-01-02 23:44:25.872775: Epoch time: 125.66 s\n",
      "2024-01-02 23:44:26.992525: \n",
      "2024-01-02 23:44:26.998023: Epoch 534\n",
      "2024-01-02 23:44:27.003025: Current learning rate: 0.00503\n",
      "2024-01-02 23:46:32.732919: train_loss -0.9165\n",
      "2024-01-02 23:46:32.739918: val_loss -0.8325\n",
      "2024-01-02 23:46:32.745918: Pseudo dice [0.9119, 0.9518, 0.9455]\n",
      "2024-01-02 23:46:32.752916: Epoch time: 125.74 s\n",
      "2024-01-02 23:46:33.950402: \n",
      "2024-01-02 23:46:33.956459: Epoch 535\n",
      "2024-01-02 23:46:33.960462: Current learning rate: 0.00502\n",
      "2024-01-02 23:48:39.537029: train_loss -0.9152\n",
      "2024-01-02 23:48:39.543029: val_loss -0.8279\n",
      "2024-01-02 23:48:39.548020: Pseudo dice [0.9122, 0.9502, 0.9447]\n",
      "2024-01-02 23:48:39.555020: Epoch time: 125.59 s\n",
      "2024-01-02 23:48:40.644605: \n",
      "2024-01-02 23:48:40.650055: Epoch 536\n",
      "2024-01-02 23:48:40.657065: Current learning rate: 0.00501\n",
      "2024-01-02 23:50:46.378493: train_loss -0.9151\n",
      "2024-01-02 23:50:46.385494: val_loss -0.8314\n",
      "2024-01-02 23:50:46.393494: Pseudo dice [0.9108, 0.9512, 0.9446]\n",
      "2024-01-02 23:50:46.400494: Epoch time: 125.73 s\n",
      "2024-01-02 23:50:47.782369: \n",
      "2024-01-02 23:50:47.788303: Epoch 537\n",
      "2024-01-02 23:50:47.793305: Current learning rate: 0.005\n",
      "2024-01-02 23:52:53.309391: train_loss -0.9171\n",
      "2024-01-02 23:52:53.316900: val_loss -0.8256\n",
      "2024-01-02 23:52:53.322900: Pseudo dice [0.9105, 0.9519, 0.9453]\n",
      "2024-01-02 23:52:53.330327: Epoch time: 125.53 s\n",
      "2024-01-02 23:52:54.826240: \n",
      "2024-01-02 23:52:54.835324: Epoch 538\n",
      "2024-01-02 23:52:54.841324: Current learning rate: 0.00499\n",
      "2024-01-02 23:55:00.407863: train_loss -0.915\n",
      "2024-01-02 23:55:00.416863: val_loss -0.8275\n",
      "2024-01-02 23:55:00.421858: Pseudo dice [0.9117, 0.951, 0.9456]\n",
      "2024-01-02 23:55:00.427856: Epoch time: 125.58 s\n",
      "2024-01-02 23:55:01.609659: \n",
      "2024-01-02 23:55:01.616295: Epoch 539\n",
      "2024-01-02 23:55:01.621321: Current learning rate: 0.00498\n",
      "2024-01-02 23:57:07.218277: train_loss -0.9163\n",
      "2024-01-02 23:57:07.225278: val_loss -0.8292\n",
      "2024-01-02 23:57:07.233279: Pseudo dice [0.9134, 0.951, 0.9435]\n",
      "2024-01-02 23:57:07.239279: Epoch time: 125.61 s\n",
      "2024-01-02 23:57:08.462240: \n",
      "2024-01-02 23:57:08.471569: Epoch 540\n",
      "2024-01-02 23:57:08.480583: Current learning rate: 0.00497\n",
      "2024-01-02 23:59:14.260801: train_loss -0.9146\n",
      "2024-01-02 23:59:14.267783: val_loss -0.828\n",
      "2024-01-02 23:59:14.273783: Pseudo dice [0.9114, 0.9513, 0.9451]\n",
      "2024-01-02 23:59:14.278783: Epoch time: 125.8 s\n",
      "2024-01-02 23:59:15.405389: \n",
      "2024-01-02 23:59:15.411371: Epoch 541\n",
      "2024-01-02 23:59:15.415365: Current learning rate: 0.00496\n",
      "2024-01-03 00:01:21.187554: train_loss -0.9146\n",
      "2024-01-03 00:01:21.196554: val_loss -0.8353\n",
      "2024-01-03 00:01:21.204554: Pseudo dice [0.9133, 0.9528, 0.9457]\n",
      "2024-01-03 00:01:21.210562: Epoch time: 125.78 s\n",
      "2024-01-03 00:01:22.282545: \n",
      "2024-01-03 00:01:22.288801: Epoch 542\n",
      "2024-01-03 00:01:22.293814: Current learning rate: 0.00495\n",
      "2024-01-03 00:03:28.319897: train_loss -0.9116\n",
      "2024-01-03 00:03:28.325891: val_loss -0.8232\n",
      "2024-01-03 00:03:28.332887: Pseudo dice [0.9079, 0.9498, 0.9458]\n",
      "2024-01-03 00:03:28.337890: Epoch time: 126.04 s\n",
      "2024-01-03 00:03:29.642735: \n",
      "2024-01-03 00:03:29.648635: Epoch 543\n",
      "2024-01-03 00:03:29.653635: Current learning rate: 0.00494\n",
      "2024-01-03 00:05:35.540000: train_loss -0.9126\n",
      "2024-01-03 00:05:35.550003: val_loss -0.8315\n",
      "2024-01-03 00:05:35.555016: Pseudo dice [0.914, 0.951, 0.9462]\n",
      "2024-01-03 00:05:35.561511: Epoch time: 125.9 s\n",
      "2024-01-03 00:05:36.815889: \n",
      "2024-01-03 00:05:36.821884: Epoch 544\n",
      "2024-01-03 00:05:36.828889: Current learning rate: 0.00493\n",
      "2024-01-03 00:07:42.550236: train_loss -0.9143\n",
      "2024-01-03 00:07:42.562298: val_loss -0.8318\n",
      "2024-01-03 00:07:42.570235: Pseudo dice [0.9124, 0.9514, 0.9455]\n",
      "2024-01-03 00:07:42.578329: Epoch time: 125.74 s\n",
      "2024-01-03 00:07:43.614929: \n",
      "2024-01-03 00:07:43.622992: Epoch 545\n",
      "2024-01-03 00:07:43.631003: Current learning rate: 0.00492\n",
      "2024-01-03 00:09:49.481561: train_loss -0.915\n",
      "2024-01-03 00:09:49.489565: val_loss -0.8296\n",
      "2024-01-03 00:09:49.496566: Pseudo dice [0.9118, 0.9518, 0.9452]\n",
      "2024-01-03 00:09:49.503565: Epoch time: 125.87 s\n",
      "2024-01-03 00:09:51.066887: \n",
      "2024-01-03 00:09:51.071899: Epoch 546\n",
      "2024-01-03 00:09:51.076433: Current learning rate: 0.00491\n",
      "2024-01-03 00:11:57.299755: train_loss -0.9144\n",
      "2024-01-03 00:11:57.308753: val_loss -0.8335\n",
      "2024-01-03 00:11:57.317757: Pseudo dice [0.9135, 0.9518, 0.9454]\n",
      "2024-01-03 00:11:57.331622: Epoch time: 126.24 s\n",
      "2024-01-03 00:11:58.725338: \n",
      "2024-01-03 00:11:58.733894: Epoch 547\n",
      "2024-01-03 00:11:58.738107: Current learning rate: 0.0049\n",
      "2024-01-03 00:14:04.242154: train_loss -0.9153\n",
      "2024-01-03 00:14:04.250150: val_loss -0.8333\n",
      "2024-01-03 00:14:04.255150: Pseudo dice [0.9109, 0.9511, 0.9437]\n",
      "2024-01-03 00:14:04.261151: Epoch time: 125.52 s\n",
      "2024-01-03 00:14:05.322510: \n",
      "2024-01-03 00:14:05.328571: Epoch 548\n",
      "2024-01-03 00:14:05.332579: Current learning rate: 0.00489\n",
      "2024-01-03 00:16:11.020153: train_loss -0.9177\n",
      "2024-01-03 00:16:11.027156: val_loss -0.8298\n",
      "2024-01-03 00:16:11.033154: Pseudo dice [0.9107, 0.9517, 0.9447]\n",
      "2024-01-03 00:16:11.039155: Epoch time: 125.7 s\n",
      "2024-01-03 00:16:12.374507: \n",
      "2024-01-03 00:16:12.381451: Epoch 549\n",
      "2024-01-03 00:16:12.389452: Current learning rate: 0.00488\n",
      "2024-01-03 00:18:17.961437: train_loss -0.9145\n",
      "2024-01-03 00:18:17.968435: val_loss -0.8375\n",
      "2024-01-03 00:18:17.977439: Pseudo dice [0.9107, 0.9504, 0.9441]\n",
      "2024-01-03 00:18:17.985440: Epoch time: 125.59 s\n",
      "2024-01-03 00:18:19.507819: \n",
      "2024-01-03 00:18:19.513820: Epoch 550\n",
      "2024-01-03 00:18:19.517832: Current learning rate: 0.00487\n",
      "2024-01-03 00:20:25.184931: train_loss -0.9149\n",
      "2024-01-03 00:20:25.191932: val_loss -0.8344\n",
      "2024-01-03 00:20:25.197940: Pseudo dice [0.9098, 0.9522, 0.9449]\n",
      "2024-01-03 00:20:25.206935: Epoch time: 125.68 s\n",
      "2024-01-03 00:20:26.413810: \n",
      "2024-01-03 00:20:26.420802: Epoch 551\n",
      "2024-01-03 00:20:26.424810: Current learning rate: 0.00486\n",
      "2024-01-03 00:22:32.349990: train_loss -0.917\n",
      "2024-01-03 00:22:32.356512: val_loss -0.8321\n",
      "2024-01-03 00:22:32.364621: Pseudo dice [0.9107, 0.9515, 0.9452]\n",
      "2024-01-03 00:22:32.369622: Epoch time: 125.94 s\n",
      "2024-01-03 00:22:33.566779: \n",
      "2024-01-03 00:22:33.572712: Epoch 552\n",
      "2024-01-03 00:22:33.576785: Current learning rate: 0.00485\n",
      "2024-01-03 00:24:40.492946: train_loss -0.9178\n",
      "2024-01-03 00:24:40.502958: val_loss -0.8279\n",
      "2024-01-03 00:24:40.507953: Pseudo dice [0.9117, 0.9513, 0.9452]\n",
      "2024-01-03 00:24:40.513960: Epoch time: 126.93 s\n",
      "2024-01-03 00:24:41.705430: \n",
      "2024-01-03 00:24:41.712579: Epoch 553\n",
      "2024-01-03 00:24:41.717424: Current learning rate: 0.00484\n",
      "2024-01-03 00:26:47.356196: train_loss -0.9162\n",
      "2024-01-03 00:26:47.365196: val_loss -0.8251\n",
      "2024-01-03 00:26:47.372201: Pseudo dice [0.9101, 0.9506, 0.9435]\n",
      "2024-01-03 00:26:47.379200: Epoch time: 125.65 s\n",
      "2024-01-03 00:26:48.806974: \n",
      "2024-01-03 00:26:48.812404: Epoch 554\n",
      "2024-01-03 00:26:48.816405: Current learning rate: 0.00484\n",
      "2024-01-03 00:28:54.139932: train_loss -0.9223\n",
      "2024-01-03 00:28:54.148949: val_loss -0.8241\n",
      "2024-01-03 00:28:54.157449: Pseudo dice [0.9123, 0.9506, 0.9442]\n",
      "2024-01-03 00:28:54.162455: Epoch time: 125.33 s\n",
      "2024-01-03 00:28:55.432366: \n",
      "2024-01-03 00:28:55.438308: Epoch 555\n",
      "2024-01-03 00:28:55.443385: Current learning rate: 0.00483\n",
      "2024-01-03 00:31:01.426853: train_loss -0.9139\n",
      "2024-01-03 00:31:01.435307: val_loss -0.8297\n",
      "2024-01-03 00:31:01.440305: Pseudo dice [0.9132, 0.9511, 0.9457]\n",
      "2024-01-03 00:31:01.446376: Epoch time: 126.0 s\n",
      "2024-01-03 00:31:02.625065: \n",
      "2024-01-03 00:31:02.632151: Epoch 556\n",
      "2024-01-03 00:31:02.642965: Current learning rate: 0.00482\n",
      "2024-01-03 00:33:08.171623: train_loss -0.9159\n",
      "2024-01-03 00:33:08.181633: val_loss -0.829\n",
      "2024-01-03 00:33:08.188637: Pseudo dice [0.9107, 0.9515, 0.9448]\n",
      "2024-01-03 00:33:08.195624: Epoch time: 125.55 s\n",
      "2024-01-03 00:33:09.355003: \n",
      "2024-01-03 00:33:09.365668: Epoch 557\n",
      "2024-01-03 00:33:09.369730: Current learning rate: 0.00481\n",
      "2024-01-03 00:35:15.248022: train_loss -0.9124\n",
      "2024-01-03 00:35:15.255021: val_loss -0.8279\n",
      "2024-01-03 00:35:15.260020: Pseudo dice [0.9123, 0.9516, 0.9459]\n",
      "2024-01-03 00:35:15.266020: Epoch time: 125.89 s\n",
      "2024-01-03 00:35:16.485388: \n",
      "2024-01-03 00:35:16.493724: Epoch 558\n",
      "2024-01-03 00:35:16.499735: Current learning rate: 0.0048\n",
      "2024-01-03 00:37:22.355407: train_loss -0.917\n",
      "2024-01-03 00:37:22.367414: val_loss -0.8372\n",
      "2024-01-03 00:37:22.375927: Pseudo dice [0.9123, 0.9536, 0.9456]\n",
      "2024-01-03 00:37:22.384927: Epoch time: 125.87 s\n",
      "2024-01-03 00:37:23.779018: \n",
      "2024-01-03 00:37:23.787017: Epoch 559\n",
      "2024-01-03 00:37:23.792009: Current learning rate: 0.00479\n",
      "2024-01-03 00:39:29.645606: train_loss -0.913\n",
      "2024-01-03 00:39:29.652146: val_loss -0.8356\n",
      "2024-01-03 00:39:29.658137: Pseudo dice [0.9127, 0.9523, 0.9456]\n",
      "2024-01-03 00:39:29.663644: Epoch time: 125.87 s\n",
      "2024-01-03 00:39:30.838451: \n",
      "2024-01-03 00:39:30.844051: Epoch 560\n",
      "2024-01-03 00:39:30.848110: Current learning rate: 0.00478\n",
      "2024-01-03 00:41:37.858506: train_loss -0.9126\n",
      "2024-01-03 00:41:37.869506: val_loss -0.8306\n",
      "2024-01-03 00:41:37.876506: Pseudo dice [0.9111, 0.9514, 0.945]\n",
      "2024-01-03 00:41:37.883508: Epoch time: 127.02 s\n",
      "2024-01-03 00:41:39.621336: \n",
      "2024-01-03 00:41:39.627402: Epoch 561\n",
      "2024-01-03 00:41:39.634410: Current learning rate: 0.00477\n",
      "2024-01-03 00:43:44.987067: train_loss -0.9165\n",
      "2024-01-03 00:43:44.993066: val_loss -0.8327\n",
      "2024-01-03 00:43:44.998067: Pseudo dice [0.9126, 0.9511, 0.9449]\n",
      "2024-01-03 00:43:45.005066: Epoch time: 125.37 s\n",
      "2024-01-03 00:43:46.204129: \n",
      "2024-01-03 00:43:46.210068: Epoch 562\n",
      "2024-01-03 00:43:46.215127: Current learning rate: 0.00476\n",
      "2024-01-03 00:45:51.932546: train_loss -0.9138\n",
      "2024-01-03 00:45:51.939546: val_loss -0.8244\n",
      "2024-01-03 00:45:51.944551: Pseudo dice [0.9142, 0.9502, 0.9463]\n",
      "2024-01-03 00:45:51.948544: Epoch time: 125.73 s\n",
      "2024-01-03 00:45:53.207926: \n",
      "2024-01-03 00:45:53.214108: Epoch 563\n",
      "2024-01-03 00:45:53.222041: Current learning rate: 0.00475\n",
      "2024-01-03 00:47:58.828882: train_loss -0.9129\n",
      "2024-01-03 00:47:58.835889: val_loss -0.8318\n",
      "2024-01-03 00:47:58.840886: Pseudo dice [0.9128, 0.951, 0.9454]\n",
      "2024-01-03 00:47:58.845887: Epoch time: 125.62 s\n",
      "2024-01-03 00:47:59.998378: \n",
      "2024-01-03 00:48:00.003847: Epoch 564\n",
      "2024-01-03 00:48:00.011905: Current learning rate: 0.00474\n",
      "2024-01-03 00:50:05.697890: train_loss -0.9163\n",
      "2024-01-03 00:50:05.705891: val_loss -0.8305\n",
      "2024-01-03 00:50:05.711891: Pseudo dice [0.9112, 0.9513, 0.945]\n",
      "2024-01-03 00:50:05.715891: Epoch time: 125.7 s\n",
      "2024-01-03 00:50:06.888354: \n",
      "2024-01-03 00:50:06.894355: Epoch 565\n",
      "2024-01-03 00:50:06.899422: Current learning rate: 0.00473\n",
      "2024-01-03 00:52:12.699345: train_loss -0.9141\n",
      "2024-01-03 00:52:12.707346: val_loss -0.8312\n",
      "2024-01-03 00:52:12.716348: Pseudo dice [0.9135, 0.9518, 0.9453]\n",
      "2024-01-03 00:52:12.723352: Epoch time: 125.81 s\n",
      "2024-01-03 00:52:13.975953: \n",
      "2024-01-03 00:52:13.982368: Epoch 566\n",
      "2024-01-03 00:52:13.988368: Current learning rate: 0.00472\n",
      "2024-01-03 00:54:19.767668: train_loss -0.9159\n",
      "2024-01-03 00:54:19.775678: val_loss -0.8179\n",
      "2024-01-03 00:54:19.783681: Pseudo dice [0.9102, 0.9494, 0.9443]\n",
      "2024-01-03 00:54:19.789675: Epoch time: 125.79 s\n",
      "2024-01-03 00:54:20.951622: \n",
      "2024-01-03 00:54:20.958131: Epoch 567\n",
      "2024-01-03 00:54:20.962755: Current learning rate: 0.00471\n",
      "2024-01-03 00:56:26.982447: train_loss -0.9122\n",
      "2024-01-03 00:56:26.988447: val_loss -0.8311\n",
      "2024-01-03 00:56:26.994447: Pseudo dice [0.9142, 0.951, 0.9447]\n",
      "2024-01-03 00:56:26.999448: Epoch time: 126.03 s\n",
      "2024-01-03 00:56:28.162826: \n",
      "2024-01-03 00:56:28.173817: Epoch 568\n",
      "2024-01-03 00:56:28.179747: Current learning rate: 0.0047\n",
      "2024-01-03 00:58:34.142766: train_loss -0.913\n",
      "2024-01-03 00:58:34.149767: val_loss -0.822\n",
      "2024-01-03 00:58:34.154766: Pseudo dice [0.9106, 0.9495, 0.9452]\n",
      "2024-01-03 00:58:34.159766: Epoch time: 125.98 s\n",
      "2024-01-03 00:58:35.605138: \n",
      "2024-01-03 00:58:35.615151: Epoch 569\n",
      "2024-01-03 00:58:35.621211: Current learning rate: 0.00469\n",
      "2024-01-03 01:00:41.569797: train_loss -0.9144\n",
      "2024-01-03 01:00:41.577798: val_loss -0.8252\n",
      "2024-01-03 01:00:41.584796: Pseudo dice [0.9116, 0.95, 0.9455]\n",
      "2024-01-03 01:00:41.590796: Epoch time: 125.97 s\n",
      "2024-01-03 01:00:42.955128: \n",
      "2024-01-03 01:00:42.965127: Epoch 570\n",
      "2024-01-03 01:00:42.971127: Current learning rate: 0.00468\n",
      "2024-01-03 01:02:48.346826: train_loss -0.9196\n",
      "2024-01-03 01:02:48.353827: val_loss -0.8309\n",
      "2024-01-03 01:02:48.358833: Pseudo dice [0.9113, 0.9518, 0.9442]\n",
      "2024-01-03 01:02:48.364825: Epoch time: 125.39 s\n",
      "2024-01-03 01:02:49.533732: \n",
      "2024-01-03 01:02:49.538800: Epoch 571\n",
      "2024-01-03 01:02:49.543795: Current learning rate: 0.00467\n",
      "2024-01-03 01:04:55.497127: train_loss -0.9158\n",
      "2024-01-03 01:04:55.505128: val_loss -0.8251\n",
      "2024-01-03 01:04:55.514127: Pseudo dice [0.9092, 0.9506, 0.9445]\n",
      "2024-01-03 01:04:55.521128: Epoch time: 125.96 s\n",
      "2024-01-03 01:04:56.724688: \n",
      "2024-01-03 01:04:56.732705: Epoch 572\n",
      "2024-01-03 01:04:56.737712: Current learning rate: 0.00466\n",
      "2024-01-03 01:07:02.465027: train_loss -0.9138\n",
      "2024-01-03 01:07:02.474043: val_loss -0.8261\n",
      "2024-01-03 01:07:02.481030: Pseudo dice [0.9088, 0.9497, 0.9425]\n",
      "2024-01-03 01:07:02.487031: Epoch time: 125.74 s\n",
      "2024-01-03 01:07:03.690873: \n",
      "2024-01-03 01:07:03.697057: Epoch 573\n",
      "2024-01-03 01:07:03.701143: Current learning rate: 0.00465\n",
      "2024-01-03 01:09:09.351729: train_loss -0.9155\n",
      "2024-01-03 01:09:09.360734: val_loss -0.8282\n",
      "2024-01-03 01:09:09.367733: Pseudo dice [0.9097, 0.9509, 0.9449]\n",
      "2024-01-03 01:09:09.374264: Epoch time: 125.66 s\n",
      "2024-01-03 01:09:10.540704: \n",
      "2024-01-03 01:09:10.548738: Epoch 574\n",
      "2024-01-03 01:09:10.555672: Current learning rate: 0.00464\n",
      "2024-01-03 01:11:16.244185: train_loss -0.9168\n",
      "2024-01-03 01:11:16.252183: val_loss -0.8273\n",
      "2024-01-03 01:11:16.259184: Pseudo dice [0.9118, 0.9504, 0.9447]\n",
      "2024-01-03 01:11:16.267184: Epoch time: 125.7 s\n",
      "2024-01-03 01:11:17.413024: \n",
      "2024-01-03 01:11:17.419039: Epoch 575\n",
      "2024-01-03 01:11:17.424040: Current learning rate: 0.00463\n",
      "2024-01-03 01:13:23.191108: train_loss -0.9174\n",
      "2024-01-03 01:13:23.198108: val_loss -0.8287\n",
      "2024-01-03 01:13:23.202112: Pseudo dice [0.9105, 0.9512, 0.9439]\n",
      "2024-01-03 01:13:23.207117: Epoch time: 125.78 s\n",
      "2024-01-03 01:13:24.282243: \n",
      "2024-01-03 01:13:24.291178: Epoch 576\n",
      "2024-01-03 01:13:24.295265: Current learning rate: 0.00462\n",
      "2024-01-03 01:15:29.869388: train_loss -0.9168\n",
      "2024-01-03 01:15:29.876386: val_loss -0.8292\n",
      "2024-01-03 01:15:29.883389: Pseudo dice [0.9122, 0.9515, 0.9441]\n",
      "2024-01-03 01:15:29.889387: Epoch time: 125.59 s\n",
      "2024-01-03 01:15:31.337851: \n",
      "2024-01-03 01:15:31.346576: Epoch 577\n",
      "2024-01-03 01:15:31.351510: Current learning rate: 0.00461\n",
      "2024-01-03 01:17:36.652523: train_loss -0.92\n",
      "2024-01-03 01:17:36.659521: val_loss -0.8264\n",
      "2024-01-03 01:17:36.664521: Pseudo dice [0.912, 0.9513, 0.9446]\n",
      "2024-01-03 01:17:36.670575: Epoch time: 125.32 s\n",
      "2024-01-03 01:17:37.848942: \n",
      "2024-01-03 01:17:37.856052: Epoch 578\n",
      "2024-01-03 01:17:37.862059: Current learning rate: 0.0046\n",
      "2024-01-03 01:19:43.402629: train_loss -0.9203\n",
      "2024-01-03 01:19:43.409629: val_loss -0.8288\n",
      "2024-01-03 01:19:43.416629: Pseudo dice [0.909, 0.9513, 0.9453]\n",
      "2024-01-03 01:19:43.421633: Epoch time: 125.55 s\n",
      "2024-01-03 01:19:44.680472: \n",
      "2024-01-03 01:19:44.688401: Epoch 579\n",
      "2024-01-03 01:19:44.693392: Current learning rate: 0.00459\n",
      "2024-01-03 01:21:50.044070: train_loss -0.92\n",
      "2024-01-03 01:21:50.053064: val_loss -0.8258\n",
      "2024-01-03 01:21:50.059068: Pseudo dice [0.9116, 0.9508, 0.9452]\n",
      "2024-01-03 01:21:50.064063: Epoch time: 125.37 s\n",
      "2024-01-03 01:21:51.204671: \n",
      "2024-01-03 01:21:51.213673: Epoch 580\n",
      "2024-01-03 01:21:51.217730: Current learning rate: 0.00458\n",
      "2024-01-03 01:23:56.774722: train_loss -0.9161\n",
      "2024-01-03 01:23:56.783727: val_loss -0.8239\n",
      "2024-01-03 01:23:56.793800: Pseudo dice [0.9119, 0.9505, 0.9451]\n",
      "2024-01-03 01:23:56.801724: Epoch time: 125.57 s\n",
      "2024-01-03 01:23:57.958015: \n",
      "2024-01-03 01:23:57.965352: Epoch 581\n",
      "2024-01-03 01:23:57.969749: Current learning rate: 0.00457\n",
      "2024-01-03 01:26:03.497059: train_loss -0.9182\n",
      "2024-01-03 01:26:03.504568: val_loss -0.8286\n",
      "2024-01-03 01:26:03.509567: Pseudo dice [0.9115, 0.951, 0.9447]\n",
      "2024-01-03 01:26:03.516568: Epoch time: 125.54 s\n",
      "2024-01-03 01:26:04.764961: \n",
      "2024-01-03 01:26:04.777028: Epoch 582\n",
      "2024-01-03 01:26:04.782985: Current learning rate: 0.00456\n",
      "2024-01-03 01:28:10.660256: train_loss -0.9132\n",
      "2024-01-03 01:28:10.671252: val_loss -0.8295\n",
      "2024-01-03 01:28:10.677255: Pseudo dice [0.9095, 0.953, 0.9448]\n",
      "2024-01-03 01:28:10.683255: Epoch time: 125.9 s\n",
      "2024-01-03 01:28:11.899119: \n",
      "2024-01-03 01:28:11.905110: Epoch 583\n",
      "2024-01-03 01:28:11.910299: Current learning rate: 0.00455\n",
      "2024-01-03 01:30:17.561893: train_loss -0.9158\n",
      "2024-01-03 01:30:17.568893: val_loss -0.8248\n",
      "2024-01-03 01:30:17.576401: Pseudo dice [0.9077, 0.9507, 0.9468]\n",
      "2024-01-03 01:30:17.581401: Epoch time: 125.66 s\n",
      "2024-01-03 01:30:18.749983: \n",
      "2024-01-03 01:30:18.755971: Epoch 584\n",
      "2024-01-03 01:30:18.761057: Current learning rate: 0.00454\n",
      "2024-01-03 01:32:24.606200: train_loss -0.9171\n",
      "2024-01-03 01:32:24.616198: val_loss -0.8229\n",
      "2024-01-03 01:32:24.624205: Pseudo dice [0.912, 0.9504, 0.9451]\n",
      "2024-01-03 01:32:24.631212: Epoch time: 125.86 s\n",
      "2024-01-03 01:32:26.055388: \n",
      "2024-01-03 01:32:26.064390: Epoch 585\n",
      "2024-01-03 01:32:26.068463: Current learning rate: 0.00453\n",
      "2024-01-03 01:34:31.619591: train_loss -0.9159\n",
      "2024-01-03 01:34:31.628584: val_loss -0.8222\n",
      "2024-01-03 01:34:31.632663: Pseudo dice [0.9101, 0.95, 0.9456]\n",
      "2024-01-03 01:34:31.637656: Epoch time: 125.57 s\n",
      "2024-01-03 01:34:32.724512: \n",
      "2024-01-03 01:34:32.731569: Epoch 586\n",
      "2024-01-03 01:34:32.736512: Current learning rate: 0.00452\n",
      "2024-01-03 01:36:38.401640: train_loss -0.9172\n",
      "2024-01-03 01:36:38.411640: val_loss -0.8253\n",
      "2024-01-03 01:36:38.416632: Pseudo dice [0.9091, 0.9507, 0.9464]\n",
      "2024-01-03 01:36:38.421633: Epoch time: 125.68 s\n",
      "2024-01-03 01:36:39.644226: \n",
      "2024-01-03 01:36:39.650162: Epoch 587\n",
      "2024-01-03 01:36:39.654166: Current learning rate: 0.00451\n",
      "2024-01-03 01:38:45.942418: train_loss -0.9169\n",
      "2024-01-03 01:38:45.950418: val_loss -0.8293\n",
      "2024-01-03 01:38:45.958424: Pseudo dice [0.9103, 0.9523, 0.945]\n",
      "2024-01-03 01:38:45.964424: Epoch time: 126.3 s\n",
      "2024-01-03 01:38:47.635558: \n",
      "2024-01-03 01:38:47.640686: Epoch 588\n",
      "2024-01-03 01:38:47.649724: Current learning rate: 0.0045\n",
      "2024-01-03 01:40:53.351820: train_loss -0.9153\n",
      "2024-01-03 01:40:53.358818: val_loss -0.8292\n",
      "2024-01-03 01:40:53.363829: Pseudo dice [0.9112, 0.9509, 0.945]\n",
      "2024-01-03 01:40:53.370819: Epoch time: 125.72 s\n",
      "2024-01-03 01:40:54.544776: \n",
      "2024-01-03 01:40:54.550740: Epoch 589\n",
      "2024-01-03 01:40:54.556827: Current learning rate: 0.00449\n",
      "2024-01-03 01:42:59.953543: train_loss -0.9184\n",
      "2024-01-03 01:42:59.959543: val_loss -0.8285\n",
      "2024-01-03 01:42:59.969053: Pseudo dice [0.9094, 0.9514, 0.9455]\n",
      "2024-01-03 01:42:59.974060: Epoch time: 125.41 s\n",
      "2024-01-03 01:43:01.213776: \n",
      "2024-01-03 01:43:01.219776: Epoch 590\n",
      "2024-01-03 01:43:01.225902: Current learning rate: 0.00448\n",
      "2024-01-03 01:45:06.773819: train_loss -0.9159\n",
      "2024-01-03 01:45:06.780819: val_loss -0.8274\n",
      "2024-01-03 01:45:06.786896: Pseudo dice [0.9088, 0.9504, 0.9439]\n",
      "2024-01-03 01:45:06.791826: Epoch time: 125.56 s\n",
      "2024-01-03 01:45:07.975736: \n",
      "2024-01-03 01:45:07.981681: Epoch 591\n",
      "2024-01-03 01:45:07.986719: Current learning rate: 0.00447\n",
      "2024-01-03 01:47:13.691525: train_loss -0.9168\n",
      "2024-01-03 01:47:13.699529: val_loss -0.8312\n",
      "2024-01-03 01:47:13.704530: Pseudo dice [0.9127, 0.9526, 0.9446]\n",
      "2024-01-03 01:47:13.711037: Epoch time: 125.72 s\n",
      "2024-01-03 01:47:15.001276: \n",
      "2024-01-03 01:47:15.009906: Epoch 592\n",
      "2024-01-03 01:47:15.014902: Current learning rate: 0.00446\n",
      "2024-01-03 01:49:20.587512: train_loss -0.918\n",
      "2024-01-03 01:49:20.593513: val_loss -0.8249\n",
      "2024-01-03 01:49:20.598513: Pseudo dice [0.9115, 0.9498, 0.9448]\n",
      "2024-01-03 01:49:20.603514: Epoch time: 125.59 s\n",
      "2024-01-03 01:49:21.814960: \n",
      "2024-01-03 01:49:21.821564: Epoch 593\n",
      "2024-01-03 01:49:21.826572: Current learning rate: 0.00445\n",
      "2024-01-03 01:51:27.946768: train_loss -0.917\n",
      "2024-01-03 01:51:27.955759: val_loss -0.8304\n",
      "2024-01-03 01:51:27.962796: Pseudo dice [0.9154, 0.9522, 0.9436]\n",
      "2024-01-03 01:51:27.968798: Epoch time: 126.13 s\n",
      "2024-01-03 01:51:29.271647: \n",
      "2024-01-03 01:51:29.284887: Epoch 594\n",
      "2024-01-03 01:51:29.296959: Current learning rate: 0.00444\n",
      "2024-01-03 01:53:34.940804: train_loss -0.9147\n",
      "2024-01-03 01:53:34.947805: val_loss -0.8249\n",
      "2024-01-03 01:53:34.954806: Pseudo dice [0.9114, 0.9509, 0.9444]\n",
      "2024-01-03 01:53:34.962807: Epoch time: 125.67 s\n",
      "2024-01-03 01:53:36.138062: \n",
      "2024-01-03 01:53:36.144285: Epoch 595\n",
      "2024-01-03 01:53:36.150298: Current learning rate: 0.00443\n",
      "2024-01-03 01:55:41.881190: train_loss -0.9164\n",
      "2024-01-03 01:55:41.892190: val_loss -0.8225\n",
      "2024-01-03 01:55:41.899190: Pseudo dice [0.9111, 0.9504, 0.9451]\n",
      "2024-01-03 01:55:41.909189: Epoch time: 125.74 s\n",
      "2024-01-03 01:55:43.098160: \n",
      "2024-01-03 01:55:43.104082: Epoch 596\n",
      "2024-01-03 01:55:43.109082: Current learning rate: 0.00442\n",
      "2024-01-03 01:57:48.984928: train_loss -0.9139\n",
      "2024-01-03 01:57:48.994929: val_loss -0.8267\n",
      "2024-01-03 01:57:49.000931: Pseudo dice [0.913, 0.9509, 0.9446]\n",
      "2024-01-03 01:57:49.006605: Epoch time: 125.89 s\n",
      "2024-01-03 01:57:50.082328: \n",
      "2024-01-03 01:57:50.088328: Epoch 597\n",
      "2024-01-03 01:57:50.092337: Current learning rate: 0.00441\n",
      "2024-01-03 01:59:55.719668: train_loss -0.9182\n",
      "2024-01-03 01:59:55.725669: val_loss -0.8333\n",
      "2024-01-03 01:59:55.731668: Pseudo dice [0.913, 0.9522, 0.9452]\n",
      "2024-01-03 01:59:55.737668: Epoch time: 125.64 s\n",
      "2024-01-03 01:59:56.824281: \n",
      "2024-01-03 01:59:56.832281: Epoch 598\n",
      "2024-01-03 01:59:56.837410: Current learning rate: 0.0044\n",
      "2024-01-03 02:02:02.468194: train_loss -0.9187\n",
      "2024-01-03 02:02:02.478194: val_loss -0.8286\n",
      "2024-01-03 02:02:02.484194: Pseudo dice [0.9141, 0.9522, 0.9456]\n",
      "2024-01-03 02:02:02.490203: Epoch time: 125.65 s\n",
      "2024-01-03 02:02:03.807877: \n",
      "2024-01-03 02:02:03.814884: Epoch 599\n",
      "2024-01-03 02:02:03.820884: Current learning rate: 0.00439\n",
      "2024-01-03 02:04:09.770494: train_loss -0.9183\n",
      "2024-01-03 02:04:09.779018: val_loss -0.8267\n",
      "2024-01-03 02:04:09.787018: Pseudo dice [0.9122, 0.9511, 0.9447]\n",
      "2024-01-03 02:04:09.792018: Epoch time: 125.96 s\n",
      "2024-01-03 02:04:11.655686: \n",
      "2024-01-03 02:04:11.667340: Epoch 600\n",
      "2024-01-03 02:04:11.677421: Current learning rate: 0.00438\n",
      "2024-01-03 02:06:16.909633: train_loss -0.9238\n",
      "2024-01-03 02:06:16.918634: val_loss -0.8335\n",
      "2024-01-03 02:06:16.924641: Pseudo dice [0.9124, 0.9533, 0.9455]\n",
      "2024-01-03 02:06:16.930633: Epoch time: 125.26 s\n",
      "2024-01-03 02:06:18.060399: \n",
      "2024-01-03 02:06:18.065393: Epoch 601\n",
      "2024-01-03 02:06:18.069394: Current learning rate: 0.00437\n",
      "2024-01-03 02:08:23.830838: train_loss -0.9179\n",
      "2024-01-03 02:08:23.837368: val_loss -0.8217\n",
      "2024-01-03 02:08:23.843365: Pseudo dice [0.9113, 0.9501, 0.9437]\n",
      "2024-01-03 02:08:23.849366: Epoch time: 125.77 s\n",
      "2024-01-03 02:08:25.084651: \n",
      "2024-01-03 02:08:25.093657: Epoch 602\n",
      "2024-01-03 02:08:25.098583: Current learning rate: 0.00436\n",
      "2024-01-03 02:10:30.978010: train_loss -0.9147\n",
      "2024-01-03 02:10:30.987014: val_loss -0.8283\n",
      "2024-01-03 02:10:30.997011: Pseudo dice [0.9127, 0.9516, 0.9436]\n",
      "2024-01-03 02:10:31.003015: Epoch time: 125.89 s\n",
      "2024-01-03 02:10:32.220445: \n",
      "2024-01-03 02:10:32.231092: Epoch 603\n",
      "2024-01-03 02:10:32.239098: Current learning rate: 0.00435\n",
      "2024-01-03 02:12:38.313847: train_loss -0.915\n",
      "2024-01-03 02:12:38.321849: val_loss -0.8339\n",
      "2024-01-03 02:12:38.328849: Pseudo dice [0.9118, 0.9532, 0.9459]\n",
      "2024-01-03 02:12:38.336853: Epoch time: 126.09 s\n",
      "2024-01-03 02:12:39.828007: \n",
      "2024-01-03 02:12:39.836005: Epoch 604\n",
      "2024-01-03 02:12:39.841006: Current learning rate: 0.00434\n",
      "2024-01-03 02:14:46.149842: train_loss -0.9148\n",
      "2024-01-03 02:14:46.158842: val_loss -0.8208\n",
      "2024-01-03 02:14:46.166844: Pseudo dice [0.9116, 0.9493, 0.943]\n",
      "2024-01-03 02:14:46.172843: Epoch time: 126.32 s\n",
      "2024-01-03 02:14:47.365980: \n",
      "2024-01-03 02:14:47.380272: Epoch 605\n",
      "2024-01-03 02:14:47.389258: Current learning rate: 0.00433\n",
      "2024-01-03 02:16:53.136953: train_loss -0.9175\n",
      "2024-01-03 02:16:53.145955: val_loss -0.822\n",
      "2024-01-03 02:16:53.158557: Pseudo dice [0.9098, 0.9495, 0.9449]\n",
      "2024-01-03 02:16:53.164556: Epoch time: 125.77 s\n",
      "2024-01-03 02:16:54.348132: \n",
      "2024-01-03 02:16:54.354526: Epoch 606\n",
      "2024-01-03 02:16:54.358593: Current learning rate: 0.00432\n",
      "2024-01-03 02:19:00.048078: train_loss -0.9162\n",
      "2024-01-03 02:19:00.055083: val_loss -0.8314\n",
      "2024-01-03 02:19:00.061092: Pseudo dice [0.9116, 0.9511, 0.9447]\n",
      "2024-01-03 02:19:00.066083: Epoch time: 125.7 s\n",
      "2024-01-03 02:19:01.391608: \n",
      "2024-01-03 02:19:01.403709: Epoch 607\n",
      "2024-01-03 02:19:01.408133: Current learning rate: 0.00431\n",
      "2024-01-03 02:21:06.980022: train_loss -0.9174\n",
      "2024-01-03 02:21:06.987013: val_loss -0.8283\n",
      "2024-01-03 02:21:06.994013: Pseudo dice [0.9109, 0.9511, 0.9442]\n",
      "2024-01-03 02:21:06.998016: Epoch time: 125.59 s\n",
      "2024-01-03 02:21:08.306270: \n",
      "2024-01-03 02:21:08.311300: Epoch 608\n",
      "2024-01-03 02:21:08.316884: Current learning rate: 0.0043\n",
      "2024-01-03 02:23:14.361953: train_loss -0.9172\n",
      "2024-01-03 02:23:14.368950: val_loss -0.8202\n",
      "2024-01-03 02:23:14.373950: Pseudo dice [0.9107, 0.9504, 0.9453]\n",
      "2024-01-03 02:23:14.380948: Epoch time: 126.06 s\n",
      "2024-01-03 02:23:15.501087: \n",
      "2024-01-03 02:23:15.509169: Epoch 609\n",
      "2024-01-03 02:23:15.514110: Current learning rate: 0.00429\n",
      "2024-01-03 02:25:21.283053: train_loss -0.9181\n",
      "2024-01-03 02:25:21.290054: val_loss -0.8321\n",
      "2024-01-03 02:25:21.295057: Pseudo dice [0.9141, 0.9516, 0.9456]\n",
      "2024-01-03 02:25:21.300056: Epoch time: 125.78 s\n",
      "2024-01-03 02:25:22.550791: \n",
      "2024-01-03 02:25:22.556847: Epoch 610\n",
      "2024-01-03 02:25:22.565831: Current learning rate: 0.00429\n",
      "2024-01-03 02:27:28.279039: train_loss -0.9173\n",
      "2024-01-03 02:27:28.286040: val_loss -0.8269\n",
      "2024-01-03 02:27:28.291050: Pseudo dice [0.911, 0.9519, 0.9454]\n",
      "2024-01-03 02:27:28.297044: Epoch time: 125.73 s\n",
      "2024-01-03 02:27:29.525651: \n",
      "2024-01-03 02:27:29.534576: Epoch 611\n",
      "2024-01-03 02:27:29.538658: Current learning rate: 0.00428\n",
      "2024-01-03 02:29:35.025035: train_loss -0.9196\n",
      "2024-01-03 02:29:35.034035: val_loss -0.8304\n",
      "2024-01-03 02:29:35.040196: Pseudo dice [0.9119, 0.9526, 0.9447]\n",
      "2024-01-03 02:29:35.045206: Epoch time: 125.5 s\n",
      "2024-01-03 02:29:36.216772: \n",
      "2024-01-03 02:29:36.222719: Epoch 612\n",
      "2024-01-03 02:29:36.227733: Current learning rate: 0.00427\n",
      "2024-01-03 02:31:41.916605: train_loss -0.9168\n",
      "2024-01-03 02:31:41.925602: val_loss -0.8279\n",
      "2024-01-03 02:31:41.933604: Pseudo dice [0.912, 0.9514, 0.944]\n",
      "2024-01-03 02:31:41.941603: Epoch time: 125.7 s\n",
      "2024-01-03 02:31:43.072530: \n",
      "2024-01-03 02:31:43.078602: Epoch 613\n",
      "2024-01-03 02:31:43.086601: Current learning rate: 0.00426\n",
      "2024-01-03 02:33:48.593130: train_loss -0.9155\n",
      "2024-01-03 02:33:48.604627: val_loss -0.8358\n",
      "2024-01-03 02:33:48.610628: Pseudo dice [0.9113, 0.9536, 0.9452]\n",
      "2024-01-03 02:33:48.616632: Epoch time: 125.52 s\n",
      "2024-01-03 02:33:49.762724: \n",
      "2024-01-03 02:33:49.768265: Epoch 614\n",
      "2024-01-03 02:33:49.776349: Current learning rate: 0.00425\n",
      "2024-01-03 02:35:55.805556: train_loss -0.9174\n",
      "2024-01-03 02:35:55.812557: val_loss -0.8268\n",
      "2024-01-03 02:35:55.817557: Pseudo dice [0.9117, 0.9496, 0.9445]\n",
      "2024-01-03 02:35:55.825559: Epoch time: 126.04 s\n",
      "2024-01-03 02:35:57.291764: \n",
      "2024-01-03 02:35:57.299763: Epoch 615\n",
      "2024-01-03 02:35:57.305761: Current learning rate: 0.00424\n",
      "2024-01-03 02:38:02.922190: train_loss -0.9187\n",
      "2024-01-03 02:38:02.929193: val_loss -0.8308\n",
      "2024-01-03 02:38:02.934192: Pseudo dice [0.911, 0.9497, 0.9421]\n",
      "2024-01-03 02:38:02.940110: Epoch time: 125.63 s\n",
      "2024-01-03 02:38:04.348958: \n",
      "2024-01-03 02:38:04.354890: Epoch 616\n",
      "2024-01-03 02:38:04.358898: Current learning rate: 0.00423\n",
      "2024-01-03 02:40:10.134589: train_loss -0.9143\n",
      "2024-01-03 02:40:10.141594: val_loss -0.8226\n",
      "2024-01-03 02:40:10.146594: Pseudo dice [0.9125, 0.951, 0.9461]\n",
      "2024-01-03 02:40:10.152102: Epoch time: 125.79 s\n",
      "2024-01-03 02:40:11.436944: \n",
      "2024-01-03 02:40:11.445879: Epoch 617\n",
      "2024-01-03 02:40:11.486823: Current learning rate: 0.00422\n",
      "2024-01-03 02:42:17.117631: train_loss -0.9188\n",
      "2024-01-03 02:42:17.124638: val_loss -0.8251\n",
      "2024-01-03 02:42:17.131635: Pseudo dice [0.9138, 0.9505, 0.9447]\n",
      "2024-01-03 02:42:17.137633: Epoch time: 125.68 s\n",
      "2024-01-03 02:42:18.308594: \n",
      "2024-01-03 02:42:18.314536: Epoch 618\n",
      "2024-01-03 02:42:18.322481: Current learning rate: 0.00421\n",
      "2024-01-03 02:44:24.117589: train_loss -0.9158\n",
      "2024-01-03 02:44:24.128591: val_loss -0.8238\n",
      "2024-01-03 02:44:24.136104: Pseudo dice [0.9088, 0.9507, 0.946]\n",
      "2024-01-03 02:44:24.143132: Epoch time: 125.81 s\n",
      "2024-01-03 02:44:25.584735: \n",
      "2024-01-03 02:44:25.591466: Epoch 619\n",
      "2024-01-03 02:44:25.595446: Current learning rate: 0.0042\n",
      "2024-01-03 02:46:31.018877: train_loss -0.919\n",
      "2024-01-03 02:46:31.028876: val_loss -0.8252\n",
      "2024-01-03 02:46:31.036377: Pseudo dice [0.9128, 0.95, 0.9443]\n",
      "2024-01-03 02:46:31.044377: Epoch time: 125.44 s\n",
      "2024-01-03 02:46:32.220445: \n",
      "2024-01-03 02:46:32.226589: Epoch 620\n",
      "2024-01-03 02:46:32.231600: Current learning rate: 0.00419\n",
      "2024-01-03 02:48:37.852554: train_loss -0.9178\n",
      "2024-01-03 02:48:37.861092: val_loss -0.8244\n",
      "2024-01-03 02:48:37.871094: Pseudo dice [0.9099, 0.9501, 0.9434]\n",
      "2024-01-03 02:48:37.879090: Epoch time: 125.63 s\n",
      "2024-01-03 02:48:39.045152: \n",
      "2024-01-03 02:48:39.052297: Epoch 621\n",
      "2024-01-03 02:48:39.058339: Current learning rate: 0.00418\n",
      "2024-01-03 02:50:44.686770: train_loss -0.9178\n",
      "2024-01-03 02:50:44.694274: val_loss -0.8264\n",
      "2024-01-03 02:50:44.701278: Pseudo dice [0.9107, 0.9511, 0.9451]\n",
      "2024-01-03 02:50:44.706278: Epoch time: 125.64 s\n",
      "2024-01-03 02:50:45.832132: \n",
      "2024-01-03 02:50:45.838145: Epoch 622\n",
      "2024-01-03 02:50:45.844208: Current learning rate: 0.00417\n",
      "2024-01-03 02:52:51.530828: train_loss -0.9187\n",
      "2024-01-03 02:52:51.538830: val_loss -0.8307\n",
      "2024-01-03 02:52:51.545830: Pseudo dice [0.9094, 0.9508, 0.9473]\n",
      "2024-01-03 02:52:51.550828: Epoch time: 125.7 s\n",
      "2024-01-03 02:52:52.984532: \n",
      "2024-01-03 02:52:52.990477: Epoch 623\n",
      "2024-01-03 02:52:52.995922: Current learning rate: 0.00416\n",
      "2024-01-03 02:54:58.934883: train_loss -0.9163\n",
      "2024-01-03 02:54:58.942886: val_loss -0.828\n",
      "2024-01-03 02:54:58.948886: Pseudo dice [0.9104, 0.9514, 0.9451]\n",
      "2024-01-03 02:54:58.952887: Epoch time: 125.95 s\n",
      "2024-01-03 02:55:00.237035: \n",
      "2024-01-03 02:55:00.246111: Epoch 624\n",
      "2024-01-03 02:55:00.251057: Current learning rate: 0.00415\n",
      "2024-01-03 02:57:06.078406: train_loss -0.916\n",
      "2024-01-03 02:57:06.087916: val_loss -0.8309\n",
      "2024-01-03 02:57:06.093917: Pseudo dice [0.9124, 0.9519, 0.9452]\n",
      "2024-01-03 02:57:06.100917: Epoch time: 125.84 s\n",
      "2024-01-03 02:57:07.473008: \n",
      "2024-01-03 02:57:07.481208: Epoch 625\n",
      "2024-01-03 02:57:07.485877: Current learning rate: 0.00414\n",
      "2024-01-03 02:59:13.384197: train_loss -0.9156\n",
      "2024-01-03 02:59:13.391197: val_loss -0.8268\n",
      "2024-01-03 02:59:13.397197: Pseudo dice [0.9124, 0.9514, 0.9457]\n",
      "2024-01-03 02:59:13.402197: Epoch time: 125.91 s\n",
      "2024-01-03 02:59:14.505821: \n",
      "2024-01-03 02:59:14.511828: Epoch 626\n",
      "2024-01-03 02:59:14.516818: Current learning rate: 0.00413\n",
      "2024-01-03 03:01:20.173150: train_loss -0.9176\n",
      "2024-01-03 03:01:20.181153: val_loss -0.8332\n",
      "2024-01-03 03:01:20.186150: Pseudo dice [0.9125, 0.9527, 0.9458]\n",
      "2024-01-03 03:01:20.194153: Epoch time: 125.67 s\n",
      "2024-01-03 03:01:21.421386: \n",
      "2024-01-03 03:01:21.427336: Epoch 627\n",
      "2024-01-03 03:01:21.431335: Current learning rate: 0.00412\n",
      "2024-01-03 03:03:26.913775: train_loss -0.922\n",
      "2024-01-03 03:03:26.921780: val_loss -0.8254\n",
      "2024-01-03 03:03:26.927779: Pseudo dice [0.9117, 0.9506, 0.9455]\n",
      "2024-01-03 03:03:26.932779: Epoch time: 125.49 s\n",
      "2024-01-03 03:03:28.146105: \n",
      "2024-01-03 03:03:28.151670: Epoch 628\n",
      "2024-01-03 03:03:28.159718: Current learning rate: 0.00411\n",
      "2024-01-03 03:05:33.779485: train_loss -0.9188\n",
      "2024-01-03 03:05:33.786486: val_loss -0.8225\n",
      "2024-01-03 03:05:33.793488: Pseudo dice [0.9138, 0.9503, 0.945]\n",
      "2024-01-03 03:05:33.798487: Epoch time: 125.63 s\n",
      "2024-01-03 03:05:35.045484: \n",
      "2024-01-03 03:05:35.051383: Epoch 629\n",
      "2024-01-03 03:05:35.055460: Current learning rate: 0.0041\n",
      "2024-01-03 03:07:40.741697: train_loss -0.9203\n",
      "2024-01-03 03:07:40.750688: val_loss -0.8325\n",
      "2024-01-03 03:07:40.755698: Pseudo dice [0.9127, 0.9525, 0.9449]\n",
      "2024-01-03 03:07:40.761693: Epoch time: 125.7 s\n",
      "2024-01-03 03:07:42.003146: \n",
      "2024-01-03 03:07:42.009179: Epoch 630\n",
      "2024-01-03 03:07:42.013241: Current learning rate: 0.00409\n",
      "2024-01-03 03:09:47.484176: train_loss -0.9202\n",
      "2024-01-03 03:09:47.491175: val_loss -0.832\n",
      "2024-01-03 03:09:47.504176: Pseudo dice [0.9125, 0.9525, 0.9442]\n",
      "2024-01-03 03:09:47.509176: Epoch time: 125.48 s\n",
      "2024-01-03 03:09:48.983105: \n",
      "2024-01-03 03:09:48.989577: Epoch 631\n",
      "2024-01-03 03:09:48.993648: Current learning rate: 0.00408\n",
      "2024-01-03 03:11:54.580790: train_loss -0.9172\n",
      "2024-01-03 03:11:54.588791: val_loss -0.8224\n",
      "2024-01-03 03:11:54.595790: Pseudo dice [0.9122, 0.9498, 0.9441]\n",
      "2024-01-03 03:11:54.601790: Epoch time: 125.6 s\n",
      "2024-01-03 03:11:55.848660: \n",
      "2024-01-03 03:11:55.855075: Epoch 632\n",
      "2024-01-03 03:11:55.863163: Current learning rate: 0.00407\n",
      "2024-01-03 03:14:01.641747: train_loss -0.9199\n",
      "2024-01-03 03:14:01.648746: val_loss -0.8217\n",
      "2024-01-03 03:14:01.656153: Pseudo dice [0.9109, 0.9502, 0.9448]\n",
      "2024-01-03 03:14:01.660161: Epoch time: 125.79 s\n",
      "2024-01-03 03:14:02.820611: \n",
      "2024-01-03 03:14:02.826134: Epoch 633\n",
      "2024-01-03 03:14:02.830143: Current learning rate: 0.00406\n",
      "2024-01-03 03:16:08.405064: train_loss -0.9185\n",
      "2024-01-03 03:16:08.413067: val_loss -0.833\n",
      "2024-01-03 03:16:08.419067: Pseudo dice [0.9128, 0.9523, 0.9457]\n",
      "2024-01-03 03:16:08.424076: Epoch time: 125.59 s\n",
      "2024-01-03 03:16:09.698227: \n",
      "2024-01-03 03:16:09.704472: Epoch 634\n",
      "2024-01-03 03:16:09.712488: Current learning rate: 0.00405\n",
      "2024-01-03 03:18:15.197812: train_loss -0.9178\n",
      "2024-01-03 03:18:15.203816: val_loss -0.8355\n",
      "2024-01-03 03:18:15.208815: Pseudo dice [0.9138, 0.9538, 0.9446]\n",
      "2024-01-03 03:18:15.214816: Epoch time: 125.5 s\n",
      "2024-01-03 03:18:16.553783: \n",
      "2024-01-03 03:18:16.563934: Epoch 635\n",
      "2024-01-03 03:18:16.572991: Current learning rate: 0.00404\n",
      "2024-01-03 03:20:22.209497: train_loss -0.9156\n",
      "2024-01-03 03:20:22.217498: val_loss -0.828\n",
      "2024-01-03 03:20:22.223519: Pseudo dice [0.9121, 0.9506, 0.945]\n",
      "2024-01-03 03:20:22.229503: Epoch time: 125.66 s\n",
      "2024-01-03 03:20:23.464815: \n",
      "2024-01-03 03:20:23.474317: Epoch 636\n",
      "2024-01-03 03:20:23.479305: Current learning rate: 0.00403\n",
      "2024-01-03 03:22:29.599953: train_loss -0.9156\n",
      "2024-01-03 03:22:29.608963: val_loss -0.8262\n",
      "2024-01-03 03:22:29.613966: Pseudo dice [0.9111, 0.9506, 0.9446]\n",
      "2024-01-03 03:22:29.620969: Epoch time: 126.14 s\n",
      "2024-01-03 03:22:30.873386: \n",
      "2024-01-03 03:22:30.879203: Epoch 637\n",
      "2024-01-03 03:22:30.884283: Current learning rate: 0.00402\n",
      "2024-01-03 03:24:36.768270: train_loss -0.9153\n",
      "2024-01-03 03:24:36.778272: val_loss -0.8275\n",
      "2024-01-03 03:24:36.785273: Pseudo dice [0.9124, 0.9514, 0.9434]\n",
      "2024-01-03 03:24:36.791280: Epoch time: 125.9 s\n",
      "2024-01-03 03:24:38.182377: \n",
      "2024-01-03 03:24:38.188576: Epoch 638\n",
      "2024-01-03 03:24:38.193595: Current learning rate: 0.00401\n",
      "2024-01-03 03:26:43.847990: train_loss -0.9162\n",
      "2024-01-03 03:26:43.854987: val_loss -0.8295\n",
      "2024-01-03 03:26:43.860988: Pseudo dice [0.91, 0.9526, 0.9464]\n",
      "2024-01-03 03:26:43.866992: Epoch time: 125.67 s\n",
      "2024-01-03 03:26:45.152218: \n",
      "2024-01-03 03:26:45.158294: Epoch 639\n",
      "2024-01-03 03:26:45.165308: Current learning rate: 0.004\n",
      "2024-01-03 03:28:51.132317: train_loss -0.9153\n",
      "2024-01-03 03:28:51.140323: val_loss -0.8201\n",
      "2024-01-03 03:28:51.147317: Pseudo dice [0.9107, 0.9499, 0.9447]\n",
      "2024-01-03 03:28:51.157318: Epoch time: 125.98 s\n",
      "2024-01-03 03:28:52.665288: \n",
      "2024-01-03 03:28:52.673368: Epoch 640\n",
      "2024-01-03 03:28:52.680348: Current learning rate: 0.00399\n",
      "2024-01-03 03:30:58.740251: train_loss -0.9136\n",
      "2024-01-03 03:30:58.748253: val_loss -0.8259\n",
      "2024-01-03 03:30:58.753253: Pseudo dice [0.9142, 0.9502, 0.9464]\n",
      "2024-01-03 03:30:58.758253: Epoch time: 126.08 s\n",
      "2024-01-03 03:30:59.926303: \n",
      "2024-01-03 03:30:59.933366: Epoch 641\n",
      "2024-01-03 03:30:59.939306: Current learning rate: 0.00398\n",
      "2024-01-03 03:33:05.338204: train_loss -0.918\n",
      "2024-01-03 03:33:05.346210: val_loss -0.8252\n",
      "2024-01-03 03:33:05.355204: Pseudo dice [0.911, 0.9494, 0.9439]\n",
      "2024-01-03 03:33:05.360203: Epoch time: 125.41 s\n",
      "2024-01-03 03:33:06.601520: \n",
      "2024-01-03 03:33:06.607970: Epoch 642\n",
      "2024-01-03 03:33:06.617490: Current learning rate: 0.00397\n",
      "2024-01-03 03:35:12.446325: train_loss -0.9172\n",
      "2024-01-03 03:35:12.456325: val_loss -0.8308\n",
      "2024-01-03 03:35:12.463327: Pseudo dice [0.9128, 0.9514, 0.9464]\n",
      "2024-01-03 03:35:12.470325: Epoch time: 125.85 s\n",
      "2024-01-03 03:35:13.921958: \n",
      "2024-01-03 03:35:13.930621: Epoch 643\n",
      "2024-01-03 03:35:13.935710: Current learning rate: 0.00396\n",
      "2024-01-03 03:37:20.630808: train_loss -0.9144\n",
      "2024-01-03 03:37:20.640809: val_loss -0.8284\n",
      "2024-01-03 03:37:20.647808: Pseudo dice [0.9114, 0.9517, 0.9445]\n",
      "2024-01-03 03:37:20.655809: Epoch time: 126.71 s\n",
      "2024-01-03 03:37:22.310352: \n",
      "2024-01-03 03:37:22.316352: Epoch 644\n",
      "2024-01-03 03:37:22.321344: Current learning rate: 0.00395\n",
      "2024-01-03 03:39:27.831014: train_loss -0.9139\n",
      "2024-01-03 03:39:27.838244: val_loss -0.8237\n",
      "2024-01-03 03:39:27.844228: Pseudo dice [0.9108, 0.9496, 0.946]\n",
      "2024-01-03 03:39:27.848240: Epoch time: 125.52 s\n",
      "2024-01-03 03:39:28.971807: \n",
      "2024-01-03 03:39:28.979876: Epoch 645\n",
      "2024-01-03 03:39:28.987889: Current learning rate: 0.00394\n",
      "2024-01-03 03:41:34.802268: train_loss -0.916\n",
      "2024-01-03 03:41:34.808268: val_loss -0.8278\n",
      "2024-01-03 03:41:34.815268: Pseudo dice [0.9125, 0.9512, 0.9454]\n",
      "2024-01-03 03:41:34.821270: Epoch time: 125.83 s\n",
      "2024-01-03 03:41:36.186628: \n",
      "2024-01-03 03:41:36.195259: Epoch 646\n",
      "2024-01-03 03:41:36.203212: Current learning rate: 0.00393\n",
      "2024-01-03 03:43:42.083218: train_loss -0.92\n",
      "2024-01-03 03:43:42.092215: val_loss -0.8338\n",
      "2024-01-03 03:43:42.102346: Pseudo dice [0.9136, 0.952, 0.9443]\n",
      "2024-01-03 03:43:42.110343: Epoch time: 125.9 s\n",
      "2024-01-03 03:43:43.283873: \n",
      "2024-01-03 03:43:43.290991: Epoch 647\n",
      "2024-01-03 03:43:43.297632: Current learning rate: 0.00392\n",
      "2024-01-03 03:45:48.990719: train_loss -0.9183\n",
      "2024-01-03 03:45:48.997715: val_loss -0.8345\n",
      "2024-01-03 03:45:49.004715: Pseudo dice [0.9118, 0.953, 0.9438]\n",
      "2024-01-03 03:45:49.010715: Epoch time: 125.71 s\n",
      "2024-01-03 03:45:50.170038: \n",
      "2024-01-03 03:45:50.175797: Epoch 648\n",
      "2024-01-03 03:45:50.180835: Current learning rate: 0.00391\n",
      "2024-01-03 03:47:56.869299: train_loss -0.919\n",
      "2024-01-03 03:47:56.878297: val_loss -0.8213\n",
      "2024-01-03 03:47:56.884303: Pseudo dice [0.9103, 0.9497, 0.9442]\n",
      "2024-01-03 03:47:56.890303: Epoch time: 126.7 s\n",
      "2024-01-03 03:47:58.260944: \n",
      "2024-01-03 03:47:58.266933: Epoch 649\n",
      "2024-01-03 03:47:58.274002: Current learning rate: 0.0039\n",
      "2024-01-03 03:50:04.540090: train_loss -0.9164\n",
      "2024-01-03 03:50:04.549089: val_loss -0.8141\n",
      "2024-01-03 03:50:04.558454: Pseudo dice [0.908, 0.9483, 0.9444]\n",
      "2024-01-03 03:50:04.563453: Epoch time: 126.28 s\n",
      "2024-01-03 03:50:05.966870: \n",
      "2024-01-03 03:50:05.981932: Epoch 650\n",
      "2024-01-03 03:50:05.987946: Current learning rate: 0.00389\n",
      "2024-01-03 03:52:12.020734: train_loss -0.9157\n",
      "2024-01-03 03:52:12.027724: val_loss -0.832\n",
      "2024-01-03 03:52:12.032723: Pseudo dice [0.9124, 0.9532, 0.9457]\n",
      "2024-01-03 03:52:12.038235: Epoch time: 126.05 s\n",
      "2024-01-03 03:52:13.298362: \n",
      "2024-01-03 03:52:13.306490: Epoch 651\n",
      "2024-01-03 03:52:13.311406: Current learning rate: 0.00388\n",
      "2024-01-03 03:54:19.119735: train_loss -0.9157\n",
      "2024-01-03 03:54:19.125732: val_loss -0.8311\n",
      "2024-01-03 03:54:19.132729: Pseudo dice [0.9109, 0.9525, 0.9428]\n",
      "2024-01-03 03:54:19.138737: Epoch time: 125.82 s\n",
      "2024-01-03 03:54:20.307078: \n",
      "2024-01-03 03:54:20.314085: Epoch 652\n",
      "2024-01-03 03:54:20.319092: Current learning rate: 0.00387\n",
      "2024-01-03 03:56:26.085596: train_loss -0.9171\n",
      "2024-01-03 03:56:26.091590: val_loss -0.8282\n",
      "2024-01-03 03:56:26.096592: Pseudo dice [0.9134, 0.9514, 0.9456]\n",
      "2024-01-03 03:56:26.101652: Epoch time: 125.78 s\n",
      "2024-01-03 03:56:27.437535: \n",
      "2024-01-03 03:56:27.444792: Epoch 653\n",
      "2024-01-03 03:56:27.449771: Current learning rate: 0.00386\n",
      "2024-01-03 03:58:33.106825: train_loss -0.9195\n",
      "2024-01-03 03:58:33.116817: val_loss -0.8282\n",
      "2024-01-03 03:58:33.122822: Pseudo dice [0.9114, 0.9511, 0.9464]\n",
      "2024-01-03 03:58:33.126818: Epoch time: 125.67 s\n",
      "2024-01-03 03:58:34.325193: \n",
      "2024-01-03 03:58:34.330807: Epoch 654\n",
      "2024-01-03 03:58:34.335821: Current learning rate: 0.00385\n",
      "2024-01-03 04:00:39.688753: train_loss -0.9204\n",
      "2024-01-03 04:00:39.696753: val_loss -0.8312\n",
      "2024-01-03 04:00:39.701753: Pseudo dice [0.9119, 0.9515, 0.9428]\n",
      "2024-01-03 04:00:39.705753: Epoch time: 125.37 s\n",
      "2024-01-03 04:00:40.924830: \n",
      "2024-01-03 04:00:40.936778: Epoch 655\n",
      "2024-01-03 04:00:40.941886: Current learning rate: 0.00384\n",
      "2024-01-03 04:02:46.870789: train_loss -0.9188\n",
      "2024-01-03 04:02:46.880740: val_loss -0.8237\n",
      "2024-01-03 04:02:46.889740: Pseudo dice [0.9102, 0.9499, 0.943]\n",
      "2024-01-03 04:02:46.895745: Epoch time: 125.95 s\n",
      "2024-01-03 04:02:48.405893: \n",
      "2024-01-03 04:02:48.411895: Epoch 656\n",
      "2024-01-03 04:02:48.415966: Current learning rate: 0.00383\n",
      "2024-01-03 04:04:54.157416: train_loss -0.9176\n",
      "2024-01-03 04:04:54.166425: val_loss -0.8301\n",
      "2024-01-03 04:04:54.173416: Pseudo dice [0.9127, 0.9522, 0.9461]\n",
      "2024-01-03 04:04:54.180417: Epoch time: 125.75 s\n",
      "2024-01-03 04:04:55.318967: \n",
      "2024-01-03 04:04:55.327966: Epoch 657\n",
      "2024-01-03 04:04:55.332976: Current learning rate: 0.00382\n",
      "2024-01-03 04:07:01.061341: train_loss -0.9172\n",
      "2024-01-03 04:07:01.067852: val_loss -0.8221\n",
      "2024-01-03 04:07:01.072851: Pseudo dice [0.91, 0.9508, 0.9454]\n",
      "2024-01-03 04:07:01.077852: Epoch time: 125.74 s\n",
      "2024-01-03 04:07:02.379221: \n",
      "2024-01-03 04:07:02.384253: Epoch 658\n",
      "2024-01-03 04:07:02.390298: Current learning rate: 0.00381\n",
      "2024-01-03 04:09:07.886553: train_loss -0.9204\n",
      "2024-01-03 04:09:07.893556: val_loss -0.8269\n",
      "2024-01-03 04:09:07.898555: Pseudo dice [0.9112, 0.9514, 0.9455]\n",
      "2024-01-03 04:09:07.902553: Epoch time: 125.51 s\n",
      "2024-01-03 04:09:09.044313: \n",
      "2024-01-03 04:09:09.049830: Epoch 659\n",
      "2024-01-03 04:09:09.054375: Current learning rate: 0.0038\n",
      "2024-01-03 04:11:14.606119: train_loss -0.9205\n",
      "2024-01-03 04:11:14.614122: val_loss -0.8286\n",
      "2024-01-03 04:11:14.619122: Pseudo dice [0.9124, 0.952, 0.9441]\n",
      "2024-01-03 04:11:14.625132: Epoch time: 125.56 s\n",
      "2024-01-03 04:11:15.799038: \n",
      "2024-01-03 04:11:15.808024: Epoch 660\n",
      "2024-01-03 04:11:15.814112: Current learning rate: 0.00379\n",
      "2024-01-03 04:13:21.274231: train_loss -0.9174\n",
      "2024-01-03 04:13:21.284739: val_loss -0.8274\n",
      "2024-01-03 04:13:21.292738: Pseudo dice [0.9126, 0.9519, 0.9455]\n",
      "2024-01-03 04:13:21.299741: Epoch time: 125.48 s\n",
      "2024-01-03 04:13:22.648062: \n",
      "2024-01-03 04:13:22.653476: Epoch 661\n",
      "2024-01-03 04:13:22.657530: Current learning rate: 0.00378\n",
      "2024-01-03 04:15:28.199837: train_loss -0.9211\n",
      "2024-01-03 04:15:28.207838: val_loss -0.8302\n",
      "2024-01-03 04:15:28.213838: Pseudo dice [0.9128, 0.9525, 0.9458]\n",
      "2024-01-03 04:15:28.218840: Epoch time: 125.55 s\n",
      "2024-01-03 04:15:29.393461: \n",
      "2024-01-03 04:15:29.399425: Epoch 662\n",
      "2024-01-03 04:15:29.409969: Current learning rate: 0.00377\n",
      "2024-01-03 04:17:35.033371: train_loss -0.9201\n",
      "2024-01-03 04:17:35.043370: val_loss -0.8251\n",
      "2024-01-03 04:17:35.048369: Pseudo dice [0.909, 0.9509, 0.9448]\n",
      "2024-01-03 04:17:35.053369: Epoch time: 125.64 s\n",
      "2024-01-03 04:17:36.343198: \n",
      "2024-01-03 04:17:36.351198: Epoch 663\n",
      "2024-01-03 04:17:36.356190: Current learning rate: 0.00376\n",
      "2024-01-03 04:19:41.895600: train_loss -0.9185\n",
      "2024-01-03 04:19:41.903600: val_loss -0.8229\n",
      "2024-01-03 04:19:41.909600: Pseudo dice [0.9116, 0.9498, 0.9444]\n",
      "2024-01-03 04:19:41.913601: Epoch time: 125.55 s\n",
      "2024-01-03 04:19:42.977942: \n",
      "2024-01-03 04:19:42.983940: Epoch 664\n",
      "2024-01-03 04:19:42.994023: Current learning rate: 0.00375\n",
      "2024-01-03 04:21:48.510467: train_loss -0.9177\n",
      "2024-01-03 04:21:48.517467: val_loss -0.833\n",
      "2024-01-03 04:21:48.522467: Pseudo dice [0.9117, 0.9531, 0.9456]\n",
      "2024-01-03 04:21:48.528471: Epoch time: 125.53 s\n",
      "2024-01-03 04:21:49.876315: \n",
      "2024-01-03 04:21:49.882321: Epoch 665\n",
      "2024-01-03 04:21:49.887368: Current learning rate: 0.00374\n",
      "2024-01-03 04:23:56.082294: train_loss -0.9172\n",
      "2024-01-03 04:23:56.089294: val_loss -0.8265\n",
      "2024-01-03 04:23:56.098294: Pseudo dice [0.9139, 0.9515, 0.9454]\n",
      "2024-01-03 04:23:56.104294: Epoch time: 126.21 s\n",
      "2024-01-03 04:23:57.259544: \n",
      "2024-01-03 04:23:57.266697: Epoch 666\n",
      "2024-01-03 04:23:57.271696: Current learning rate: 0.00373\n",
      "2024-01-03 04:26:03.196510: train_loss -0.9166\n",
      "2024-01-03 04:26:03.203510: val_loss -0.8228\n",
      "2024-01-03 04:26:03.209513: Pseudo dice [0.913, 0.9508, 0.9469]\n",
      "2024-01-03 04:26:03.213513: Epoch time: 125.94 s\n",
      "2024-01-03 04:26:04.399967: \n",
      "2024-01-03 04:26:04.405963: Epoch 667\n",
      "2024-01-03 04:26:04.409978: Current learning rate: 0.00372\n",
      "2024-01-03 04:28:10.221840: train_loss -0.9165\n",
      "2024-01-03 04:28:10.227839: val_loss -0.8224\n",
      "2024-01-03 04:28:10.232840: Pseudo dice [0.9091, 0.9521, 0.9451]\n",
      "2024-01-03 04:28:10.237838: Epoch time: 125.82 s\n",
      "2024-01-03 04:28:11.588729: \n",
      "2024-01-03 04:28:11.594719: Epoch 668\n",
      "2024-01-03 04:28:11.598728: Current learning rate: 0.00371\n",
      "2024-01-03 04:30:17.138697: train_loss -0.9189\n",
      "2024-01-03 04:30:17.148697: val_loss -0.827\n",
      "2024-01-03 04:30:17.156867: Pseudo dice [0.9117, 0.9504, 0.9443]\n",
      "2024-01-03 04:30:17.166379: Epoch time: 125.55 s\n",
      "2024-01-03 04:30:18.470118: \n",
      "2024-01-03 04:30:18.475845: Epoch 669\n",
      "2024-01-03 04:30:18.480845: Current learning rate: 0.0037\n",
      "2024-01-03 04:32:24.040848: train_loss -0.9208\n",
      "2024-01-03 04:32:24.048359: val_loss -0.8297\n",
      "2024-01-03 04:32:24.054358: Pseudo dice [0.9136, 0.9516, 0.9465]\n",
      "2024-01-03 04:32:24.059358: Epoch time: 125.57 s\n",
      "2024-01-03 04:32:25.273752: \n",
      "2024-01-03 04:32:25.279972: Epoch 670\n",
      "2024-01-03 04:32:25.290462: Current learning rate: 0.00369\n",
      "2024-01-03 04:34:31.122154: train_loss -0.9211\n",
      "2024-01-03 04:34:31.131147: val_loss -0.8281\n",
      "2024-01-03 04:34:31.138151: Pseudo dice [0.9107, 0.9518, 0.945]\n",
      "2024-01-03 04:34:31.144204: Epoch time: 125.85 s\n",
      "2024-01-03 04:34:32.419353: \n",
      "2024-01-03 04:34:32.431444: Epoch 671\n",
      "2024-01-03 04:34:32.436669: Current learning rate: 0.00368\n",
      "2024-01-03 04:36:38.125283: train_loss -0.9184\n",
      "2024-01-03 04:36:38.132285: val_loss -0.8216\n",
      "2024-01-03 04:36:38.141289: Pseudo dice [0.9102, 0.9511, 0.9423]\n",
      "2024-01-03 04:36:38.146286: Epoch time: 125.71 s\n",
      "2024-01-03 04:36:39.414047: \n",
      "2024-01-03 04:36:39.422047: Epoch 672\n",
      "2024-01-03 04:36:39.427047: Current learning rate: 0.00367\n",
      "2024-01-03 04:38:45.649534: train_loss -0.9173\n",
      "2024-01-03 04:38:45.658533: val_loss -0.8277\n",
      "2024-01-03 04:38:45.667533: Pseudo dice [0.9093, 0.9521, 0.9453]\n",
      "2024-01-03 04:38:45.674534: Epoch time: 126.24 s\n",
      "2024-01-03 04:38:46.994184: \n",
      "2024-01-03 04:38:47.003148: Epoch 673\n",
      "2024-01-03 04:38:47.008257: Current learning rate: 0.00366\n",
      "2024-01-03 04:40:52.915422: train_loss -0.9173\n",
      "2024-01-03 04:40:52.924423: val_loss -0.8241\n",
      "2024-01-03 04:40:52.932130: Pseudo dice [0.9109, 0.9501, 0.9451]\n",
      "2024-01-03 04:40:52.940032: Epoch time: 125.92 s\n",
      "2024-01-03 04:40:54.193963: \n",
      "2024-01-03 04:40:54.200220: Epoch 674\n",
      "2024-01-03 04:40:54.204284: Current learning rate: 0.00365\n",
      "2024-01-03 04:42:59.837750: train_loss -0.9193\n",
      "2024-01-03 04:42:59.844748: val_loss -0.8285\n",
      "2024-01-03 04:42:59.851755: Pseudo dice [0.9149, 0.9524, 0.9464]\n",
      "2024-01-03 04:42:59.855763: Epoch time: 125.64 s\n",
      "2024-01-03 04:43:01.029532: \n",
      "2024-01-03 04:43:01.035531: Epoch 675\n",
      "2024-01-03 04:43:01.040531: Current learning rate: 0.00364\n",
      "2024-01-03 04:45:06.478379: train_loss -0.9196\n",
      "2024-01-03 04:45:06.490385: val_loss -0.8249\n",
      "2024-01-03 04:45:06.509672: Pseudo dice [0.9119, 0.9503, 0.946]\n",
      "2024-01-03 04:45:06.514677: Epoch time: 125.45 s\n",
      "2024-01-03 04:45:07.893548: \n",
      "2024-01-03 04:45:07.901334: Epoch 676\n",
      "2024-01-03 04:45:07.906414: Current learning rate: 0.00363\n",
      "2024-01-03 04:47:13.523065: train_loss -0.9209\n",
      "2024-01-03 04:47:13.530063: val_loss -0.8296\n",
      "2024-01-03 04:47:13.535059: Pseudo dice [0.9118, 0.952, 0.9445]\n",
      "2024-01-03 04:47:13.539068: Epoch time: 125.63 s\n",
      "2024-01-03 04:47:14.692251: \n",
      "2024-01-03 04:47:14.706776: Epoch 677\n",
      "2024-01-03 04:47:14.714851: Current learning rate: 0.00362\n",
      "2024-01-03 04:49:20.458039: train_loss -0.9198\n",
      "2024-01-03 04:49:20.467040: val_loss -0.8332\n",
      "2024-01-03 04:49:20.478040: Pseudo dice [0.9124, 0.9515, 0.9453]\n",
      "2024-01-03 04:49:20.487040: Epoch time: 125.77 s\n",
      "2024-01-03 04:49:21.866755: \n",
      "2024-01-03 04:49:21.872758: Epoch 678\n",
      "2024-01-03 04:49:21.877825: Current learning rate: 0.00361\n",
      "2024-01-03 04:51:27.486816: train_loss -0.9211\n",
      "2024-01-03 04:51:27.492816: val_loss -0.8232\n",
      "2024-01-03 04:51:27.498817: Pseudo dice [0.9113, 0.9506, 0.9446]\n",
      "2024-01-03 04:51:27.503325: Epoch time: 125.62 s\n",
      "2024-01-03 04:51:28.644481: \n",
      "2024-01-03 04:51:28.650662: Epoch 679\n",
      "2024-01-03 04:51:28.655655: Current learning rate: 0.0036\n",
      "2024-01-03 04:53:34.412488: train_loss -0.9191\n",
      "2024-01-03 04:53:34.422483: val_loss -0.8269\n",
      "2024-01-03 04:53:34.428480: Pseudo dice [0.911, 0.9513, 0.9464]\n",
      "2024-01-03 04:53:34.433480: Epoch time: 125.77 s\n",
      "2024-01-03 04:53:35.646416: \n",
      "2024-01-03 04:53:35.652548: Epoch 680\n",
      "2024-01-03 04:53:35.657177: Current learning rate: 0.00359\n",
      "2024-01-03 04:55:41.639692: train_loss -0.9183\n",
      "2024-01-03 04:55:41.647691: val_loss -0.8252\n",
      "2024-01-03 04:55:41.654699: Pseudo dice [0.9078, 0.9507, 0.9461]\n",
      "2024-01-03 04:55:41.662173: Epoch time: 125.99 s\n",
      "2024-01-03 04:55:42.889767: \n",
      "2024-01-03 04:55:42.895477: Epoch 681\n",
      "2024-01-03 04:55:42.905553: Current learning rate: 0.00358\n",
      "2024-01-03 04:57:48.744425: train_loss -0.9193\n",
      "2024-01-03 04:57:48.752424: val_loss -0.8224\n",
      "2024-01-03 04:57:48.757426: Pseudo dice [0.9099, 0.9495, 0.9457]\n",
      "2024-01-03 04:57:48.763424: Epoch time: 125.86 s\n",
      "2024-01-03 04:57:49.955053: \n",
      "2024-01-03 04:57:49.960548: Epoch 682\n",
      "2024-01-03 04:57:49.965523: Current learning rate: 0.00357\n",
      "2024-01-03 04:59:55.397925: train_loss -0.9191\n",
      "2024-01-03 04:59:55.406926: val_loss -0.8247\n",
      "2024-01-03 04:59:55.413918: Pseudo dice [0.9118, 0.9497, 0.945]\n",
      "2024-01-03 04:59:55.419927: Epoch time: 125.44 s\n",
      "2024-01-03 04:59:56.854887: \n",
      "2024-01-03 04:59:56.865055: Epoch 683\n",
      "2024-01-03 04:59:56.869150: Current learning rate: 0.00356\n",
      "2024-01-03 05:02:03.256892: train_loss -0.9193\n",
      "2024-01-03 05:02:03.265893: val_loss -0.8303\n",
      "2024-01-03 05:02:03.272892: Pseudo dice [0.9138, 0.9533, 0.9467]\n",
      "2024-01-03 05:02:03.281893: Epoch time: 126.4 s\n",
      "2024-01-03 05:02:04.669098: \n",
      "2024-01-03 05:02:04.677120: Epoch 684\n",
      "2024-01-03 05:02:04.682144: Current learning rate: 0.00355\n",
      "2024-01-03 05:04:10.500266: train_loss -0.9165\n",
      "2024-01-03 05:04:10.507257: val_loss -0.8267\n",
      "2024-01-03 05:04:10.512257: Pseudo dice [0.9115, 0.9516, 0.9452]\n",
      "2024-01-03 05:04:10.518262: Epoch time: 125.83 s\n",
      "2024-01-03 05:04:11.755007: \n",
      "2024-01-03 05:04:11.761005: Epoch 685\n",
      "2024-01-03 05:04:11.766005: Current learning rate: 0.00354\n",
      "2024-01-03 05:06:17.467655: train_loss -0.917\n",
      "2024-01-03 05:06:17.475655: val_loss -0.8047\n",
      "2024-01-03 05:06:17.482655: Pseudo dice [0.9063, 0.9481, 0.9426]\n",
      "2024-01-03 05:06:17.491655: Epoch time: 125.71 s\n",
      "2024-01-03 05:06:18.694125: \n",
      "2024-01-03 05:06:18.705637: Epoch 686\n",
      "2024-01-03 05:06:18.710688: Current learning rate: 0.00353\n",
      "2024-01-03 05:08:24.304435: train_loss -0.9181\n",
      "2024-01-03 05:08:24.314427: val_loss -0.8209\n",
      "2024-01-03 05:08:24.319438: Pseudo dice [0.9123, 0.9504, 0.9444]\n",
      "2024-01-03 05:08:24.325430: Epoch time: 125.61 s\n",
      "2024-01-03 05:08:25.613200: \n",
      "2024-01-03 05:08:25.619190: Epoch 687\n",
      "2024-01-03 05:08:25.625198: Current learning rate: 0.00352\n",
      "2024-01-03 05:10:30.897550: train_loss -0.9204\n",
      "2024-01-03 05:10:30.906551: val_loss -0.8319\n",
      "2024-01-03 05:10:30.913553: Pseudo dice [0.9115, 0.9508, 0.9445]\n",
      "2024-01-03 05:10:30.918551: Epoch time: 125.29 s\n",
      "2024-01-03 05:10:32.205509: \n",
      "2024-01-03 05:10:32.213217: Epoch 688\n",
      "2024-01-03 05:10:32.224235: Current learning rate: 0.00351\n",
      "2024-01-03 05:12:38.180710: train_loss -0.9186\n",
      "2024-01-03 05:12:38.186712: val_loss -0.8265\n",
      "2024-01-03 05:12:38.193711: Pseudo dice [0.9113, 0.952, 0.9458]\n",
      "2024-01-03 05:12:38.199712: Epoch time: 125.98 s\n",
      "2024-01-03 05:12:39.513217: \n",
      "2024-01-03 05:12:39.522310: Epoch 689\n",
      "2024-01-03 05:12:39.527966: Current learning rate: 0.0035\n",
      "2024-01-03 05:14:45.189203: train_loss -0.9186\n",
      "2024-01-03 05:14:45.198204: val_loss -0.8308\n",
      "2024-01-03 05:14:45.204206: Pseudo dice [0.9128, 0.9525, 0.9453]\n",
      "2024-01-03 05:14:45.210205: Epoch time: 125.68 s\n",
      "2024-01-03 05:14:46.709157: \n",
      "2024-01-03 05:14:46.721155: Epoch 690\n",
      "2024-01-03 05:14:46.726079: Current learning rate: 0.00349\n",
      "2024-01-03 05:16:52.566894: train_loss -0.9184\n",
      "2024-01-03 05:16:52.575894: val_loss -0.8286\n",
      "2024-01-03 05:16:52.582894: Pseudo dice [0.9098, 0.9514, 0.9454]\n",
      "2024-01-03 05:16:52.586898: Epoch time: 125.86 s\n",
      "2024-01-03 05:16:53.854727: \n",
      "2024-01-03 05:16:53.860654: Epoch 691\n",
      "2024-01-03 05:16:53.865822: Current learning rate: 0.00348\n",
      "2024-01-03 05:18:59.500202: train_loss -0.9167\n",
      "2024-01-03 05:18:59.507211: val_loss -0.8186\n",
      "2024-01-03 05:18:59.512206: Pseudo dice [0.9112, 0.9494, 0.9446]\n",
      "2024-01-03 05:18:59.519207: Epoch time: 125.65 s\n",
      "2024-01-03 05:19:00.714549: \n",
      "2024-01-03 05:19:00.721326: Epoch 692\n",
      "2024-01-03 05:19:00.725974: Current learning rate: 0.00346\n",
      "2024-01-03 05:21:06.197327: train_loss -0.9211\n",
      "2024-01-03 05:21:06.204320: val_loss -0.8291\n",
      "2024-01-03 05:21:06.210340: Pseudo dice [0.9127, 0.9527, 0.9453]\n",
      "2024-01-03 05:21:06.216341: Epoch time: 125.48 s\n",
      "2024-01-03 05:21:07.465752: \n",
      "2024-01-03 05:21:07.476213: Epoch 693\n",
      "2024-01-03 05:21:07.483201: Current learning rate: 0.00345\n",
      "2024-01-03 05:23:13.178149: train_loss -0.9187\n",
      "2024-01-03 05:23:13.185161: val_loss -0.8282\n",
      "2024-01-03 05:23:13.191166: Pseudo dice [0.909, 0.9526, 0.9458]\n",
      "2024-01-03 05:23:13.198166: Epoch time: 125.72 s\n",
      "2024-01-03 05:23:14.306096: \n",
      "2024-01-03 05:23:14.313174: Epoch 694\n",
      "2024-01-03 05:23:14.319158: Current learning rate: 0.00344\n",
      "2024-01-03 05:25:20.063355: train_loss -0.919\n",
      "2024-01-03 05:25:20.070357: val_loss -0.8286\n",
      "2024-01-03 05:25:20.076356: Pseudo dice [0.9111, 0.9519, 0.9443]\n",
      "2024-01-03 05:25:20.081355: Epoch time: 125.76 s\n",
      "2024-01-03 05:25:21.346492: \n",
      "2024-01-03 05:25:21.353064: Epoch 695\n",
      "2024-01-03 05:25:21.358061: Current learning rate: 0.00343\n",
      "2024-01-03 05:27:27.161154: train_loss -0.9191\n",
      "2024-01-03 05:27:27.168152: val_loss -0.829\n",
      "2024-01-03 05:27:27.174152: Pseudo dice [0.9118, 0.9516, 0.9446]\n",
      "2024-01-03 05:27:27.179152: Epoch time: 125.82 s\n",
      "2024-01-03 05:27:28.372683: \n",
      "2024-01-03 05:27:28.381937: Epoch 696\n",
      "2024-01-03 05:27:28.389944: Current learning rate: 0.00342\n",
      "2024-01-03 05:29:34.064723: train_loss -0.9184\n",
      "2024-01-03 05:29:34.074800: val_loss -0.8316\n",
      "2024-01-03 05:29:34.080713: Pseudo dice [0.9125, 0.9517, 0.9448]\n",
      "2024-01-03 05:29:34.085712: Epoch time: 125.69 s\n",
      "2024-01-03 05:29:35.334778: \n",
      "2024-01-03 05:29:35.339853: Epoch 697\n",
      "2024-01-03 05:29:35.344839: Current learning rate: 0.00341\n",
      "2024-01-03 05:31:40.958858: train_loss -0.9175\n",
      "2024-01-03 05:31:40.970371: val_loss -0.8263\n",
      "2024-01-03 05:31:40.979371: Pseudo dice [0.9106, 0.9505, 0.9435]\n",
      "2024-01-03 05:31:40.985368: Epoch time: 125.63 s\n",
      "2024-01-03 05:31:42.389162: \n",
      "2024-01-03 05:31:42.399312: Epoch 698\n",
      "2024-01-03 05:31:42.409253: Current learning rate: 0.0034\n",
      "2024-01-03 05:33:47.862803: train_loss -0.9217\n",
      "2024-01-03 05:33:47.872803: val_loss -0.8318\n",
      "2024-01-03 05:33:47.877803: Pseudo dice [0.9096, 0.9521, 0.9451]\n",
      "2024-01-03 05:33:47.883803: Epoch time: 125.47 s\n",
      "2024-01-03 05:33:49.083159: \n",
      "2024-01-03 05:33:49.089159: Epoch 699\n",
      "2024-01-03 05:33:49.095151: Current learning rate: 0.00339\n",
      "2024-01-03 05:35:55.022696: train_loss -0.919\n",
      "2024-01-03 05:35:55.030203: val_loss -0.828\n",
      "2024-01-03 05:35:55.035203: Pseudo dice [0.9124, 0.9517, 0.9463]\n",
      "2024-01-03 05:35:55.041202: Epoch time: 125.94 s\n",
      "2024-01-03 05:35:56.713577: \n",
      "2024-01-03 05:35:56.719222: Epoch 700\n",
      "2024-01-03 05:35:56.726232: Current learning rate: 0.00338\n",
      "2024-01-03 05:38:02.329648: train_loss -0.921\n",
      "2024-01-03 05:38:02.338649: val_loss -0.8272\n",
      "2024-01-03 05:38:02.344651: Pseudo dice [0.912, 0.9518, 0.9456]\n",
      "2024-01-03 05:38:02.350649: Epoch time: 125.62 s\n",
      "2024-01-03 05:38:03.746940: \n",
      "2024-01-03 05:38:03.754943: Epoch 701\n",
      "2024-01-03 05:38:03.762944: Current learning rate: 0.00337\n",
      "2024-01-03 05:40:09.385528: train_loss -0.9211\n",
      "2024-01-03 05:40:09.392528: val_loss -0.8283\n",
      "2024-01-03 05:40:09.398529: Pseudo dice [0.9117, 0.951, 0.944]\n",
      "2024-01-03 05:40:09.404530: Epoch time: 125.64 s\n",
      "2024-01-03 05:40:10.538938: \n",
      "2024-01-03 05:40:10.546936: Epoch 702\n",
      "2024-01-03 05:40:10.552098: Current learning rate: 0.00336\n",
      "2024-01-03 05:42:16.088781: train_loss -0.9231\n",
      "2024-01-03 05:42:16.099781: val_loss -0.8265\n",
      "2024-01-03 05:42:16.107290: Pseudo dice [0.9129, 0.9521, 0.9453]\n",
      "2024-01-03 05:42:16.115290: Epoch time: 125.55 s\n",
      "2024-01-03 05:42:17.322793: \n",
      "2024-01-03 05:42:17.333741: Epoch 703\n",
      "2024-01-03 05:42:17.338791: Current learning rate: 0.00335\n",
      "2024-01-03 05:44:23.360824: train_loss -0.9172\n",
      "2024-01-03 05:44:23.367826: val_loss -0.8289\n",
      "2024-01-03 05:44:23.373823: Pseudo dice [0.912, 0.9519, 0.9448]\n",
      "2024-01-03 05:44:23.378823: Epoch time: 126.04 s\n",
      "2024-01-03 05:44:24.564516: \n",
      "2024-01-03 05:44:24.572581: Epoch 704\n",
      "2024-01-03 05:44:24.577514: Current learning rate: 0.00334\n",
      "2024-01-03 05:46:30.292546: train_loss -0.9205\n",
      "2024-01-03 05:46:30.299546: val_loss -0.8275\n",
      "2024-01-03 05:46:30.309548: Pseudo dice [0.9135, 0.9511, 0.9462]\n",
      "2024-01-03 05:46:30.315547: Epoch time: 125.73 s\n",
      "2024-01-03 05:46:31.628001: \n",
      "2024-01-03 05:46:31.636086: Epoch 705\n",
      "2024-01-03 05:46:31.641084: Current learning rate: 0.00333\n",
      "2024-01-03 05:48:37.455447: train_loss -0.9176\n",
      "2024-01-03 05:48:37.465447: val_loss -0.8284\n",
      "2024-01-03 05:48:37.471451: Pseudo dice [0.9111, 0.9518, 0.9447]\n",
      "2024-01-03 05:48:37.480447: Epoch time: 125.83 s\n",
      "2024-01-03 05:48:38.925458: \n",
      "2024-01-03 05:48:38.933868: Epoch 706\n",
      "2024-01-03 05:48:38.938926: Current learning rate: 0.00332\n",
      "2024-01-03 05:50:44.711273: train_loss -0.917\n",
      "2024-01-03 05:50:44.721274: val_loss -0.8264\n",
      "2024-01-03 05:50:44.728273: Pseudo dice [0.9124, 0.9513, 0.9461]\n",
      "2024-01-03 05:50:44.737273: Epoch time: 125.79 s\n",
      "2024-01-03 05:50:45.966758: \n",
      "2024-01-03 05:50:45.971828: Epoch 707\n",
      "2024-01-03 05:50:45.976823: Current learning rate: 0.00331\n",
      "2024-01-03 05:52:51.877124: train_loss -0.9202\n",
      "2024-01-03 05:52:51.887123: val_loss -0.8275\n",
      "2024-01-03 05:52:51.894161: Pseudo dice [0.9126, 0.9518, 0.9446]\n",
      "2024-01-03 05:52:51.900152: Epoch time: 125.91 s\n",
      "2024-01-03 05:52:53.124017: \n",
      "2024-01-03 05:52:53.135949: Epoch 708\n",
      "2024-01-03 05:52:53.140016: Current learning rate: 0.0033\n",
      "2024-01-03 05:54:58.590129: train_loss -0.9208\n",
      "2024-01-03 05:54:58.601130: val_loss -0.8334\n",
      "2024-01-03 05:54:58.608119: Pseudo dice [0.9138, 0.952, 0.9474]\n",
      "2024-01-03 05:54:58.614118: Epoch time: 125.47 s\n",
      "2024-01-03 05:54:59.961546: \n",
      "2024-01-03 05:54:59.967531: Epoch 709\n",
      "2024-01-03 05:54:59.976182: Current learning rate: 0.00329\n",
      "2024-01-03 05:57:05.744403: train_loss -0.9178\n",
      "2024-01-03 05:57:05.754403: val_loss -0.8287\n",
      "2024-01-03 05:57:05.762403: Pseudo dice [0.9144, 0.9516, 0.9454]\n",
      "2024-01-03 05:57:05.770401: Epoch time: 125.78 s\n",
      "2024-01-03 05:57:07.108908: \n",
      "2024-01-03 05:57:07.115412: Epoch 710\n",
      "2024-01-03 05:57:07.120424: Current learning rate: 0.00328\n",
      "2024-01-03 05:59:12.712356: train_loss -0.9208\n",
      "2024-01-03 05:59:12.720374: val_loss -0.8338\n",
      "2024-01-03 05:59:12.726360: Pseudo dice [0.9111, 0.9509, 0.9444]\n",
      "2024-01-03 05:59:12.731360: Epoch time: 125.6 s\n",
      "2024-01-03 05:59:14.018115: \n",
      "2024-01-03 05:59:14.025115: Epoch 711\n",
      "2024-01-03 05:59:14.039725: Current learning rate: 0.00327\n",
      "2024-01-03 06:01:19.705633: train_loss -0.9192\n",
      "2024-01-03 06:01:19.713636: val_loss -0.8274\n",
      "2024-01-03 06:01:19.719646: Pseudo dice [0.9128, 0.9522, 0.9451]\n",
      "2024-01-03 06:01:19.725649: Epoch time: 125.69 s\n",
      "2024-01-03 06:01:21.018153: \n",
      "2024-01-03 06:01:21.023812: Epoch 712\n",
      "2024-01-03 06:01:21.028828: Current learning rate: 0.00326\n",
      "2024-01-03 06:03:26.502061: train_loss -0.9204\n",
      "2024-01-03 06:03:26.509057: val_loss -0.8311\n",
      "2024-01-03 06:03:26.514053: Pseudo dice [0.9125, 0.9515, 0.9448]\n",
      "2024-01-03 06:03:26.519053: Epoch time: 125.49 s\n",
      "2024-01-03 06:03:27.895742: \n",
      "2024-01-03 06:03:27.901325: Epoch 713\n",
      "2024-01-03 06:03:27.908566: Current learning rate: 0.00325\n",
      "2024-01-03 06:05:33.522233: train_loss -0.92\n",
      "2024-01-03 06:05:33.528232: val_loss -0.8204\n",
      "2024-01-03 06:05:33.533236: Pseudo dice [0.912, 0.9499, 0.9442]\n",
      "2024-01-03 06:05:33.538238: Epoch time: 125.63 s\n",
      "2024-01-03 06:05:34.748924: \n",
      "2024-01-03 06:05:34.755299: Epoch 714\n",
      "2024-01-03 06:05:34.760310: Current learning rate: 0.00324\n",
      "2024-01-03 06:07:40.214671: train_loss -0.9201\n",
      "2024-01-03 06:07:40.223179: val_loss -0.8191\n",
      "2024-01-03 06:07:40.227183: Pseudo dice [0.91, 0.9505, 0.9456]\n",
      "2024-01-03 06:07:40.233185: Epoch time: 125.47 s\n",
      "2024-01-03 06:07:41.446304: \n",
      "2024-01-03 06:07:41.455303: Epoch 715\n",
      "2024-01-03 06:07:41.460328: Current learning rate: 0.00323\n",
      "2024-01-03 06:09:47.268852: train_loss -0.921\n",
      "2024-01-03 06:09:47.277858: val_loss -0.8307\n",
      "2024-01-03 06:09:47.288372: Pseudo dice [0.9113, 0.9523, 0.946]\n",
      "2024-01-03 06:09:47.295372: Epoch time: 125.82 s\n",
      "2024-01-03 06:09:48.678232: \n",
      "2024-01-03 06:09:48.683171: Epoch 716\n",
      "2024-01-03 06:09:48.688892: Current learning rate: 0.00322\n",
      "2024-01-03 06:11:54.288933: train_loss -0.9198\n",
      "2024-01-03 06:11:54.295932: val_loss -0.8231\n",
      "2024-01-03 06:11:54.303933: Pseudo dice [0.9145, 0.9502, 0.9443]\n",
      "2024-01-03 06:11:54.309936: Epoch time: 125.61 s\n",
      "2024-01-03 06:11:55.520281: \n",
      "2024-01-03 06:11:55.525940: Epoch 717\n",
      "2024-01-03 06:11:55.531136: Current learning rate: 0.00321\n",
      "2024-01-03 06:14:01.332176: train_loss -0.9194\n",
      "2024-01-03 06:14:01.340176: val_loss -0.8295\n",
      "2024-01-03 06:14:01.345176: Pseudo dice [0.9126, 0.9523, 0.9441]\n",
      "2024-01-03 06:14:01.350191: Epoch time: 125.81 s\n",
      "2024-01-03 06:14:02.503443: \n",
      "2024-01-03 06:14:02.513167: Epoch 718\n",
      "2024-01-03 06:14:02.517600: Current learning rate: 0.0032\n",
      "2024-01-03 06:16:07.895687: train_loss -0.9205\n",
      "2024-01-03 06:16:07.905771: val_loss -0.8237\n",
      "2024-01-03 06:16:07.914749: Pseudo dice [0.9118, 0.9511, 0.9447]\n",
      "2024-01-03 06:16:07.921679: Epoch time: 125.39 s\n",
      "2024-01-03 06:16:09.106219: \n",
      "2024-01-03 06:16:09.111862: Epoch 719\n",
      "2024-01-03 06:16:09.122950: Current learning rate: 0.00319\n",
      "2024-01-03 06:18:14.367409: train_loss -0.9257\n",
      "2024-01-03 06:18:14.378397: val_loss -0.8189\n",
      "2024-01-03 06:18:14.386396: Pseudo dice [0.9076, 0.9499, 0.946]\n",
      "2024-01-03 06:18:14.394395: Epoch time: 125.26 s\n",
      "2024-01-03 06:18:15.820272: \n",
      "2024-01-03 06:18:15.825474: Epoch 720\n",
      "2024-01-03 06:18:15.829556: Current learning rate: 0.00318\n",
      "2024-01-03 06:20:21.219735: train_loss -0.919\n",
      "2024-01-03 06:20:21.227747: val_loss -0.8292\n",
      "2024-01-03 06:20:21.233741: Pseudo dice [0.914, 0.9503, 0.944]\n",
      "2024-01-03 06:20:21.242737: Epoch time: 125.4 s\n",
      "2024-01-03 06:20:22.418481: \n",
      "2024-01-03 06:20:22.424706: Epoch 721\n",
      "2024-01-03 06:20:22.429697: Current learning rate: 0.00317\n",
      "2024-01-03 06:22:28.034175: train_loss -0.9199\n",
      "2024-01-03 06:22:28.040160: val_loss -0.8189\n",
      "2024-01-03 06:22:28.048172: Pseudo dice [0.9095, 0.9499, 0.9461]\n",
      "2024-01-03 06:22:28.054160: Epoch time: 125.62 s\n",
      "2024-01-03 06:22:29.363978: \n",
      "2024-01-03 06:22:29.375983: Epoch 722\n",
      "2024-01-03 06:22:29.383048: Current learning rate: 0.00316\n",
      "2024-01-03 06:24:34.993387: train_loss -0.9195\n",
      "2024-01-03 06:24:35.002388: val_loss -0.8242\n",
      "2024-01-03 06:24:35.007388: Pseudo dice [0.9118, 0.9507, 0.9453]\n",
      "2024-01-03 06:24:35.012388: Epoch time: 125.63 s\n",
      "2024-01-03 06:24:36.241215: \n",
      "2024-01-03 06:24:36.249811: Epoch 723\n",
      "2024-01-03 06:24:36.257841: Current learning rate: 0.00315\n",
      "2024-01-03 06:26:41.796027: train_loss -0.9228\n",
      "2024-01-03 06:26:41.804028: val_loss -0.8329\n",
      "2024-01-03 06:26:41.811028: Pseudo dice [0.9131, 0.9523, 0.9446]\n",
      "2024-01-03 06:26:41.816028: Epoch time: 125.56 s\n",
      "2024-01-03 06:26:43.158288: \n",
      "2024-01-03 06:26:43.167362: Epoch 724\n",
      "2024-01-03 06:26:43.171355: Current learning rate: 0.00314\n",
      "2024-01-03 06:28:48.834094: train_loss -0.9168\n",
      "2024-01-03 06:28:48.840098: val_loss -0.8215\n",
      "2024-01-03 06:28:48.847098: Pseudo dice [0.9123, 0.9501, 0.9451]\n",
      "2024-01-03 06:28:48.855098: Epoch time: 125.68 s\n",
      "2024-01-03 06:28:50.008208: \n",
      "2024-01-03 06:28:50.014775: Epoch 725\n",
      "2024-01-03 06:28:50.018737: Current learning rate: 0.00313\n",
      "2024-01-03 06:30:55.606941: train_loss -0.9186\n",
      "2024-01-03 06:30:55.615941: val_loss -0.8223\n",
      "2024-01-03 06:30:55.624941: Pseudo dice [0.9115, 0.9508, 0.9443]\n",
      "2024-01-03 06:30:55.629941: Epoch time: 125.6 s\n",
      "2024-01-03 06:30:56.967824: \n",
      "2024-01-03 06:30:56.974896: Epoch 726\n",
      "2024-01-03 06:30:56.979616: Current learning rate: 0.00312\n",
      "2024-01-03 06:33:02.513655: train_loss -0.9181\n",
      "2024-01-03 06:33:02.521664: val_loss -0.8195\n",
      "2024-01-03 06:33:02.527657: Pseudo dice [0.9099, 0.951, 0.9443]\n",
      "2024-01-03 06:33:02.533656: Epoch time: 125.55 s\n",
      "2024-01-03 06:33:03.758867: \n",
      "2024-01-03 06:33:03.768908: Epoch 727\n",
      "2024-01-03 06:33:03.773864: Current learning rate: 0.00311\n",
      "2024-01-03 06:35:09.412974: train_loss -0.9181\n",
      "2024-01-03 06:35:09.422971: val_loss -0.8224\n",
      "2024-01-03 06:35:09.428972: Pseudo dice [0.9102, 0.95, 0.9439]\n",
      "2024-01-03 06:35:09.432971: Epoch time: 125.66 s\n",
      "2024-01-03 06:35:10.788937: \n",
      "2024-01-03 06:35:10.798304: Epoch 728\n",
      "2024-01-03 06:35:10.802319: Current learning rate: 0.0031\n",
      "2024-01-03 06:37:16.352818: train_loss -0.9208\n",
      "2024-01-03 06:37:16.358817: val_loss -0.8242\n",
      "2024-01-03 06:37:16.364816: Pseudo dice [0.9128, 0.9505, 0.9451]\n",
      "2024-01-03 06:37:16.370828: Epoch time: 125.56 s\n",
      "2024-01-03 06:37:17.521354: \n",
      "2024-01-03 06:37:17.527298: Epoch 729\n",
      "2024-01-03 06:37:17.533299: Current learning rate: 0.00309\n",
      "2024-01-03 06:39:23.058144: train_loss -0.9227\n",
      "2024-01-03 06:39:23.065134: val_loss -0.8203\n",
      "2024-01-03 06:39:23.070147: Pseudo dice [0.9094, 0.9507, 0.9445]\n",
      "2024-01-03 06:39:23.075150: Epoch time: 125.54 s\n",
      "2024-01-03 06:39:24.186730: \n",
      "2024-01-03 06:39:24.193161: Epoch 730\n",
      "2024-01-03 06:39:24.201180: Current learning rate: 0.00308\n",
      "2024-01-03 06:41:29.524877: train_loss -0.9224\n",
      "2024-01-03 06:41:29.532872: val_loss -0.8284\n",
      "2024-01-03 06:41:29.538872: Pseudo dice [0.9107, 0.9517, 0.946]\n",
      "2024-01-03 06:41:29.543872: Epoch time: 125.34 s\n",
      "2024-01-03 06:41:30.831627: \n",
      "2024-01-03 06:41:30.837693: Epoch 731\n",
      "2024-01-03 06:41:30.846630: Current learning rate: 0.00307\n",
      "2024-01-03 06:43:36.521379: train_loss -0.9213\n",
      "2024-01-03 06:43:36.528380: val_loss -0.8245\n",
      "2024-01-03 06:43:36.533381: Pseudo dice [0.9086, 0.9512, 0.9456]\n",
      "2024-01-03 06:43:36.540382: Epoch time: 125.69 s\n",
      "2024-01-03 06:43:37.942314: \n",
      "2024-01-03 06:43:37.951595: Epoch 732\n",
      "2024-01-03 06:43:37.958663: Current learning rate: 0.00306\n",
      "2024-01-03 06:45:43.767196: train_loss -0.9186\n",
      "2024-01-03 06:45:43.773196: val_loss -0.8279\n",
      "2024-01-03 06:45:43.780196: Pseudo dice [0.911, 0.9524, 0.946]\n",
      "2024-01-03 06:45:43.785196: Epoch time: 125.83 s\n",
      "2024-01-03 06:45:44.960608: \n",
      "2024-01-03 06:45:44.966731: Epoch 733\n",
      "2024-01-03 06:45:44.971747: Current learning rate: 0.00305\n",
      "2024-01-03 06:47:51.138055: train_loss -0.9195\n",
      "2024-01-03 06:47:51.149051: val_loss -0.8242\n",
      "2024-01-03 06:47:51.156057: Pseudo dice [0.9113, 0.9512, 0.9448]\n",
      "2024-01-03 06:47:51.161044: Epoch time: 126.18 s\n",
      "2024-01-03 06:47:52.333472: \n",
      "2024-01-03 06:47:52.342630: Epoch 734\n",
      "2024-01-03 06:47:52.350639: Current learning rate: 0.00304\n",
      "2024-01-03 06:49:57.823313: train_loss -0.9213\n",
      "2024-01-03 06:49:57.832303: val_loss -0.8266\n",
      "2024-01-03 06:49:57.839303: Pseudo dice [0.9122, 0.9512, 0.9446]\n",
      "2024-01-03 06:49:57.847372: Epoch time: 125.49 s\n",
      "2024-01-03 06:49:59.282896: \n",
      "2024-01-03 06:49:59.289894: Epoch 735\n",
      "2024-01-03 06:49:59.294536: Current learning rate: 0.00303\n",
      "2024-01-03 06:52:04.902030: train_loss -0.9205\n",
      "2024-01-03 06:52:04.913031: val_loss -0.8266\n",
      "2024-01-03 06:52:04.920030: Pseudo dice [0.9096, 0.9519, 0.9438]\n",
      "2024-01-03 06:52:04.927030: Epoch time: 125.62 s\n",
      "2024-01-03 06:52:06.003650: \n",
      "2024-01-03 06:52:06.012726: Epoch 736\n",
      "2024-01-03 06:52:06.018725: Current learning rate: 0.00302\n",
      "2024-01-03 06:54:11.815602: train_loss -0.9207\n",
      "2024-01-03 06:54:11.822607: val_loss -0.8221\n",
      "2024-01-03 06:54:11.829606: Pseudo dice [0.9117, 0.9495, 0.9457]\n",
      "2024-01-03 06:54:11.835601: Epoch time: 125.81 s\n",
      "2024-01-03 06:54:12.962214: \n",
      "2024-01-03 06:54:12.968993: Epoch 737\n",
      "2024-01-03 06:54:12.977049: Current learning rate: 0.00301\n",
      "2024-01-03 06:56:18.606708: train_loss -0.9221\n",
      "2024-01-03 06:56:18.616698: val_loss -0.8221\n",
      "2024-01-03 06:56:18.624697: Pseudo dice [0.9124, 0.9495, 0.9452]\n",
      "2024-01-03 06:56:18.631700: Epoch time: 125.65 s\n",
      "2024-01-03 06:56:19.803370: \n",
      "2024-01-03 06:56:19.816857: Epoch 738\n",
      "2024-01-03 06:56:19.821927: Current learning rate: 0.003\n",
      "2024-01-03 06:58:25.491321: train_loss -0.9224\n",
      "2024-01-03 06:58:25.498321: val_loss -0.8222\n",
      "2024-01-03 06:58:25.505321: Pseudo dice [0.9109, 0.9503, 0.9457]\n",
      "2024-01-03 06:58:25.511334: Epoch time: 125.69 s\n",
      "2024-01-03 06:58:26.689076: \n",
      "2024-01-03 06:58:26.699013: Epoch 739\n",
      "2024-01-03 06:58:26.703667: Current learning rate: 0.00299\n",
      "2024-01-03 07:00:32.348106: train_loss -0.9222\n",
      "2024-01-03 07:00:32.356111: val_loss -0.8255\n",
      "2024-01-03 07:00:32.361112: Pseudo dice [0.9118, 0.9504, 0.9459]\n",
      "2024-01-03 07:00:32.367113: Epoch time: 125.66 s\n",
      "2024-01-03 07:00:33.752043: \n",
      "2024-01-03 07:00:33.760111: Epoch 740\n",
      "2024-01-03 07:00:33.765110: Current learning rate: 0.00297\n",
      "2024-01-03 07:02:39.290734: train_loss -0.9205\n",
      "2024-01-03 07:02:39.297724: val_loss -0.8233\n",
      "2024-01-03 07:02:39.304722: Pseudo dice [0.91, 0.9513, 0.9464]\n",
      "2024-01-03 07:02:39.309731: Epoch time: 125.54 s\n",
      "2024-01-03 07:02:40.459819: \n",
      "2024-01-03 07:02:40.465701: Epoch 741\n",
      "2024-01-03 07:02:40.472778: Current learning rate: 0.00296\n",
      "2024-01-03 07:04:46.151989: train_loss -0.9212\n",
      "2024-01-03 07:04:46.162985: val_loss -0.8207\n",
      "2024-01-03 07:04:46.169996: Pseudo dice [0.9096, 0.9495, 0.944]\n",
      "2024-01-03 07:04:46.174987: Epoch time: 125.69 s\n",
      "2024-01-03 07:04:47.625652: \n",
      "2024-01-03 07:04:47.633086: Epoch 742\n",
      "2024-01-03 07:04:47.641089: Current learning rate: 0.00295\n",
      "2024-01-03 07:06:53.338979: train_loss -0.9214\n",
      "2024-01-03 07:06:53.350990: val_loss -0.819\n",
      "2024-01-03 07:06:53.359982: Pseudo dice [0.908, 0.9498, 0.9436]\n",
      "2024-01-03 07:06:53.364993: Epoch time: 125.71 s\n",
      "2024-01-03 07:06:54.704496: \n",
      "2024-01-03 07:06:54.713651: Epoch 743\n",
      "2024-01-03 07:06:54.718663: Current learning rate: 0.00294\n",
      "2024-01-03 07:09:00.567684: train_loss -0.9209\n",
      "2024-01-03 07:09:00.579686: val_loss -0.8287\n",
      "2024-01-03 07:09:00.588202: Pseudo dice [0.9115, 0.9525, 0.9452]\n",
      "2024-01-03 07:09:00.597214: Epoch time: 125.86 s\n",
      "2024-01-03 07:09:01.928754: \n",
      "2024-01-03 07:09:01.934815: Epoch 744\n",
      "2024-01-03 07:09:01.939808: Current learning rate: 0.00293\n",
      "2024-01-03 07:11:07.349776: train_loss -0.9225\n",
      "2024-01-03 07:11:07.360302: val_loss -0.8322\n",
      "2024-01-03 07:11:07.370302: Pseudo dice [0.9119, 0.9527, 0.9444]\n",
      "2024-01-03 07:11:07.379836: Epoch time: 125.42 s\n",
      "2024-01-03 07:11:08.714149: \n",
      "2024-01-03 07:11:08.719569: Epoch 745\n",
      "2024-01-03 07:11:08.724633: Current learning rate: 0.00292\n",
      "2024-01-03 07:13:14.091815: train_loss -0.92\n",
      "2024-01-03 07:13:14.100841: val_loss -0.8244\n",
      "2024-01-03 07:13:14.107831: Pseudo dice [0.9099, 0.9512, 0.9446]\n",
      "2024-01-03 07:13:14.112831: Epoch time: 125.38 s\n",
      "2024-01-03 07:13:15.348626: \n",
      "2024-01-03 07:13:15.356618: Epoch 746\n",
      "2024-01-03 07:13:15.361627: Current learning rate: 0.00291\n",
      "2024-01-03 07:15:20.894983: train_loss -0.9195\n",
      "2024-01-03 07:15:20.903981: val_loss -0.8325\n",
      "2024-01-03 07:15:20.909982: Pseudo dice [0.9105, 0.952, 0.9446]\n",
      "2024-01-03 07:15:20.918984: Epoch time: 125.55 s\n",
      "2024-01-03 07:15:22.147618: \n",
      "2024-01-03 07:15:22.153684: Epoch 747\n",
      "2024-01-03 07:15:22.159690: Current learning rate: 0.0029\n",
      "2024-01-03 07:17:27.577295: train_loss -0.9234\n",
      "2024-01-03 07:17:27.584295: val_loss -0.8236\n",
      "2024-01-03 07:17:27.591295: Pseudo dice [0.9126, 0.9504, 0.9442]\n",
      "2024-01-03 07:17:27.597297: Epoch time: 125.43 s\n",
      "2024-01-03 07:17:28.766032: \n",
      "2024-01-03 07:17:28.772017: Epoch 748\n",
      "2024-01-03 07:17:28.776974: Current learning rate: 0.00289\n",
      "2024-01-03 07:19:34.659585: train_loss -0.9207\n",
      "2024-01-03 07:19:34.667577: val_loss -0.8246\n",
      "2024-01-03 07:19:34.674575: Pseudo dice [0.9108, 0.9521, 0.9458]\n",
      "2024-01-03 07:19:34.679575: Epoch time: 125.9 s\n",
      "2024-01-03 07:19:35.836028: \n",
      "2024-01-03 07:19:35.846494: Epoch 749\n",
      "2024-01-03 07:19:35.851564: Current learning rate: 0.00288\n",
      "2024-01-03 07:21:41.900353: train_loss -0.9205\n",
      "2024-01-03 07:21:41.912354: val_loss -0.8244\n",
      "2024-01-03 07:21:41.921354: Pseudo dice [0.9121, 0.9521, 0.9444]\n",
      "2024-01-03 07:21:41.926353: Epoch time: 126.07 s\n",
      "2024-01-03 07:21:43.923580: \n",
      "2024-01-03 07:21:43.930429: Epoch 750\n",
      "2024-01-03 07:21:43.938426: Current learning rate: 0.00287\n",
      "2024-01-03 07:23:50.161470: train_loss -0.9186\n",
      "2024-01-03 07:23:50.169471: val_loss -0.814\n",
      "2024-01-03 07:23:50.175470: Pseudo dice [0.9097, 0.9484, 0.9447]\n",
      "2024-01-03 07:23:50.180470: Epoch time: 126.24 s\n",
      "2024-01-03 07:23:51.519705: \n",
      "2024-01-03 07:23:51.525770: Epoch 751\n",
      "2024-01-03 07:23:51.530785: Current learning rate: 0.00286\n",
      "2024-01-03 07:25:57.147897: train_loss -0.9212\n",
      "2024-01-03 07:25:57.154896: val_loss -0.8296\n",
      "2024-01-03 07:25:57.161896: Pseudo dice [0.9122, 0.9524, 0.9451]\n",
      "2024-01-03 07:25:57.167906: Epoch time: 125.63 s\n",
      "2024-01-03 07:25:58.405514: \n",
      "2024-01-03 07:25:58.411501: Epoch 752\n",
      "2024-01-03 07:25:58.416567: Current learning rate: 0.00285\n",
      "2024-01-03 07:28:04.778732: train_loss -0.9178\n",
      "2024-01-03 07:28:04.788752: val_loss -0.8206\n",
      "2024-01-03 07:28:04.797751: Pseudo dice [0.9129, 0.9508, 0.9453]\n",
      "2024-01-03 07:28:04.803255: Epoch time: 126.37 s\n",
      "2024-01-03 07:28:06.150721: \n",
      "2024-01-03 07:28:06.157731: Epoch 753\n",
      "2024-01-03 07:28:06.162662: Current learning rate: 0.00284\n",
      "2024-01-03 07:30:11.502666: train_loss -0.9251\n",
      "2024-01-03 07:30:11.512667: val_loss -0.8225\n",
      "2024-01-03 07:30:11.517666: Pseudo dice [0.9121, 0.9523, 0.9454]\n",
      "2024-01-03 07:30:11.525667: Epoch time: 125.35 s\n",
      "2024-01-03 07:30:12.753620: \n",
      "2024-01-03 07:30:12.760698: Epoch 754\n",
      "2024-01-03 07:30:12.765770: Current learning rate: 0.00283\n",
      "2024-01-03 07:32:18.243475: train_loss -0.9204\n",
      "2024-01-03 07:32:18.253987: val_loss -0.824\n",
      "2024-01-03 07:32:18.261996: Pseudo dice [0.9112, 0.9515, 0.9443]\n",
      "2024-01-03 07:32:18.266995: Epoch time: 125.49 s\n",
      "2024-01-03 07:32:19.411383: \n",
      "2024-01-03 07:32:19.417973: Epoch 755\n",
      "2024-01-03 07:32:19.426029: Current learning rate: 0.00282\n",
      "2024-01-03 07:34:24.999818: train_loss -0.9229\n",
      "2024-01-03 07:34:25.008822: val_loss -0.822\n",
      "2024-01-03 07:34:25.014819: Pseudo dice [0.912, 0.9498, 0.9449]\n",
      "2024-01-03 07:34:25.019814: Epoch time: 125.59 s\n",
      "2024-01-03 07:34:26.195522: \n",
      "2024-01-03 07:34:26.201850: Epoch 756\n",
      "2024-01-03 07:34:26.209853: Current learning rate: 0.00281\n",
      "2024-01-03 07:36:31.947629: train_loss -0.9212\n",
      "2024-01-03 07:36:31.954629: val_loss -0.8245\n",
      "2024-01-03 07:36:31.960629: Pseudo dice [0.9097, 0.9519, 0.9459]\n",
      "2024-01-03 07:36:31.966631: Epoch time: 125.75 s\n",
      "2024-01-03 07:36:33.368451: \n",
      "2024-01-03 07:36:33.374311: Epoch 757\n",
      "2024-01-03 07:36:33.379314: Current learning rate: 0.0028\n",
      "2024-01-03 07:38:39.055574: train_loss -0.9219\n",
      "2024-01-03 07:38:39.064095: val_loss -0.8221\n",
      "2024-01-03 07:38:39.070100: Pseudo dice [0.9106, 0.9498, 0.9442]\n",
      "2024-01-03 07:38:39.076099: Epoch time: 125.69 s\n",
      "2024-01-03 07:38:40.307571: \n",
      "2024-01-03 07:38:40.314590: Epoch 758\n",
      "2024-01-03 07:38:40.319898: Current learning rate: 0.00279\n",
      "2024-01-03 07:40:45.867150: train_loss -0.9226\n",
      "2024-01-03 07:40:45.874141: val_loss -0.8229\n",
      "2024-01-03 07:40:45.880141: Pseudo dice [0.9113, 0.9512, 0.9448]\n",
      "2024-01-03 07:40:45.885142: Epoch time: 125.56 s\n",
      "2024-01-03 07:40:47.079619: \n",
      "2024-01-03 07:40:47.093713: Epoch 759\n",
      "2024-01-03 07:40:47.098797: Current learning rate: 0.00278\n",
      "2024-01-03 07:42:52.962924: train_loss -0.9215\n",
      "2024-01-03 07:42:52.970925: val_loss -0.8283\n",
      "2024-01-03 07:42:52.976934: Pseudo dice [0.9133, 0.9522, 0.9446]\n",
      "2024-01-03 07:42:52.983942: Epoch time: 125.88 s\n",
      "2024-01-03 07:42:54.290182: \n",
      "2024-01-03 07:42:54.295824: Epoch 760\n",
      "2024-01-03 07:42:54.300822: Current learning rate: 0.00277\n",
      "2024-01-03 07:44:59.870094: train_loss -0.9212\n",
      "2024-01-03 07:44:59.880097: val_loss -0.8212\n",
      "2024-01-03 07:44:59.885099: Pseudo dice [0.9128, 0.9501, 0.9438]\n",
      "2024-01-03 07:44:59.891184: Epoch time: 125.58 s\n",
      "2024-01-03 07:45:01.121057: \n",
      "2024-01-03 07:45:01.133057: Epoch 761\n",
      "2024-01-03 07:45:01.141131: Current learning rate: 0.00276\n",
      "2024-01-03 07:47:06.795900: train_loss -0.9218\n",
      "2024-01-03 07:47:06.803432: val_loss -0.8228\n",
      "2024-01-03 07:47:06.809456: Pseudo dice [0.9114, 0.9513, 0.9452]\n",
      "2024-01-03 07:47:06.815959: Epoch time: 125.68 s\n",
      "2024-01-03 07:47:07.919582: \n",
      "2024-01-03 07:47:07.926311: Epoch 762\n",
      "2024-01-03 07:47:07.931319: Current learning rate: 0.00275\n",
      "2024-01-03 07:49:13.697903: train_loss -0.9216\n",
      "2024-01-03 07:49:13.704905: val_loss -0.8232\n",
      "2024-01-03 07:49:13.712904: Pseudo dice [0.9106, 0.9509, 0.9451]\n",
      "2024-01-03 07:49:13.721906: Epoch time: 125.78 s\n",
      "2024-01-03 07:49:15.112984: \n",
      "2024-01-03 07:49:15.120982: Epoch 763\n",
      "2024-01-03 07:49:15.131169: Current learning rate: 0.00274\n",
      "2024-01-03 07:51:20.932675: train_loss -0.9205\n",
      "2024-01-03 07:51:20.941674: val_loss -0.8208\n",
      "2024-01-03 07:51:20.948676: Pseudo dice [0.9106, 0.9514, 0.9457]\n",
      "2024-01-03 07:51:20.955681: Epoch time: 125.82 s\n",
      "2024-01-03 07:51:22.313890: \n",
      "2024-01-03 07:51:22.321249: Epoch 764\n",
      "2024-01-03 07:51:22.327177: Current learning rate: 0.00273\n",
      "2024-01-03 07:53:27.690095: train_loss -0.9217\n",
      "2024-01-03 07:53:27.697095: val_loss -0.8233\n",
      "2024-01-03 07:53:27.704096: Pseudo dice [0.9108, 0.951, 0.9445]\n",
      "2024-01-03 07:53:27.710096: Epoch time: 125.38 s\n",
      "2024-01-03 07:53:28.852478: \n",
      "2024-01-03 07:53:28.859653: Epoch 765\n",
      "2024-01-03 07:53:28.867652: Current learning rate: 0.00272\n",
      "2024-01-03 07:55:34.752649: train_loss -0.9225\n",
      "2024-01-03 07:55:34.760652: val_loss -0.8279\n",
      "2024-01-03 07:55:34.765652: Pseudo dice [0.9098, 0.9514, 0.9458]\n",
      "2024-01-03 07:55:34.770652: Epoch time: 125.9 s\n",
      "2024-01-03 07:55:36.006597: \n",
      "2024-01-03 07:55:36.013183: Epoch 766\n",
      "2024-01-03 07:55:36.022262: Current learning rate: 0.00271\n",
      "2024-01-03 07:57:41.676971: train_loss -0.9221\n",
      "2024-01-03 07:57:41.686969: val_loss -0.8301\n",
      "2024-01-03 07:57:41.696476: Pseudo dice [0.9107, 0.9527, 0.9436]\n",
      "2024-01-03 07:57:41.705482: Epoch time: 125.67 s\n",
      "2024-01-03 07:57:43.118011: \n",
      "2024-01-03 07:57:43.123010: Epoch 767\n",
      "2024-01-03 07:57:43.128042: Current learning rate: 0.0027\n",
      "2024-01-03 07:59:48.738403: train_loss -0.9212\n",
      "2024-01-03 07:59:48.749405: val_loss -0.8233\n",
      "2024-01-03 07:59:48.758404: Pseudo dice [0.9117, 0.9504, 0.9451]\n",
      "2024-01-03 07:59:48.770402: Epoch time: 125.62 s\n",
      "2024-01-03 07:59:50.111635: \n",
      "2024-01-03 07:59:50.119564: Epoch 768\n",
      "2024-01-03 07:59:50.125597: Current learning rate: 0.00268\n",
      "2024-01-03 08:01:55.962394: train_loss -0.9217\n",
      "2024-01-03 08:01:55.969394: val_loss -0.8203\n",
      "2024-01-03 08:01:55.978393: Pseudo dice [0.9105, 0.9505, 0.9445]\n",
      "2024-01-03 08:01:55.985394: Epoch time: 125.85 s\n",
      "2024-01-03 08:01:57.158400: \n",
      "2024-01-03 08:01:57.164400: Epoch 769\n",
      "2024-01-03 08:01:57.172585: Current learning rate: 0.00267\n",
      "2024-01-03 08:04:02.702585: train_loss -0.9222\n",
      "2024-01-03 08:04:02.710585: val_loss -0.8186\n",
      "2024-01-03 08:04:02.716586: Pseudo dice [0.9099, 0.9503, 0.9458]\n",
      "2024-01-03 08:04:02.721587: Epoch time: 125.54 s\n",
      "2024-01-03 08:04:04.053409: \n",
      "2024-01-03 08:04:04.065915: Epoch 770\n",
      "2024-01-03 08:04:04.071966: Current learning rate: 0.00266\n",
      "2024-01-03 08:06:09.700043: train_loss -0.92\n",
      "2024-01-03 08:06:09.708564: val_loss -0.821\n",
      "2024-01-03 08:06:09.714550: Pseudo dice [0.91, 0.9499, 0.9455]\n",
      "2024-01-03 08:06:09.718553: Epoch time: 125.65 s\n",
      "2024-01-03 08:06:11.216559: \n",
      "2024-01-03 08:06:11.228582: Epoch 771\n",
      "2024-01-03 08:06:11.233504: Current learning rate: 0.00265\n",
      "2024-01-03 08:08:16.921188: train_loss -0.9236\n",
      "2024-01-03 08:08:16.930187: val_loss -0.8248\n",
      "2024-01-03 08:08:16.937187: Pseudo dice [0.9102, 0.952, 0.9464]\n",
      "2024-01-03 08:08:16.943187: Epoch time: 125.71 s\n",
      "2024-01-03 08:08:18.230402: \n",
      "2024-01-03 08:08:18.237400: Epoch 772\n",
      "2024-01-03 08:08:18.242393: Current learning rate: 0.00264\n",
      "2024-01-03 08:10:23.860843: train_loss -0.9191\n",
      "2024-01-03 08:10:23.868347: val_loss -0.8228\n",
      "2024-01-03 08:10:23.875353: Pseudo dice [0.9108, 0.9506, 0.9432]\n",
      "2024-01-03 08:10:23.883352: Epoch time: 125.63 s\n",
      "2024-01-03 08:10:25.203425: \n",
      "2024-01-03 08:10:25.212438: Epoch 773\n",
      "2024-01-03 08:10:25.216414: Current learning rate: 0.00263\n",
      "2024-01-03 08:12:30.723031: train_loss -0.9207\n",
      "2024-01-03 08:12:30.734026: val_loss -0.8238\n",
      "2024-01-03 08:12:30.742027: Pseudo dice [0.9121, 0.9508, 0.9444]\n",
      "2024-01-03 08:12:30.751025: Epoch time: 125.52 s\n",
      "2024-01-03 08:12:32.142316: \n",
      "2024-01-03 08:12:32.147255: Epoch 774\n",
      "2024-01-03 08:12:32.152255: Current learning rate: 0.00262\n",
      "2024-01-03 08:14:37.805015: train_loss -0.9205\n",
      "2024-01-03 08:14:37.814017: val_loss -0.8256\n",
      "2024-01-03 08:14:37.822025: Pseudo dice [0.9121, 0.9513, 0.9463]\n",
      "2024-01-03 08:14:37.828022: Epoch time: 125.66 s\n",
      "2024-01-03 08:14:39.302755: \n",
      "2024-01-03 08:14:39.310756: Epoch 775\n",
      "2024-01-03 08:14:39.316759: Current learning rate: 0.00261\n",
      "2024-01-03 08:16:44.910117: train_loss -0.9207\n",
      "2024-01-03 08:16:44.918109: val_loss -0.8247\n",
      "2024-01-03 08:16:44.927124: Pseudo dice [0.9135, 0.9507, 0.9456]\n",
      "2024-01-03 08:16:44.933126: Epoch time: 125.61 s\n",
      "2024-01-03 08:16:46.154999: \n",
      "2024-01-03 08:16:46.164309: Epoch 776\n",
      "2024-01-03 08:16:46.169262: Current learning rate: 0.0026\n",
      "2024-01-03 08:18:51.807981: train_loss -0.9201\n",
      "2024-01-03 08:18:51.817980: val_loss -0.8277\n",
      "2024-01-03 08:18:51.825983: Pseudo dice [0.9126, 0.9528, 0.9446]\n",
      "2024-01-03 08:18:51.832985: Epoch time: 125.65 s\n",
      "2024-01-03 08:18:53.087822: \n",
      "2024-01-03 08:18:53.096809: Epoch 777\n",
      "2024-01-03 08:18:53.101289: Current learning rate: 0.00259\n",
      "2024-01-03 08:20:58.786626: train_loss -0.9216\n",
      "2024-01-03 08:20:58.794636: val_loss -0.8249\n",
      "2024-01-03 08:20:58.800630: Pseudo dice [0.9095, 0.9513, 0.9436]\n",
      "2024-01-03 08:20:58.806630: Epoch time: 125.7 s\n",
      "2024-01-03 08:21:00.102650: \n",
      "2024-01-03 08:21:00.112764: Epoch 778\n",
      "2024-01-03 08:21:00.117748: Current learning rate: 0.00258\n",
      "2024-01-03 08:23:05.589547: train_loss -0.9216\n",
      "2024-01-03 08:23:05.595548: val_loss -0.828\n",
      "2024-01-03 08:23:05.602058: Pseudo dice [0.9115, 0.9525, 0.9449]\n",
      "2024-01-03 08:23:05.606527: Epoch time: 125.49 s\n",
      "2024-01-03 08:23:07.079272: \n",
      "2024-01-03 08:23:07.087339: Epoch 779\n",
      "2024-01-03 08:23:07.092340: Current learning rate: 0.00257\n",
      "2024-01-03 08:25:12.649830: train_loss -0.9225\n",
      "2024-01-03 08:25:12.656836: val_loss -0.8293\n",
      "2024-01-03 08:25:12.662835: Pseudo dice [0.9126, 0.9526, 0.945]\n",
      "2024-01-03 08:25:12.670831: Epoch time: 125.57 s\n",
      "2024-01-03 08:25:13.884153: \n",
      "2024-01-03 08:25:13.890215: Epoch 780\n",
      "2024-01-03 08:25:13.895252: Current learning rate: 0.00256\n",
      "2024-01-03 08:27:19.480654: train_loss -0.923\n",
      "2024-01-03 08:27:19.489655: val_loss -0.8247\n",
      "2024-01-03 08:27:19.495656: Pseudo dice [0.9095, 0.9508, 0.9442]\n",
      "2024-01-03 08:27:19.504658: Epoch time: 125.6 s\n",
      "2024-01-03 08:27:20.893951: \n",
      "2024-01-03 08:27:20.899961: Epoch 781\n",
      "2024-01-03 08:27:20.905960: Current learning rate: 0.00255\n",
      "2024-01-03 08:29:26.333082: train_loss -0.925\n",
      "2024-01-03 08:29:26.340082: val_loss -0.8263\n",
      "2024-01-03 08:29:26.345082: Pseudo dice [0.9117, 0.9511, 0.944]\n",
      "2024-01-03 08:29:26.349082: Epoch time: 125.44 s\n",
      "2024-01-03 08:29:27.581034: \n",
      "2024-01-03 08:29:27.590939: Epoch 782\n",
      "2024-01-03 08:29:27.596097: Current learning rate: 0.00254\n",
      "2024-01-03 08:31:33.111355: train_loss -0.9256\n",
      "2024-01-03 08:31:33.120358: val_loss -0.8255\n",
      "2024-01-03 08:31:33.126357: Pseudo dice [0.9109, 0.9513, 0.9445]\n",
      "2024-01-03 08:31:33.132355: Epoch time: 125.53 s\n",
      "2024-01-03 08:31:34.397518: \n",
      "2024-01-03 08:31:34.407598: Epoch 783\n",
      "2024-01-03 08:31:34.414594: Current learning rate: 0.00253\n",
      "2024-01-03 08:33:40.308004: train_loss -0.9225\n",
      "2024-01-03 08:33:40.318005: val_loss -0.8188\n",
      "2024-01-03 08:33:40.325007: Pseudo dice [0.9106, 0.9494, 0.9432]\n",
      "2024-01-03 08:33:40.331015: Epoch time: 125.91 s\n",
      "2024-01-03 08:33:41.513441: \n",
      "2024-01-03 08:33:41.520523: Epoch 784\n",
      "2024-01-03 08:33:41.525549: Current learning rate: 0.00252\n",
      "2024-01-03 08:35:47.433156: train_loss -0.9192\n",
      "2024-01-03 08:35:47.449160: val_loss -0.8236\n",
      "2024-01-03 08:35:47.457158: Pseudo dice [0.9114, 0.9506, 0.945]\n",
      "2024-01-03 08:35:47.462157: Epoch time: 125.92 s\n",
      "2024-01-03 08:35:48.771628: \n",
      "2024-01-03 08:35:48.780543: Epoch 785\n",
      "2024-01-03 08:35:48.788615: Current learning rate: 0.00251\n",
      "2024-01-03 08:37:54.568296: train_loss -0.918\n",
      "2024-01-03 08:37:54.576805: val_loss -0.8261\n",
      "2024-01-03 08:37:54.585812: Pseudo dice [0.913, 0.9507, 0.9448]\n",
      "2024-01-03 08:37:54.593810: Epoch time: 125.8 s\n",
      "2024-01-03 08:37:55.953042: \n",
      "2024-01-03 08:37:55.960048: Epoch 786\n",
      "2024-01-03 08:37:55.965106: Current learning rate: 0.0025\n",
      "2024-01-03 08:40:01.462222: train_loss -0.9228\n",
      "2024-01-03 08:40:01.470222: val_loss -0.8261\n",
      "2024-01-03 08:40:01.477237: Pseudo dice [0.9128, 0.9521, 0.9447]\n",
      "2024-01-03 08:40:01.482231: Epoch time: 125.51 s\n",
      "2024-01-03 08:40:02.843282: \n",
      "2024-01-03 08:40:02.851281: Epoch 787\n",
      "2024-01-03 08:40:02.856280: Current learning rate: 0.00249\n",
      "2024-01-03 08:42:08.172765: train_loss -0.9229\n",
      "2024-01-03 08:42:08.182761: val_loss -0.8266\n",
      "2024-01-03 08:42:08.192762: Pseudo dice [0.9108, 0.9522, 0.9455]\n",
      "2024-01-03 08:42:08.201761: Epoch time: 125.33 s\n",
      "2024-01-03 08:42:09.375336: \n",
      "2024-01-03 08:42:09.382339: Epoch 788\n",
      "2024-01-03 08:42:09.387336: Current learning rate: 0.00248\n",
      "2024-01-03 08:44:14.849671: train_loss -0.9251\n",
      "2024-01-03 08:44:14.859669: val_loss -0.8186\n",
      "2024-01-03 08:44:14.865670: Pseudo dice [0.9107, 0.9491, 0.9454]\n",
      "2024-01-03 08:44:14.871673: Epoch time: 125.48 s\n",
      "2024-01-03 08:44:16.076030: \n",
      "2024-01-03 08:44:16.083006: Epoch 789\n",
      "2024-01-03 08:44:16.096017: Current learning rate: 0.00247\n",
      "2024-01-03 08:46:21.750775: train_loss -0.921\n",
      "2024-01-03 08:46:21.757775: val_loss -0.8095\n",
      "2024-01-03 08:46:21.763289: Pseudo dice [0.9086, 0.9481, 0.9447]\n",
      "2024-01-03 08:46:21.768289: Epoch time: 125.68 s\n",
      "2024-01-03 08:46:23.084630: \n",
      "2024-01-03 08:46:23.091636: Epoch 790\n",
      "2024-01-03 08:46:23.096635: Current learning rate: 0.00245\n",
      "2024-01-03 08:48:28.739361: train_loss -0.9214\n",
      "2024-01-03 08:48:28.748363: val_loss -0.8291\n",
      "2024-01-03 08:48:28.757369: Pseudo dice [0.9129, 0.9518, 0.9441]\n",
      "2024-01-03 08:48:28.764360: Epoch time: 125.66 s\n",
      "2024-01-03 08:48:30.021904: \n",
      "2024-01-03 08:48:30.034110: Epoch 791\n",
      "2024-01-03 08:48:30.039197: Current learning rate: 0.00244\n",
      "2024-01-03 08:50:35.562463: train_loss -0.9233\n",
      "2024-01-03 08:50:35.569471: val_loss -0.8169\n",
      "2024-01-03 08:50:35.575471: Pseudo dice [0.911, 0.9488, 0.9457]\n",
      "2024-01-03 08:50:35.579474: Epoch time: 125.54 s\n",
      "2024-01-03 08:50:36.975956: \n",
      "2024-01-03 08:50:36.981969: Epoch 792\n",
      "2024-01-03 08:50:36.987603: Current learning rate: 0.00243\n",
      "2024-01-03 08:52:42.476056: train_loss -0.9211\n",
      "2024-01-03 08:52:42.486069: val_loss -0.826\n",
      "2024-01-03 08:52:42.493056: Pseudo dice [0.9119, 0.9512, 0.9439]\n",
      "2024-01-03 08:52:42.499057: Epoch time: 125.5 s\n",
      "2024-01-03 08:52:43.787989: \n",
      "2024-01-03 08:52:43.794421: Epoch 793\n",
      "2024-01-03 08:52:43.802336: Current learning rate: 0.00242\n",
      "2024-01-03 08:54:49.118172: train_loss -0.9244\n",
      "2024-01-03 08:54:49.125679: val_loss -0.8214\n",
      "2024-01-03 08:54:49.131679: Pseudo dice [0.9108, 0.9506, 0.9442]\n",
      "2024-01-03 08:54:49.137680: Epoch time: 125.33 s\n",
      "2024-01-03 08:54:50.380681: \n",
      "2024-01-03 08:54:50.395670: Epoch 794\n",
      "2024-01-03 08:54:50.407674: Current learning rate: 0.00241\n",
      "2024-01-03 08:56:55.948803: train_loss -0.9233\n",
      "2024-01-03 08:56:55.957805: val_loss -0.8311\n",
      "2024-01-03 08:56:55.965803: Pseudo dice [0.9132, 0.9533, 0.9454]\n",
      "2024-01-03 08:56:55.974803: Epoch time: 125.57 s\n",
      "2024-01-03 08:56:57.348003: \n",
      "2024-01-03 08:56:57.355912: Epoch 795\n",
      "2024-01-03 08:56:57.360910: Current learning rate: 0.0024\n",
      "2024-01-03 08:59:03.448992: train_loss -0.9194\n",
      "2024-01-03 08:59:03.458509: val_loss -0.822\n",
      "2024-01-03 08:59:03.467510: Pseudo dice [0.9106, 0.9506, 0.945]\n",
      "2024-01-03 08:59:03.476509: Epoch time: 126.1 s\n",
      "2024-01-03 08:59:04.915056: \n",
      "2024-01-03 08:59:04.926058: Epoch 796\n",
      "2024-01-03 08:59:04.931055: Current learning rate: 0.00239\n",
      "2024-01-03 09:01:10.717990: train_loss -0.9232\n",
      "2024-01-03 09:01:10.729065: val_loss -0.8231\n",
      "2024-01-03 09:01:10.738990: Pseudo dice [0.9107, 0.9504, 0.945]\n",
      "2024-01-03 09:01:10.743989: Epoch time: 125.8 s\n",
      "2024-01-03 09:01:12.119765: \n",
      "2024-01-03 09:01:12.125699: Epoch 797\n",
      "2024-01-03 09:01:12.130969: Current learning rate: 0.00238\n",
      "2024-01-03 09:03:17.810580: train_loss -0.9223\n",
      "2024-01-03 09:03:17.822572: val_loss -0.8286\n",
      "2024-01-03 09:03:17.827573: Pseudo dice [0.913, 0.9526, 0.9458]\n",
      "2024-01-03 09:03:17.834153: Epoch time: 125.69 s\n",
      "2024-01-03 09:03:19.067428: \n",
      "2024-01-03 09:03:19.074413: Epoch 798\n",
      "2024-01-03 09:03:19.080415: Current learning rate: 0.00237\n",
      "2024-01-03 09:05:24.976087: train_loss -0.9222\n",
      "2024-01-03 09:05:24.983077: val_loss -0.8242\n",
      "2024-01-03 09:05:24.992078: Pseudo dice [0.9113, 0.9516, 0.9459]\n",
      "2024-01-03 09:05:24.997077: Epoch time: 125.91 s\n",
      "2024-01-03 09:05:26.205117: \n",
      "2024-01-03 09:05:26.211112: Epoch 799\n",
      "2024-01-03 09:05:26.216112: Current learning rate: 0.00236\n",
      "2024-01-03 09:07:31.811496: train_loss -0.9233\n",
      "2024-01-03 09:07:31.821495: val_loss -0.8254\n",
      "2024-01-03 09:07:31.829495: Pseudo dice [0.911, 0.9513, 0.9458]\n",
      "2024-01-03 09:07:31.836496: Epoch time: 125.61 s\n",
      "2024-01-03 09:07:33.457155: \n",
      "2024-01-03 09:07:33.463145: Epoch 800\n",
      "2024-01-03 09:07:33.469152: Current learning rate: 0.00235\n",
      "2024-01-03 09:09:39.473013: train_loss -0.9218\n",
      "2024-01-03 09:09:39.483023: val_loss -0.8292\n",
      "2024-01-03 09:09:39.490017: Pseudo dice [0.9104, 0.9517, 0.9447]\n",
      "2024-01-03 09:09:39.496047: Epoch time: 126.02 s\n",
      "2024-01-03 09:09:41.157040: \n",
      "2024-01-03 09:09:41.163040: Epoch 801\n",
      "2024-01-03 09:09:41.168032: Current learning rate: 0.00234\n",
      "2024-01-03 09:11:46.592271: train_loss -0.9247\n",
      "2024-01-03 09:11:46.601271: val_loss -0.8312\n",
      "2024-01-03 09:11:46.607778: Pseudo dice [0.9122, 0.9536, 0.9466]\n",
      "2024-01-03 09:11:46.612778: Epoch time: 125.44 s\n",
      "2024-01-03 09:11:47.893925: \n",
      "2024-01-03 09:11:47.900637: Epoch 802\n",
      "2024-01-03 09:11:47.956955: Current learning rate: 0.00233\n",
      "2024-01-03 09:13:53.510049: train_loss -0.9238\n",
      "2024-01-03 09:13:53.517049: val_loss -0.8262\n",
      "2024-01-03 09:13:53.522707: Pseudo dice [0.909, 0.9514, 0.9448]\n",
      "2024-01-03 09:13:53.526721: Epoch time: 125.62 s\n",
      "2024-01-03 09:13:54.737880: \n",
      "2024-01-03 09:13:54.744820: Epoch 803\n",
      "2024-01-03 09:13:54.749903: Current learning rate: 0.00232\n",
      "2024-01-03 09:16:00.592489: train_loss -0.9216\n",
      "2024-01-03 09:16:00.600496: val_loss -0.8225\n",
      "2024-01-03 09:16:00.606495: Pseudo dice [0.9116, 0.9504, 0.9438]\n",
      "2024-01-03 09:16:00.614488: Epoch time: 125.86 s\n",
      "2024-01-03 09:16:01.941753: \n",
      "2024-01-03 09:16:01.948838: Epoch 804\n",
      "2024-01-03 09:16:01.954769: Current learning rate: 0.00231\n",
      "2024-01-03 09:18:07.511789: train_loss -0.9245\n",
      "2024-01-03 09:18:07.519785: val_loss -0.8204\n",
      "2024-01-03 09:18:07.525785: Pseudo dice [0.9123, 0.9506, 0.9447]\n",
      "2024-01-03 09:18:07.531789: Epoch time: 125.57 s\n",
      "2024-01-03 09:18:08.972792: \n",
      "2024-01-03 09:18:08.978731: Epoch 805\n",
      "2024-01-03 09:18:08.983731: Current learning rate: 0.0023\n",
      "2024-01-03 09:20:14.496134: train_loss -0.9219\n",
      "2024-01-03 09:20:14.504127: val_loss -0.8304\n",
      "2024-01-03 09:20:14.513126: Pseudo dice [0.9113, 0.9529, 0.9457]\n",
      "2024-01-03 09:20:14.518127: Epoch time: 125.52 s\n",
      "2024-01-03 09:20:15.656851: \n",
      "2024-01-03 09:20:15.663908: Epoch 806\n",
      "2024-01-03 09:20:15.668911: Current learning rate: 0.00229\n",
      "2024-01-03 09:22:21.558232: train_loss -0.9214\n",
      "2024-01-03 09:22:21.565231: val_loss -0.8312\n",
      "2024-01-03 09:22:21.571230: Pseudo dice [0.912, 0.9526, 0.9433]\n",
      "2024-01-03 09:22:21.576231: Epoch time: 125.9 s\n",
      "2024-01-03 09:22:22.863782: \n",
      "2024-01-03 09:22:22.874305: Epoch 807\n",
      "2024-01-03 09:22:22.880305: Current learning rate: 0.00228\n",
      "2024-01-03 09:24:28.518840: train_loss -0.9226\n",
      "2024-01-03 09:24:28.525839: val_loss -0.8267\n",
      "2024-01-03 09:24:28.530839: Pseudo dice [0.9124, 0.9514, 0.9445]\n",
      "2024-01-03 09:24:28.537841: Epoch time: 125.66 s\n",
      "2024-01-03 09:24:30.059624: \n",
      "2024-01-03 09:24:30.066495: Epoch 808\n",
      "2024-01-03 09:24:30.074567: Current learning rate: 0.00226\n",
      "2024-01-03 09:26:35.720699: train_loss -0.922\n",
      "2024-01-03 09:26:35.727699: val_loss -0.8177\n",
      "2024-01-03 09:26:35.734699: Pseudo dice [0.9111, 0.9497, 0.946]\n",
      "2024-01-03 09:26:35.740699: Epoch time: 125.66 s\n",
      "2024-01-03 09:26:36.972565: \n",
      "2024-01-03 09:26:36.982756: Epoch 809\n",
      "2024-01-03 09:26:36.988745: Current learning rate: 0.00225\n",
      "2024-01-03 09:28:42.733885: train_loss -0.9222\n",
      "2024-01-03 09:28:42.740881: val_loss -0.823\n",
      "2024-01-03 09:28:42.747887: Pseudo dice [0.9103, 0.9502, 0.9445]\n",
      "2024-01-03 09:28:42.752889: Epoch time: 125.76 s\n",
      "2024-01-03 09:28:43.896563: \n",
      "2024-01-03 09:28:43.910545: Epoch 810\n",
      "2024-01-03 09:28:43.916605: Current learning rate: 0.00224\n",
      "2024-01-03 09:30:49.592704: train_loss -0.9202\n",
      "2024-01-03 09:30:49.604214: val_loss -0.8315\n",
      "2024-01-03 09:30:49.612214: Pseudo dice [0.9123, 0.9522, 0.946]\n",
      "2024-01-03 09:30:49.618213: Epoch time: 125.7 s\n",
      "2024-01-03 09:30:51.096105: \n",
      "2024-01-03 09:30:51.103163: Epoch 811\n",
      "2024-01-03 09:30:51.108706: Current learning rate: 0.00223\n",
      "2024-01-03 09:32:56.810730: train_loss -0.9207\n",
      "2024-01-03 09:32:56.817731: val_loss -0.8221\n",
      "2024-01-03 09:32:56.826741: Pseudo dice [0.909, 0.9512, 0.9446]\n",
      "2024-01-03 09:32:56.836730: Epoch time: 125.72 s\n",
      "2024-01-03 09:32:58.140742: \n",
      "2024-01-03 09:32:58.149184: Epoch 812\n",
      "2024-01-03 09:32:58.159707: Current learning rate: 0.00222\n",
      "2024-01-03 09:35:03.637244: train_loss -0.9238\n",
      "2024-01-03 09:35:03.648263: val_loss -0.8231\n",
      "2024-01-03 09:35:03.657262: Pseudo dice [0.9109, 0.9512, 0.9441]\n",
      "2024-01-03 09:35:03.666262: Epoch time: 125.5 s\n",
      "2024-01-03 09:35:05.016160: \n",
      "2024-01-03 09:35:05.026283: Epoch 813\n",
      "2024-01-03 09:35:05.030711: Current learning rate: 0.00221\n",
      "2024-01-03 09:37:10.963424: train_loss -0.9214\n",
      "2024-01-03 09:37:10.971422: val_loss -0.8224\n",
      "2024-01-03 09:37:10.976422: Pseudo dice [0.9109, 0.9508, 0.9453]\n",
      "2024-01-03 09:37:10.981431: Epoch time: 125.95 s\n",
      "2024-01-03 09:37:12.156187: \n",
      "2024-01-03 09:37:12.163660: Epoch 814\n",
      "2024-01-03 09:37:12.168732: Current learning rate: 0.0022\n",
      "2024-01-03 09:39:18.230459: train_loss -0.9222\n",
      "2024-01-03 09:39:18.240469: val_loss -0.8173\n",
      "2024-01-03 09:39:18.246471: Pseudo dice [0.9121, 0.9499, 0.945]\n",
      "2024-01-03 09:39:18.251477: Epoch time: 126.08 s\n",
      "2024-01-03 09:39:19.688402: \n",
      "2024-01-03 09:39:19.695401: Epoch 815\n",
      "2024-01-03 09:39:19.700393: Current learning rate: 0.00219\n",
      "2024-01-03 09:41:25.356693: train_loss -0.9209\n",
      "2024-01-03 09:41:25.364692: val_loss -0.8182\n",
      "2024-01-03 09:41:25.370699: Pseudo dice [0.91, 0.9498, 0.9453]\n",
      "2024-01-03 09:41:25.376704: Epoch time: 125.67 s\n",
      "2024-01-03 09:41:26.546526: \n",
      "2024-01-03 09:41:26.553422: Epoch 816\n",
      "2024-01-03 09:41:26.558475: Current learning rate: 0.00218\n",
      "2024-01-03 09:43:32.313788: train_loss -0.9232\n",
      "2024-01-03 09:43:32.320788: val_loss -0.8253\n",
      "2024-01-03 09:43:32.326013: Pseudo dice [0.9127, 0.9517, 0.9464]\n",
      "2024-01-03 09:43:32.332005: Epoch time: 125.77 s\n",
      "2024-01-03 09:43:33.525429: \n",
      "2024-01-03 09:43:33.531652: Epoch 817\n",
      "2024-01-03 09:43:33.539917: Current learning rate: 0.00217\n",
      "2024-01-03 09:45:39.545562: train_loss -0.9176\n",
      "2024-01-03 09:45:39.553570: val_loss -0.8162\n",
      "2024-01-03 09:45:39.561571: Pseudo dice [0.9114, 0.9491, 0.9455]\n",
      "2024-01-03 09:45:39.571079: Epoch time: 126.02 s\n",
      "2024-01-03 09:45:40.750186: \n",
      "2024-01-03 09:45:40.759193: Epoch 818\n",
      "2024-01-03 09:45:40.767712: Current learning rate: 0.00216\n",
      "2024-01-03 09:47:46.505918: train_loss -0.9218\n",
      "2024-01-03 09:47:46.515435: val_loss -0.8211\n",
      "2024-01-03 09:47:46.521434: Pseudo dice [0.9112, 0.9514, 0.9453]\n",
      "2024-01-03 09:47:46.527436: Epoch time: 125.76 s\n",
      "2024-01-03 09:47:47.765410: \n",
      "2024-01-03 09:47:47.776595: Epoch 819\n",
      "2024-01-03 09:47:47.785180: Current learning rate: 0.00215\n",
      "2024-01-03 09:49:53.411143: train_loss -0.9206\n",
      "2024-01-03 09:49:53.420142: val_loss -0.8218\n",
      "2024-01-03 09:49:53.428062: Pseudo dice [0.9123, 0.9508, 0.9452]\n",
      "2024-01-03 09:49:53.433061: Epoch time: 125.65 s\n",
      "2024-01-03 09:49:54.586853: \n",
      "2024-01-03 09:49:54.592861: Epoch 820\n",
      "2024-01-03 09:49:54.597854: Current learning rate: 0.00214\n",
      "2024-01-03 09:52:00.268897: train_loss -0.923\n",
      "2024-01-03 09:52:00.275897: val_loss -0.8189\n",
      "2024-01-03 09:52:00.280897: Pseudo dice [0.9096, 0.9512, 0.944]\n",
      "2024-01-03 09:52:00.285897: Epoch time: 125.69 s\n",
      "2024-01-03 09:52:01.460509: \n",
      "2024-01-03 09:52:01.466103: Epoch 821\n",
      "2024-01-03 09:52:01.474102: Current learning rate: 0.00213\n",
      "2024-01-03 09:54:07.192775: train_loss -0.922\n",
      "2024-01-03 09:54:07.200283: val_loss -0.8262\n",
      "2024-01-03 09:54:07.210284: Pseudo dice [0.9128, 0.9514, 0.9448]\n",
      "2024-01-03 09:54:07.217293: Epoch time: 125.73 s\n",
      "2024-01-03 09:54:08.558650: \n",
      "2024-01-03 09:54:08.564641: Epoch 822\n",
      "2024-01-03 09:54:08.569639: Current learning rate: 0.00212\n",
      "2024-01-03 09:56:14.431637: train_loss -0.9223\n",
      "2024-01-03 09:56:14.440636: val_loss -0.8222\n",
      "2024-01-03 09:56:14.447637: Pseudo dice [0.9114, 0.9509, 0.9443]\n",
      "2024-01-03 09:56:14.457637: Epoch time: 125.87 s\n",
      "2024-01-03 09:56:15.734722: \n",
      "2024-01-03 09:56:15.744502: Epoch 823\n",
      "2024-01-03 09:56:15.749523: Current learning rate: 0.0021\n",
      "2024-01-03 09:58:21.239979: train_loss -0.9263\n",
      "2024-01-03 09:58:21.252975: val_loss -0.8228\n",
      "2024-01-03 09:58:21.259480: Pseudo dice [0.9084, 0.951, 0.9437]\n",
      "2024-01-03 09:58:21.265480: Epoch time: 125.51 s\n",
      "2024-01-03 09:58:22.421583: \n",
      "2024-01-03 09:58:22.429125: Epoch 824\n",
      "2024-01-03 09:58:22.438206: Current learning rate: 0.00209\n",
      "2024-01-03 10:00:27.935890: train_loss -0.9236\n",
      "2024-01-03 10:00:27.945404: val_loss -0.8258\n",
      "2024-01-03 10:00:27.955403: Pseudo dice [0.9119, 0.9514, 0.9439]\n",
      "2024-01-03 10:00:27.961403: Epoch time: 125.52 s\n",
      "2024-01-03 10:00:29.185732: \n",
      "2024-01-03 10:00:29.192895: Epoch 825\n",
      "2024-01-03 10:00:29.199900: Current learning rate: 0.00208\n",
      "2024-01-03 10:02:34.679348: train_loss -0.9244\n",
      "2024-01-03 10:02:34.687348: val_loss -0.8227\n",
      "2024-01-03 10:02:34.695359: Pseudo dice [0.9108, 0.9514, 0.9438]\n",
      "2024-01-03 10:02:34.702357: Epoch time: 125.49 s\n",
      "2024-01-03 10:02:35.777686: \n",
      "2024-01-03 10:02:35.787741: Epoch 826\n",
      "2024-01-03 10:02:35.792734: Current learning rate: 0.00207\n",
      "2024-01-03 10:04:41.473271: train_loss -0.9242\n",
      "2024-01-03 10:04:41.483211: val_loss -0.8239\n",
      "2024-01-03 10:04:41.490303: Pseudo dice [0.9115, 0.9507, 0.9456]\n",
      "2024-01-03 10:04:41.496292: Epoch time: 125.7 s\n",
      "2024-01-03 10:04:42.680727: \n",
      "2024-01-03 10:04:42.686721: Epoch 827\n",
      "2024-01-03 10:04:42.692729: Current learning rate: 0.00206\n",
      "2024-01-03 10:06:48.400738: train_loss -0.9223\n",
      "2024-01-03 10:06:48.407723: val_loss -0.8212\n",
      "2024-01-03 10:06:48.415268: Pseudo dice [0.9117, 0.9508, 0.9452]\n",
      "2024-01-03 10:06:48.423270: Epoch time: 125.72 s\n",
      "2024-01-03 10:06:49.587464: \n",
      "2024-01-03 10:06:49.597445: Epoch 828\n",
      "2024-01-03 10:06:49.602534: Current learning rate: 0.00205\n",
      "2024-01-03 10:08:55.277639: train_loss -0.9261\n",
      "2024-01-03 10:08:55.284644: val_loss -0.8181\n",
      "2024-01-03 10:08:55.291645: Pseudo dice [0.9097, 0.9497, 0.9448]\n",
      "2024-01-03 10:08:55.297163: Epoch time: 125.69 s\n",
      "2024-01-03 10:08:56.523709: \n",
      "2024-01-03 10:08:56.528792: Epoch 829\n",
      "2024-01-03 10:08:56.537768: Current learning rate: 0.00204\n",
      "2024-01-03 10:11:02.055686: train_loss -0.9234\n",
      "2024-01-03 10:11:02.063685: val_loss -0.8228\n",
      "2024-01-03 10:11:02.069685: Pseudo dice [0.911, 0.9513, 0.9457]\n",
      "2024-01-03 10:11:02.074684: Epoch time: 125.53 s\n",
      "2024-01-03 10:11:03.272471: \n",
      "2024-01-03 10:11:03.277471: Epoch 830\n",
      "2024-01-03 10:11:03.282471: Current learning rate: 0.00203\n",
      "2024-01-03 10:13:09.162875: train_loss -0.9213\n",
      "2024-01-03 10:13:09.172876: val_loss -0.8154\n",
      "2024-01-03 10:13:09.181875: Pseudo dice [0.91, 0.9501, 0.9452]\n",
      "2024-01-03 10:13:09.189891: Epoch time: 125.89 s\n",
      "2024-01-03 10:13:10.740457: \n",
      "2024-01-03 10:13:10.750162: Epoch 831\n",
      "2024-01-03 10:13:10.754772: Current learning rate: 0.00202\n",
      "2024-01-03 10:15:16.295118: train_loss -0.9242\n",
      "2024-01-03 10:15:16.302114: val_loss -0.823\n",
      "2024-01-03 10:15:16.308115: Pseudo dice [0.9098, 0.9514, 0.9462]\n",
      "2024-01-03 10:15:16.312115: Epoch time: 125.56 s\n",
      "2024-01-03 10:15:17.475747: \n",
      "2024-01-03 10:15:17.483381: Epoch 832\n",
      "2024-01-03 10:15:17.490378: Current learning rate: 0.00201\n",
      "2024-01-03 10:17:23.378579: train_loss -0.923\n",
      "2024-01-03 10:17:23.385580: val_loss -0.8339\n",
      "2024-01-03 10:17:23.392578: Pseudo dice [0.9119, 0.9537, 0.9456]\n",
      "2024-01-03 10:17:23.396579: Epoch time: 125.9 s\n",
      "2024-01-03 10:17:24.577469: \n",
      "2024-01-03 10:17:24.583677: Epoch 833\n",
      "2024-01-03 10:17:24.588725: Current learning rate: 0.002\n",
      "2024-01-03 10:19:30.390317: train_loss -0.9218\n",
      "2024-01-03 10:19:30.398317: val_loss -0.8179\n",
      "2024-01-03 10:19:30.404322: Pseudo dice [0.9104, 0.949, 0.9457]\n",
      "2024-01-03 10:19:30.410321: Epoch time: 125.81 s\n",
      "2024-01-03 10:19:31.663629: \n",
      "2024-01-03 10:19:31.675633: Epoch 834\n",
      "2024-01-03 10:19:31.680627: Current learning rate: 0.00199\n",
      "2024-01-03 10:21:37.562754: train_loss -0.9231\n",
      "2024-01-03 10:21:37.574754: val_loss -0.8217\n",
      "2024-01-03 10:21:37.580756: Pseudo dice [0.9113, 0.9503, 0.9455]\n",
      "2024-01-03 10:21:37.585761: Epoch time: 125.9 s\n",
      "2024-01-03 10:21:38.753431: \n",
      "2024-01-03 10:21:38.758819: Epoch 835\n",
      "2024-01-03 10:21:38.763748: Current learning rate: 0.00198\n",
      "2024-01-03 10:23:44.166713: train_loss -0.9244\n",
      "2024-01-03 10:23:44.173720: val_loss -0.8211\n",
      "2024-01-03 10:23:44.179718: Pseudo dice [0.9098, 0.9517, 0.9451]\n",
      "2024-01-03 10:23:44.184719: Epoch time: 125.41 s\n",
      "2024-01-03 10:23:45.325180: \n",
      "2024-01-03 10:23:45.331180: Epoch 836\n",
      "2024-01-03 10:23:45.338171: Current learning rate: 0.00196\n",
      "2024-01-03 10:25:51.369363: train_loss -0.9171\n",
      "2024-01-03 10:25:51.378363: val_loss -0.8343\n",
      "2024-01-03 10:25:51.387362: Pseudo dice [0.9127, 0.9522, 0.9446]\n",
      "2024-01-03 10:25:51.393363: Epoch time: 126.05 s\n",
      "2024-01-03 10:25:52.775064: \n",
      "2024-01-03 10:25:52.787066: Epoch 837\n",
      "2024-01-03 10:25:52.795141: Current learning rate: 0.00195\n",
      "2024-01-03 10:27:58.306719: train_loss -0.9222\n",
      "2024-01-03 10:27:58.314720: val_loss -0.8213\n",
      "2024-01-03 10:27:58.321719: Pseudo dice [0.9107, 0.9503, 0.9456]\n",
      "2024-01-03 10:27:58.329722: Epoch time: 125.53 s\n",
      "2024-01-03 10:27:59.539732: \n",
      "2024-01-03 10:27:59.545805: Epoch 838\n",
      "2024-01-03 10:27:59.550738: Current learning rate: 0.00194\n",
      "2024-01-03 10:30:05.919657: train_loss -0.9196\n",
      "2024-01-03 10:30:05.929655: val_loss -0.8276\n",
      "2024-01-03 10:30:05.936657: Pseudo dice [0.9098, 0.9518, 0.9435]\n",
      "2024-01-03 10:30:05.941656: Epoch time: 126.38 s\n",
      "2024-01-03 10:30:07.550858: \n",
      "2024-01-03 10:30:07.556857: Epoch 839\n",
      "2024-01-03 10:30:07.561857: Current learning rate: 0.00193\n",
      "2024-01-03 10:32:13.215077: train_loss -0.9236\n",
      "2024-01-03 10:32:13.223082: val_loss -0.824\n",
      "2024-01-03 10:32:13.230092: Pseudo dice [0.9107, 0.9519, 0.945]\n",
      "2024-01-03 10:32:13.237085: Epoch time: 125.67 s\n",
      "2024-01-03 10:32:14.519619: \n",
      "2024-01-03 10:32:14.525544: Epoch 840\n",
      "2024-01-03 10:32:14.531713: Current learning rate: 0.00192\n",
      "2024-01-03 10:34:20.107692: train_loss -0.9232\n",
      "2024-01-03 10:34:20.116194: val_loss -0.8207\n",
      "2024-01-03 10:34:20.123200: Pseudo dice [0.9101, 0.9506, 0.9457]\n",
      "2024-01-03 10:34:20.130200: Epoch time: 125.59 s\n",
      "2024-01-03 10:34:21.322053: \n",
      "2024-01-03 10:34:21.334138: Epoch 841\n",
      "2024-01-03 10:34:21.340051: Current learning rate: 0.00191\n",
      "2024-01-03 10:36:26.784219: train_loss -0.9253\n",
      "2024-01-03 10:36:26.793216: val_loss -0.8259\n",
      "2024-01-03 10:36:26.801216: Pseudo dice [0.9106, 0.9519, 0.9446]\n",
      "2024-01-03 10:36:26.806216: Epoch time: 125.46 s\n",
      "2024-01-03 10:36:27.889420: \n",
      "2024-01-03 10:36:27.898437: Epoch 842\n",
      "2024-01-03 10:36:27.906536: Current learning rate: 0.0019\n",
      "2024-01-03 10:38:33.504879: train_loss -0.9238\n",
      "2024-01-03 10:38:33.511879: val_loss -0.8195\n",
      "2024-01-03 10:38:33.517879: Pseudo dice [0.9111, 0.9504, 0.9445]\n",
      "2024-01-03 10:38:33.522878: Epoch time: 125.62 s\n",
      "2024-01-03 10:38:34.674845: \n",
      "2024-01-03 10:38:34.680928: Epoch 843\n",
      "2024-01-03 10:38:34.685986: Current learning rate: 0.00189\n",
      "2024-01-03 10:40:40.629427: train_loss -0.921\n",
      "2024-01-03 10:40:40.635963: val_loss -0.8206\n",
      "2024-01-03 10:40:40.644961: Pseudo dice [0.9091, 0.9516, 0.9443]\n",
      "2024-01-03 10:40:40.650957: Epoch time: 125.96 s\n",
      "2024-01-03 10:40:41.760735: \n",
      "2024-01-03 10:40:41.766724: Epoch 844\n",
      "2024-01-03 10:40:41.771733: Current learning rate: 0.00188\n",
      "2024-01-03 10:42:47.613476: train_loss -0.9246\n",
      "2024-01-03 10:42:47.623123: val_loss -0.8262\n",
      "2024-01-03 10:42:47.631044: Pseudo dice [0.9116, 0.9525, 0.9448]\n",
      "2024-01-03 10:42:47.636044: Epoch time: 125.85 s\n",
      "2024-01-03 10:42:48.732020: \n",
      "2024-01-03 10:42:48.744124: Epoch 845\n",
      "2024-01-03 10:42:48.753201: Current learning rate: 0.00187\n",
      "2024-01-03 10:44:54.741468: train_loss -0.9204\n",
      "2024-01-03 10:44:54.751470: val_loss -0.8286\n",
      "2024-01-03 10:44:54.759477: Pseudo dice [0.9108, 0.9523, 0.9457]\n",
      "2024-01-03 10:44:54.768477: Epoch time: 126.01 s\n",
      "2024-01-03 10:44:56.082955: \n",
      "2024-01-03 10:44:56.089328: Epoch 846\n",
      "2024-01-03 10:44:56.101450: Current learning rate: 0.00186\n",
      "2024-01-03 10:47:02.039841: train_loss -0.9202\n",
      "2024-01-03 10:47:02.047840: val_loss -0.8254\n",
      "2024-01-03 10:47:02.054840: Pseudo dice [0.9117, 0.9516, 0.9451]\n",
      "2024-01-03 10:47:02.063840: Epoch time: 125.96 s\n",
      "2024-01-03 10:47:03.179453: \n",
      "2024-01-03 10:47:03.188061: Epoch 847\n",
      "2024-01-03 10:47:03.193060: Current learning rate: 0.00185\n",
      "2024-01-03 10:49:08.796323: train_loss -0.9279\n",
      "2024-01-03 10:49:08.803323: val_loss -0.8213\n",
      "2024-01-03 10:49:08.808327: Pseudo dice [0.9115, 0.9502, 0.9446]\n",
      "2024-01-03 10:49:08.813323: Epoch time: 125.62 s\n",
      "2024-01-03 10:49:10.102933: \n",
      "2024-01-03 10:49:10.109938: Epoch 848\n",
      "2024-01-03 10:49:10.114938: Current learning rate: 0.00184\n",
      "2024-01-03 10:51:15.866091: train_loss -0.9235\n",
      "2024-01-03 10:51:15.873092: val_loss -0.8219\n",
      "2024-01-03 10:51:15.879092: Pseudo dice [0.9107, 0.9514, 0.9448]\n",
      "2024-01-03 10:51:15.885092: Epoch time: 125.77 s\n",
      "2024-01-03 10:51:17.124141: \n",
      "2024-01-03 10:51:17.131175: Epoch 849\n",
      "2024-01-03 10:51:17.136189: Current learning rate: 0.00182\n",
      "2024-01-03 10:53:22.758661: train_loss -0.9244\n",
      "2024-01-03 10:53:22.772646: val_loss -0.8291\n",
      "2024-01-03 10:53:22.781718: Pseudo dice [0.9102, 0.952, 0.9462]\n",
      "2024-01-03 10:53:22.789643: Epoch time: 125.64 s\n",
      "2024-01-03 10:53:24.282040: \n",
      "2024-01-03 10:53:24.297045: Epoch 850\n",
      "2024-01-03 10:53:24.302099: Current learning rate: 0.00181\n",
      "2024-01-03 10:55:30.067330: train_loss -0.9255\n",
      "2024-01-03 10:55:30.076835: val_loss -0.8218\n",
      "2024-01-03 10:55:30.083841: Pseudo dice [0.9105, 0.9512, 0.9458]\n",
      "2024-01-03 10:55:30.088844: Epoch time: 125.79 s\n",
      "2024-01-03 10:55:31.181860: \n",
      "2024-01-03 10:55:31.187888: Epoch 851\n",
      "2024-01-03 10:55:31.191972: Current learning rate: 0.0018\n",
      "2024-01-03 10:57:37.232737: train_loss -0.922\n",
      "2024-01-03 10:57:37.244747: val_loss -0.8218\n",
      "2024-01-03 10:57:37.251739: Pseudo dice [0.9117, 0.9506, 0.944]\n",
      "2024-01-03 10:57:37.267864: Epoch time: 126.05 s\n",
      "2024-01-03 10:57:38.479150: \n",
      "2024-01-03 10:57:38.485153: Epoch 852\n",
      "2024-01-03 10:57:38.492212: Current learning rate: 0.00179\n",
      "2024-01-03 10:59:44.177326: train_loss -0.9228\n",
      "2024-01-03 10:59:44.188328: val_loss -0.8217\n",
      "2024-01-03 10:59:44.198327: Pseudo dice [0.9117, 0.9509, 0.9451]\n",
      "2024-01-03 10:59:44.205325: Epoch time: 125.7 s\n",
      "2024-01-03 10:59:45.449917: \n",
      "2024-01-03 10:59:45.463135: Epoch 853\n",
      "2024-01-03 10:59:45.470211: Current learning rate: 0.00178\n",
      "2024-01-03 11:01:51.178896: train_loss -0.9231\n",
      "2024-01-03 11:01:51.188892: val_loss -0.8198\n",
      "2024-01-03 11:01:51.198894: Pseudo dice [0.9101, 0.9505, 0.9437]\n",
      "2024-01-03 11:01:51.204892: Epoch time: 125.73 s\n",
      "2024-01-03 11:01:52.409749: \n",
      "2024-01-03 11:01:52.419928: Epoch 854\n",
      "2024-01-03 11:01:52.428552: Current learning rate: 0.00177\n",
      "2024-01-03 11:04:01.163903: train_loss -0.9252\n",
      "2024-01-03 11:04:01.172538: val_loss -0.8183\n",
      "2024-01-03 11:04:01.182539: Pseudo dice [0.9105, 0.9494, 0.9451]\n",
      "2024-01-03 11:04:01.192019: Epoch time: 128.76 s\n",
      "2024-01-03 11:04:02.620388: \n",
      "2024-01-03 11:04:02.626517: Epoch 855\n",
      "2024-01-03 11:04:02.631660: Current learning rate: 0.00176\n",
      "2024-01-03 11:06:08.787754: train_loss -0.9237\n",
      "2024-01-03 11:06:08.796754: val_loss -0.8232\n",
      "2024-01-03 11:06:08.805754: Pseudo dice [0.9097, 0.9512, 0.9449]\n",
      "2024-01-03 11:06:08.814753: Epoch time: 126.17 s\n",
      "2024-01-03 11:06:10.141077: \n",
      "2024-01-03 11:06:10.147442: Epoch 856\n",
      "2024-01-03 11:06:10.152510: Current learning rate: 0.00175\n",
      "2024-01-03 11:08:17.096777: train_loss -0.9248\n",
      "2024-01-03 11:08:17.104785: val_loss -0.8264\n",
      "2024-01-03 11:08:17.112783: Pseudo dice [0.912, 0.9516, 0.9466]\n",
      "2024-01-03 11:08:17.118784: Epoch time: 126.96 s\n",
      "2024-01-03 11:08:18.457108: \n",
      "2024-01-03 11:08:18.466037: Epoch 857\n",
      "2024-01-03 11:08:18.471102: Current learning rate: 0.00174\n",
      "2024-01-03 11:10:24.883576: train_loss -0.922\n",
      "2024-01-03 11:10:24.890568: val_loss -0.8281\n",
      "2024-01-03 11:10:24.897574: Pseudo dice [0.9104, 0.9525, 0.9454]\n",
      "2024-01-03 11:10:24.902570: Epoch time: 126.43 s\n",
      "2024-01-03 11:10:26.152543: \n",
      "2024-01-03 11:10:26.158475: Epoch 858\n",
      "2024-01-03 11:10:26.166826: Current learning rate: 0.00173\n",
      "2024-01-03 11:12:31.780291: train_loss -0.9258\n",
      "2024-01-03 11:12:31.790280: val_loss -0.8285\n",
      "2024-01-03 11:12:31.796281: Pseudo dice [0.912, 0.9522, 0.9433]\n",
      "2024-01-03 11:12:31.801280: Epoch time: 125.63 s\n",
      "2024-01-03 11:12:32.865630: \n",
      "2024-01-03 11:12:32.871650: Epoch 859\n",
      "2024-01-03 11:12:32.876731: Current learning rate: 0.00172\n",
      "2024-01-03 11:14:38.804516: train_loss -0.9242\n",
      "2024-01-03 11:14:38.812515: val_loss -0.8186\n",
      "2024-01-03 11:14:38.819516: Pseudo dice [0.9109, 0.9499, 0.9447]\n",
      "2024-01-03 11:14:38.826516: Epoch time: 125.94 s\n",
      "2024-01-03 11:14:40.096166: \n",
      "2024-01-03 11:14:40.104897: Epoch 860\n",
      "2024-01-03 11:14:40.139128: Current learning rate: 0.0017\n",
      "2024-01-03 11:16:46.131766: train_loss -0.924\n",
      "2024-01-03 11:16:46.138768: val_loss -0.82\n",
      "2024-01-03 11:16:46.145762: Pseudo dice [0.91, 0.9504, 0.9441]\n",
      "2024-01-03 11:16:46.153785: Epoch time: 126.04 s\n",
      "2024-01-03 11:16:47.370901: \n",
      "2024-01-03 11:16:47.376900: Epoch 861\n",
      "2024-01-03 11:16:47.381901: Current learning rate: 0.00169\n",
      "2024-01-03 11:18:53.233972: train_loss -0.9251\n",
      "2024-01-03 11:18:53.240972: val_loss -0.8236\n",
      "2024-01-03 11:18:53.249480: Pseudo dice [0.9122, 0.9514, 0.9441]\n",
      "2024-01-03 11:18:53.255480: Epoch time: 125.86 s\n",
      "2024-01-03 11:18:54.646800: \n",
      "2024-01-03 11:18:54.652263: Epoch 862\n",
      "2024-01-03 11:18:54.656263: Current learning rate: 0.00168\n",
      "2024-01-03 11:21:00.619905: train_loss -0.923\n",
      "2024-01-03 11:21:00.626874: val_loss -0.8259\n",
      "2024-01-03 11:21:00.631904: Pseudo dice [0.9111, 0.9518, 0.9453]\n",
      "2024-01-03 11:21:00.637893: Epoch time: 125.97 s\n",
      "2024-01-03 11:21:01.762729: \n",
      "2024-01-03 11:21:01.768459: Epoch 863\n",
      "2024-01-03 11:21:01.773474: Current learning rate: 0.00167\n",
      "2024-01-03 11:23:07.962782: train_loss -0.9207\n",
      "2024-01-03 11:23:07.970782: val_loss -0.8241\n",
      "2024-01-03 11:23:07.980783: Pseudo dice [0.91, 0.9512, 0.9444]\n",
      "2024-01-03 11:23:07.988784: Epoch time: 126.2 s\n",
      "2024-01-03 11:23:09.224005: \n",
      "2024-01-03 11:23:09.237183: Epoch 864\n",
      "2024-01-03 11:23:09.243121: Current learning rate: 0.00166\n",
      "2024-01-03 11:25:14.748456: train_loss -0.924\n",
      "2024-01-03 11:25:14.757442: val_loss -0.8272\n",
      "2024-01-03 11:25:14.764457: Pseudo dice [0.9134, 0.9523, 0.9457]\n",
      "2024-01-03 11:25:14.772443: Epoch time: 125.53 s\n",
      "2024-01-03 11:25:15.940620: \n",
      "2024-01-03 11:25:15.947639: Epoch 865\n",
      "2024-01-03 11:25:15.951621: Current learning rate: 0.00165\n",
      "2024-01-03 11:27:21.948061: train_loss -0.9249\n",
      "2024-01-03 11:27:21.957051: val_loss -0.8275\n",
      "2024-01-03 11:27:21.966063: Pseudo dice [0.9114, 0.952, 0.945]\n",
      "2024-01-03 11:27:21.972056: Epoch time: 126.01 s\n",
      "2024-01-03 11:27:23.102384: \n",
      "2024-01-03 11:27:23.109381: Epoch 866\n",
      "2024-01-03 11:27:23.117552: Current learning rate: 0.00164\n",
      "2024-01-03 11:29:28.733567: train_loss -0.9223\n",
      "2024-01-03 11:29:28.743567: val_loss -0.8286\n",
      "2024-01-03 11:29:28.753568: Pseudo dice [0.9121, 0.9518, 0.9452]\n",
      "2024-01-03 11:29:28.761574: Epoch time: 125.63 s\n",
      "2024-01-03 11:29:29.873617: \n",
      "2024-01-03 11:29:29.881622: Epoch 867\n",
      "2024-01-03 11:29:29.886677: Current learning rate: 0.00163\n",
      "2024-01-03 11:31:35.807316: train_loss -0.9234\n",
      "2024-01-03 11:31:35.818323: val_loss -0.8257\n",
      "2024-01-03 11:31:35.827321: Pseudo dice [0.9119, 0.9518, 0.944]\n",
      "2024-01-03 11:31:35.834839: Epoch time: 125.93 s\n",
      "2024-01-03 11:31:37.040480: \n",
      "2024-01-03 11:31:37.047409: Epoch 868\n",
      "2024-01-03 11:31:37.058488: Current learning rate: 0.00162\n",
      "2024-01-03 11:33:42.560297: train_loss -0.9235\n",
      "2024-01-03 11:33:42.567305: val_loss -0.8152\n",
      "2024-01-03 11:33:42.575306: Pseudo dice [0.9089, 0.9492, 0.945]\n",
      "2024-01-03 11:33:42.580304: Epoch time: 125.52 s\n",
      "2024-01-03 11:33:43.969047: \n",
      "2024-01-03 11:33:43.976111: Epoch 869\n",
      "2024-01-03 11:33:43.981114: Current learning rate: 0.00161\n",
      "2024-01-03 11:35:49.855069: train_loss -0.9216\n",
      "2024-01-03 11:35:49.866071: val_loss -0.8225\n",
      "2024-01-03 11:35:49.874070: Pseudo dice [0.9112, 0.951, 0.9447]\n",
      "2024-01-03 11:35:49.881070: Epoch time: 125.89 s\n",
      "2024-01-03 11:35:51.165012: \n",
      "2024-01-03 11:35:51.171278: Epoch 870\n",
      "2024-01-03 11:35:51.176300: Current learning rate: 0.00159\n",
      "2024-01-03 11:37:57.074276: train_loss -0.9222\n",
      "2024-01-03 11:37:57.081274: val_loss -0.8218\n",
      "2024-01-03 11:37:57.086273: Pseudo dice [0.9118, 0.9512, 0.9457]\n",
      "2024-01-03 11:37:57.093833: Epoch time: 125.91 s\n",
      "2024-01-03 11:37:58.186011: \n",
      "2024-01-03 11:37:58.200117: Epoch 871\n",
      "2024-01-03 11:37:58.205074: Current learning rate: 0.00158\n",
      "2024-01-03 11:40:03.663304: train_loss -0.9252\n",
      "2024-01-03 11:40:03.674312: val_loss -0.8236\n",
      "2024-01-03 11:40:03.681312: Pseudo dice [0.9102, 0.9519, 0.9451]\n",
      "2024-01-03 11:40:03.689822: Epoch time: 125.48 s\n",
      "2024-01-03 11:40:04.896543: \n",
      "2024-01-03 11:40:04.905559: Epoch 872\n",
      "2024-01-03 11:40:04.913554: Current learning rate: 0.00157\n",
      "2024-01-03 11:42:10.677993: train_loss -0.9244\n",
      "2024-01-03 11:42:10.684993: val_loss -0.8184\n",
      "2024-01-03 11:42:10.689993: Pseudo dice [0.9092, 0.95, 0.9451]\n",
      "2024-01-03 11:42:10.694995: Epoch time: 125.78 s\n",
      "2024-01-03 11:42:11.872725: \n",
      "2024-01-03 11:42:11.879662: Epoch 873\n",
      "2024-01-03 11:42:11.884653: Current learning rate: 0.00156\n",
      "2024-01-03 11:44:17.480481: train_loss -0.924\n",
      "2024-01-03 11:44:17.487481: val_loss -0.8191\n",
      "2024-01-03 11:44:17.493481: Pseudo dice [0.9122, 0.9511, 0.9455]\n",
      "2024-01-03 11:44:17.499480: Epoch time: 125.61 s\n",
      "2024-01-03 11:44:18.596759: \n",
      "2024-01-03 11:44:18.607189: Epoch 874\n",
      "2024-01-03 11:44:18.612263: Current learning rate: 0.00155\n",
      "2024-01-03 11:46:24.354005: train_loss -0.9257\n",
      "2024-01-03 11:46:24.365007: val_loss -0.8211\n",
      "2024-01-03 11:46:24.373519: Pseudo dice [0.9096, 0.951, 0.9446]\n",
      "2024-01-03 11:46:24.381528: Epoch time: 125.76 s\n",
      "2024-01-03 11:46:25.476954: \n",
      "2024-01-03 11:46:25.483860: Epoch 875\n",
      "2024-01-03 11:46:25.487878: Current learning rate: 0.00154\n",
      "2024-01-03 11:48:31.793098: train_loss -0.9217\n",
      "2024-01-03 11:48:31.803099: val_loss -0.8204\n",
      "2024-01-03 11:48:31.809098: Pseudo dice [0.9096, 0.9511, 0.9451]\n",
      "2024-01-03 11:48:31.814098: Epoch time: 126.32 s\n",
      "2024-01-03 11:48:33.010724: \n",
      "2024-01-03 11:48:33.016727: Epoch 876\n",
      "2024-01-03 11:48:33.021742: Current learning rate: 0.00153\n",
      "2024-01-03 11:50:38.642079: train_loss -0.924\n",
      "2024-01-03 11:50:38.650080: val_loss -0.8245\n",
      "2024-01-03 11:50:38.656082: Pseudo dice [0.9102, 0.9519, 0.9449]\n",
      "2024-01-03 11:50:38.662078: Epoch time: 125.63 s\n",
      "2024-01-03 11:50:40.082872: \n",
      "2024-01-03 11:50:40.092154: Epoch 877\n",
      "2024-01-03 11:50:40.100212: Current learning rate: 0.00152\n",
      "2024-01-03 11:52:45.865258: train_loss -0.9251\n",
      "2024-01-03 11:52:45.875260: val_loss -0.8295\n",
      "2024-01-03 11:52:45.881263: Pseudo dice [0.9119, 0.9536, 0.9449]\n",
      "2024-01-03 11:52:45.887261: Epoch time: 125.78 s\n",
      "2024-01-03 11:52:47.115978: \n",
      "2024-01-03 11:52:47.121926: Epoch 878\n",
      "2024-01-03 11:52:47.127442: Current learning rate: 0.00151\n",
      "2024-01-03 11:54:52.990484: train_loss -0.9229\n",
      "2024-01-03 11:54:53.001478: val_loss -0.8153\n",
      "2024-01-03 11:54:53.008482: Pseudo dice [0.9087, 0.9493, 0.9439]\n",
      "2024-01-03 11:54:53.014485: Epoch time: 125.88 s\n",
      "2024-01-03 11:54:54.246860: \n",
      "2024-01-03 11:54:54.253803: Epoch 879\n",
      "2024-01-03 11:54:54.258801: Current learning rate: 0.00149\n",
      "2024-01-03 11:56:59.686940: train_loss -0.926\n",
      "2024-01-03 11:56:59.698948: val_loss -0.8278\n",
      "2024-01-03 11:56:59.705947: Pseudo dice [0.9116, 0.9524, 0.9453]\n",
      "2024-01-03 11:56:59.711945: Epoch time: 125.44 s\n",
      "2024-01-03 11:57:00.806661: \n",
      "2024-01-03 11:57:00.813271: Epoch 880\n",
      "2024-01-03 11:57:00.818279: Current learning rate: 0.00148\n",
      "2024-01-03 11:59:06.949189: train_loss -0.9253\n",
      "2024-01-03 11:59:06.960188: val_loss -0.82\n",
      "2024-01-03 11:59:06.968196: Pseudo dice [0.91, 0.951, 0.9446]\n",
      "2024-01-03 11:59:06.976196: Epoch time: 126.14 s\n",
      "2024-01-03 11:59:08.030621: \n",
      "2024-01-03 11:59:08.039163: Epoch 881\n",
      "2024-01-03 11:59:08.047216: Current learning rate: 0.00147\n",
      "2024-01-03 12:01:13.703151: train_loss -0.9249\n",
      "2024-01-03 12:01:13.712157: val_loss -0.8183\n",
      "2024-01-03 12:01:13.719157: Pseudo dice [0.9087, 0.9506, 0.9446]\n",
      "2024-01-03 12:01:13.725160: Epoch time: 125.67 s\n",
      "2024-01-03 12:01:14.944592: \n",
      "2024-01-03 12:01:14.953533: Epoch 882\n",
      "2024-01-03 12:01:14.959155: Current learning rate: 0.00146\n",
      "2024-01-03 12:03:20.644758: train_loss -0.927\n",
      "2024-01-03 12:03:20.652749: val_loss -0.8159\n",
      "2024-01-03 12:03:20.659751: Pseudo dice [0.9104, 0.9499, 0.9449]\n",
      "2024-01-03 12:03:20.665751: Epoch time: 125.7 s\n",
      "2024-01-03 12:03:21.933463: \n",
      "2024-01-03 12:03:21.946054: Epoch 883\n",
      "2024-01-03 12:03:21.951040: Current learning rate: 0.00145\n",
      "2024-01-03 12:05:27.822777: train_loss -0.9231\n",
      "2024-01-03 12:05:27.832780: val_loss -0.8242\n",
      "2024-01-03 12:05:27.838778: Pseudo dice [0.9114, 0.9519, 0.9446]\n",
      "2024-01-03 12:05:27.846777: Epoch time: 125.89 s\n",
      "2024-01-03 12:05:29.157377: \n",
      "2024-01-03 12:05:29.163410: Epoch 884\n",
      "2024-01-03 12:05:29.168394: Current learning rate: 0.00144\n",
      "2024-01-03 12:07:34.807197: train_loss -0.9244\n",
      "2024-01-03 12:07:34.814200: val_loss -0.8199\n",
      "2024-01-03 12:07:34.820197: Pseudo dice [0.9104, 0.9497, 0.9446]\n",
      "2024-01-03 12:07:34.825197: Epoch time: 125.65 s\n",
      "2024-01-03 12:07:36.232756: \n",
      "2024-01-03 12:07:36.240608: Epoch 885\n",
      "2024-01-03 12:07:36.244786: Current learning rate: 0.00143\n",
      "2024-01-03 12:09:41.893605: train_loss -0.9242\n",
      "2024-01-03 12:09:41.902118: val_loss -0.8187\n",
      "2024-01-03 12:09:41.908123: Pseudo dice [0.9108, 0.95, 0.9447]\n",
      "2024-01-03 12:09:41.914122: Epoch time: 125.66 s\n",
      "2024-01-03 12:09:43.013854: \n",
      "2024-01-03 12:09:43.019852: Epoch 886\n",
      "2024-01-03 12:09:43.027843: Current learning rate: 0.00142\n",
      "2024-01-03 12:11:48.847591: train_loss -0.9228\n",
      "2024-01-03 12:11:48.857582: val_loss -0.819\n",
      "2024-01-03 12:11:48.865591: Pseudo dice [0.9107, 0.9505, 0.945]\n",
      "2024-01-03 12:11:48.874612: Epoch time: 125.84 s\n",
      "2024-01-03 12:11:50.087260: \n",
      "2024-01-03 12:11:50.096801: Epoch 887\n",
      "2024-01-03 12:11:50.101866: Current learning rate: 0.00141\n",
      "2024-01-03 12:13:55.827674: train_loss -0.9244\n",
      "2024-01-03 12:13:55.834676: val_loss -0.8214\n",
      "2024-01-03 12:13:55.840680: Pseudo dice [0.9124, 0.9512, 0.9446]\n",
      "2024-01-03 12:13:55.844681: Epoch time: 125.74 s\n",
      "2024-01-03 12:13:57.086408: \n",
      "2024-01-03 12:13:57.103311: Epoch 888\n",
      "2024-01-03 12:13:57.108314: Current learning rate: 0.00139\n",
      "2024-01-03 12:16:02.854789: train_loss -0.9245\n",
      "2024-01-03 12:16:02.862789: val_loss -0.8231\n",
      "2024-01-03 12:16:02.871782: Pseudo dice [0.9106, 0.9515, 0.9437]\n",
      "2024-01-03 12:16:02.880791: Epoch time: 125.77 s\n",
      "2024-01-03 12:16:03.933797: \n",
      "2024-01-03 12:16:03.939811: Epoch 889\n",
      "2024-01-03 12:16:03.946811: Current learning rate: 0.00138\n",
      "2024-01-03 12:18:09.497894: train_loss -0.9286\n",
      "2024-01-03 12:18:09.507895: val_loss -0.8276\n",
      "2024-01-03 12:18:09.514406: Pseudo dice [0.9111, 0.952, 0.944]\n",
      "2024-01-03 12:18:09.520406: Epoch time: 125.57 s\n",
      "2024-01-03 12:18:10.700692: \n",
      "2024-01-03 12:18:10.706750: Epoch 890\n",
      "2024-01-03 12:18:10.711768: Current learning rate: 0.00137\n",
      "2024-01-03 12:20:16.493578: train_loss -0.9252\n",
      "2024-01-03 12:20:16.500578: val_loss -0.829\n",
      "2024-01-03 12:20:16.505579: Pseudo dice [0.9125, 0.9527, 0.9452]\n",
      "2024-01-03 12:20:16.510589: Epoch time: 125.8 s\n",
      "2024-01-03 12:20:17.637608: \n",
      "2024-01-03 12:20:17.643680: Epoch 891\n",
      "2024-01-03 12:20:17.648609: Current learning rate: 0.00136\n",
      "2024-01-03 12:22:23.399822: train_loss -0.9247\n",
      "2024-01-03 12:22:23.411830: val_loss -0.8197\n",
      "2024-01-03 12:22:23.419844: Pseudo dice [0.9128, 0.9513, 0.9443]\n",
      "2024-01-03 12:22:23.425538: Epoch time: 125.76 s\n",
      "2024-01-03 12:22:24.645232: \n",
      "2024-01-03 12:22:24.652906: Epoch 892\n",
      "2024-01-03 12:22:24.663264: Current learning rate: 0.00135\n",
      "2024-01-03 12:24:30.417737: train_loss -0.9249\n",
      "2024-01-03 12:24:30.427739: val_loss -0.8235\n",
      "2024-01-03 12:24:30.433738: Pseudo dice [0.9114, 0.9516, 0.9455]\n",
      "2024-01-03 12:24:30.439737: Epoch time: 125.77 s\n",
      "2024-01-03 12:24:31.806329: \n",
      "2024-01-03 12:24:31.812320: Epoch 893\n",
      "2024-01-03 12:24:31.816328: Current learning rate: 0.00134\n",
      "2024-01-03 12:26:37.518301: train_loss -0.9256\n",
      "2024-01-03 12:26:37.528303: val_loss -0.8231\n",
      "2024-01-03 12:26:37.538302: Pseudo dice [0.9103, 0.9503, 0.9455]\n",
      "2024-01-03 12:26:37.543302: Epoch time: 125.71 s\n",
      "2024-01-03 12:26:38.704272: \n",
      "2024-01-03 12:26:38.715198: Epoch 894\n",
      "2024-01-03 12:26:38.720212: Current learning rate: 0.00133\n",
      "2024-01-03 12:28:44.157221: train_loss -0.9259\n",
      "2024-01-03 12:28:44.166222: val_loss -0.8262\n",
      "2024-01-03 12:28:44.172221: Pseudo dice [0.9101, 0.9521, 0.9443]\n",
      "2024-01-03 12:28:44.177221: Epoch time: 125.45 s\n",
      "2024-01-03 12:28:45.340947: \n",
      "2024-01-03 12:28:45.349635: Epoch 895\n",
      "2024-01-03 12:28:45.354621: Current learning rate: 0.00132\n",
      "2024-01-03 12:30:50.955598: train_loss -0.9248\n",
      "2024-01-03 12:30:50.962596: val_loss -0.8206\n",
      "2024-01-03 12:30:50.969596: Pseudo dice [0.91, 0.9502, 0.9439]\n",
      "2024-01-03 12:30:50.975597: Epoch time: 125.62 s\n",
      "2024-01-03 12:30:52.100914: \n",
      "2024-01-03 12:30:52.109899: Epoch 896\n",
      "2024-01-03 12:30:52.119000: Current learning rate: 0.0013\n",
      "2024-01-03 12:32:57.612781: train_loss -0.9254\n",
      "2024-01-03 12:32:57.621780: val_loss -0.82\n",
      "2024-01-03 12:32:57.627782: Pseudo dice [0.9097, 0.95, 0.9454]\n",
      "2024-01-03 12:32:57.633781: Epoch time: 125.51 s\n",
      "2024-01-03 12:32:58.765378: \n",
      "2024-01-03 12:32:58.774131: Epoch 897\n",
      "2024-01-03 12:32:58.780197: Current learning rate: 0.00129\n",
      "2024-01-03 12:35:04.183344: train_loss -0.9283\n",
      "2024-01-03 12:35:04.190343: val_loss -0.811\n",
      "2024-01-03 12:35:04.196344: Pseudo dice [0.9087, 0.949, 0.9447]\n",
      "2024-01-03 12:35:04.204346: Epoch time: 125.42 s\n",
      "2024-01-03 12:35:05.322169: \n",
      "2024-01-03 12:35:05.329885: Epoch 898\n",
      "2024-01-03 12:35:05.339549: Current learning rate: 0.00128\n",
      "2024-01-03 12:37:10.963902: train_loss -0.9264\n",
      "2024-01-03 12:37:10.971903: val_loss -0.8201\n",
      "2024-01-03 12:37:10.978899: Pseudo dice [0.9116, 0.951, 0.9447]\n",
      "2024-01-03 12:37:10.983900: Epoch time: 125.64 s\n",
      "2024-01-03 12:37:12.124580: \n",
      "2024-01-03 12:37:12.133646: Epoch 899\n",
      "2024-01-03 12:37:12.139609: Current learning rate: 0.00127\n",
      "2024-01-03 12:39:17.684215: train_loss -0.93\n",
      "2024-01-03 12:39:17.694212: val_loss -0.822\n",
      "2024-01-03 12:39:17.702211: Pseudo dice [0.91, 0.951, 0.9449]\n",
      "2024-01-03 12:39:17.708208: Epoch time: 125.56 s\n",
      "2024-01-03 12:39:19.284513: \n",
      "2024-01-03 12:39:19.290576: Epoch 900\n",
      "2024-01-03 12:39:19.295575: Current learning rate: 0.00126\n",
      "2024-01-03 12:41:24.644084: train_loss -0.9277\n",
      "2024-01-03 12:41:24.652629: val_loss -0.8194\n",
      "2024-01-03 12:41:24.659632: Pseudo dice [0.9103, 0.9505, 0.945]\n",
      "2024-01-03 12:41:24.666674: Epoch time: 125.36 s\n",
      "2024-01-03 12:41:25.925775: \n",
      "2024-01-03 12:41:25.932051: Epoch 901\n",
      "2024-01-03 12:41:25.936643: Current learning rate: 0.00125\n",
      "2024-01-03 12:43:31.551242: train_loss -0.9234\n",
      "2024-01-03 12:43:31.559243: val_loss -0.8204\n",
      "2024-01-03 12:43:31.564243: Pseudo dice [0.9109, 0.9507, 0.9444]\n",
      "2024-01-03 12:43:31.569242: Epoch time: 125.63 s\n",
      "2024-01-03 12:43:32.854810: \n",
      "2024-01-03 12:43:32.866831: Epoch 902\n",
      "2024-01-03 12:43:32.871759: Current learning rate: 0.00124\n",
      "2024-01-03 12:45:38.850601: train_loss -0.9213\n",
      "2024-01-03 12:45:38.858586: val_loss -0.8237\n",
      "2024-01-03 12:45:38.866585: Pseudo dice [0.9107, 0.9525, 0.9455]\n",
      "2024-01-03 12:45:38.872590: Epoch time: 126.0 s\n",
      "2024-01-03 12:45:40.247139: \n",
      "2024-01-03 12:45:40.256837: Epoch 903\n",
      "2024-01-03 12:45:40.261506: Current learning rate: 0.00122\n",
      "2024-01-03 12:47:45.844208: train_loss -0.9257\n",
      "2024-01-03 12:47:45.851212: val_loss -0.8175\n",
      "2024-01-03 12:47:45.857211: Pseudo dice [0.9117, 0.9497, 0.9454]\n",
      "2024-01-03 12:47:45.865721: Epoch time: 125.6 s\n",
      "2024-01-03 12:47:47.082499: \n",
      "2024-01-03 12:47:47.093457: Epoch 904\n",
      "2024-01-03 12:47:47.101442: Current learning rate: 0.00121\n",
      "2024-01-03 12:49:53.002959: train_loss -0.925\n",
      "2024-01-03 12:49:53.011963: val_loss -0.8149\n",
      "2024-01-03 12:49:53.016962: Pseudo dice [0.9113, 0.9498, 0.9444]\n",
      "2024-01-03 12:49:53.021962: Epoch time: 125.92 s\n",
      "2024-01-03 12:49:54.126416: \n",
      "2024-01-03 12:49:54.133281: Epoch 905\n",
      "2024-01-03 12:49:54.138286: Current learning rate: 0.0012\n",
      "2024-01-03 12:52:00.051850: train_loss -0.9229\n",
      "2024-01-03 12:52:00.060851: val_loss -0.8188\n",
      "2024-01-03 12:52:00.066857: Pseudo dice [0.9113, 0.9511, 0.9452]\n",
      "2024-01-03 12:52:00.073857: Epoch time: 125.93 s\n",
      "2024-01-03 12:52:01.142425: \n",
      "2024-01-03 12:52:01.149511: Epoch 906\n",
      "2024-01-03 12:52:01.154427: Current learning rate: 0.00119\n",
      "2024-01-03 12:54:07.041648: train_loss -0.9232\n",
      "2024-01-03 12:54:07.051648: val_loss -0.8184\n",
      "2024-01-03 12:54:07.058652: Pseudo dice [0.9119, 0.9502, 0.9455]\n",
      "2024-01-03 12:54:07.063653: Epoch time: 125.9 s\n",
      "2024-01-03 12:54:08.165606: \n",
      "2024-01-03 12:54:08.171120: Epoch 907\n",
      "2024-01-03 12:54:08.176131: Current learning rate: 0.00118\n",
      "2024-01-03 12:56:14.034670: train_loss -0.9258\n",
      "2024-01-03 12:56:14.041670: val_loss -0.8256\n",
      "2024-01-03 12:56:14.046673: Pseudo dice [0.9129, 0.9513, 0.9449]\n",
      "2024-01-03 12:56:14.051672: Epoch time: 125.87 s\n",
      "2024-01-03 12:56:15.123968: \n",
      "2024-01-03 12:56:15.130038: Epoch 908\n",
      "2024-01-03 12:56:15.136041: Current learning rate: 0.00117\n",
      "2024-01-03 12:58:20.825456: train_loss -0.9231\n",
      "2024-01-03 12:58:20.834462: val_loss -0.8206\n",
      "2024-01-03 12:58:20.842456: Pseudo dice [0.9114, 0.9504, 0.9448]\n",
      "2024-01-03 12:58:20.849455: Epoch time: 125.7 s\n",
      "2024-01-03 12:58:22.098819: \n",
      "2024-01-03 12:58:22.106611: Epoch 909\n",
      "2024-01-03 12:58:22.112689: Current learning rate: 0.00116\n",
      "2024-01-03 13:00:28.141963: train_loss -0.9254\n",
      "2024-01-03 13:00:28.151566: val_loss -0.8169\n",
      "2024-01-03 13:00:28.158567: Pseudo dice [0.9093, 0.9496, 0.9451]\n",
      "2024-01-03 13:00:28.169086: Epoch time: 126.05 s\n",
      "2024-01-03 13:00:29.365560: \n",
      "2024-01-03 13:00:29.375679: Epoch 910\n",
      "2024-01-03 13:00:29.381678: Current learning rate: 0.00115\n",
      "2024-01-03 13:02:35.372564: train_loss -0.9262\n",
      "2024-01-03 13:02:35.380563: val_loss -0.8206\n",
      "2024-01-03 13:02:35.386563: Pseudo dice [0.9121, 0.9507, 0.9453]\n",
      "2024-01-03 13:02:35.392564: Epoch time: 126.01 s\n",
      "2024-01-03 13:02:36.590599: \n",
      "2024-01-03 13:02:36.651850: Epoch 911\n",
      "2024-01-03 13:02:36.656880: Current learning rate: 0.00113\n",
      "2024-01-03 13:04:42.672992: train_loss -0.9226\n",
      "2024-01-03 13:04:42.684508: val_loss -0.822\n",
      "2024-01-03 13:04:42.693505: Pseudo dice [0.9099, 0.9509, 0.9441]\n",
      "2024-01-03 13:04:42.699512: Epoch time: 126.08 s\n",
      "2024-01-03 13:04:43.840968: \n",
      "2024-01-03 13:04:43.847995: Epoch 912\n",
      "2024-01-03 13:04:43.852605: Current learning rate: 0.00112\n",
      "2024-01-03 13:06:49.419724: train_loss -0.9252\n",
      "2024-01-03 13:06:49.426723: val_loss -0.8213\n",
      "2024-01-03 13:06:49.432723: Pseudo dice [0.9115, 0.951, 0.9447]\n",
      "2024-01-03 13:06:49.439723: Epoch time: 125.58 s\n",
      "2024-01-03 13:06:50.675553: \n",
      "2024-01-03 13:06:50.681617: Epoch 913\n",
      "2024-01-03 13:06:50.686616: Current learning rate: 0.00111\n",
      "2024-01-03 13:08:55.926710: train_loss -0.9306\n",
      "2024-01-03 13:08:55.933710: val_loss -0.8234\n",
      "2024-01-03 13:08:55.940710: Pseudo dice [0.9106, 0.9511, 0.9448]\n",
      "2024-01-03 13:08:55.944710: Epoch time: 125.25 s\n",
      "2024-01-03 13:08:57.099934: \n",
      "2024-01-03 13:08:57.106931: Epoch 914\n",
      "2024-01-03 13:08:57.111948: Current learning rate: 0.0011\n",
      "2024-01-03 13:11:03.030809: train_loss -0.9236\n",
      "2024-01-03 13:11:03.039320: val_loss -0.8191\n",
      "2024-01-03 13:11:03.048323: Pseudo dice [0.9097, 0.9501, 0.9436]\n",
      "2024-01-03 13:11:03.057320: Epoch time: 125.93 s\n",
      "2024-01-03 13:11:04.411668: \n",
      "2024-01-03 13:11:04.417715: Epoch 915\n",
      "2024-01-03 13:11:04.422715: Current learning rate: 0.00109\n",
      "2024-01-03 13:13:10.267907: train_loss -0.9252\n",
      "2024-01-03 13:13:10.274909: val_loss -0.8246\n",
      "2024-01-03 13:13:10.281907: Pseudo dice [0.9116, 0.9507, 0.9452]\n",
      "2024-01-03 13:13:10.287906: Epoch time: 125.86 s\n",
      "2024-01-03 13:13:11.628221: \n",
      "2024-01-03 13:13:11.642401: Epoch 916\n",
      "2024-01-03 13:13:11.649486: Current learning rate: 0.00108\n",
      "2024-01-03 13:15:17.463795: train_loss -0.9252\n",
      "2024-01-03 13:15:17.470794: val_loss -0.8185\n",
      "2024-01-03 13:15:17.476794: Pseudo dice [0.9125, 0.9498, 0.9454]\n",
      "2024-01-03 13:15:17.481795: Epoch time: 125.84 s\n",
      "2024-01-03 13:15:18.636937: \n",
      "2024-01-03 13:15:18.642929: Epoch 917\n",
      "2024-01-03 13:15:18.647930: Current learning rate: 0.00106\n",
      "2024-01-03 13:17:24.450252: train_loss -0.926\n",
      "2024-01-03 13:17:24.459759: val_loss -0.8245\n",
      "2024-01-03 13:17:24.468766: Pseudo dice [0.9112, 0.9517, 0.9444]\n",
      "2024-01-03 13:17:24.474766: Epoch time: 125.82 s\n",
      "2024-01-03 13:17:25.890427: \n",
      "2024-01-03 13:17:25.896401: Epoch 918\n",
      "2024-01-03 13:17:25.901428: Current learning rate: 0.00105\n",
      "2024-01-03 13:19:31.374764: train_loss -0.9271\n",
      "2024-01-03 13:19:31.383764: val_loss -0.8212\n",
      "2024-01-03 13:19:31.391780: Pseudo dice [0.9115, 0.9511, 0.9451]\n",
      "2024-01-03 13:19:31.397766: Epoch time: 125.49 s\n",
      "2024-01-03 13:19:32.600835: \n",
      "2024-01-03 13:19:32.606781: Epoch 919\n",
      "2024-01-03 13:19:32.610815: Current learning rate: 0.00104\n",
      "2024-01-03 13:21:38.475597: train_loss -0.9245\n",
      "2024-01-03 13:21:38.485106: val_loss -0.8297\n",
      "2024-01-03 13:21:38.491107: Pseudo dice [0.9115, 0.9523, 0.9444]\n",
      "2024-01-03 13:21:38.495108: Epoch time: 125.88 s\n",
      "2024-01-03 13:21:39.561165: \n",
      "2024-01-03 13:21:39.571326: Epoch 920\n",
      "2024-01-03 13:21:39.580968: Current learning rate: 0.00103\n",
      "2024-01-03 13:23:45.119431: train_loss -0.926\n",
      "2024-01-03 13:23:45.127447: val_loss -0.8158\n",
      "2024-01-03 13:23:45.136962: Pseudo dice [0.9097, 0.95, 0.9446]\n",
      "2024-01-03 13:23:45.142957: Epoch time: 125.56 s\n",
      "2024-01-03 13:23:46.267941: \n",
      "2024-01-03 13:23:46.275888: Epoch 921\n",
      "2024-01-03 13:23:46.280960: Current learning rate: 0.00102\n",
      "2024-01-03 13:25:51.975244: train_loss -0.9261\n",
      "2024-01-03 13:25:51.983254: val_loss -0.8237\n",
      "2024-01-03 13:25:51.991766: Pseudo dice [0.9103, 0.9511, 0.9451]\n",
      "2024-01-03 13:25:51.996768: Epoch time: 125.71 s\n",
      "2024-01-03 13:25:53.177587: \n",
      "2024-01-03 13:25:53.183587: Epoch 922\n",
      "2024-01-03 13:25:53.188117: Current learning rate: 0.00101\n",
      "2024-01-03 13:27:58.702429: train_loss -0.9273\n",
      "2024-01-03 13:27:58.710431: val_loss -0.8226\n",
      "2024-01-03 13:27:58.717437: Pseudo dice [0.91, 0.9518, 0.9454]\n",
      "2024-01-03 13:27:58.726428: Epoch time: 125.53 s\n",
      "2024-01-03 13:27:59.877968: \n",
      "2024-01-03 13:27:59.886977: Epoch 923\n",
      "2024-01-03 13:27:59.891977: Current learning rate: 0.001\n",
      "2024-01-03 13:30:05.583801: train_loss -0.9256\n",
      "2024-01-03 13:30:05.590802: val_loss -0.8182\n",
      "2024-01-03 13:30:05.595802: Pseudo dice [0.9114, 0.9502, 0.9439]\n",
      "2024-01-03 13:30:05.601801: Epoch time: 125.71 s\n",
      "2024-01-03 13:30:06.742437: \n",
      "2024-01-03 13:30:06.752502: Epoch 924\n",
      "2024-01-03 13:30:06.756532: Current learning rate: 0.00098\n",
      "2024-01-03 13:32:12.593733: train_loss -0.9232\n",
      "2024-01-03 13:32:12.600734: val_loss -0.8153\n",
      "2024-01-03 13:32:12.608734: Pseudo dice [0.9093, 0.9497, 0.9453]\n",
      "2024-01-03 13:32:12.616734: Epoch time: 125.85 s\n",
      "2024-01-03 13:32:13.726861: \n",
      "2024-01-03 13:32:13.736164: Epoch 925\n",
      "2024-01-03 13:32:13.741136: Current learning rate: 0.00097\n",
      "2024-01-03 13:34:19.336831: train_loss -0.9257\n",
      "2024-01-03 13:34:19.346370: val_loss -0.8269\n",
      "2024-01-03 13:34:19.353371: Pseudo dice [0.9137, 0.9521, 0.9436]\n",
      "2024-01-03 13:34:19.359370: Epoch time: 125.61 s\n",
      "2024-01-03 13:34:20.719245: \n",
      "2024-01-03 13:34:20.728315: Epoch 926\n",
      "2024-01-03 13:34:20.732382: Current learning rate: 0.00096\n",
      "2024-01-03 13:36:26.315778: train_loss -0.9274\n",
      "2024-01-03 13:36:26.321778: val_loss -0.8138\n",
      "2024-01-03 13:36:26.327780: Pseudo dice [0.909, 0.949, 0.9456]\n",
      "2024-01-03 13:36:26.336780: Epoch time: 125.6 s\n",
      "2024-01-03 13:36:27.495380: \n",
      "2024-01-03 13:36:27.504346: Epoch 927\n",
      "2024-01-03 13:36:27.509303: Current learning rate: 0.00095\n",
      "2024-01-03 13:38:33.003288: train_loss -0.9268\n",
      "2024-01-03 13:38:33.012290: val_loss -0.8262\n",
      "2024-01-03 13:38:33.018287: Pseudo dice [0.9122, 0.952, 0.9451]\n",
      "2024-01-03 13:38:33.025281: Epoch time: 125.51 s\n",
      "2024-01-03 13:38:34.129065: \n",
      "2024-01-03 13:38:34.137566: Epoch 928\n",
      "2024-01-03 13:38:34.145628: Current learning rate: 0.00094\n",
      "2024-01-03 13:40:39.720351: train_loss -0.9258\n",
      "2024-01-03 13:40:39.733355: val_loss -0.8254\n",
      "2024-01-03 13:40:39.738354: Pseudo dice [0.9119, 0.9516, 0.9447]\n",
      "2024-01-03 13:40:39.746864: Epoch time: 125.59 s\n",
      "2024-01-03 13:40:40.895536: \n",
      "2024-01-03 13:40:40.904882: Epoch 929\n",
      "2024-01-03 13:40:40.909943: Current learning rate: 0.00092\n",
      "2024-01-03 13:42:46.659009: train_loss -0.9253\n",
      "2024-01-03 13:42:46.665998: val_loss -0.8182\n",
      "2024-01-03 13:42:46.672997: Pseudo dice [0.9102, 0.9506, 0.9449]\n",
      "2024-01-03 13:42:46.677997: Epoch time: 125.77 s\n",
      "2024-01-03 13:42:47.864328: \n",
      "2024-01-03 13:42:47.872416: Epoch 930\n",
      "2024-01-03 13:42:47.876384: Current learning rate: 0.00091\n",
      "2024-01-03 13:44:53.397886: train_loss -0.9252\n",
      "2024-01-03 13:44:53.406888: val_loss -0.8275\n",
      "2024-01-03 13:44:53.413893: Pseudo dice [0.9105, 0.9524, 0.945]\n",
      "2024-01-03 13:44:53.418893: Epoch time: 125.53 s\n",
      "2024-01-03 13:44:54.527117: \n",
      "2024-01-03 13:44:54.533114: Epoch 931\n",
      "2024-01-03 13:44:54.538114: Current learning rate: 0.0009\n",
      "2024-01-03 13:47:00.585271: train_loss -0.9266\n",
      "2024-01-03 13:47:00.592271: val_loss -0.8221\n",
      "2024-01-03 13:47:00.598271: Pseudo dice [0.9119, 0.9505, 0.9451]\n",
      "2024-01-03 13:47:00.603270: Epoch time: 126.06 s\n",
      "2024-01-03 13:47:01.826662: \n",
      "2024-01-03 13:47:01.834673: Epoch 932\n",
      "2024-01-03 13:47:01.844682: Current learning rate: 0.00089\n",
      "2024-01-03 13:49:07.317211: train_loss -0.9277\n",
      "2024-01-03 13:49:07.327211: val_loss -0.8208\n",
      "2024-01-03 13:49:07.333213: Pseudo dice [0.9119, 0.9505, 0.9451]\n",
      "2024-01-03 13:49:07.340211: Epoch time: 125.49 s\n",
      "2024-01-03 13:49:08.678785: \n",
      "2024-01-03 13:49:08.684389: Epoch 933\n",
      "2024-01-03 13:49:08.689991: Current learning rate: 0.00088\n",
      "2024-01-03 13:51:14.225914: train_loss -0.928\n",
      "2024-01-03 13:51:14.232914: val_loss -0.8257\n",
      "2024-01-03 13:51:14.240940: Pseudo dice [0.911, 0.9523, 0.9448]\n",
      "2024-01-03 13:51:14.245929: Epoch time: 125.55 s\n",
      "2024-01-03 13:51:15.485971: \n",
      "2024-01-03 13:51:15.491508: Epoch 934\n",
      "2024-01-03 13:51:15.496527: Current learning rate: 0.00087\n",
      "2024-01-03 13:53:21.141716: train_loss -0.9294\n",
      "2024-01-03 13:53:21.147716: val_loss -0.8164\n",
      "2024-01-03 13:53:21.154716: Pseudo dice [0.9117, 0.95, 0.9448]\n",
      "2024-01-03 13:53:21.161611: Epoch time: 125.66 s\n",
      "2024-01-03 13:53:22.285090: \n",
      "2024-01-03 13:53:22.291109: Epoch 935\n",
      "2024-01-03 13:53:22.296029: Current learning rate: 0.00085\n",
      "2024-01-03 13:55:28.469545: train_loss -0.9262\n",
      "2024-01-03 13:55:28.479556: val_loss -0.8237\n",
      "2024-01-03 13:55:28.485548: Pseudo dice [0.9118, 0.9519, 0.9447]\n",
      "2024-01-03 13:55:28.490544: Epoch time: 126.19 s\n",
      "2024-01-03 13:55:29.569007: \n",
      "2024-01-03 13:55:29.577627: Epoch 936\n",
      "2024-01-03 13:55:29.586696: Current learning rate: 0.00084\n",
      "2024-01-03 13:57:35.294341: train_loss -0.9273\n",
      "2024-01-03 13:57:35.301338: val_loss -0.8162\n",
      "2024-01-03 13:57:35.308335: Pseudo dice [0.9108, 0.9498, 0.9456]\n",
      "2024-01-03 13:57:35.314336: Epoch time: 125.73 s\n",
      "2024-01-03 13:57:36.475446: \n",
      "2024-01-03 13:57:36.484009: Epoch 937\n",
      "2024-01-03 13:57:36.489626: Current learning rate: 0.00083\n",
      "2024-01-03 13:59:42.104813: train_loss -0.9249\n",
      "2024-01-03 13:59:42.114814: val_loss -0.8292\n",
      "2024-01-03 13:59:42.121814: Pseudo dice [0.9123, 0.9522, 0.9451]\n",
      "2024-01-03 13:59:42.129816: Epoch time: 125.63 s\n",
      "2024-01-03 13:59:43.215628: \n",
      "2024-01-03 13:59:43.221688: Epoch 938\n",
      "2024-01-03 13:59:43.226687: Current learning rate: 0.00082\n",
      "2024-01-03 14:01:48.553299: train_loss -0.9288\n",
      "2024-01-03 14:01:48.559300: val_loss -0.8197\n",
      "2024-01-03 14:01:48.565300: Pseudo dice [0.9118, 0.9507, 0.9441]\n",
      "2024-01-03 14:01:48.570304: Epoch time: 125.34 s\n",
      "2024-01-03 14:01:49.615151: \n",
      "2024-01-03 14:01:49.624149: Epoch 939\n",
      "2024-01-03 14:01:49.628709: Current learning rate: 0.00081\n",
      "2024-01-03 14:03:55.243578: train_loss -0.9285\n",
      "2024-01-03 14:03:55.250589: val_loss -0.8199\n",
      "2024-01-03 14:03:55.257588: Pseudo dice [0.9121, 0.9514, 0.9448]\n",
      "2024-01-03 14:03:55.262580: Epoch time: 125.63 s\n",
      "2024-01-03 14:03:56.396477: \n",
      "2024-01-03 14:03:56.404942: Epoch 940\n",
      "2024-01-03 14:03:56.410894: Current learning rate: 0.00079\n",
      "2024-01-03 14:06:02.594090: train_loss -0.9262\n",
      "2024-01-03 14:06:02.605089: val_loss -0.816\n",
      "2024-01-03 14:06:02.614089: Pseudo dice [0.9101, 0.9502, 0.944]\n",
      "2024-01-03 14:06:02.621089: Epoch time: 126.2 s\n",
      "2024-01-03 14:06:03.989547: \n",
      "2024-01-03 14:06:03.994543: Epoch 941\n",
      "2024-01-03 14:06:03.998622: Current learning rate: 0.00078\n",
      "2024-01-03 14:08:09.584829: train_loss -0.9265\n",
      "2024-01-03 14:08:09.591829: val_loss -0.8232\n",
      "2024-01-03 14:08:09.601828: Pseudo dice [0.9104, 0.9512, 0.9445]\n",
      "2024-01-03 14:08:09.606828: Epoch time: 125.6 s\n",
      "2024-01-03 14:08:10.739872: \n",
      "2024-01-03 14:08:10.745866: Epoch 942\n",
      "2024-01-03 14:08:10.750384: Current learning rate: 0.00077\n",
      "2024-01-03 14:10:16.385866: train_loss -0.9289\n",
      "2024-01-03 14:10:16.395862: val_loss -0.8173\n",
      "2024-01-03 14:10:16.402374: Pseudo dice [0.9109, 0.9503, 0.9445]\n",
      "2024-01-03 14:10:16.408376: Epoch time: 125.65 s\n",
      "2024-01-03 14:10:17.564342: \n",
      "2024-01-03 14:10:17.571004: Epoch 943\n",
      "2024-01-03 14:10:17.575675: Current learning rate: 0.00076\n",
      "2024-01-03 14:12:23.381341: train_loss -0.9268\n",
      "2024-01-03 14:12:23.388338: val_loss -0.8231\n",
      "2024-01-03 14:12:23.393336: Pseudo dice [0.9112, 0.9514, 0.9444]\n",
      "2024-01-03 14:12:23.398341: Epoch time: 125.82 s\n",
      "2024-01-03 14:12:24.570994: \n",
      "2024-01-03 14:12:24.583511: Epoch 944\n",
      "2024-01-03 14:12:24.594557: Current learning rate: 0.00075\n",
      "2024-01-03 14:14:30.043939: train_loss -0.9306\n",
      "2024-01-03 14:14:30.049939: val_loss -0.8202\n",
      "2024-01-03 14:14:30.055939: Pseudo dice [0.9121, 0.9504, 0.945]\n",
      "2024-01-03 14:14:30.060938: Epoch time: 125.48 s\n",
      "2024-01-03 14:14:31.236835: \n",
      "2024-01-03 14:14:31.243752: Epoch 945\n",
      "2024-01-03 14:14:31.247759: Current learning rate: 0.00074\n",
      "2024-01-03 14:16:37.265027: train_loss -0.9245\n",
      "2024-01-03 14:16:37.274028: val_loss -0.8223\n",
      "2024-01-03 14:16:37.279027: Pseudo dice [0.9116, 0.9512, 0.9451]\n",
      "2024-01-03 14:16:37.284027: Epoch time: 126.03 s\n",
      "2024-01-03 14:16:38.387996: \n",
      "2024-01-03 14:16:38.394124: Epoch 946\n",
      "2024-01-03 14:16:38.399123: Current learning rate: 0.00072\n",
      "2024-01-03 14:18:44.308099: train_loss -0.9285\n",
      "2024-01-03 14:18:44.320096: val_loss -0.8245\n",
      "2024-01-03 14:18:44.327107: Pseudo dice [0.9136, 0.9518, 0.9452]\n",
      "2024-01-03 14:18:44.334102: Epoch time: 125.92 s\n",
      "2024-01-03 14:18:45.532533: \n",
      "2024-01-03 14:18:45.538526: Epoch 947\n",
      "2024-01-03 14:18:45.543714: Current learning rate: 0.00071\n",
      "2024-01-03 14:20:51.426952: train_loss -0.9292\n",
      "2024-01-03 14:20:51.433474: val_loss -0.8188\n",
      "2024-01-03 14:20:51.438475: Pseudo dice [0.9101, 0.9506, 0.9454]\n",
      "2024-01-03 14:20:51.445477: Epoch time: 125.9 s\n",
      "2024-01-03 14:20:52.643722: \n",
      "2024-01-03 14:20:52.650661: Epoch 948\n",
      "2024-01-03 14:20:52.655661: Current learning rate: 0.0007\n",
      "2024-01-03 14:22:58.573715: train_loss -0.9254\n",
      "2024-01-03 14:22:58.580716: val_loss -0.8263\n",
      "2024-01-03 14:22:58.585716: Pseudo dice [0.9121, 0.9526, 0.945]\n",
      "2024-01-03 14:22:58.590725: Epoch time: 125.93 s\n",
      "2024-01-03 14:23:00.044616: \n",
      "2024-01-03 14:23:00.050616: Epoch 949\n",
      "2024-01-03 14:23:00.055615: Current learning rate: 0.00069\n",
      "2024-01-03 14:25:05.855857: train_loss -0.9267\n",
      "2024-01-03 14:25:05.863861: val_loss -0.823\n",
      "2024-01-03 14:25:05.869862: Pseudo dice [0.9117, 0.9516, 0.9449]\n",
      "2024-01-03 14:25:05.874859: Epoch time: 125.81 s\n",
      "2024-01-03 14:25:07.457154: \n",
      "2024-01-03 14:25:07.466155: Epoch 950\n",
      "2024-01-03 14:25:07.471229: Current learning rate: 0.00067\n",
      "2024-01-03 14:27:13.399640: train_loss -0.9261\n",
      "2024-01-03 14:27:13.408642: val_loss -0.8164\n",
      "2024-01-03 14:27:13.413644: Pseudo dice [0.9115, 0.9499, 0.9451]\n",
      "2024-01-03 14:27:13.419643: Epoch time: 125.94 s\n",
      "2024-01-03 14:27:14.687177: \n",
      "2024-01-03 14:27:14.693242: Epoch 951\n",
      "2024-01-03 14:27:14.698237: Current learning rate: 0.00066\n",
      "2024-01-03 14:29:21.040732: train_loss -0.9252\n",
      "2024-01-03 14:29:21.048244: val_loss -0.817\n",
      "2024-01-03 14:29:21.054241: Pseudo dice [0.9103, 0.9505, 0.9451]\n",
      "2024-01-03 14:29:21.059242: Epoch time: 126.35 s\n",
      "2024-01-03 14:29:22.377831: \n",
      "2024-01-03 14:29:22.387861: Epoch 952\n",
      "2024-01-03 14:29:22.392887: Current learning rate: 0.00065\n",
      "2024-01-03 14:31:28.182852: train_loss -0.9281\n",
      "2024-01-03 14:31:28.192852: val_loss -0.8224\n",
      "2024-01-03 14:31:28.200863: Pseudo dice [0.912, 0.9513, 0.9453]\n",
      "2024-01-03 14:31:28.206853: Epoch time: 125.81 s\n",
      "2024-01-03 14:31:29.344825: \n",
      "2024-01-03 14:31:29.352122: Epoch 953\n",
      "2024-01-03 14:31:29.357168: Current learning rate: 0.00064\n",
      "2024-01-03 14:33:34.976603: train_loss -0.9271\n",
      "2024-01-03 14:33:34.985110: val_loss -0.8241\n",
      "2024-01-03 14:33:34.991119: Pseudo dice [0.9121, 0.9524, 0.9445]\n",
      "2024-01-03 14:33:34.996115: Epoch time: 125.63 s\n",
      "2024-01-03 14:33:36.128347: \n",
      "2024-01-03 14:33:36.135421: Epoch 954\n",
      "2024-01-03 14:33:36.140504: Current learning rate: 0.00063\n",
      "2024-01-03 14:35:42.320062: train_loss -0.9275\n",
      "2024-01-03 14:35:42.326575: val_loss -0.8152\n",
      "2024-01-03 14:35:42.332577: Pseudo dice [0.912, 0.9496, 0.9453]\n",
      "2024-01-03 14:35:42.339573: Epoch time: 126.19 s\n",
      "2024-01-03 14:35:43.608376: \n",
      "2024-01-03 14:35:43.613375: Epoch 955\n",
      "2024-01-03 14:35:43.618371: Current learning rate: 0.00061\n",
      "2024-01-03 14:37:49.221842: train_loss -0.9266\n",
      "2024-01-03 14:37:49.228845: val_loss -0.8172\n",
      "2024-01-03 14:37:49.234846: Pseudo dice [0.9112, 0.9502, 0.9451]\n",
      "2024-01-03 14:37:49.239842: Epoch time: 125.62 s\n",
      "2024-01-03 14:37:50.276272: \n",
      "2024-01-03 14:37:50.285425: Epoch 956\n",
      "2024-01-03 14:37:50.290430: Current learning rate: 0.0006\n",
      "2024-01-03 14:39:56.394761: train_loss -0.9253\n",
      "2024-01-03 14:39:56.406760: val_loss -0.8228\n",
      "2024-01-03 14:39:56.413753: Pseudo dice [0.9111, 0.9518, 0.945]\n",
      "2024-01-03 14:39:56.419756: Epoch time: 126.12 s\n",
      "2024-01-03 14:39:57.678392: \n",
      "2024-01-03 14:39:57.693655: Epoch 957\n",
      "2024-01-03 14:39:57.701654: Current learning rate: 0.00059\n",
      "2024-01-03 14:42:02.967941: train_loss -0.9294\n",
      "2024-01-03 14:42:02.976940: val_loss -0.8163\n",
      "2024-01-03 14:42:02.983934: Pseudo dice [0.9106, 0.9497, 0.9441]\n",
      "2024-01-03 14:42:02.988948: Epoch time: 125.29 s\n",
      "2024-01-03 14:42:04.128165: \n",
      "2024-01-03 14:42:04.136375: Epoch 958\n",
      "2024-01-03 14:42:04.142491: Current learning rate: 0.00058\n",
      "2024-01-03 14:44:09.614763: train_loss -0.9311\n",
      "2024-01-03 14:44:09.623760: val_loss -0.8144\n",
      "2024-01-03 14:44:09.629754: Pseudo dice [0.9115, 0.9495, 0.9447]\n",
      "2024-01-03 14:44:09.635755: Epoch time: 125.49 s\n",
      "2024-01-03 14:44:10.806047: \n",
      "2024-01-03 14:44:10.811581: Epoch 959\n",
      "2024-01-03 14:44:10.816581: Current learning rate: 0.00056\n",
      "2024-01-03 14:46:16.422976: train_loss -0.9277\n",
      "2024-01-03 14:46:16.430482: val_loss -0.823\n",
      "2024-01-03 14:46:16.439488: Pseudo dice [0.9124, 0.9519, 0.9445]\n",
      "2024-01-03 14:46:16.446480: Epoch time: 125.62 s\n",
      "2024-01-03 14:46:17.690565: \n",
      "2024-01-03 14:46:17.697978: Epoch 960\n",
      "2024-01-03 14:46:17.703006: Current learning rate: 0.00055\n",
      "2024-01-03 14:48:23.254723: train_loss -0.9284\n",
      "2024-01-03 14:48:23.261722: val_loss -0.8195\n",
      "2024-01-03 14:48:23.266723: Pseudo dice [0.9109, 0.9501, 0.9445]\n",
      "2024-01-03 14:48:23.272722: Epoch time: 125.57 s\n",
      "2024-01-03 14:48:24.433468: \n",
      "2024-01-03 14:48:24.440287: Epoch 961\n",
      "2024-01-03 14:48:24.445441: Current learning rate: 0.00054\n",
      "2024-01-03 14:50:30.203803: train_loss -0.9263\n",
      "2024-01-03 14:50:30.214797: val_loss -0.8224\n",
      "2024-01-03 14:50:30.222795: Pseudo dice [0.9117, 0.9515, 0.9453]\n",
      "2024-01-03 14:50:30.229806: Epoch time: 125.77 s\n",
      "2024-01-03 14:50:31.457700: \n",
      "2024-01-03 14:50:31.465884: Epoch 962\n",
      "2024-01-03 14:50:31.471881: Current learning rate: 0.00053\n",
      "2024-01-03 14:52:37.233506: train_loss -0.9262\n",
      "2024-01-03 14:52:37.240510: val_loss -0.8204\n",
      "2024-01-03 14:52:37.247512: Pseudo dice [0.9119, 0.951, 0.9449]\n",
      "2024-01-03 14:52:37.256026: Epoch time: 125.78 s\n",
      "2024-01-03 14:52:38.390481: \n",
      "2024-01-03 14:52:38.397846: Epoch 963\n",
      "2024-01-03 14:52:38.404780: Current learning rate: 0.00051\n",
      "2024-01-03 14:54:43.906772: train_loss -0.927\n",
      "2024-01-03 14:54:43.913812: val_loss -0.8215\n",
      "2024-01-03 14:54:43.919831: Pseudo dice [0.9118, 0.952, 0.9449]\n",
      "2024-01-03 14:54:43.924828: Epoch time: 125.52 s\n",
      "2024-01-03 14:54:45.012899: \n",
      "2024-01-03 14:54:45.018923: Epoch 964\n",
      "2024-01-03 14:54:45.024371: Current learning rate: 0.0005\n",
      "2024-01-03 14:56:50.442693: train_loss -0.9301\n",
      "2024-01-03 14:56:50.452696: val_loss -0.8235\n",
      "2024-01-03 14:56:50.461708: Pseudo dice [0.9115, 0.9516, 0.9449]\n",
      "2024-01-03 14:56:50.466707: Epoch time: 125.43 s\n",
      "2024-01-03 14:56:51.867719: \n",
      "2024-01-03 14:56:51.876536: Epoch 965\n",
      "2024-01-03 14:56:51.882215: Current learning rate: 0.00049\n",
      "2024-01-03 14:58:57.516742: train_loss -0.9319\n",
      "2024-01-03 14:58:57.523742: val_loss -0.822\n",
      "2024-01-03 14:58:57.533252: Pseudo dice [0.9121, 0.9508, 0.9451]\n",
      "2024-01-03 14:58:57.538253: Epoch time: 125.65 s\n",
      "2024-01-03 14:58:58.641955: \n",
      "2024-01-03 14:58:58.649964: Epoch 966\n",
      "2024-01-03 14:58:58.659894: Current learning rate: 0.00048\n",
      "2024-01-03 15:01:04.213520: train_loss -0.9273\n",
      "2024-01-03 15:01:04.222520: val_loss -0.8224\n",
      "2024-01-03 15:01:04.228026: Pseudo dice [0.9116, 0.9509, 0.9456]\n",
      "2024-01-03 15:01:04.234026: Epoch time: 125.57 s\n",
      "2024-01-03 15:01:05.368579: \n",
      "2024-01-03 15:01:05.381931: Epoch 967\n",
      "2024-01-03 15:01:05.386932: Current learning rate: 0.00046\n",
      "2024-01-03 15:03:11.205391: train_loss -0.9256\n",
      "2024-01-03 15:03:11.211895: val_loss -0.822\n",
      "2024-01-03 15:03:11.218406: Pseudo dice [0.9123, 0.9523, 0.9451]\n",
      "2024-01-03 15:03:11.223413: Epoch time: 125.84 s\n",
      "2024-01-03 15:03:12.506196: \n",
      "2024-01-03 15:03:12.512189: Epoch 968\n",
      "2024-01-03 15:03:12.516195: Current learning rate: 0.00045\n",
      "2024-01-03 15:05:17.935673: train_loss -0.9299\n",
      "2024-01-03 15:05:17.943679: val_loss -0.8225\n",
      "2024-01-03 15:05:17.948675: Pseudo dice [0.9125, 0.9512, 0.9449]\n",
      "2024-01-03 15:05:17.953672: Epoch time: 125.43 s\n",
      "2024-01-03 15:05:19.190795: \n",
      "2024-01-03 15:05:19.196777: Epoch 969\n",
      "2024-01-03 15:05:19.201783: Current learning rate: 0.00044\n",
      "2024-01-03 15:07:24.804569: train_loss -0.9278\n",
      "2024-01-03 15:07:24.811569: val_loss -0.8204\n",
      "2024-01-03 15:07:24.817570: Pseudo dice [0.9112, 0.9507, 0.9457]\n",
      "2024-01-03 15:07:24.823569: Epoch time: 125.62 s\n",
      "2024-01-03 15:07:25.988035: \n",
      "2024-01-03 15:07:25.995039: Epoch 970\n",
      "2024-01-03 15:07:26.004048: Current learning rate: 0.00043\n",
      "2024-01-03 15:09:31.485946: train_loss -0.9285\n",
      "2024-01-03 15:09:31.492943: val_loss -0.82\n",
      "2024-01-03 15:09:31.497863: Pseudo dice [0.9117, 0.951, 0.9448]\n",
      "2024-01-03 15:09:31.503862: Epoch time: 125.5 s\n",
      "2024-01-03 15:09:32.742140: \n",
      "2024-01-03 15:09:32.751470: Epoch 971\n",
      "2024-01-03 15:09:32.756489: Current learning rate: 0.00041\n",
      "2024-01-03 15:11:38.459984: train_loss -0.9281\n",
      "2024-01-03 15:11:38.466985: val_loss -0.8204\n",
      "2024-01-03 15:11:38.473985: Pseudo dice [0.9121, 0.9511, 0.9444]\n",
      "2024-01-03 15:11:38.479984: Epoch time: 125.72 s\n",
      "2024-01-03 15:11:39.830197: \n",
      "2024-01-03 15:11:39.838261: Epoch 972\n",
      "2024-01-03 15:11:39.843256: Current learning rate: 0.0004\n",
      "2024-01-03 15:13:45.736754: train_loss -0.9272\n",
      "2024-01-03 15:13:45.746753: val_loss -0.825\n",
      "2024-01-03 15:13:45.755754: Pseudo dice [0.9107, 0.9516, 0.9449]\n",
      "2024-01-03 15:13:45.762757: Epoch time: 125.91 s\n",
      "2024-01-03 15:13:47.198419: \n",
      "2024-01-03 15:13:47.207486: Epoch 973\n",
      "2024-01-03 15:13:47.212426: Current learning rate: 0.00039\n",
      "2024-01-03 15:15:52.931432: train_loss -0.9268\n",
      "2024-01-03 15:15:52.941431: val_loss -0.8197\n",
      "2024-01-03 15:15:52.950440: Pseudo dice [0.912, 0.9504, 0.9451]\n",
      "2024-01-03 15:15:52.958197: Epoch time: 125.73 s\n",
      "2024-01-03 15:15:54.170861: \n",
      "2024-01-03 15:15:54.179167: Epoch 974\n",
      "2024-01-03 15:15:54.187230: Current learning rate: 0.00037\n",
      "2024-01-03 15:17:59.696842: train_loss -0.9316\n",
      "2024-01-03 15:17:59.703851: val_loss -0.8203\n",
      "2024-01-03 15:17:59.708848: Pseudo dice [0.9117, 0.9502, 0.9453]\n",
      "2024-01-03 15:17:59.714843: Epoch time: 125.53 s\n",
      "2024-01-03 15:18:00.939401: \n",
      "2024-01-03 15:18:00.947409: Epoch 975\n",
      "2024-01-03 15:18:00.952401: Current learning rate: 0.00036\n",
      "2024-01-03 15:20:06.516757: train_loss -0.926\n",
      "2024-01-03 15:20:06.523758: val_loss -0.8172\n",
      "2024-01-03 15:20:06.528757: Pseudo dice [0.9117, 0.9501, 0.9453]\n",
      "2024-01-03 15:20:06.533757: Epoch time: 125.58 s\n",
      "2024-01-03 15:20:07.625606: \n",
      "2024-01-03 15:20:07.634618: Epoch 976\n",
      "2024-01-03 15:20:07.638555: Current learning rate: 0.00035\n",
      "2024-01-03 15:22:13.193023: train_loss -0.9268\n",
      "2024-01-03 15:22:13.200023: val_loss -0.8213\n",
      "2024-01-03 15:22:13.208024: Pseudo dice [0.9111, 0.9515, 0.9453]\n",
      "2024-01-03 15:22:13.215029: Epoch time: 125.57 s\n",
      "2024-01-03 15:22:14.335412: \n",
      "2024-01-03 15:22:14.341442: Epoch 977\n",
      "2024-01-03 15:22:14.345528: Current learning rate: 0.00034\n",
      "2024-01-03 15:24:19.949132: train_loss -0.9299\n",
      "2024-01-03 15:24:19.956140: val_loss -0.8195\n",
      "2024-01-03 15:24:19.961132: Pseudo dice [0.9112, 0.9514, 0.9447]\n",
      "2024-01-03 15:24:19.966642: Epoch time: 125.61 s\n",
      "2024-01-03 15:24:21.105163: \n",
      "2024-01-03 15:24:21.111238: Epoch 978\n",
      "2024-01-03 15:24:21.116809: Current learning rate: 0.00032\n",
      "2024-01-03 15:26:26.708331: train_loss -0.9279\n",
      "2024-01-03 15:26:26.715339: val_loss -0.8165\n",
      "2024-01-03 15:26:26.722857: Pseudo dice [0.9108, 0.9506, 0.9447]\n",
      "2024-01-03 15:26:26.727852: Epoch time: 125.6 s\n",
      "2024-01-03 15:26:27.812860: \n",
      "2024-01-03 15:26:27.821853: Epoch 979\n",
      "2024-01-03 15:26:27.825852: Current learning rate: 0.00031\n",
      "2024-01-03 15:28:33.185029: train_loss -0.9307\n",
      "2024-01-03 15:28:33.194030: val_loss -0.8218\n",
      "2024-01-03 15:28:33.201031: Pseudo dice [0.9123, 0.9507, 0.9454]\n",
      "2024-01-03 15:28:33.206031: Epoch time: 125.37 s\n",
      "2024-01-03 15:28:34.659970: \n",
      "2024-01-03 15:28:34.674051: Epoch 980\n",
      "2024-01-03 15:28:34.682689: Current learning rate: 0.0003\n",
      "2024-01-03 15:30:40.433950: train_loss -0.926\n",
      "2024-01-03 15:30:40.440962: val_loss -0.8205\n",
      "2024-01-03 15:30:40.445951: Pseudo dice [0.9109, 0.9514, 0.9453]\n",
      "2024-01-03 15:30:40.453469: Epoch time: 125.77 s\n",
      "2024-01-03 15:30:41.640818: \n",
      "2024-01-03 15:30:41.689750: Epoch 981\n",
      "2024-01-03 15:30:41.693802: Current learning rate: 0.00028\n",
      "2024-01-03 15:32:47.417449: train_loss -0.93\n",
      "2024-01-03 15:32:47.424440: val_loss -0.8216\n",
      "2024-01-03 15:32:47.430448: Pseudo dice [0.9118, 0.9512, 0.9453]\n",
      "2024-01-03 15:32:47.434956: Epoch time: 125.78 s\n",
      "2024-01-03 15:32:48.575775: \n",
      "2024-01-03 15:32:48.584832: Epoch 982\n",
      "2024-01-03 15:32:48.589823: Current learning rate: 0.00027\n",
      "2024-01-03 15:34:54.608896: train_loss -0.9272\n",
      "2024-01-03 15:34:54.623896: val_loss -0.8185\n",
      "2024-01-03 15:34:54.631896: Pseudo dice [0.9114, 0.9507, 0.9442]\n",
      "2024-01-03 15:34:54.636896: Epoch time: 126.03 s\n",
      "2024-01-03 15:34:55.737108: \n",
      "2024-01-03 15:34:55.744193: Epoch 983\n",
      "2024-01-03 15:34:55.752189: Current learning rate: 0.00026\n",
      "2024-01-03 15:37:01.572581: train_loss -0.9235\n",
      "2024-01-03 15:37:01.581096: val_loss -0.817\n",
      "2024-01-03 15:37:01.589095: Pseudo dice [0.9113, 0.9504, 0.945]\n",
      "2024-01-03 15:37:01.596097: Epoch time: 125.84 s\n",
      "2024-01-03 15:37:02.734478: \n",
      "2024-01-03 15:37:02.744231: Epoch 984\n",
      "2024-01-03 15:37:02.749253: Current learning rate: 0.00024\n",
      "2024-01-03 15:39:08.649106: train_loss -0.9281\n",
      "2024-01-03 15:39:08.656106: val_loss -0.8215\n",
      "2024-01-03 15:39:08.662616: Pseudo dice [0.9117, 0.951, 0.945]\n",
      "2024-01-03 15:39:08.668616: Epoch time: 125.92 s\n",
      "2024-01-03 15:39:09.829620: \n",
      "2024-01-03 15:39:09.835119: Epoch 985\n",
      "2024-01-03 15:39:09.843056: Current learning rate: 0.00023\n",
      "2024-01-03 15:41:15.758498: train_loss -0.9273\n",
      "2024-01-03 15:41:15.769276: val_loss -0.8172\n",
      "2024-01-03 15:41:15.778276: Pseudo dice [0.9109, 0.9502, 0.9444]\n",
      "2024-01-03 15:41:15.784279: Epoch time: 125.93 s\n",
      "2024-01-03 15:41:16.944066: \n",
      "2024-01-03 15:41:16.954353: Epoch 986\n",
      "2024-01-03 15:41:16.964351: Current learning rate: 0.00021\n",
      "2024-01-03 15:43:22.350642: train_loss -0.9309\n",
      "2024-01-03 15:43:22.360631: val_loss -0.8159\n",
      "2024-01-03 15:43:22.369730: Pseudo dice [0.9113, 0.9497, 0.9452]\n",
      "2024-01-03 15:43:22.374629: Epoch time: 125.41 s\n",
      "2024-01-03 15:43:23.474789: \n",
      "2024-01-03 15:43:23.481791: Epoch 987\n",
      "2024-01-03 15:43:23.492054: Current learning rate: 0.0002\n",
      "2024-01-03 15:45:29.103009: train_loss -0.9275\n",
      "2024-01-03 15:45:29.114004: val_loss -0.8183\n",
      "2024-01-03 15:45:29.123005: Pseudo dice [0.9118, 0.9509, 0.9443]\n",
      "2024-01-03 15:45:29.128005: Epoch time: 125.63 s\n",
      "2024-01-03 15:45:30.290548: \n",
      "2024-01-03 15:45:30.299547: Epoch 988\n",
      "2024-01-03 15:45:30.304624: Current learning rate: 0.00019\n",
      "2024-01-03 15:47:36.062401: train_loss -0.9313\n",
      "2024-01-03 15:47:36.070402: val_loss -0.8211\n",
      "2024-01-03 15:47:36.077762: Pseudo dice [0.9118, 0.9514, 0.9444]\n",
      "2024-01-03 15:47:36.084764: Epoch time: 125.77 s\n",
      "2024-01-03 15:47:37.210803: \n",
      "2024-01-03 15:47:37.217807: Epoch 989\n",
      "2024-01-03 15:47:37.222277: Current learning rate: 0.00017\n",
      "2024-01-03 15:49:42.977128: train_loss -0.9282\n",
      "2024-01-03 15:49:42.987120: val_loss -0.8185\n",
      "2024-01-03 15:49:42.995120: Pseudo dice [0.9113, 0.9507, 0.9449]\n",
      "2024-01-03 15:49:43.005695: Epoch time: 125.77 s\n",
      "2024-01-03 15:49:44.361181: \n",
      "2024-01-03 15:49:44.367173: Epoch 990\n",
      "2024-01-03 15:49:44.371181: Current learning rate: 0.00016\n",
      "2024-01-03 15:51:50.297940: train_loss -0.9257\n",
      "2024-01-03 15:51:50.304940: val_loss -0.8222\n",
      "2024-01-03 15:51:50.312938: Pseudo dice [0.9116, 0.9513, 0.9448]\n",
      "2024-01-03 15:51:50.319939: Epoch time: 125.94 s\n",
      "2024-01-03 15:51:51.451311: \n",
      "2024-01-03 15:51:51.459403: Epoch 991\n",
      "2024-01-03 15:51:51.464369: Current learning rate: 0.00014\n",
      "2024-01-03 15:53:57.342779: train_loss -0.9283\n",
      "2024-01-03 15:53:57.352287: val_loss -0.8281\n",
      "2024-01-03 15:53:57.360287: Pseudo dice [0.9127, 0.9525, 0.945]\n",
      "2024-01-03 15:53:57.366289: Epoch time: 125.89 s\n",
      "2024-01-03 15:53:58.645738: \n",
      "2024-01-03 15:53:58.654400: Epoch 992\n",
      "2024-01-03 15:53:58.661407: Current learning rate: 0.00013\n",
      "2024-01-03 15:56:04.430130: train_loss -0.9304\n",
      "2024-01-03 15:56:04.437639: val_loss -0.8177\n",
      "2024-01-03 15:56:04.444642: Pseudo dice [0.9113, 0.9507, 0.945]\n",
      "2024-01-03 15:56:04.452640: Epoch time: 125.79 s\n",
      "2024-01-03 15:56:05.924611: \n",
      "2024-01-03 15:56:05.932611: Epoch 993\n",
      "2024-01-03 15:56:05.937125: Current learning rate: 0.00011\n",
      "2024-01-03 15:58:11.828612: train_loss -0.9297\n",
      "2024-01-03 15:58:11.835612: val_loss -0.8215\n",
      "2024-01-03 15:58:11.842612: Pseudo dice [0.9125, 0.9512, 0.9447]\n",
      "2024-01-03 15:58:11.847612: Epoch time: 125.91 s\n",
      "2024-01-03 15:58:12.907638: \n",
      "2024-01-03 15:58:12.913473: Epoch 994\n",
      "2024-01-03 15:58:12.922085: Current learning rate: 0.0001\n",
      "2024-01-03 16:00:18.537578: train_loss -0.929\n",
      "2024-01-03 16:00:18.543579: val_loss -0.8217\n",
      "2024-01-03 16:00:18.548579: Pseudo dice [0.9123, 0.9508, 0.945]\n",
      "2024-01-03 16:00:18.553579: Epoch time: 125.63 s\n",
      "2024-01-03 16:00:19.715298: \n",
      "2024-01-03 16:00:19.722308: Epoch 995\n",
      "2024-01-03 16:00:19.727303: Current learning rate: 8e-05\n",
      "2024-01-03 16:02:25.547218: train_loss -0.929\n",
      "2024-01-03 16:02:25.558218: val_loss -0.8187\n",
      "2024-01-03 16:02:25.564226: Pseudo dice [0.9119, 0.9511, 0.9455]\n",
      "2024-01-03 16:02:25.570226: Epoch time: 125.83 s\n",
      "2024-01-03 16:02:27.032625: \n",
      "2024-01-03 16:02:27.038979: Epoch 996\n",
      "2024-01-03 16:02:27.044033: Current learning rate: 7e-05\n",
      "2024-01-03 16:04:32.628020: train_loss -0.9295\n",
      "2024-01-03 16:04:32.636013: val_loss -0.8162\n",
      "2024-01-03 16:04:32.641015: Pseudo dice [0.9113, 0.9503, 0.9449]\n",
      "2024-01-03 16:04:32.647009: Epoch time: 125.6 s\n",
      "2024-01-03 16:04:33.787926: \n",
      "2024-01-03 16:04:33.796565: Epoch 997\n",
      "2024-01-03 16:04:33.805640: Current learning rate: 5e-05\n",
      "2024-01-03 16:06:39.533637: train_loss -0.9292\n",
      "2024-01-03 16:06:39.541062: val_loss -0.8141\n",
      "2024-01-03 16:06:39.547069: Pseudo dice [0.9114, 0.9498, 0.9451]\n",
      "2024-01-03 16:06:39.553066: Epoch time: 125.75 s\n",
      "2024-01-03 16:06:40.679940: \n",
      "2024-01-03 16:06:40.688873: Epoch 998\n",
      "2024-01-03 16:06:40.693878: Current learning rate: 4e-05\n",
      "2024-01-03 16:08:46.737800: train_loss -0.9262\n",
      "2024-01-03 16:08:46.745800: val_loss -0.8175\n",
      "2024-01-03 16:08:46.751806: Pseudo dice [0.9114, 0.9498, 0.9456]\n",
      "2024-01-03 16:08:46.757809: Epoch time: 126.06 s\n",
      "2024-01-03 16:08:47.912243: \n",
      "2024-01-03 16:08:47.920193: Epoch 999\n",
      "2024-01-03 16:08:47.925212: Current learning rate: 2e-05\n",
      "2024-01-03 16:10:53.428475: train_loss -0.9282\n",
      "2024-01-03 16:10:53.435475: val_loss -0.8165\n",
      "2024-01-03 16:10:53.440476: Pseudo dice [0.9112, 0.95, 0.9451]\n",
      "2024-01-03 16:10:53.446476: Epoch time: 125.52 s\n",
      "2024-01-03 16:10:55.147018: Training done.\n",
      "2024-01-03 16:10:55.198092: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-03 16:10:55.209555: The split file contains 5 splits.\n",
      "2024-01-03 16:10:55.214547: Desired fold for training: 1\n",
      "2024-01-03 16:10:55.218550: This split has 8 training and 2 validation cases.\n",
      "2024-01-03 16:10:55.224549: predicting case_3\n",
      "2024-01-03 16:10:57.919440: predicting case_9\n",
      "2024-01-03 16:11:09.717693: Validation complete\n",
      "2024-01-03 16:11:09.723691: Mean Validation Dice:  0.9380062869520893\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-03 16:11:11.129168: unpacking dataset...\n",
      "2024-01-03 16:11:11.383551: unpacking done...\n",
      "2024-01-03 16:11:11.389562: do_dummy_2d_data_aug: False\n",
      "2024-01-03 16:11:11.393554: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-03 16:11:11.397624: The split file contains 5 splits.\n",
      "2024-01-03 16:11:11.400554: Desired fold for training: 2\n",
      "2024-01-03 16:11:11.404069: This split has 8 training and 2 validation cases.\n",
      "2024-01-03 16:11:11.432242: Unable to plot network architecture:\n",
      "2024-01-03 16:11:11.436869: No module named 'hiddenlayer'\n",
      "2024-01-03 16:11:11.460947: \n",
      "2024-01-03 16:11:11.465594: Epoch 0\n",
      "2024-01-03 16:11:11.469110: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-03 16:13:26.646904: train_loss -0.3969\n",
      "2024-01-03 16:13:26.654905: val_loss -0.6227\n",
      "2024-01-03 16:13:26.661904: Pseudo dice [0.8497, 0.8652, 0.8942]\n",
      "2024-01-03 16:13:26.666903: Epoch time: 135.19 s\n",
      "2024-01-03 16:13:26.670903: Yayy! New best EMA pseudo Dice: 0.8697\n",
      "2024-01-03 16:13:28.151122: \n",
      "2024-01-03 16:13:28.157193: Epoch 1\n",
      "2024-01-03 16:13:28.162161: Current learning rate: 0.00999\n",
      "2024-01-03 16:15:33.194569: train_loss -0.6873\n",
      "2024-01-03 16:15:33.204570: val_loss -0.7053\n",
      "2024-01-03 16:15:33.212570: Pseudo dice [0.8953, 0.8862, 0.9023]\n",
      "2024-01-03 16:15:33.218568: Epoch time: 125.05 s\n",
      "2024-01-03 16:15:33.223569: Yayy! New best EMA pseudo Dice: 0.8722\n",
      "2024-01-03 16:15:34.727035: \n",
      "2024-01-03 16:15:34.736499: Epoch 2\n",
      "2024-01-03 16:15:34.740484: Current learning rate: 0.00998\n",
      "2024-01-03 16:17:39.806555: train_loss -0.7374\n",
      "2024-01-03 16:17:39.816556: val_loss -0.7346\n",
      "2024-01-03 16:17:39.823556: Pseudo dice [0.9076, 0.8963, 0.9083]\n",
      "2024-01-03 16:17:39.828556: Epoch time: 125.08 s\n",
      "2024-01-03 16:17:39.834555: Yayy! New best EMA pseudo Dice: 0.8754\n",
      "2024-01-03 16:17:41.440666: \n",
      "2024-01-03 16:17:41.451936: Epoch 3\n",
      "2024-01-03 16:17:41.460416: Current learning rate: 0.00997\n",
      "2024-01-03 16:19:46.332358: train_loss -0.772\n",
      "2024-01-03 16:19:46.343122: val_loss -0.7412\n",
      "2024-01-03 16:19:46.352123: Pseudo dice [0.8927, 0.8994, 0.9109]\n",
      "2024-01-03 16:19:46.359123: Epoch time: 124.89 s\n",
      "2024-01-03 16:19:46.364122: Yayy! New best EMA pseudo Dice: 0.878\n",
      "2024-01-03 16:19:48.122085: \n",
      "2024-01-03 16:19:48.127616: Epoch 4\n",
      "2024-01-03 16:19:48.131647: Current learning rate: 0.00996\n",
      "2024-01-03 16:21:53.568975: train_loss -0.7889\n",
      "2024-01-03 16:21:53.575886: val_loss -0.7556\n",
      "2024-01-03 16:21:53.584892: Pseudo dice [0.9031, 0.902, 0.9126]\n",
      "2024-01-03 16:21:53.590892: Epoch time: 125.45 s\n",
      "2024-01-03 16:21:53.596891: Yayy! New best EMA pseudo Dice: 0.8807\n",
      "2024-01-03 16:21:55.027591: \n",
      "2024-01-03 16:21:55.033792: Epoch 5\n",
      "2024-01-03 16:21:55.039851: Current learning rate: 0.00995\n",
      "2024-01-03 16:23:59.979755: train_loss -0.8002\n",
      "2024-01-03 16:23:59.986755: val_loss -0.7602\n",
      "2024-01-03 16:23:59.992755: Pseudo dice [0.9018, 0.9059, 0.9171]\n",
      "2024-01-03 16:23:59.997755: Epoch time: 124.95 s\n",
      "2024-01-03 16:24:00.002755: Yayy! New best EMA pseudo Dice: 0.8835\n",
      "2024-01-03 16:24:01.479456: \n",
      "2024-01-03 16:24:01.484678: Epoch 6\n",
      "2024-01-03 16:24:01.489685: Current learning rate: 0.00995\n",
      "2024-01-03 16:26:06.525204: train_loss -0.8067\n",
      "2024-01-03 16:26:06.535711: val_loss -0.7813\n",
      "2024-01-03 16:26:06.543711: Pseudo dice [0.9072, 0.914, 0.9233]\n",
      "2024-01-03 16:26:06.551713: Epoch time: 125.05 s\n",
      "2024-01-03 16:26:06.556713: Yayy! New best EMA pseudo Dice: 0.8866\n",
      "2024-01-03 16:26:08.119683: \n",
      "2024-01-03 16:26:08.125698: Epoch 7\n",
      "2024-01-03 16:26:08.129698: Current learning rate: 0.00994\n",
      "2024-01-03 16:28:13.343683: train_loss -0.8131\n",
      "2024-01-03 16:28:13.351675: val_loss -0.7675\n",
      "2024-01-03 16:28:13.356674: Pseudo dice [0.9059, 0.9084, 0.9179]\n",
      "2024-01-03 16:28:13.360676: Epoch time: 125.23 s\n",
      "2024-01-03 16:28:13.365685: Yayy! New best EMA pseudo Dice: 0.889\n",
      "2024-01-03 16:28:14.896797: \n",
      "2024-01-03 16:28:14.907873: Epoch 8\n",
      "2024-01-03 16:28:14.914862: Current learning rate: 0.00993\n",
      "2024-01-03 16:30:20.366814: train_loss -0.8195\n",
      "2024-01-03 16:30:20.374818: val_loss -0.7815\n",
      "2024-01-03 16:30:20.379819: Pseudo dice [0.9096, 0.9118, 0.9191]\n",
      "2024-01-03 16:30:20.385818: Epoch time: 125.47 s\n",
      "2024-01-03 16:30:20.391818: Yayy! New best EMA pseudo Dice: 0.8915\n",
      "2024-01-03 16:30:21.995721: \n",
      "2024-01-03 16:30:22.002728: Epoch 9\n",
      "2024-01-03 16:30:22.006813: Current learning rate: 0.00992\n",
      "2024-01-03 16:32:27.242109: train_loss -0.8218\n",
      "2024-01-03 16:32:27.252111: val_loss -0.7691\n",
      "2024-01-03 16:32:27.260109: Pseudo dice [0.9109, 0.9085, 0.9182]\n",
      "2024-01-03 16:32:27.268109: Epoch time: 125.25 s\n",
      "2024-01-03 16:32:27.272109: Yayy! New best EMA pseudo Dice: 0.8936\n",
      "2024-01-03 16:32:28.725295: \n",
      "2024-01-03 16:32:28.735357: Epoch 10\n",
      "2024-01-03 16:32:28.740294: Current learning rate: 0.00991\n",
      "2024-01-03 16:34:33.952299: train_loss -0.8248\n",
      "2024-01-03 16:34:33.961299: val_loss -0.7814\n",
      "2024-01-03 16:34:33.968300: Pseudo dice [0.9132, 0.9107, 0.9187]\n",
      "2024-01-03 16:34:33.972299: Epoch time: 125.23 s\n",
      "2024-01-03 16:34:33.977299: Yayy! New best EMA pseudo Dice: 0.8957\n",
      "2024-01-03 16:34:35.692916: \n",
      "2024-01-03 16:34:35.698437: Epoch 11\n",
      "2024-01-03 16:34:35.704438: Current learning rate: 0.0099\n",
      "2024-01-03 16:36:40.936733: train_loss -0.8293\n",
      "2024-01-03 16:36:40.942733: val_loss -0.7842\n",
      "2024-01-03 16:36:40.949737: Pseudo dice [0.9162, 0.9127, 0.9194]\n",
      "2024-01-03 16:36:40.954741: Epoch time: 125.25 s\n",
      "2024-01-03 16:36:40.958737: Yayy! New best EMA pseudo Dice: 0.8977\n",
      "2024-01-03 16:36:42.439784: \n",
      "2024-01-03 16:36:42.445719: Epoch 12\n",
      "2024-01-03 16:36:42.453730: Current learning rate: 0.00989\n",
      "2024-01-03 16:38:47.811699: train_loss -0.8323\n",
      "2024-01-03 16:38:47.817708: val_loss -0.775\n",
      "2024-01-03 16:38:47.824713: Pseudo dice [0.9099, 0.9103, 0.9181]\n",
      "2024-01-03 16:38:47.829712: Epoch time: 125.37 s\n",
      "2024-01-03 16:38:47.836221: Yayy! New best EMA pseudo Dice: 0.8992\n",
      "2024-01-03 16:38:49.405258: \n",
      "2024-01-03 16:38:49.412255: Epoch 13\n",
      "2024-01-03 16:38:49.416236: Current learning rate: 0.00988\n",
      "2024-01-03 16:40:54.482933: train_loss -0.837\n",
      "2024-01-03 16:40:54.491440: val_loss -0.7863\n",
      "2024-01-03 16:40:54.496441: Pseudo dice [0.9181, 0.9135, 0.922]\n",
      "2024-01-03 16:40:54.500441: Epoch time: 125.08 s\n",
      "2024-01-03 16:40:54.508441: Yayy! New best EMA pseudo Dice: 0.9011\n",
      "2024-01-03 16:40:55.994941: \n",
      "2024-01-03 16:40:56.000500: Epoch 14\n",
      "2024-01-03 16:40:56.005516: Current learning rate: 0.00987\n",
      "2024-01-03 16:43:01.254338: train_loss -0.8364\n",
      "2024-01-03 16:43:01.260592: val_loss -0.7897\n",
      "2024-01-03 16:43:01.267593: Pseudo dice [0.9214, 0.9133, 0.9193]\n",
      "2024-01-03 16:43:01.274592: Epoch time: 125.26 s\n",
      "2024-01-03 16:43:01.278592: Yayy! New best EMA pseudo Dice: 0.9028\n",
      "2024-01-03 16:43:02.680979: \n",
      "2024-01-03 16:43:02.689664: Epoch 15\n",
      "2024-01-03 16:43:02.700708: Current learning rate: 0.00986\n",
      "2024-01-03 16:45:08.272942: train_loss -0.8359\n",
      "2024-01-03 16:45:08.279941: val_loss -0.7896\n",
      "2024-01-03 16:45:08.285942: Pseudo dice [0.9146, 0.9167, 0.9239]\n",
      "2024-01-03 16:45:08.290943: Epoch time: 125.59 s\n",
      "2024-01-03 16:45:08.294942: Yayy! New best EMA pseudo Dice: 0.9043\n",
      "2024-01-03 16:45:09.884258: \n",
      "2024-01-03 16:45:09.890220: Epoch 16\n",
      "2024-01-03 16:45:09.894229: Current learning rate: 0.00986\n",
      "2024-01-03 16:47:15.026328: train_loss -0.842\n",
      "2024-01-03 16:47:15.035320: val_loss -0.7913\n",
      "2024-01-03 16:47:15.043335: Pseudo dice [0.9198, 0.9167, 0.9236]\n",
      "2024-01-03 16:47:15.049316: Epoch time: 125.14 s\n",
      "2024-01-03 16:47:15.054316: Yayy! New best EMA pseudo Dice: 0.9059\n",
      "2024-01-03 16:47:16.566914: \n",
      "2024-01-03 16:47:16.572990: Epoch 17\n",
      "2024-01-03 16:47:16.577065: Current learning rate: 0.00985\n",
      "2024-01-03 16:49:22.122358: train_loss -0.8444\n",
      "2024-01-03 16:49:22.132357: val_loss -0.7893\n",
      "2024-01-03 16:49:22.141357: Pseudo dice [0.915, 0.9145, 0.9225]\n",
      "2024-01-03 16:49:22.150360: Epoch time: 125.56 s\n",
      "2024-01-03 16:49:22.156358: Yayy! New best EMA pseudo Dice: 0.907\n",
      "2024-01-03 16:49:23.720804: \n",
      "2024-01-03 16:49:23.726810: Epoch 18\n",
      "2024-01-03 16:49:23.731810: Current learning rate: 0.00984\n",
      "2024-01-03 16:51:29.213595: train_loss -0.8465\n",
      "2024-01-03 16:51:29.220594: val_loss -0.7846\n",
      "2024-01-03 16:51:29.225596: Pseudo dice [0.9159, 0.9135, 0.92]\n",
      "2024-01-03 16:51:29.230599: Epoch time: 125.49 s\n",
      "2024-01-03 16:51:29.235595: Yayy! New best EMA pseudo Dice: 0.908\n",
      "2024-01-03 16:51:30.830229: \n",
      "2024-01-03 16:51:30.835229: Epoch 19\n",
      "2024-01-03 16:51:30.840065: Current learning rate: 0.00983\n",
      "2024-01-03 16:53:35.837637: train_loss -0.8477\n",
      "2024-01-03 16:53:35.843636: val_loss -0.7897\n",
      "2024-01-03 16:53:35.850636: Pseudo dice [0.9204, 0.9153, 0.9223]\n",
      "2024-01-03 16:53:35.854146: Epoch time: 125.01 s\n",
      "2024-01-03 16:53:35.858141: Yayy! New best EMA pseudo Dice: 0.9091\n",
      "2024-01-03 16:53:37.298804: \n",
      "2024-01-03 16:53:37.304878: Epoch 20\n",
      "2024-01-03 16:53:37.309861: Current learning rate: 0.00982\n",
      "2024-01-03 16:55:42.804971: train_loss -0.8462\n",
      "2024-01-03 16:55:42.813975: val_loss -0.7952\n",
      "2024-01-03 16:55:42.819974: Pseudo dice [0.9157, 0.9155, 0.924]\n",
      "2024-01-03 16:55:42.826975: Epoch time: 125.51 s\n",
      "2024-01-03 16:55:42.832482: Yayy! New best EMA pseudo Dice: 0.91\n",
      "2024-01-03 16:55:44.332006: \n",
      "2024-01-03 16:55:44.343763: Epoch 21\n",
      "2024-01-03 16:55:44.349820: Current learning rate: 0.00981\n",
      "2024-01-03 16:57:49.671416: train_loss -0.8468\n",
      "2024-01-03 16:57:49.677416: val_loss -0.7898\n",
      "2024-01-03 16:57:49.682415: Pseudo dice [0.9182, 0.9134, 0.9219]\n",
      "2024-01-03 16:57:49.686414: Epoch time: 125.34 s\n",
      "2024-01-03 16:57:49.690415: Yayy! New best EMA pseudo Dice: 0.9108\n",
      "2024-01-03 16:57:51.089524: \n",
      "2024-01-03 16:57:51.096009: Epoch 22\n",
      "2024-01-03 16:57:51.101088: Current learning rate: 0.0098\n",
      "2024-01-03 16:59:56.432684: train_loss -0.85\n",
      "2024-01-03 16:59:56.439686: val_loss -0.7978\n",
      "2024-01-03 16:59:56.446685: Pseudo dice [0.9222, 0.9183, 0.9255]\n",
      "2024-01-03 16:59:56.453507: Epoch time: 125.34 s\n",
      "2024-01-03 16:59:56.458507: Yayy! New best EMA pseudo Dice: 0.9119\n",
      "2024-01-03 16:59:57.977415: \n",
      "2024-01-03 16:59:57.983622: Epoch 23\n",
      "2024-01-03 16:59:57.991315: Current learning rate: 0.00979\n",
      "2024-01-03 17:02:03.304424: train_loss -0.8499\n",
      "2024-01-03 17:02:03.313425: val_loss -0.798\n",
      "2024-01-03 17:02:03.322428: Pseudo dice [0.9215, 0.9193, 0.9269]\n",
      "2024-01-03 17:02:03.328441: Epoch time: 125.33 s\n",
      "2024-01-03 17:02:03.336432: Yayy! New best EMA pseudo Dice: 0.913\n",
      "2024-01-03 17:02:04.906386: \n",
      "2024-01-03 17:02:04.912216: Epoch 24\n",
      "2024-01-03 17:02:04.916301: Current learning rate: 0.00978\n",
      "2024-01-03 17:04:10.030036: train_loss -0.8545\n",
      "2024-01-03 17:04:10.039571: val_loss -0.791\n",
      "2024-01-03 17:04:10.045637: Pseudo dice [0.9199, 0.9149, 0.9236]\n",
      "2024-01-03 17:04:10.054640: Epoch time: 125.13 s\n",
      "2024-01-03 17:04:10.061641: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2024-01-03 17:04:11.584978: \n",
      "2024-01-03 17:04:11.597662: Epoch 25\n",
      "2024-01-03 17:04:11.604565: Current learning rate: 0.00977\n",
      "2024-01-03 17:06:16.910030: train_loss -0.8539\n",
      "2024-01-03 17:06:16.919032: val_loss -0.8012\n",
      "2024-01-03 17:06:16.926530: Pseudo dice [0.9201, 0.9196, 0.9293]\n",
      "2024-01-03 17:06:16.930535: Epoch time: 125.33 s\n",
      "2024-01-03 17:06:16.935535: Yayy! New best EMA pseudo Dice: 0.9146\n",
      "2024-01-03 17:06:18.660080: \n",
      "2024-01-03 17:06:18.668067: Epoch 26\n",
      "2024-01-03 17:06:18.673062: Current learning rate: 0.00977\n",
      "2024-01-03 17:08:24.174170: train_loss -0.856\n",
      "2024-01-03 17:08:24.185170: val_loss -0.7879\n",
      "2024-01-03 17:08:24.192172: Pseudo dice [0.9169, 0.9129, 0.9207]\n",
      "2024-01-03 17:08:24.200176: Epoch time: 125.52 s\n",
      "2024-01-03 17:08:24.207176: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2024-01-03 17:08:25.723239: \n",
      "2024-01-03 17:08:25.731169: Epoch 27\n",
      "2024-01-03 17:08:25.740257: Current learning rate: 0.00976\n",
      "2024-01-03 17:10:30.801541: train_loss -0.8584\n",
      "2024-01-03 17:10:30.811540: val_loss -0.7955\n",
      "2024-01-03 17:10:30.820548: Pseudo dice [0.9207, 0.9171, 0.9225]\n",
      "2024-01-03 17:10:30.829539: Epoch time: 125.08 s\n",
      "2024-01-03 17:10:30.837540: Yayy! New best EMA pseudo Dice: 0.9153\n",
      "2024-01-03 17:10:32.385751: \n",
      "2024-01-03 17:10:32.391276: Epoch 28\n",
      "2024-01-03 17:10:32.397318: Current learning rate: 0.00975\n",
      "2024-01-03 17:12:37.805353: train_loss -0.8567\n",
      "2024-01-03 17:12:37.812352: val_loss -0.8035\n",
      "2024-01-03 17:12:37.818353: Pseudo dice [0.9186, 0.9199, 0.926]\n",
      "2024-01-03 17:12:37.824357: Epoch time: 125.42 s\n",
      "2024-01-03 17:12:37.831357: Yayy! New best EMA pseudo Dice: 0.9159\n",
      "2024-01-03 17:12:39.201563: \n",
      "2024-01-03 17:12:39.208112: Epoch 29\n",
      "2024-01-03 17:12:39.216123: Current learning rate: 0.00974\n",
      "2024-01-03 17:14:44.443964: train_loss -0.86\n",
      "2024-01-03 17:14:44.450975: val_loss -0.8022\n",
      "2024-01-03 17:14:44.456965: Pseudo dice [0.9227, 0.919, 0.9242]\n",
      "2024-01-03 17:14:44.462973: Epoch time: 125.24 s\n",
      "2024-01-03 17:14:44.468963: Yayy! New best EMA pseudo Dice: 0.9165\n",
      "2024-01-03 17:14:45.921712: \n",
      "2024-01-03 17:14:45.926707: Epoch 30\n",
      "2024-01-03 17:14:45.931797: Current learning rate: 0.00973\n",
      "2024-01-03 17:16:51.448958: train_loss -0.8613\n",
      "2024-01-03 17:16:51.454959: val_loss -0.8051\n",
      "2024-01-03 17:16:51.459959: Pseudo dice [0.9218, 0.921, 0.9273]\n",
      "2024-01-03 17:16:51.464959: Epoch time: 125.53 s\n",
      "2024-01-03 17:16:51.469972: Yayy! New best EMA pseudo Dice: 0.9172\n",
      "2024-01-03 17:16:52.964357: \n",
      "2024-01-03 17:16:52.969448: Epoch 31\n",
      "2024-01-03 17:16:52.977536: Current learning rate: 0.00972\n",
      "2024-01-03 17:18:58.277514: train_loss -0.8609\n",
      "2024-01-03 17:18:58.284516: val_loss -0.8008\n",
      "2024-01-03 17:18:58.291512: Pseudo dice [0.9204, 0.9185, 0.9255]\n",
      "2024-01-03 17:18:58.297513: Epoch time: 125.32 s\n",
      "2024-01-03 17:18:58.301513: Yayy! New best EMA pseudo Dice: 0.9176\n",
      "2024-01-03 17:18:59.922720: \n",
      "2024-01-03 17:18:59.928720: Epoch 32\n",
      "2024-01-03 17:18:59.933665: Current learning rate: 0.00971\n",
      "2024-01-03 17:21:05.136093: train_loss -0.8615\n",
      "2024-01-03 17:21:05.143092: val_loss -0.8005\n",
      "2024-01-03 17:21:05.148092: Pseudo dice [0.9202, 0.9186, 0.9248]\n",
      "2024-01-03 17:21:05.153092: Epoch time: 125.22 s\n",
      "2024-01-03 17:21:05.157092: Yayy! New best EMA pseudo Dice: 0.918\n",
      "2024-01-03 17:21:06.566739: \n",
      "2024-01-03 17:21:06.572383: Epoch 33\n",
      "2024-01-03 17:21:06.576400: Current learning rate: 0.0097\n",
      "2024-01-03 17:23:12.096360: train_loss -0.8625\n",
      "2024-01-03 17:23:12.103269: val_loss -0.8014\n",
      "2024-01-03 17:23:12.108271: Pseudo dice [0.9276, 0.9176, 0.9279]\n",
      "2024-01-03 17:23:12.114271: Epoch time: 125.53 s\n",
      "2024-01-03 17:23:12.121268: Yayy! New best EMA pseudo Dice: 0.9186\n",
      "2024-01-03 17:23:13.784082: \n",
      "2024-01-03 17:23:13.793083: Epoch 34\n",
      "2024-01-03 17:23:13.803399: Current learning rate: 0.00969\n",
      "2024-01-03 17:25:19.059772: train_loss -0.8628\n",
      "2024-01-03 17:25:19.070786: val_loss -0.7951\n",
      "2024-01-03 17:25:19.079786: Pseudo dice [0.9216, 0.916, 0.9257]\n",
      "2024-01-03 17:25:19.085285: Epoch time: 125.28 s\n",
      "2024-01-03 17:25:19.090290: Yayy! New best EMA pseudo Dice: 0.9189\n",
      "2024-01-03 17:25:20.656462: \n",
      "2024-01-03 17:25:20.661393: Epoch 35\n",
      "2024-01-03 17:25:20.668394: Current learning rate: 0.00968\n",
      "2024-01-03 17:27:25.984147: train_loss -0.8641\n",
      "2024-01-03 17:27:25.990147: val_loss -0.8041\n",
      "2024-01-03 17:27:25.996146: Pseudo dice [0.9217, 0.9221, 0.9294]\n",
      "2024-01-03 17:27:26.003150: Epoch time: 125.33 s\n",
      "2024-01-03 17:27:26.007146: Yayy! New best EMA pseudo Dice: 0.9194\n",
      "2024-01-03 17:27:27.511484: \n",
      "2024-01-03 17:27:27.517430: Epoch 36\n",
      "2024-01-03 17:27:27.521429: Current learning rate: 0.00968\n",
      "2024-01-03 17:29:33.072137: train_loss -0.8633\n",
      "2024-01-03 17:29:33.079143: val_loss -0.8042\n",
      "2024-01-03 17:29:33.086147: Pseudo dice [0.9215, 0.9199, 0.9276]\n",
      "2024-01-03 17:29:33.091164: Epoch time: 125.56 s\n",
      "2024-01-03 17:29:33.095163: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2024-01-03 17:29:34.590022: \n",
      "2024-01-03 17:29:34.596024: Epoch 37\n",
      "2024-01-03 17:29:34.600090: Current learning rate: 0.00967\n",
      "2024-01-03 17:31:39.744144: train_loss -0.8691\n",
      "2024-01-03 17:31:39.750142: val_loss -0.8019\n",
      "2024-01-03 17:31:39.757141: Pseudo dice [0.9198, 0.9205, 0.9289]\n",
      "2024-01-03 17:31:39.761145: Epoch time: 125.16 s\n",
      "2024-01-03 17:31:39.766145: Yayy! New best EMA pseudo Dice: 0.9201\n",
      "2024-01-03 17:31:41.213891: \n",
      "2024-01-03 17:31:41.220834: Epoch 38\n",
      "2024-01-03 17:31:41.228275: Current learning rate: 0.00966\n",
      "2024-01-03 17:33:46.613881: train_loss -0.869\n",
      "2024-01-03 17:33:46.622893: val_loss -0.8058\n",
      "2024-01-03 17:33:46.629903: Pseudo dice [0.9218, 0.921, 0.931]\n",
      "2024-01-03 17:33:46.633898: Epoch time: 125.4 s\n",
      "2024-01-03 17:33:46.639413: Yayy! New best EMA pseudo Dice: 0.9206\n",
      "2024-01-03 17:33:48.249755: \n",
      "2024-01-03 17:33:48.260803: Epoch 39\n",
      "2024-01-03 17:33:48.264813: Current learning rate: 0.00965\n",
      "2024-01-03 17:35:53.893040: train_loss -0.8681\n",
      "2024-01-03 17:35:53.899035: val_loss -0.81\n",
      "2024-01-03 17:35:53.904542: Pseudo dice [0.9229, 0.9234, 0.9322]\n",
      "2024-01-03 17:35:53.910542: Epoch time: 125.64 s\n",
      "2024-01-03 17:35:53.917542: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-03 17:35:55.423135: \n",
      "2024-01-03 17:35:55.428129: Epoch 40\n",
      "2024-01-03 17:35:55.433217: Current learning rate: 0.00964\n",
      "2024-01-03 17:38:00.859303: train_loss -0.8708\n",
      "2024-01-03 17:38:00.869311: val_loss -0.8094\n",
      "2024-01-03 17:38:00.878313: Pseudo dice [0.925, 0.9237, 0.9322]\n",
      "2024-01-03 17:38:00.885823: Epoch time: 125.44 s\n",
      "2024-01-03 17:38:00.891823: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2024-01-03 17:38:02.721798: \n",
      "2024-01-03 17:38:02.730875: Epoch 41\n",
      "2024-01-03 17:38:02.735878: Current learning rate: 0.00963\n",
      "2024-01-03 17:40:08.403769: train_loss -0.8719\n",
      "2024-01-03 17:40:08.411786: val_loss -0.8056\n",
      "2024-01-03 17:40:08.418782: Pseudo dice [0.9257, 0.9216, 0.9297]\n",
      "2024-01-03 17:40:08.422781: Epoch time: 125.68 s\n",
      "2024-01-03 17:40:08.427786: Yayy! New best EMA pseudo Dice: 0.9221\n",
      "2024-01-03 17:40:09.877487: \n",
      "2024-01-03 17:40:09.882491: Epoch 42\n",
      "2024-01-03 17:40:09.886476: Current learning rate: 0.00962\n",
      "2024-01-03 17:42:15.277843: train_loss -0.8739\n",
      "2024-01-03 17:42:15.286850: val_loss -0.8138\n",
      "2024-01-03 17:42:15.292851: Pseudo dice [0.9234, 0.9259, 0.9324]\n",
      "2024-01-03 17:42:15.299379: Epoch time: 125.4 s\n",
      "2024-01-03 17:42:15.304379: Yayy! New best EMA pseudo Dice: 0.9226\n",
      "2024-01-03 17:42:16.758004: \n",
      "2024-01-03 17:42:16.763999: Epoch 43\n",
      "2024-01-03 17:42:16.772984: Current learning rate: 0.00961\n",
      "2024-01-03 17:44:22.588094: train_loss -0.8731\n",
      "2024-01-03 17:44:22.595094: val_loss -0.8033\n",
      "2024-01-03 17:44:22.603094: Pseudo dice [0.926, 0.9197, 0.9291]\n",
      "2024-01-03 17:44:22.609094: Epoch time: 125.83 s\n",
      "2024-01-03 17:44:22.613094: Yayy! New best EMA pseudo Dice: 0.9229\n",
      "2024-01-03 17:44:24.114748: \n",
      "2024-01-03 17:44:24.128535: Epoch 44\n",
      "2024-01-03 17:44:24.132573: Current learning rate: 0.0096\n",
      "2024-01-03 17:46:29.585336: train_loss -0.8748\n",
      "2024-01-03 17:46:29.591337: val_loss -0.8091\n",
      "2024-01-03 17:46:29.598336: Pseudo dice [0.9274, 0.9233, 0.9326]\n",
      "2024-01-03 17:46:29.603341: Epoch time: 125.47 s\n",
      "2024-01-03 17:46:29.607352: Yayy! New best EMA pseudo Dice: 0.9233\n",
      "2024-01-03 17:46:31.104239: \n",
      "2024-01-03 17:46:31.113216: Epoch 45\n",
      "2024-01-03 17:46:31.117799: Current learning rate: 0.00959\n",
      "2024-01-03 17:48:36.730662: train_loss -0.872\n",
      "2024-01-03 17:48:36.736658: val_loss -0.807\n",
      "2024-01-03 17:48:36.744586: Pseudo dice [0.9252, 0.9212, 0.9299]\n",
      "2024-01-03 17:48:36.751585: Epoch time: 125.63 s\n",
      "2024-01-03 17:48:36.758586: Yayy! New best EMA pseudo Dice: 0.9236\n",
      "2024-01-03 17:48:38.164059: \n",
      "2024-01-03 17:48:38.170132: Epoch 46\n",
      "2024-01-03 17:48:38.174962: Current learning rate: 0.00959\n",
      "2024-01-03 17:50:43.778908: train_loss -0.8755\n",
      "2024-01-03 17:50:43.788906: val_loss -0.8107\n",
      "2024-01-03 17:50:43.805516: Pseudo dice [0.9244, 0.9234, 0.9311]\n",
      "2024-01-03 17:50:43.814039: Epoch time: 125.62 s\n",
      "2024-01-03 17:50:43.821040: Yayy! New best EMA pseudo Dice: 0.9238\n",
      "2024-01-03 17:50:45.333000: \n",
      "2024-01-03 17:50:45.338944: Epoch 47\n",
      "2024-01-03 17:50:45.342944: Current learning rate: 0.00958\n",
      "2024-01-03 17:52:50.886363: train_loss -0.8748\n",
      "2024-01-03 17:52:50.895370: val_loss -0.804\n",
      "2024-01-03 17:52:50.903881: Pseudo dice [0.9262, 0.9201, 0.9302]\n",
      "2024-01-03 17:52:50.913881: Epoch time: 125.55 s\n",
      "2024-01-03 17:52:50.920880: Yayy! New best EMA pseudo Dice: 0.924\n",
      "2024-01-03 17:52:52.648569: \n",
      "2024-01-03 17:52:52.660557: Epoch 48\n",
      "2024-01-03 17:52:52.672624: Current learning rate: 0.00957\n",
      "2024-01-03 17:54:58.440168: train_loss -0.8731\n",
      "2024-01-03 17:54:58.447179: val_loss -0.8076\n",
      "2024-01-03 17:54:58.452178: Pseudo dice [0.9253, 0.9235, 0.9328]\n",
      "2024-01-03 17:54:58.458244: Epoch time: 125.79 s\n",
      "2024-01-03 17:54:58.462914: Yayy! New best EMA pseudo Dice: 0.9243\n",
      "2024-01-03 17:54:59.928302: \n",
      "2024-01-03 17:54:59.936384: Epoch 49\n",
      "2024-01-03 17:54:59.944318: Current learning rate: 0.00956\n",
      "2024-01-03 17:57:05.565807: train_loss -0.8788\n",
      "2024-01-03 17:57:05.572809: val_loss -0.803\n",
      "2024-01-03 17:57:05.577808: Pseudo dice [0.9243, 0.9215, 0.9298]\n",
      "2024-01-03 17:57:05.582807: Epoch time: 125.64 s\n",
      "2024-01-03 17:57:05.917097: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2024-01-03 17:57:07.358858: \n",
      "2024-01-03 17:57:07.367800: Epoch 50\n",
      "2024-01-03 17:57:07.374136: Current learning rate: 0.00955\n",
      "2024-01-03 17:59:12.717721: train_loss -0.8778\n",
      "2024-01-03 17:59:12.728722: val_loss -0.8107\n",
      "2024-01-03 17:59:12.734721: Pseudo dice [0.9261, 0.9232, 0.9318]\n",
      "2024-01-03 17:59:12.743721: Epoch time: 125.36 s\n",
      "2024-01-03 17:59:12.748721: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2024-01-03 17:59:14.244293: \n",
      "2024-01-03 17:59:14.250818: Epoch 51\n",
      "2024-01-03 17:59:14.258890: Current learning rate: 0.00954\n",
      "2024-01-03 18:01:19.626385: train_loss -0.8793\n",
      "2024-01-03 18:01:19.633385: val_loss -0.8057\n",
      "2024-01-03 18:01:19.638385: Pseudo dice [0.9319, 0.92, 0.9273]\n",
      "2024-01-03 18:01:19.644385: Epoch time: 125.38 s\n",
      "2024-01-03 18:01:19.650388: Yayy! New best EMA pseudo Dice: 0.9248\n",
      "2024-01-03 18:01:21.212638: \n",
      "2024-01-03 18:01:21.220573: Epoch 52\n",
      "2024-01-03 18:01:21.225631: Current learning rate: 0.00953\n",
      "2024-01-03 18:03:26.723579: train_loss -0.8786\n",
      "2024-01-03 18:03:26.730593: val_loss -0.8103\n",
      "2024-01-03 18:03:26.735592: Pseudo dice [0.928, 0.9226, 0.9299]\n",
      "2024-01-03 18:03:26.740593: Epoch time: 125.51 s\n",
      "2024-01-03 18:03:26.747116: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2024-01-03 18:03:28.233671: \n",
      "2024-01-03 18:03:28.238671: Epoch 53\n",
      "2024-01-03 18:03:28.243078: Current learning rate: 0.00952\n",
      "2024-01-03 18:05:34.137925: train_loss -0.8785\n",
      "2024-01-03 18:05:34.143926: val_loss -0.8106\n",
      "2024-01-03 18:05:34.150375: Pseudo dice [0.9231, 0.923, 0.9316]\n",
      "2024-01-03 18:05:34.156384: Epoch time: 125.91 s\n",
      "2024-01-03 18:05:34.161383: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2024-01-03 18:05:35.674235: \n",
      "2024-01-03 18:05:35.680232: Epoch 54\n",
      "2024-01-03 18:05:35.684359: Current learning rate: 0.00951\n",
      "2024-01-03 18:07:41.754252: train_loss -0.878\n",
      "2024-01-03 18:07:41.763253: val_loss -0.8048\n",
      "2024-01-03 18:07:41.771200: Pseudo dice [0.9305, 0.9206, 0.9289]\n",
      "2024-01-03 18:07:41.780595: Epoch time: 126.08 s\n",
      "2024-01-03 18:07:41.788596: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-03 18:07:43.517030: \n",
      "2024-01-03 18:07:43.523030: Epoch 55\n",
      "2024-01-03 18:07:43.529022: Current learning rate: 0.0095\n",
      "2024-01-03 18:09:49.220790: train_loss -0.8787\n",
      "2024-01-03 18:09:49.229782: val_loss -0.8085\n",
      "2024-01-03 18:09:49.236782: Pseudo dice [0.9245, 0.9225, 0.9296]\n",
      "2024-01-03 18:09:49.242782: Epoch time: 125.7 s\n",
      "2024-01-03 18:09:49.246937: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-03 18:09:50.992309: \n",
      "2024-01-03 18:09:51.000302: Epoch 56\n",
      "2024-01-03 18:09:51.007328: Current learning rate: 0.00949\n",
      "2024-01-03 18:11:56.766032: train_loss -0.8788\n",
      "2024-01-03 18:11:56.774033: val_loss -0.8102\n",
      "2024-01-03 18:11:56.781032: Pseudo dice [0.9266, 0.9226, 0.932]\n",
      "2024-01-03 18:11:56.786032: Epoch time: 125.78 s\n",
      "2024-01-03 18:11:56.791032: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2024-01-03 18:11:58.283428: \n",
      "2024-01-03 18:11:58.289177: Epoch 57\n",
      "2024-01-03 18:11:58.294254: Current learning rate: 0.00949\n",
      "2024-01-03 18:14:03.828030: train_loss -0.8796\n",
      "2024-01-03 18:14:03.839020: val_loss -0.8069\n",
      "2024-01-03 18:14:03.846034: Pseudo dice [0.9234, 0.9231, 0.9304]\n",
      "2024-01-03 18:14:03.855021: Epoch time: 125.55 s\n",
      "2024-01-03 18:14:03.860025: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2024-01-03 18:14:05.374267: \n",
      "2024-01-03 18:14:05.380212: Epoch 58\n",
      "2024-01-03 18:14:05.385264: Current learning rate: 0.00948\n",
      "2024-01-03 18:16:10.809994: train_loss -0.8815\n",
      "2024-01-03 18:16:10.817005: val_loss -0.8084\n",
      "2024-01-03 18:16:10.822003: Pseudo dice [0.9272, 0.9217, 0.9307]\n",
      "2024-01-03 18:16:10.826995: Epoch time: 125.44 s\n",
      "2024-01-03 18:16:10.831132: Yayy! New best EMA pseudo Dice: 0.9256\n",
      "2024-01-03 18:16:12.234414: \n",
      "2024-01-03 18:16:12.245423: Epoch 59\n",
      "2024-01-03 18:16:12.250992: Current learning rate: 0.00947\n",
      "2024-01-03 18:18:17.625196: train_loss -0.8802\n",
      "2024-01-03 18:18:17.633196: val_loss -0.8098\n",
      "2024-01-03 18:18:17.678960: Pseudo dice [0.9255, 0.9253, 0.9326]\n",
      "2024-01-03 18:18:17.683962: Epoch time: 125.39 s\n",
      "2024-01-03 18:18:17.688960: Yayy! New best EMA pseudo Dice: 0.9258\n",
      "2024-01-03 18:18:19.169214: \n",
      "2024-01-03 18:18:19.176061: Epoch 60\n",
      "2024-01-03 18:18:19.180404: Current learning rate: 0.00946\n",
      "2024-01-03 18:20:24.786181: train_loss -0.8795\n",
      "2024-01-03 18:20:24.795174: val_loss -0.8088\n",
      "2024-01-03 18:20:24.802176: Pseudo dice [0.9279, 0.9224, 0.9303]\n",
      "2024-01-03 18:20:24.806183: Epoch time: 125.62 s\n",
      "2024-01-03 18:20:24.813199: Yayy! New best EMA pseudo Dice: 0.9259\n",
      "2024-01-03 18:20:26.215635: \n",
      "2024-01-03 18:20:26.220777: Epoch 61\n",
      "2024-01-03 18:20:26.228759: Current learning rate: 0.00945\n",
      "2024-01-03 18:22:31.571653: train_loss -0.8837\n",
      "2024-01-03 18:22:31.581652: val_loss -0.8088\n",
      "2024-01-03 18:22:31.588658: Pseudo dice [0.9255, 0.9234, 0.9309]\n",
      "2024-01-03 18:22:31.593657: Epoch time: 125.36 s\n",
      "2024-01-03 18:22:31.598659: Yayy! New best EMA pseudo Dice: 0.926\n",
      "2024-01-03 18:22:33.073857: \n",
      "2024-01-03 18:22:33.080282: Epoch 62\n",
      "2024-01-03 18:22:33.087360: Current learning rate: 0.00944\n",
      "2024-01-03 18:24:38.658755: train_loss -0.8839\n",
      "2024-01-03 18:24:38.664747: val_loss -0.8074\n",
      "2024-01-03 18:24:38.670253: Pseudo dice [0.9278, 0.9214, 0.931]\n",
      "2024-01-03 18:24:38.674261: Epoch time: 125.59 s\n",
      "2024-01-03 18:24:38.678261: Yayy! New best EMA pseudo Dice: 0.9261\n",
      "2024-01-03 18:24:40.266845: \n",
      "2024-01-03 18:24:40.274384: Epoch 63\n",
      "2024-01-03 18:24:40.278392: Current learning rate: 0.00943\n",
      "2024-01-03 18:26:45.516077: train_loss -0.8824\n",
      "2024-01-03 18:26:45.524077: val_loss -0.8073\n",
      "2024-01-03 18:26:45.528581: Pseudo dice [0.9276, 0.922, 0.9321]\n",
      "2024-01-03 18:26:45.533592: Epoch time: 125.25 s\n",
      "2024-01-03 18:26:45.538587: Yayy! New best EMA pseudo Dice: 0.9262\n",
      "2024-01-03 18:26:47.234003: \n",
      "2024-01-03 18:26:47.245631: Epoch 64\n",
      "2024-01-03 18:26:47.255623: Current learning rate: 0.00942\n",
      "2024-01-03 18:28:52.732954: train_loss -0.8844\n",
      "2024-01-03 18:28:52.741954: val_loss -0.8096\n",
      "2024-01-03 18:28:52.749952: Pseudo dice [0.928, 0.9232, 0.9323]\n",
      "2024-01-03 18:28:52.755954: Epoch time: 125.5 s\n",
      "2024-01-03 18:28:52.761953: Yayy! New best EMA pseudo Dice: 0.9263\n",
      "2024-01-03 18:28:54.265266: \n",
      "2024-01-03 18:28:54.271557: Epoch 65\n",
      "2024-01-03 18:28:54.277638: Current learning rate: 0.00941\n",
      "2024-01-03 18:30:59.694795: train_loss -0.8846\n",
      "2024-01-03 18:30:59.701798: val_loss -0.813\n",
      "2024-01-03 18:30:59.706795: Pseudo dice [0.927, 0.9252, 0.933]\n",
      "2024-01-03 18:30:59.711794: Epoch time: 125.43 s\n",
      "2024-01-03 18:30:59.716037: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2024-01-03 18:31:01.202638: \n",
      "2024-01-03 18:31:01.210629: Epoch 66\n",
      "2024-01-03 18:31:01.214637: Current learning rate: 0.0094\n",
      "2024-01-03 18:33:06.851020: train_loss -0.8833\n",
      "2024-01-03 18:33:06.860023: val_loss -0.809\n",
      "2024-01-03 18:33:06.870020: Pseudo dice [0.9246, 0.9242, 0.9311]\n",
      "2024-01-03 18:33:06.879019: Epoch time: 125.65 s\n",
      "2024-01-03 18:33:06.884027: Yayy! New best EMA pseudo Dice: 0.9266\n",
      "2024-01-03 18:33:08.610892: \n",
      "2024-01-03 18:33:08.624680: Epoch 67\n",
      "2024-01-03 18:33:08.632100: Current learning rate: 0.00939\n",
      "2024-01-03 18:35:14.414613: train_loss -0.8835\n",
      "2024-01-03 18:35:14.421613: val_loss -0.8135\n",
      "2024-01-03 18:35:14.426614: Pseudo dice [0.9252, 0.927, 0.9331]\n",
      "2024-01-03 18:35:14.431616: Epoch time: 125.8 s\n",
      "2024-01-03 18:35:14.436613: Yayy! New best EMA pseudo Dice: 0.9267\n",
      "2024-01-03 18:35:15.980282: \n",
      "2024-01-03 18:35:15.985890: Epoch 68\n",
      "2024-01-03 18:35:15.989855: Current learning rate: 0.00939\n",
      "2024-01-03 18:37:21.794806: train_loss -0.8856\n",
      "2024-01-03 18:37:21.801804: val_loss -0.8142\n",
      "2024-01-03 18:37:21.807803: Pseudo dice [0.9297, 0.9249, 0.9329]\n",
      "2024-01-03 18:37:21.814803: Epoch time: 125.82 s\n",
      "2024-01-03 18:37:21.819805: Yayy! New best EMA pseudo Dice: 0.927\n",
      "2024-01-03 18:37:23.366300: \n",
      "2024-01-03 18:37:23.377301: Epoch 69\n",
      "2024-01-03 18:37:23.382301: Current learning rate: 0.00938\n",
      "2024-01-03 18:39:28.816959: train_loss -0.8805\n",
      "2024-01-03 18:39:28.823960: val_loss -0.8144\n",
      "2024-01-03 18:39:28.829959: Pseudo dice [0.9258, 0.9269, 0.9355]\n",
      "2024-01-03 18:39:28.833960: Epoch time: 125.45 s\n",
      "2024-01-03 18:39:28.838962: Yayy! New best EMA pseudo Dice: 0.9272\n",
      "2024-01-03 18:39:30.325255: \n",
      "2024-01-03 18:39:30.331372: Epoch 70\n",
      "2024-01-03 18:39:30.336474: Current learning rate: 0.00937\n",
      "2024-01-03 18:41:35.744537: train_loss -0.885\n",
      "2024-01-03 18:41:35.753528: val_loss -0.8131\n",
      "2024-01-03 18:41:35.759534: Pseudo dice [0.9286, 0.9266, 0.9337]\n",
      "2024-01-03 18:41:35.764533: Epoch time: 125.42 s\n",
      "2024-01-03 18:41:35.769534: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2024-01-03 18:41:37.455182: \n",
      "2024-01-03 18:41:37.461265: Epoch 71\n",
      "2024-01-03 18:41:37.470786: Current learning rate: 0.00936\n",
      "2024-01-03 18:43:42.838451: train_loss -0.8851\n",
      "2024-01-03 18:43:42.844451: val_loss -0.8093\n",
      "2024-01-03 18:43:42.850451: Pseudo dice [0.9318, 0.923, 0.9302]\n",
      "2024-01-03 18:43:42.855450: Epoch time: 125.38 s\n",
      "2024-01-03 18:43:42.859450: Yayy! New best EMA pseudo Dice: 0.9276\n",
      "2024-01-03 18:43:44.427093: \n",
      "2024-01-03 18:43:44.439200: Epoch 72\n",
      "2024-01-03 18:43:44.450121: Current learning rate: 0.00935\n",
      "2024-01-03 18:45:49.792836: train_loss -0.8841\n",
      "2024-01-03 18:45:49.798837: val_loss -0.8074\n",
      "2024-01-03 18:45:49.805841: Pseudo dice [0.9275, 0.9226, 0.9324]\n",
      "2024-01-03 18:45:49.812826: Epoch time: 125.37 s\n",
      "2024-01-03 18:45:51.058099: \n",
      "2024-01-03 18:45:51.069419: Epoch 73\n",
      "2024-01-03 18:45:51.080498: Current learning rate: 0.00934\n",
      "2024-01-03 18:47:56.189483: train_loss -0.8898\n",
      "2024-01-03 18:47:56.195484: val_loss -0.8106\n",
      "2024-01-03 18:47:56.202471: Pseudo dice [0.9279, 0.9246, 0.9325]\n",
      "2024-01-03 18:47:56.206471: Epoch time: 125.13 s\n",
      "2024-01-03 18:47:56.212471: Yayy! New best EMA pseudo Dice: 0.9276\n",
      "2024-01-03 18:47:57.685254: \n",
      "2024-01-03 18:47:57.691249: Epoch 74\n",
      "2024-01-03 18:47:57.696190: Current learning rate: 0.00933\n",
      "2024-01-03 18:50:03.167407: train_loss -0.8888\n",
      "2024-01-03 18:50:03.173402: val_loss -0.8099\n",
      "2024-01-03 18:50:03.178402: Pseudo dice [0.9267, 0.9237, 0.9336]\n",
      "2024-01-03 18:50:03.183402: Epoch time: 125.48 s\n",
      "2024-01-03 18:50:03.188400: Yayy! New best EMA pseudo Dice: 0.9277\n",
      "2024-01-03 18:50:04.720161: \n",
      "2024-01-03 18:50:04.728200: Epoch 75\n",
      "2024-01-03 18:50:04.732188: Current learning rate: 0.00932\n",
      "2024-01-03 18:52:09.749204: train_loss -0.8875\n",
      "2024-01-03 18:52:09.757432: val_loss -0.8109\n",
      "2024-01-03 18:52:09.762431: Pseudo dice [0.9271, 0.9242, 0.9314]\n",
      "2024-01-03 18:52:09.767438: Epoch time: 125.03 s\n",
      "2024-01-03 18:52:10.917973: \n",
      "2024-01-03 18:52:10.923588: Epoch 76\n",
      "2024-01-03 18:52:10.929153: Current learning rate: 0.00931\n",
      "2024-01-03 18:54:16.402358: train_loss -0.8875\n",
      "2024-01-03 18:54:16.409357: val_loss -0.8073\n",
      "2024-01-03 18:54:16.419358: Pseudo dice [0.9268, 0.9228, 0.9322]\n",
      "2024-01-03 18:54:16.426358: Epoch time: 125.49 s\n",
      "2024-01-03 18:54:17.605637: \n",
      "2024-01-03 18:54:17.611638: Epoch 77\n",
      "2024-01-03 18:54:17.615637: Current learning rate: 0.0093\n",
      "2024-01-03 18:56:23.205318: train_loss -0.8858\n",
      "2024-01-03 18:56:23.215318: val_loss -0.8119\n",
      "2024-01-03 18:56:23.222317: Pseudo dice [0.9304, 0.9237, 0.931]\n",
      "2024-01-03 18:56:23.227317: Epoch time: 125.6 s\n",
      "2024-01-03 18:56:23.232317: Yayy! New best EMA pseudo Dice: 0.9277\n",
      "2024-01-03 18:56:24.974228: \n",
      "2024-01-03 18:56:24.979964: Epoch 78\n",
      "2024-01-03 18:56:24.988023: Current learning rate: 0.0093\n",
      "2024-01-03 18:58:30.684751: train_loss -0.8844\n",
      "2024-01-03 18:58:30.690752: val_loss -0.8013\n",
      "2024-01-03 18:58:30.696752: Pseudo dice [0.9257, 0.9195, 0.9288]\n",
      "2024-01-03 18:58:30.703751: Epoch time: 125.71 s\n",
      "2024-01-03 18:58:31.882968: \n",
      "2024-01-03 18:58:31.890958: Epoch 79\n",
      "2024-01-03 18:58:31.895311: Current learning rate: 0.00929\n",
      "2024-01-03 19:00:37.658980: train_loss -0.8879\n",
      "2024-01-03 19:00:37.664984: val_loss -0.8107\n",
      "2024-01-03 19:00:37.670982: Pseudo dice [0.9271, 0.9257, 0.9345]\n",
      "2024-01-03 19:00:37.675980: Epoch time: 125.78 s\n",
      "2024-01-03 19:00:38.839698: \n",
      "2024-01-03 19:00:38.846518: Epoch 80\n",
      "2024-01-03 19:00:38.850600: Current learning rate: 0.00928\n",
      "2024-01-03 19:02:44.421716: train_loss -0.8852\n",
      "2024-01-03 19:02:44.429712: val_loss -0.8086\n",
      "2024-01-03 19:02:44.435716: Pseudo dice [0.9276, 0.923, 0.9318]\n",
      "2024-01-03 19:02:44.440712: Epoch time: 125.58 s\n",
      "2024-01-03 19:02:45.634631: \n",
      "2024-01-03 19:02:45.643632: Epoch 81\n",
      "2024-01-03 19:02:45.648169: Current learning rate: 0.00927\n",
      "2024-01-03 19:04:51.125042: train_loss -0.8877\n",
      "2024-01-03 19:04:51.133043: val_loss -0.8101\n",
      "2024-01-03 19:04:51.138043: Pseudo dice [0.9251, 0.9249, 0.9329]\n",
      "2024-01-03 19:04:51.144042: Epoch time: 125.49 s\n",
      "2024-01-03 19:04:52.368274: \n",
      "2024-01-03 19:04:52.374264: Epoch 82\n",
      "2024-01-03 19:04:52.380289: Current learning rate: 0.00926\n",
      "2024-01-03 19:06:58.066182: train_loss -0.8878\n",
      "2024-01-03 19:06:58.075183: val_loss -0.8109\n",
      "2024-01-03 19:06:58.080182: Pseudo dice [0.9254, 0.924, 0.9343]\n",
      "2024-01-03 19:06:58.084182: Epoch time: 125.7 s\n",
      "2024-01-03 19:06:59.296638: \n",
      "2024-01-03 19:06:59.302717: Epoch 83\n",
      "2024-01-03 19:06:59.307658: Current learning rate: 0.00925\n",
      "2024-01-03 19:09:04.647638: train_loss -0.8853\n",
      "2024-01-03 19:09:04.655637: val_loss -0.8103\n",
      "2024-01-03 19:09:04.661640: Pseudo dice [0.9304, 0.9233, 0.9311]\n",
      "2024-01-03 19:09:04.666640: Epoch time: 125.35 s\n",
      "2024-01-03 19:09:05.731040: \n",
      "2024-01-03 19:09:05.736058: Epoch 84\n",
      "2024-01-03 19:09:05.744094: Current learning rate: 0.00924\n",
      "2024-01-03 19:11:10.958297: train_loss -0.8899\n",
      "2024-01-03 19:11:10.965296: val_loss -0.8144\n",
      "2024-01-03 19:11:10.973296: Pseudo dice [0.9287, 0.9274, 0.9356]\n",
      "2024-01-03 19:11:10.978296: Epoch time: 125.23 s\n",
      "2024-01-03 19:11:10.983295: Yayy! New best EMA pseudo Dice: 0.9279\n",
      "2024-01-03 19:11:12.279579: \n",
      "2024-01-03 19:11:12.290590: Epoch 85\n",
      "2024-01-03 19:11:12.295499: Current learning rate: 0.00923\n",
      "2024-01-03 19:13:17.874216: train_loss -0.8866\n",
      "2024-01-03 19:13:17.882216: val_loss -0.8123\n",
      "2024-01-03 19:13:17.889223: Pseudo dice [0.9306, 0.9233, 0.9298]\n",
      "2024-01-03 19:13:17.895220: Epoch time: 125.6 s\n",
      "2024-01-03 19:13:19.198428: \n",
      "2024-01-03 19:13:19.206087: Epoch 86\n",
      "2024-01-03 19:13:19.210749: Current learning rate: 0.00922\n",
      "2024-01-03 19:15:27.389978: train_loss -0.8884\n",
      "2024-01-03 19:15:27.396986: val_loss -0.8064\n",
      "2024-01-03 19:15:27.403494: Pseudo dice [0.9236, 0.9242, 0.9326]\n",
      "2024-01-03 19:15:27.408494: Epoch time: 128.19 s\n",
      "2024-01-03 19:15:28.550269: \n",
      "2024-01-03 19:15:28.556566: Epoch 87\n",
      "2024-01-03 19:15:28.561503: Current learning rate: 0.00921\n",
      "2024-01-03 19:17:34.845885: train_loss -0.8885\n",
      "2024-01-03 19:17:34.852885: val_loss -0.7987\n",
      "2024-01-03 19:17:34.857891: Pseudo dice [0.9189, 0.9225, 0.9304]\n",
      "2024-01-03 19:17:34.863886: Epoch time: 126.3 s\n",
      "2024-01-03 19:17:36.012725: \n",
      "2024-01-03 19:17:36.017725: Epoch 88\n",
      "2024-01-03 19:17:36.022676: Current learning rate: 0.0092\n",
      "2024-01-03 19:19:42.298102: train_loss -0.8894\n",
      "2024-01-03 19:19:42.307103: val_loss -0.8082\n",
      "2024-01-03 19:19:42.314118: Pseudo dice [0.9298, 0.9237, 0.932]\n",
      "2024-01-03 19:19:42.324662: Epoch time: 126.29 s\n",
      "2024-01-03 19:19:43.582394: \n",
      "2024-01-03 19:19:43.587389: Epoch 89\n",
      "2024-01-03 19:19:43.592391: Current learning rate: 0.0092\n",
      "2024-01-03 19:21:49.153113: train_loss -0.8898\n",
      "2024-01-03 19:21:49.161126: val_loss -0.8098\n",
      "2024-01-03 19:21:49.168133: Pseudo dice [0.9276, 0.9231, 0.9316]\n",
      "2024-01-03 19:21:49.175631: Epoch time: 125.57 s\n",
      "2024-01-03 19:21:50.197122: \n",
      "2024-01-03 19:21:50.205201: Epoch 90\n",
      "2024-01-03 19:21:50.210237: Current learning rate: 0.00919\n",
      "2024-01-03 19:23:55.871809: train_loss -0.888\n",
      "2024-01-03 19:23:55.877808: val_loss -0.81\n",
      "2024-01-03 19:23:55.882797: Pseudo dice [0.9267, 0.9246, 0.9336]\n",
      "2024-01-03 19:23:55.886796: Epoch time: 125.68 s\n",
      "2024-01-03 19:23:57.042341: \n",
      "2024-01-03 19:23:57.055013: Epoch 91\n",
      "2024-01-03 19:23:57.066697: Current learning rate: 0.00918\n",
      "2024-01-03 19:26:02.703841: train_loss -0.8892\n",
      "2024-01-03 19:26:02.709835: val_loss -0.8112\n",
      "2024-01-03 19:26:02.714840: Pseudo dice [0.9291, 0.9252, 0.9333]\n",
      "2024-01-03 19:26:02.718836: Epoch time: 125.66 s\n",
      "2024-01-03 19:26:03.851053: \n",
      "2024-01-03 19:26:03.859688: Epoch 92\n",
      "2024-01-03 19:26:03.863686: Current learning rate: 0.00917\n",
      "2024-01-03 19:28:09.456636: train_loss -0.8886\n",
      "2024-01-03 19:28:09.465636: val_loss -0.8116\n",
      "2024-01-03 19:28:09.470636: Pseudo dice [0.924, 0.9268, 0.9339]\n",
      "2024-01-03 19:28:09.474638: Epoch time: 125.61 s\n",
      "2024-01-03 19:28:10.602792: \n",
      "2024-01-03 19:28:10.610865: Epoch 93\n",
      "2024-01-03 19:28:10.615865: Current learning rate: 0.00916\n",
      "2024-01-03 19:30:15.996019: train_loss -0.8913\n",
      "2024-01-03 19:30:16.006021: val_loss -0.8056\n",
      "2024-01-03 19:30:16.011019: Pseudo dice [0.9266, 0.9239, 0.9312]\n",
      "2024-01-03 19:30:16.019019: Epoch time: 125.39 s\n",
      "2024-01-03 19:30:17.076288: \n",
      "2024-01-03 19:30:17.085455: Epoch 94\n",
      "2024-01-03 19:30:17.089450: Current learning rate: 0.00915\n",
      "2024-01-03 19:32:23.099118: train_loss -0.8862\n",
      "2024-01-03 19:32:23.109120: val_loss -0.809\n",
      "2024-01-03 19:32:23.115118: Pseudo dice [0.9266, 0.925, 0.9329]\n",
      "2024-01-03 19:32:23.122118: Epoch time: 126.02 s\n",
      "2024-01-03 19:32:24.425319: \n",
      "2024-01-03 19:32:24.431311: Epoch 95\n",
      "2024-01-03 19:32:24.436411: Current learning rate: 0.00914\n",
      "2024-01-03 19:34:30.077690: train_loss -0.8914\n",
      "2024-01-03 19:34:30.086695: val_loss -0.8023\n",
      "2024-01-03 19:34:30.093696: Pseudo dice [0.9258, 0.921, 0.93]\n",
      "2024-01-03 19:34:30.101748: Epoch time: 125.65 s\n",
      "2024-01-03 19:34:31.177203: \n",
      "2024-01-03 19:34:31.182209: Epoch 96\n",
      "2024-01-03 19:34:31.187205: Current learning rate: 0.00913\n",
      "2024-01-03 19:36:36.945265: train_loss -0.8912\n",
      "2024-01-03 19:36:36.955265: val_loss -0.8064\n",
      "2024-01-03 19:36:36.965267: Pseudo dice [0.9288, 0.9217, 0.9319]\n",
      "2024-01-03 19:36:36.970265: Epoch time: 125.77 s\n",
      "2024-01-03 19:36:38.096837: \n",
      "2024-01-03 19:36:38.103188: Epoch 97\n",
      "2024-01-03 19:36:38.114130: Current learning rate: 0.00912\n",
      "2024-01-03 19:38:43.581064: train_loss -0.8893\n",
      "2024-01-03 19:38:43.588571: val_loss -0.8127\n",
      "2024-01-03 19:38:43.593571: Pseudo dice [0.9297, 0.9251, 0.9333]\n",
      "2024-01-03 19:38:43.599571: Epoch time: 125.49 s\n",
      "2024-01-03 19:38:44.602532: \n",
      "2024-01-03 19:38:44.608859: Epoch 98\n",
      "2024-01-03 19:38:44.612849: Current learning rate: 0.00911\n",
      "2024-01-03 19:40:49.821785: train_loss -0.8932\n",
      "2024-01-03 19:40:49.830782: val_loss -0.8076\n",
      "2024-01-03 19:40:49.835783: Pseudo dice [0.9295, 0.9238, 0.9331]\n",
      "2024-01-03 19:40:49.839782: Epoch time: 125.22 s\n",
      "2024-01-03 19:40:50.908424: \n",
      "2024-01-03 19:40:50.914489: Epoch 99\n",
      "2024-01-03 19:40:50.920425: Current learning rate: 0.0091\n",
      "2024-01-03 19:42:56.570897: train_loss -0.8931\n",
      "2024-01-03 19:42:56.577408: val_loss -0.8141\n",
      "2024-01-03 19:42:56.582407: Pseudo dice [0.9282, 0.9259, 0.9343]\n",
      "2024-01-03 19:42:56.587407: Epoch time: 125.66 s\n",
      "2024-01-03 19:42:56.972148: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2024-01-03 19:42:58.365953: \n",
      "2024-01-03 19:42:58.377987: Epoch 100\n",
      "2024-01-03 19:42:58.383221: Current learning rate: 0.0091\n",
      "2024-01-03 19:45:04.014924: train_loss -0.8921\n",
      "2024-01-03 19:45:04.022921: val_loss -0.8104\n",
      "2024-01-03 19:45:04.028913: Pseudo dice [0.9269, 0.9247, 0.9327]\n",
      "2024-01-03 19:45:04.036921: Epoch time: 125.65 s\n",
      "2024-01-03 19:45:04.043503: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2024-01-03 19:45:05.573150: \n",
      "2024-01-03 19:45:05.582231: Epoch 101\n",
      "2024-01-03 19:45:05.587232: Current learning rate: 0.00909\n",
      "2024-01-03 19:47:11.149751: train_loss -0.8903\n",
      "2024-01-03 19:47:11.156756: val_loss -0.8099\n",
      "2024-01-03 19:47:11.164974: Pseudo dice [0.9264, 0.9253, 0.9344]\n",
      "2024-01-03 19:47:11.171975: Epoch time: 125.58 s\n",
      "2024-01-03 19:47:11.179988: Yayy! New best EMA pseudo Dice: 0.9281\n",
      "2024-01-03 19:47:12.650401: \n",
      "2024-01-03 19:47:12.661516: Epoch 102\n",
      "2024-01-03 19:47:12.671498: Current learning rate: 0.00908\n",
      "2024-01-03 19:49:18.283426: train_loss -0.8902\n",
      "2024-01-03 19:49:18.292426: val_loss -0.8177\n",
      "2024-01-03 19:49:18.297426: Pseudo dice [0.9266, 0.9294, 0.9378]\n",
      "2024-01-03 19:49:18.304430: Epoch time: 125.64 s\n",
      "2024-01-03 19:49:18.309431: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2024-01-03 19:49:19.796327: \n",
      "2024-01-03 19:49:19.803416: Epoch 103\n",
      "2024-01-03 19:49:19.810421: Current learning rate: 0.00907\n",
      "2024-01-03 19:51:25.633416: train_loss -0.8898\n",
      "2024-01-03 19:51:25.642414: val_loss -0.82\n",
      "2024-01-03 19:51:25.648421: Pseudo dice [0.9301, 0.9283, 0.9348]\n",
      "2024-01-03 19:51:25.653420: Epoch time: 125.84 s\n",
      "2024-01-03 19:51:25.658421: Yayy! New best EMA pseudo Dice: 0.9287\n",
      "2024-01-03 19:51:27.146263: \n",
      "2024-01-03 19:51:27.152007: Epoch 104\n",
      "2024-01-03 19:51:27.156078: Current learning rate: 0.00906\n",
      "2024-01-03 19:53:32.865836: train_loss -0.8885\n",
      "2024-01-03 19:53:32.874476: val_loss -0.8155\n",
      "2024-01-03 19:53:32.879483: Pseudo dice [0.9271, 0.9264, 0.9344]\n",
      "2024-01-03 19:53:32.885480: Epoch time: 125.72 s\n",
      "2024-01-03 19:53:32.891479: Yayy! New best EMA pseudo Dice: 0.9287\n",
      "2024-01-03 19:53:34.236658: \n",
      "2024-01-03 19:53:34.242372: Epoch 105\n",
      "2024-01-03 19:53:34.246378: Current learning rate: 0.00905\n",
      "2024-01-03 19:55:39.564599: train_loss -0.8906\n",
      "2024-01-03 19:55:39.570511: val_loss -0.8105\n",
      "2024-01-03 19:55:39.576512: Pseudo dice [0.9272, 0.9248, 0.9322]\n",
      "2024-01-03 19:55:39.581586: Epoch time: 125.33 s\n",
      "2024-01-03 19:55:40.667358: \n",
      "2024-01-03 19:55:40.678292: Epoch 106\n",
      "2024-01-03 19:55:40.683355: Current learning rate: 0.00904\n",
      "2024-01-03 19:57:46.156128: train_loss -0.8928\n",
      "2024-01-03 19:57:46.162640: val_loss -0.8082\n",
      "2024-01-03 19:57:46.167649: Pseudo dice [0.9272, 0.9249, 0.933]\n",
      "2024-01-03 19:57:46.171642: Epoch time: 125.49 s\n",
      "2024-01-03 19:57:47.284601: \n",
      "2024-01-03 19:57:47.290596: Epoch 107\n",
      "2024-01-03 19:57:47.295600: Current learning rate: 0.00903\n",
      "2024-01-03 19:59:52.635645: train_loss -0.8897\n",
      "2024-01-03 19:59:52.641645: val_loss -0.8077\n",
      "2024-01-03 19:59:52.648654: Pseudo dice [0.9234, 0.9232, 0.9317]\n",
      "2024-01-03 19:59:52.654646: Epoch time: 125.35 s\n",
      "2024-01-03 19:59:53.799887: \n",
      "2024-01-03 19:59:53.805598: Epoch 108\n",
      "2024-01-03 19:59:53.809676: Current learning rate: 0.00902\n",
      "2024-01-03 20:01:59.392566: train_loss -0.8918\n",
      "2024-01-03 20:01:59.398563: val_loss -0.8112\n",
      "2024-01-03 20:01:59.403562: Pseudo dice [0.9274, 0.9254, 0.9337]\n",
      "2024-01-03 20:01:59.407562: Epoch time: 125.6 s\n",
      "2024-01-03 20:02:00.746317: \n",
      "2024-01-03 20:02:00.752514: Epoch 109\n",
      "2024-01-03 20:02:00.759576: Current learning rate: 0.00901\n",
      "2024-01-03 20:04:05.955728: train_loss -0.8948\n",
      "2024-01-03 20:04:05.964724: val_loss -0.8088\n",
      "2024-01-03 20:04:05.969725: Pseudo dice [0.9249, 0.9252, 0.9336]\n",
      "2024-01-03 20:04:05.975732: Epoch time: 125.21 s\n",
      "2024-01-03 20:04:07.141382: \n",
      "2024-01-03 20:04:07.146382: Epoch 110\n",
      "2024-01-03 20:04:07.151901: Current learning rate: 0.009\n",
      "2024-01-03 20:06:12.494425: train_loss -0.894\n",
      "2024-01-03 20:06:12.500426: val_loss -0.8126\n",
      "2024-01-03 20:06:12.506425: Pseudo dice [0.9316, 0.9252, 0.9323]\n",
      "2024-01-03 20:06:12.510425: Epoch time: 125.35 s\n",
      "2024-01-03 20:06:13.699217: \n",
      "2024-01-03 20:06:13.708151: Epoch 111\n",
      "2024-01-03 20:06:13.716230: Current learning rate: 0.009\n",
      "2024-01-03 20:08:19.163743: train_loss -0.8935\n",
      "2024-01-03 20:08:19.170745: val_loss -0.8135\n",
      "2024-01-03 20:08:19.175745: Pseudo dice [0.9297, 0.9259, 0.9336]\n",
      "2024-01-03 20:08:19.180744: Epoch time: 125.47 s\n",
      "2024-01-03 20:08:20.550959: \n",
      "2024-01-03 20:08:20.559846: Epoch 112\n",
      "2024-01-03 20:08:20.570925: Current learning rate: 0.00899\n",
      "2024-01-03 20:10:26.092292: train_loss -0.8929\n",
      "2024-01-03 20:10:26.101292: val_loss -0.8191\n",
      "2024-01-03 20:10:26.107292: Pseudo dice [0.9273, 0.9293, 0.9377]\n",
      "2024-01-03 20:10:26.112293: Epoch time: 125.54 s\n",
      "2024-01-03 20:10:26.117292: Yayy! New best EMA pseudo Dice: 0.9289\n",
      "2024-01-03 20:10:27.578039: \n",
      "2024-01-03 20:10:27.584766: Epoch 113\n",
      "2024-01-03 20:10:27.590778: Current learning rate: 0.00898\n",
      "2024-01-03 20:12:33.260944: train_loss -0.8886\n",
      "2024-01-03 20:12:33.267947: val_loss -0.8068\n",
      "2024-01-03 20:12:33.274946: Pseudo dice [0.9295, 0.9231, 0.93]\n",
      "2024-01-03 20:12:33.279946: Epoch time: 125.68 s\n",
      "2024-01-03 20:12:34.362124: \n",
      "2024-01-03 20:12:34.371145: Epoch 114\n",
      "2024-01-03 20:12:34.375198: Current learning rate: 0.00897\n",
      "2024-01-03 20:14:39.752044: train_loss -0.8921\n",
      "2024-01-03 20:14:39.758035: val_loss -0.8142\n",
      "2024-01-03 20:14:39.765038: Pseudo dice [0.9285, 0.9242, 0.9335]\n",
      "2024-01-03 20:14:39.773041: Epoch time: 125.39 s\n",
      "2024-01-03 20:14:40.891771: \n",
      "2024-01-03 20:14:40.901416: Epoch 115\n",
      "2024-01-03 20:14:40.906488: Current learning rate: 0.00896\n",
      "2024-01-03 20:16:46.502149: train_loss -0.8911\n",
      "2024-01-03 20:16:46.508150: val_loss -0.8124\n",
      "2024-01-03 20:16:46.514150: Pseudo dice [0.9253, 0.9258, 0.9353]\n",
      "2024-01-03 20:16:46.519151: Epoch time: 125.61 s\n",
      "2024-01-03 20:16:47.606516: \n",
      "2024-01-03 20:16:47.612628: Epoch 116\n",
      "2024-01-03 20:16:47.616812: Current learning rate: 0.00895\n",
      "2024-01-03 20:18:52.900001: train_loss -0.8933\n",
      "2024-01-03 20:18:52.910002: val_loss -0.8081\n",
      "2024-01-03 20:18:52.917002: Pseudo dice [0.9281, 0.9246, 0.9331]\n",
      "2024-01-03 20:18:52.923007: Epoch time: 125.29 s\n",
      "2024-01-03 20:18:54.315012: \n",
      "2024-01-03 20:18:54.321012: Epoch 117\n",
      "2024-01-03 20:18:54.325203: Current learning rate: 0.00894\n",
      "2024-01-03 20:20:59.843230: train_loss -0.8905\n",
      "2024-01-03 20:20:59.850230: val_loss -0.7891\n",
      "2024-01-03 20:20:59.855231: Pseudo dice [0.9141, 0.9182, 0.9252]\n",
      "2024-01-03 20:20:59.860231: Epoch time: 125.53 s\n",
      "2024-01-03 20:21:00.964944: \n",
      "2024-01-03 20:21:00.970353: Epoch 118\n",
      "2024-01-03 20:21:00.978426: Current learning rate: 0.00893\n",
      "2024-01-03 20:23:06.557352: train_loss -0.8936\n",
      "2024-01-03 20:23:06.563352: val_loss -0.813\n",
      "2024-01-03 20:23:06.567669: Pseudo dice [0.927, 0.9264, 0.9335]\n",
      "2024-01-03 20:23:06.572668: Epoch time: 125.59 s\n",
      "2024-01-03 20:23:07.753266: \n",
      "2024-01-03 20:23:07.759240: Epoch 119\n",
      "2024-01-03 20:23:07.764246: Current learning rate: 0.00892\n",
      "2024-01-03 20:25:13.090865: train_loss -0.8911\n",
      "2024-01-03 20:25:13.100866: val_loss -0.8072\n",
      "2024-01-03 20:25:13.107865: Pseudo dice [0.927, 0.9235, 0.9322]\n",
      "2024-01-03 20:25:13.113871: Epoch time: 125.34 s\n",
      "2024-01-03 20:25:14.298847: \n",
      "2024-01-03 20:25:14.304851: Epoch 120\n",
      "2024-01-03 20:25:14.309952: Current learning rate: 0.00891\n",
      "2024-01-03 20:27:19.849064: train_loss -0.89\n",
      "2024-01-03 20:27:19.857065: val_loss -0.8181\n",
      "2024-01-03 20:27:19.864063: Pseudo dice [0.926, 0.9291, 0.9356]\n",
      "2024-01-03 20:27:19.869063: Epoch time: 125.55 s\n",
      "2024-01-03 20:27:21.129878: \n",
      "2024-01-03 20:27:21.135405: Epoch 121\n",
      "2024-01-03 20:27:21.140413: Current learning rate: 0.0089\n",
      "2024-01-03 20:29:26.577616: train_loss -0.8933\n",
      "2024-01-03 20:29:26.586612: val_loss -0.8098\n",
      "2024-01-03 20:29:26.592613: Pseudo dice [0.9267, 0.9233, 0.932]\n",
      "2024-01-03 20:29:26.607491: Epoch time: 125.45 s\n",
      "2024-01-03 20:29:27.722308: \n",
      "2024-01-03 20:29:27.728238: Epoch 122\n",
      "2024-01-03 20:29:27.732240: Current learning rate: 0.00889\n",
      "2024-01-03 20:31:32.525743: train_loss -0.8801\n",
      "2024-01-03 20:31:32.532665: val_loss -0.8085\n",
      "2024-01-03 20:31:32.538829: Pseudo dice [0.9309, 0.9223, 0.9326]\n",
      "2024-01-03 20:31:32.542833: Epoch time: 124.8 s\n",
      "2024-01-03 20:31:33.788360: \n",
      "2024-01-03 20:31:33.797833: Epoch 123\n",
      "2024-01-03 20:31:33.806844: Current learning rate: 0.00889\n",
      "2024-01-03 20:33:38.794084: train_loss -0.8965\n",
      "2024-01-03 20:33:38.801084: val_loss -0.806\n",
      "2024-01-03 20:33:38.807083: Pseudo dice [0.927, 0.9238, 0.9325]\n",
      "2024-01-03 20:33:38.816084: Epoch time: 125.01 s\n",
      "2024-01-03 20:33:39.982296: \n",
      "2024-01-03 20:33:39.989775: Epoch 124\n",
      "2024-01-03 20:33:39.997853: Current learning rate: 0.00888\n",
      "2024-01-03 20:35:45.024759: train_loss -0.8954\n",
      "2024-01-03 20:35:45.033268: val_loss -0.8077\n",
      "2024-01-03 20:35:45.039268: Pseudo dice [0.9284, 0.9236, 0.9318]\n",
      "2024-01-03 20:35:45.046268: Epoch time: 125.04 s\n",
      "2024-01-03 20:35:46.367860: \n",
      "2024-01-03 20:35:46.373873: Epoch 125\n",
      "2024-01-03 20:35:46.385896: Current learning rate: 0.00887\n",
      "2024-01-03 20:37:51.722730: train_loss -0.8938\n",
      "2024-01-03 20:37:51.730725: val_loss -0.8116\n",
      "2024-01-03 20:37:51.736725: Pseudo dice [0.9291, 0.9252, 0.9341]\n",
      "2024-01-03 20:37:51.741723: Epoch time: 125.36 s\n",
      "2024-01-03 20:37:53.097836: \n",
      "2024-01-03 20:37:53.103780: Epoch 126\n",
      "2024-01-03 20:37:53.107836: Current learning rate: 0.00886\n",
      "2024-01-03 20:39:58.329614: train_loss -0.8956\n",
      "2024-01-03 20:39:58.336610: val_loss -0.8106\n",
      "2024-01-03 20:39:58.343619: Pseudo dice [0.925, 0.9254, 0.9325]\n",
      "2024-01-03 20:39:58.349614: Epoch time: 125.23 s\n",
      "2024-01-03 20:39:59.471231: \n",
      "2024-01-03 20:39:59.479312: Epoch 127\n",
      "2024-01-03 20:39:59.483289: Current learning rate: 0.00885\n",
      "2024-01-03 20:42:04.681004: train_loss -0.8642\n",
      "2024-01-03 20:42:04.689008: val_loss -0.807\n",
      "2024-01-03 20:42:04.695003: Pseudo dice [0.9261, 0.9234, 0.9367]\n",
      "2024-01-03 20:42:04.700006: Epoch time: 125.21 s\n",
      "2024-01-03 20:42:05.897557: \n",
      "2024-01-03 20:42:05.903666: Epoch 128\n",
      "2024-01-03 20:42:05.907740: Current learning rate: 0.00884\n",
      "2024-01-03 20:44:10.858975: train_loss -0.8743\n",
      "2024-01-03 20:44:10.870975: val_loss -0.8003\n",
      "2024-01-03 20:44:10.879973: Pseudo dice [0.9263, 0.9202, 0.9292]\n",
      "2024-01-03 20:44:10.884974: Epoch time: 124.96 s\n",
      "2024-01-03 20:44:12.089692: \n",
      "2024-01-03 20:44:12.097765: Epoch 129\n",
      "2024-01-03 20:44:12.101764: Current learning rate: 0.00883\n",
      "2024-01-03 20:46:17.213835: train_loss -0.883\n",
      "2024-01-03 20:46:17.222368: val_loss -0.8054\n",
      "2024-01-03 20:46:17.228374: Pseudo dice [0.9257, 0.9224, 0.9311]\n",
      "2024-01-03 20:46:17.233374: Epoch time: 125.13 s\n",
      "2024-01-03 20:46:18.405340: \n",
      "2024-01-03 20:46:18.411376: Epoch 130\n",
      "2024-01-03 20:46:18.419450: Current learning rate: 0.00882\n",
      "2024-01-03 20:48:23.141658: train_loss -0.8873\n",
      "2024-01-03 20:48:23.148652: val_loss -0.8127\n",
      "2024-01-03 20:48:23.155648: Pseudo dice [0.9233, 0.9265, 0.9344]\n",
      "2024-01-03 20:48:23.159648: Epoch time: 124.74 s\n",
      "2024-01-03 20:48:24.375823: \n",
      "2024-01-03 20:48:24.384352: Epoch 131\n",
      "2024-01-03 20:48:24.390237: Current learning rate: 0.00881\n",
      "2024-01-03 20:50:29.381928: train_loss -0.8735\n",
      "2024-01-03 20:50:29.388847: val_loss -0.8035\n",
      "2024-01-03 20:50:29.393846: Pseudo dice [0.9203, 0.9206, 0.9311]\n",
      "2024-01-03 20:50:29.399844: Epoch time: 125.01 s\n",
      "2024-01-03 20:50:30.811518: \n",
      "2024-01-03 20:50:30.816996: Epoch 132\n",
      "2024-01-03 20:50:30.821936: Current learning rate: 0.0088\n",
      "2024-01-03 20:52:35.561069: train_loss -0.8722\n",
      "2024-01-03 20:52:35.568076: val_loss -0.8121\n",
      "2024-01-03 20:52:35.576076: Pseudo dice [0.9252, 0.9245, 0.9341]\n",
      "2024-01-03 20:52:35.584076: Epoch time: 124.75 s\n",
      "2024-01-03 20:52:36.684658: \n",
      "2024-01-03 20:52:36.691658: Epoch 133\n",
      "2024-01-03 20:52:36.695658: Current learning rate: 0.00879\n",
      "2024-01-03 20:54:41.641526: train_loss -0.8816\n",
      "2024-01-03 20:54:41.650526: val_loss -0.8043\n",
      "2024-01-03 20:54:41.656526: Pseudo dice [0.9296, 0.9209, 0.9314]\n",
      "2024-01-03 20:54:41.662078: Epoch time: 124.96 s\n",
      "2024-01-03 20:54:42.769834: \n",
      "2024-01-03 20:54:42.782487: Epoch 134\n",
      "2024-01-03 20:54:42.790214: Current learning rate: 0.00879\n",
      "2024-01-03 20:56:47.390096: train_loss -0.8855\n",
      "2024-01-03 20:56:47.399104: val_loss -0.8027\n",
      "2024-01-03 20:56:47.404100: Pseudo dice [0.9294, 0.9212, 0.9321]\n",
      "2024-01-03 20:56:47.410099: Epoch time: 124.62 s\n",
      "2024-01-03 20:56:48.598085: \n",
      "2024-01-03 20:56:48.604160: Epoch 135\n",
      "2024-01-03 20:56:48.609158: Current learning rate: 0.00878\n",
      "2024-01-03 20:58:53.217233: train_loss -0.8914\n",
      "2024-01-03 20:58:53.226236: val_loss -0.8105\n",
      "2024-01-03 20:58:53.232234: Pseudo dice [0.9248, 0.9249, 0.9328]\n",
      "2024-01-03 20:58:53.239234: Epoch time: 124.62 s\n",
      "2024-01-03 20:58:54.499082: \n",
      "2024-01-03 20:58:54.509159: Epoch 136\n",
      "2024-01-03 20:58:54.514119: Current learning rate: 0.00877\n",
      "2024-01-03 21:00:59.228044: train_loss -0.8915\n",
      "2024-01-03 21:00:59.234552: val_loss -0.8065\n",
      "2024-01-03 21:00:59.240552: Pseudo dice [0.9282, 0.9212, 0.9333]\n",
      "2024-01-03 21:00:59.248551: Epoch time: 124.73 s\n",
      "2024-01-03 21:01:00.399782: \n",
      "2024-01-03 21:01:00.404782: Epoch 137\n",
      "2024-01-03 21:01:00.410681: Current learning rate: 0.00876\n",
      "2024-01-03 21:03:05.562663: train_loss -0.8925\n",
      "2024-01-03 21:03:05.569664: val_loss -0.8059\n",
      "2024-01-03 21:03:05.574673: Pseudo dice [0.9242, 0.9239, 0.9334]\n",
      "2024-01-03 21:03:05.579670: Epoch time: 125.16 s\n",
      "2024-01-03 21:03:06.890387: \n",
      "2024-01-03 21:03:06.897398: Epoch 138\n",
      "2024-01-03 21:03:06.908117: Current learning rate: 0.00875\n",
      "2024-01-03 21:05:11.893301: train_loss -0.8904\n",
      "2024-01-03 21:05:11.904383: val_loss -0.8104\n",
      "2024-01-03 21:05:11.912336: Pseudo dice [0.9249, 0.924, 0.9331]\n",
      "2024-01-03 21:05:11.915380: Epoch time: 125.0 s\n",
      "2024-01-03 21:05:13.062901: \n",
      "2024-01-03 21:05:13.070596: Epoch 139\n",
      "2024-01-03 21:05:13.074669: Current learning rate: 0.00874\n",
      "2024-01-03 21:07:17.810208: train_loss -0.8939\n",
      "2024-01-03 21:07:17.818300: val_loss -0.8137\n",
      "2024-01-03 21:07:17.827900: Pseudo dice [0.9262, 0.9252, 0.9351]\n",
      "2024-01-03 21:07:17.833893: Epoch time: 124.75 s\n",
      "2024-01-03 21:07:19.237288: \n",
      "2024-01-03 21:07:19.243551: Epoch 140\n",
      "2024-01-03 21:07:19.247613: Current learning rate: 0.00873\n",
      "2024-01-03 21:09:24.093977: train_loss -0.8936\n",
      "2024-01-03 21:09:24.102010: val_loss -0.8099\n",
      "2024-01-03 21:09:24.107008: Pseudo dice [0.9266, 0.9243, 0.9338]\n",
      "2024-01-03 21:09:24.111501: Epoch time: 124.86 s\n",
      "2024-01-03 21:09:25.411304: \n",
      "2024-01-03 21:09:25.419834: Epoch 141\n",
      "2024-01-03 21:09:25.423885: Current learning rate: 0.00872\n",
      "2024-01-03 21:11:30.208041: train_loss -0.8949\n",
      "2024-01-03 21:11:30.214041: val_loss -0.8107\n",
      "2024-01-03 21:11:30.265691: Pseudo dice [0.9261, 0.9252, 0.9328]\n",
      "2024-01-03 21:11:30.272692: Epoch time: 124.8 s\n",
      "2024-01-03 21:11:31.406193: \n",
      "2024-01-03 21:11:31.415485: Epoch 142\n",
      "2024-01-03 21:11:31.422534: Current learning rate: 0.00871\n",
      "2024-01-03 21:13:36.710933: train_loss -0.8927\n",
      "2024-01-03 21:13:36.716935: val_loss -0.8138\n",
      "2024-01-03 21:13:36.721934: Pseudo dice [0.9262, 0.9257, 0.9351]\n",
      "2024-01-03 21:13:36.726935: Epoch time: 125.3 s\n",
      "2024-01-03 21:13:37.924067: \n",
      "2024-01-03 21:13:37.930126: Epoch 143\n",
      "2024-01-03 21:13:37.935139: Current learning rate: 0.0087\n",
      "2024-01-03 21:15:43.273733: train_loss -0.8935\n",
      "2024-01-03 21:15:43.279741: val_loss -0.8092\n",
      "2024-01-03 21:15:43.284735: Pseudo dice [0.9267, 0.9224, 0.9312]\n",
      "2024-01-03 21:15:43.289742: Epoch time: 125.35 s\n",
      "2024-01-03 21:15:44.608403: \n",
      "2024-01-03 21:15:44.617419: Epoch 144\n",
      "2024-01-03 21:15:44.627425: Current learning rate: 0.00869\n",
      "2024-01-03 21:17:50.127234: train_loss -0.8908\n",
      "2024-01-03 21:17:50.135234: val_loss -0.8121\n",
      "2024-01-03 21:17:50.140235: Pseudo dice [0.9272, 0.9251, 0.9334]\n",
      "2024-01-03 21:17:50.147241: Epoch time: 125.52 s\n",
      "2024-01-03 21:17:51.335934: \n",
      "2024-01-03 21:17:51.342007: Epoch 145\n",
      "2024-01-03 21:17:51.350263: Current learning rate: 0.00868\n",
      "2024-01-03 21:19:56.787297: train_loss -0.8935\n",
      "2024-01-03 21:19:56.795297: val_loss -0.8092\n",
      "2024-01-03 21:19:56.802297: Pseudo dice [0.9288, 0.9254, 0.9341]\n",
      "2024-01-03 21:19:56.808296: Epoch time: 125.45 s\n",
      "2024-01-03 21:19:58.020095: \n",
      "2024-01-03 21:19:58.030764: Epoch 146\n",
      "2024-01-03 21:19:58.035765: Current learning rate: 0.00868\n",
      "2024-01-03 21:22:03.223103: train_loss -0.8933\n",
      "2024-01-03 21:22:03.230102: val_loss -0.8105\n",
      "2024-01-03 21:22:03.240104: Pseudo dice [0.9273, 0.926, 0.9369]\n",
      "2024-01-03 21:22:03.261900: Epoch time: 125.2 s\n",
      "2024-01-03 21:22:04.629858: \n",
      "2024-01-03 21:22:04.637184: Epoch 147\n",
      "2024-01-03 21:22:04.641603: Current learning rate: 0.00867\n",
      "2024-01-03 21:24:10.042641: train_loss -0.8932\n",
      "2024-01-03 21:24:10.053741: val_loss -0.8114\n",
      "2024-01-03 21:24:10.061741: Pseudo dice [0.9271, 0.9264, 0.9344]\n",
      "2024-01-03 21:24:10.070743: Epoch time: 125.41 s\n",
      "2024-01-03 21:24:11.281793: \n",
      "2024-01-03 21:24:11.290804: Epoch 148\n",
      "2024-01-03 21:24:11.295418: Current learning rate: 0.00866\n",
      "2024-01-03 21:26:16.498692: train_loss -0.8947\n",
      "2024-01-03 21:26:16.507210: val_loss -0.8098\n",
      "2024-01-03 21:26:16.515202: Pseudo dice [0.9261, 0.9233, 0.9329]\n",
      "2024-01-03 21:26:16.520202: Epoch time: 125.22 s\n",
      "2024-01-03 21:26:17.717731: \n",
      "2024-01-03 21:26:17.730824: Epoch 149\n",
      "2024-01-03 21:26:17.735883: Current learning rate: 0.00865\n",
      "2024-01-03 21:28:23.234675: train_loss -0.8948\n",
      "2024-01-03 21:28:23.241676: val_loss -0.8086\n",
      "2024-01-03 21:28:23.246682: Pseudo dice [0.9228, 0.9254, 0.9318]\n",
      "2024-01-03 21:28:23.250679: Epoch time: 125.52 s\n",
      "2024-01-03 21:28:24.841045: \n",
      "2024-01-03 21:28:24.847118: Epoch 150\n",
      "2024-01-03 21:28:24.851056: Current learning rate: 0.00864\n",
      "2024-01-03 21:30:30.242402: train_loss -0.8965\n",
      "2024-01-03 21:30:30.250402: val_loss -0.8033\n",
      "2024-01-03 21:30:30.256403: Pseudo dice [0.9263, 0.9225, 0.9329]\n",
      "2024-01-03 21:30:30.261402: Epoch time: 125.4 s\n",
      "2024-01-03 21:30:31.433910: \n",
      "2024-01-03 21:30:31.440269: Epoch 151\n",
      "2024-01-03 21:30:31.445354: Current learning rate: 0.00863\n",
      "2024-01-03 21:32:36.727231: train_loss -0.8948\n",
      "2024-01-03 21:32:36.733229: val_loss -0.8199\n",
      "2024-01-03 21:32:36.738228: Pseudo dice [0.9263, 0.9285, 0.9357]\n",
      "2024-01-03 21:32:36.742229: Epoch time: 125.29 s\n",
      "2024-01-03 21:32:37.885357: \n",
      "2024-01-03 21:32:37.891348: Epoch 152\n",
      "2024-01-03 21:32:37.895357: Current learning rate: 0.00862\n",
      "2024-01-03 21:34:43.348684: train_loss -0.8957\n",
      "2024-01-03 21:34:43.358200: val_loss -0.8119\n",
      "2024-01-03 21:34:43.364198: Pseudo dice [0.9237, 0.9272, 0.9363]\n",
      "2024-01-03 21:34:43.370198: Epoch time: 125.46 s\n",
      "2024-01-03 21:34:44.594361: \n",
      "2024-01-03 21:34:44.599424: Epoch 153\n",
      "2024-01-03 21:34:44.607421: Current learning rate: 0.00861\n",
      "2024-01-03 21:36:49.751914: train_loss -0.8966\n",
      "2024-01-03 21:36:49.759912: val_loss -0.8075\n",
      "2024-01-03 21:36:49.764918: Pseudo dice [0.9256, 0.9236, 0.934]\n",
      "2024-01-03 21:36:49.770911: Epoch time: 125.16 s\n",
      "2024-01-03 21:36:51.068998: \n",
      "2024-01-03 21:36:51.077936: Epoch 154\n",
      "2024-01-03 21:36:51.081944: Current learning rate: 0.0086\n",
      "2024-01-03 21:38:56.859982: train_loss -0.8921\n",
      "2024-01-03 21:38:56.868989: val_loss -0.8053\n",
      "2024-01-03 21:38:56.875803: Pseudo dice [0.9258, 0.9219, 0.9329]\n",
      "2024-01-03 21:38:56.884809: Epoch time: 125.79 s\n",
      "2024-01-03 21:38:58.384218: \n",
      "2024-01-03 21:38:58.391218: Epoch 155\n",
      "2024-01-03 21:38:58.396803: Current learning rate: 0.00859\n",
      "2024-01-03 21:41:04.127906: train_loss -0.8894\n",
      "2024-01-03 21:41:04.134907: val_loss -0.8147\n",
      "2024-01-03 21:41:04.141911: Pseudo dice [0.9223, 0.9267, 0.9341]\n",
      "2024-01-03 21:41:04.146911: Epoch time: 125.74 s\n",
      "2024-01-03 21:41:05.504784: \n",
      "2024-01-03 21:41:05.512782: Epoch 156\n",
      "2024-01-03 21:41:05.517783: Current learning rate: 0.00858\n",
      "2024-01-03 21:43:11.458201: train_loss -0.8962\n",
      "2024-01-03 21:43:11.464201: val_loss -0.8137\n",
      "2024-01-03 21:43:11.470199: Pseudo dice [0.9279, 0.9262, 0.9351]\n",
      "2024-01-03 21:43:11.475199: Epoch time: 125.96 s\n",
      "2024-01-03 21:43:12.724665: \n",
      "2024-01-03 21:43:12.733325: Epoch 157\n",
      "2024-01-03 21:43:12.738329: Current learning rate: 0.00858\n",
      "2024-01-03 21:45:20.894044: train_loss -0.8949\n",
      "2024-01-03 21:45:20.904052: val_loss -0.813\n",
      "2024-01-03 21:45:20.910054: Pseudo dice [0.9268, 0.9282, 0.9348]\n",
      "2024-01-03 21:45:20.918566: Epoch time: 128.17 s\n",
      "2024-01-03 21:45:22.070132: \n",
      "2024-01-03 21:45:22.075892: Epoch 158\n",
      "2024-01-03 21:45:22.080034: Current learning rate: 0.00857\n",
      "2024-01-03 21:47:28.949325: train_loss -0.8967\n",
      "2024-01-03 21:47:28.957324: val_loss -0.8111\n",
      "2024-01-03 21:47:28.965327: Pseudo dice [0.9239, 0.926, 0.9333]\n",
      "2024-01-03 21:47:28.970326: Epoch time: 126.88 s\n",
      "2024-01-03 21:47:30.198767: \n",
      "2024-01-03 21:47:30.206575: Epoch 159\n",
      "2024-01-03 21:47:30.211516: Current learning rate: 0.00856\n",
      "2024-01-03 21:49:36.534276: train_loss -0.8972\n",
      "2024-01-03 21:49:36.542280: val_loss -0.81\n",
      "2024-01-03 21:49:36.548280: Pseudo dice [0.9268, 0.9251, 0.9331]\n",
      "2024-01-03 21:49:36.554791: Epoch time: 126.34 s\n",
      "2024-01-03 21:49:37.893109: \n",
      "2024-01-03 21:49:37.901182: Epoch 160\n",
      "2024-01-03 21:49:37.905170: Current learning rate: 0.00855\n",
      "2024-01-03 21:51:44.442652: train_loss -0.8953\n",
      "2024-01-03 21:51:44.452161: val_loss -0.8156\n",
      "2024-01-03 21:51:44.461174: Pseudo dice [0.9242, 0.9268, 0.9347]\n",
      "2024-01-03 21:51:44.466167: Epoch time: 126.55 s\n",
      "2024-01-03 21:51:45.588803: \n",
      "2024-01-03 21:51:45.598805: Epoch 161\n",
      "2024-01-03 21:51:45.603906: Current learning rate: 0.00854\n",
      "2024-01-03 21:53:51.940545: train_loss -0.8995\n",
      "2024-01-03 21:53:51.948546: val_loss -0.8105\n",
      "2024-01-03 21:53:51.958549: Pseudo dice [0.9281, 0.9246, 0.9346]\n",
      "2024-01-03 21:53:51.986280: Epoch time: 126.35 s\n",
      "2024-01-03 21:53:53.255698: \n",
      "2024-01-03 21:53:53.261698: Epoch 162\n",
      "2024-01-03 21:53:53.266475: Current learning rate: 0.00853\n",
      "2024-01-03 21:55:58.670329: train_loss -0.8974\n",
      "2024-01-03 21:55:58.676327: val_loss -0.8142\n",
      "2024-01-03 21:55:58.681327: Pseudo dice [0.9288, 0.9272, 0.9366]\n",
      "2024-01-03 21:55:58.687753: Epoch time: 125.42 s\n",
      "2024-01-03 21:56:00.038559: \n",
      "2024-01-03 21:56:00.045563: Epoch 163\n",
      "2024-01-03 21:56:00.051559: Current learning rate: 0.00852\n",
      "2024-01-03 21:58:05.607631: train_loss -0.8987\n",
      "2024-01-03 21:58:05.615645: val_loss -0.8143\n",
      "2024-01-03 21:58:05.622641: Pseudo dice [0.9282, 0.9279, 0.9361]\n",
      "2024-01-03 21:58:05.627633: Epoch time: 125.57 s\n",
      "2024-01-03 21:58:06.916785: \n",
      "2024-01-03 21:58:06.928377: Epoch 164\n",
      "2024-01-03 21:58:06.933470: Current learning rate: 0.00851\n",
      "2024-01-03 22:00:12.700945: train_loss -0.8961\n",
      "2024-01-03 22:00:12.708957: val_loss -0.8094\n",
      "2024-01-03 22:00:12.715948: Pseudo dice [0.9243, 0.9247, 0.9337]\n",
      "2024-01-03 22:00:12.721951: Epoch time: 125.79 s\n",
      "2024-01-03 22:00:13.987669: \n",
      "2024-01-03 22:00:13.995671: Epoch 165\n",
      "2024-01-03 22:00:14.000683: Current learning rate: 0.0085\n",
      "2024-01-03 22:02:19.579607: train_loss -0.8963\n",
      "2024-01-03 22:02:19.586607: val_loss -0.8109\n",
      "2024-01-03 22:02:19.592607: Pseudo dice [0.9275, 0.9252, 0.934]\n",
      "2024-01-03 22:02:19.597677: Epoch time: 125.59 s\n",
      "2024-01-03 22:02:20.750900: \n",
      "2024-01-03 22:02:20.757608: Epoch 166\n",
      "2024-01-03 22:02:20.762669: Current learning rate: 0.00849\n",
      "2024-01-03 22:04:26.196297: train_loss -0.8989\n",
      "2024-01-03 22:04:26.207488: val_loss -0.8106\n",
      "2024-01-03 22:04:26.216486: Pseudo dice [0.9255, 0.9252, 0.9319]\n",
      "2024-01-03 22:04:26.221494: Epoch time: 125.45 s\n",
      "2024-01-03 22:04:27.483257: \n",
      "2024-01-03 22:04:27.495815: Epoch 167\n",
      "2024-01-03 22:04:27.508941: Current learning rate: 0.00848\n",
      "2024-01-03 22:06:32.839780: train_loss -0.898\n",
      "2024-01-03 22:06:32.852298: val_loss -0.804\n",
      "2024-01-03 22:06:32.861300: Pseudo dice [0.9257, 0.9239, 0.934]\n",
      "2024-01-03 22:06:32.870301: Epoch time: 125.36 s\n",
      "2024-01-03 22:06:34.150940: \n",
      "2024-01-03 22:06:34.158361: Epoch 168\n",
      "2024-01-03 22:06:34.162916: Current learning rate: 0.00847\n",
      "2024-01-03 22:08:39.780269: train_loss -0.8987\n",
      "2024-01-03 22:08:39.788274: val_loss -0.8119\n",
      "2024-01-03 22:08:39.793283: Pseudo dice [0.9256, 0.925, 0.9331]\n",
      "2024-01-03 22:08:39.798285: Epoch time: 125.63 s\n",
      "2024-01-03 22:08:41.109566: \n",
      "2024-01-03 22:08:41.117564: Epoch 169\n",
      "2024-01-03 22:08:41.124564: Current learning rate: 0.00847\n",
      "2024-01-03 22:10:46.795303: train_loss -0.8988\n",
      "2024-01-03 22:10:46.802304: val_loss -0.8088\n",
      "2024-01-03 22:10:46.807304: Pseudo dice [0.9235, 0.9255, 0.9351]\n",
      "2024-01-03 22:10:46.811304: Epoch time: 125.69 s\n",
      "2024-01-03 22:10:48.125190: \n",
      "2024-01-03 22:10:48.130687: Epoch 170\n",
      "2024-01-03 22:10:48.135708: Current learning rate: 0.00846\n",
      "2024-01-03 22:12:53.701920: train_loss -0.8992\n",
      "2024-01-03 22:12:53.709913: val_loss -0.8099\n",
      "2024-01-03 22:12:53.714915: Pseudo dice [0.9251, 0.9246, 0.9342]\n",
      "2024-01-03 22:12:53.719994: Epoch time: 125.58 s\n",
      "2024-01-03 22:12:54.828113: \n",
      "2024-01-03 22:12:54.837236: Epoch 171\n",
      "2024-01-03 22:12:54.843227: Current learning rate: 0.00845\n",
      "2024-01-03 22:15:00.310615: train_loss -0.8983\n",
      "2024-01-03 22:15:00.316625: val_loss -0.8087\n",
      "2024-01-03 22:15:00.322620: Pseudo dice [0.9251, 0.9262, 0.9346]\n",
      "2024-01-03 22:15:00.327538: Epoch time: 125.48 s\n",
      "2024-01-03 22:15:01.534642: \n",
      "2024-01-03 22:15:01.540887: Epoch 172\n",
      "2024-01-03 22:15:01.545908: Current learning rate: 0.00844\n",
      "2024-01-03 22:17:07.181838: train_loss -0.8991\n",
      "2024-01-03 22:17:07.191845: val_loss -0.8078\n",
      "2024-01-03 22:17:07.198845: Pseudo dice [0.9264, 0.9242, 0.9344]\n",
      "2024-01-03 22:17:07.204351: Epoch time: 125.65 s\n",
      "2024-01-03 22:17:08.434125: \n",
      "2024-01-03 22:17:08.441128: Epoch 173\n",
      "2024-01-03 22:17:08.446119: Current learning rate: 0.00843\n",
      "2024-01-03 22:19:13.795120: train_loss -0.9015\n",
      "2024-01-03 22:19:13.806099: val_loss -0.811\n",
      "2024-01-03 22:19:13.814100: Pseudo dice [0.9254, 0.9251, 0.9337]\n",
      "2024-01-03 22:19:13.821099: Epoch time: 125.36 s\n",
      "2024-01-03 22:19:15.160650: \n",
      "2024-01-03 22:19:15.165716: Epoch 174\n",
      "2024-01-03 22:19:15.170666: Current learning rate: 0.00842\n",
      "2024-01-03 22:21:21.176931: train_loss -0.8982\n",
      "2024-01-03 22:21:21.185931: val_loss -0.8071\n",
      "2024-01-03 22:21:21.191933: Pseudo dice [0.929, 0.9237, 0.9329]\n",
      "2024-01-03 22:21:21.197932: Epoch time: 126.02 s\n",
      "2024-01-03 22:21:22.422205: \n",
      "2024-01-03 22:21:22.427203: Epoch 175\n",
      "2024-01-03 22:21:22.432203: Current learning rate: 0.00841\n",
      "2024-01-03 22:23:28.072322: train_loss -0.8993\n",
      "2024-01-03 22:23:28.078323: val_loss -0.8139\n",
      "2024-01-03 22:23:28.085338: Pseudo dice [0.9269, 0.9281, 0.9367]\n",
      "2024-01-03 22:23:28.090328: Epoch time: 125.65 s\n",
      "2024-01-03 22:23:29.314690: \n",
      "2024-01-03 22:23:29.321789: Epoch 176\n",
      "2024-01-03 22:23:29.325778: Current learning rate: 0.0084\n",
      "2024-01-03 22:25:34.744592: train_loss -0.9018\n",
      "2024-01-03 22:25:34.754591: val_loss -0.8116\n",
      "2024-01-03 22:25:34.759591: Pseudo dice [0.9256, 0.9253, 0.9335]\n",
      "2024-01-03 22:25:34.763592: Epoch time: 125.43 s\n",
      "2024-01-03 22:25:35.966775: \n",
      "2024-01-03 22:25:35.975775: Epoch 177\n",
      "2024-01-03 22:25:35.980767: Current learning rate: 0.00839\n",
      "2024-01-03 22:27:41.412911: train_loss -0.9025\n",
      "2024-01-03 22:27:41.421422: val_loss -0.8075\n",
      "2024-01-03 22:27:41.430423: Pseudo dice [0.9251, 0.9237, 0.9328]\n",
      "2024-01-03 22:27:41.437420: Epoch time: 125.45 s\n",
      "2024-01-03 22:27:42.925715: \n",
      "2024-01-03 22:27:42.938729: Epoch 178\n",
      "2024-01-03 22:27:42.943650: Current learning rate: 0.00838\n",
      "2024-01-03 22:29:48.611919: train_loss -0.9011\n",
      "2024-01-03 22:29:48.622919: val_loss -0.8155\n",
      "2024-01-03 22:29:48.628918: Pseudo dice [0.9265, 0.9293, 0.9375]\n",
      "2024-01-03 22:29:48.633919: Epoch time: 125.69 s\n",
      "2024-01-03 22:29:49.746472: \n",
      "2024-01-03 22:29:49.756632: Epoch 179\n",
      "2024-01-03 22:29:49.761622: Current learning rate: 0.00837\n",
      "2024-01-03 22:31:55.553260: train_loss -0.8993\n",
      "2024-01-03 22:31:55.560260: val_loss -0.8105\n",
      "2024-01-03 22:31:55.568259: Pseudo dice [0.9225, 0.9253, 0.9331]\n",
      "2024-01-03 22:31:55.573258: Epoch time: 125.81 s\n",
      "2024-01-03 22:31:56.719764: \n",
      "2024-01-03 22:31:56.725200: Epoch 180\n",
      "2024-01-03 22:31:56.730206: Current learning rate: 0.00836\n",
      "2024-01-03 22:34:02.609037: train_loss -0.9002\n",
      "2024-01-03 22:34:02.619036: val_loss -0.8041\n",
      "2024-01-03 22:34:02.627036: Pseudo dice [0.9235, 0.9251, 0.9358]\n",
      "2024-01-03 22:34:02.633036: Epoch time: 125.89 s\n",
      "2024-01-03 22:34:04.069653: \n",
      "2024-01-03 22:34:04.078656: Epoch 181\n",
      "2024-01-03 22:34:04.084353: Current learning rate: 0.00836\n",
      "2024-01-03 22:36:09.687252: train_loss -0.8997\n",
      "2024-01-03 22:36:09.693240: val_loss -0.8085\n",
      "2024-01-03 22:36:09.701247: Pseudo dice [0.9269, 0.9243, 0.933]\n",
      "2024-01-03 22:36:09.705239: Epoch time: 125.62 s\n",
      "2024-01-03 22:36:10.908016: \n",
      "2024-01-03 22:36:10.913011: Epoch 182\n",
      "2024-01-03 22:36:10.918007: Current learning rate: 0.00835\n",
      "2024-01-03 22:38:16.853424: train_loss -0.8986\n",
      "2024-01-03 22:38:16.863421: val_loss -0.8106\n",
      "2024-01-03 22:38:16.870422: Pseudo dice [0.9249, 0.9277, 0.9361]\n",
      "2024-01-03 22:38:16.878423: Epoch time: 125.95 s\n",
      "2024-01-03 22:38:18.090462: \n",
      "2024-01-03 22:38:18.096407: Epoch 183\n",
      "2024-01-03 22:38:18.101546: Current learning rate: 0.00834\n",
      "2024-01-03 22:40:23.868086: train_loss -0.8949\n",
      "2024-01-03 22:40:23.875095: val_loss -0.803\n",
      "2024-01-03 22:40:23.880097: Pseudo dice [0.9231, 0.9218, 0.9332]\n",
      "2024-01-03 22:40:23.885175: Epoch time: 125.78 s\n",
      "2024-01-03 22:40:25.056434: \n",
      "2024-01-03 22:40:25.065494: Epoch 184\n",
      "2024-01-03 22:40:25.069901: Current learning rate: 0.00833\n",
      "2024-01-03 22:42:30.841108: train_loss -0.8945\n",
      "2024-01-03 22:42:30.849108: val_loss -0.81\n",
      "2024-01-03 22:42:30.855120: Pseudo dice [0.9251, 0.9276, 0.9365]\n",
      "2024-01-03 22:42:30.860112: Epoch time: 125.79 s\n",
      "2024-01-03 22:42:32.294986: \n",
      "2024-01-03 22:42:32.304692: Epoch 185\n",
      "2024-01-03 22:42:32.310679: Current learning rate: 0.00832\n",
      "2024-01-03 22:44:37.935327: train_loss -0.9\n",
      "2024-01-03 22:44:37.943314: val_loss -0.8112\n",
      "2024-01-03 22:44:37.953315: Pseudo dice [0.9256, 0.9272, 0.9352]\n",
      "2024-01-03 22:44:37.962315: Epoch time: 125.64 s\n",
      "2024-01-03 22:44:39.072105: \n",
      "2024-01-03 22:44:39.084176: Epoch 186\n",
      "2024-01-03 22:44:39.088835: Current learning rate: 0.00831\n",
      "2024-01-03 22:46:44.977481: train_loss -0.8986\n",
      "2024-01-03 22:46:44.983475: val_loss -0.8145\n",
      "2024-01-03 22:46:44.989474: Pseudo dice [0.9262, 0.9282, 0.9362]\n",
      "2024-01-03 22:46:44.995473: Epoch time: 125.91 s\n",
      "2024-01-03 22:46:46.226836: \n",
      "2024-01-03 22:46:46.231836: Epoch 187\n",
      "2024-01-03 22:46:46.235836: Current learning rate: 0.0083\n",
      "2024-01-03 22:48:51.865310: train_loss -0.9018\n",
      "2024-01-03 22:48:51.872308: val_loss -0.8204\n",
      "2024-01-03 22:48:51.877309: Pseudo dice [0.9246, 0.9308, 0.9365]\n",
      "2024-01-03 22:48:51.882320: Epoch time: 125.64 s\n",
      "2024-01-03 22:48:53.002506: \n",
      "2024-01-03 22:48:53.009497: Epoch 188\n",
      "2024-01-03 22:48:53.013505: Current learning rate: 0.00829\n",
      "2024-01-03 22:50:58.287409: train_loss -0.9011\n",
      "2024-01-03 22:50:58.293407: val_loss -0.816\n",
      "2024-01-03 22:50:58.301411: Pseudo dice [0.9271, 0.9263, 0.9321]\n",
      "2024-01-03 22:50:58.309413: Epoch time: 125.29 s\n",
      "2024-01-03 22:50:59.467735: \n",
      "2024-01-03 22:50:59.474745: Epoch 189\n",
      "2024-01-03 22:50:59.482792: Current learning rate: 0.00828\n",
      "2024-01-03 22:53:04.963608: train_loss -0.9034\n",
      "2024-01-03 22:53:04.970116: val_loss -0.8054\n",
      "2024-01-03 22:53:04.976118: Pseudo dice [0.9233, 0.9257, 0.935]\n",
      "2024-01-03 22:53:04.981118: Epoch time: 125.5 s\n",
      "2024-01-03 22:53:06.039033: \n",
      "2024-01-03 22:53:06.044037: Epoch 190\n",
      "2024-01-03 22:53:06.048816: Current learning rate: 0.00827\n",
      "2024-01-03 22:55:11.763510: train_loss -0.9016\n",
      "2024-01-03 22:55:11.770433: val_loss -0.8122\n",
      "2024-01-03 22:55:11.775510: Pseudo dice [0.9265, 0.9265, 0.9336]\n",
      "2024-01-03 22:55:11.781518: Epoch time: 125.73 s\n",
      "2024-01-03 22:55:13.008006: \n",
      "2024-01-03 22:55:13.012304: Epoch 191\n",
      "2024-01-03 22:55:13.017349: Current learning rate: 0.00826\n",
      "2024-01-03 22:57:18.855637: train_loss -0.9015\n",
      "2024-01-03 22:57:18.864637: val_loss -0.8142\n",
      "2024-01-03 22:57:18.871638: Pseudo dice [0.9261, 0.9277, 0.9343]\n",
      "2024-01-03 22:57:18.878638: Epoch time: 125.85 s\n",
      "2024-01-03 22:57:20.382217: \n",
      "2024-01-03 22:57:20.390217: Epoch 192\n",
      "2024-01-03 22:57:20.395218: Current learning rate: 0.00825\n",
      "2024-01-03 22:59:25.821115: train_loss -0.9015\n",
      "2024-01-03 22:59:25.827627: val_loss -0.8176\n",
      "2024-01-03 22:59:25.836633: Pseudo dice [0.9225, 0.9286, 0.9362]\n",
      "2024-01-03 22:59:25.845629: Epoch time: 125.44 s\n",
      "2024-01-03 22:59:27.176851: \n",
      "2024-01-03 22:59:27.183004: Epoch 193\n",
      "2024-01-03 22:59:27.187082: Current learning rate: 0.00824\n",
      "2024-01-03 23:01:32.714794: train_loss -0.9009\n",
      "2024-01-03 23:01:32.722785: val_loss -0.8121\n",
      "2024-01-03 23:01:32.727795: Pseudo dice [0.9238, 0.9275, 0.9366]\n",
      "2024-01-03 23:01:32.732792: Epoch time: 125.54 s\n",
      "2024-01-03 23:01:33.922420: \n",
      "2024-01-03 23:01:33.930408: Epoch 194\n",
      "2024-01-03 23:01:33.941570: Current learning rate: 0.00824\n",
      "2024-01-03 23:03:39.333404: train_loss -0.9023\n",
      "2024-01-03 23:03:39.339403: val_loss -0.8076\n",
      "2024-01-03 23:03:39.345403: Pseudo dice [0.9206, 0.9271, 0.9339]\n",
      "2024-01-03 23:03:39.351403: Epoch time: 125.41 s\n",
      "2024-01-03 23:03:40.467566: \n",
      "2024-01-03 23:03:40.477845: Epoch 195\n",
      "2024-01-03 23:03:40.482912: Current learning rate: 0.00823\n",
      "2024-01-03 23:05:46.265705: train_loss -0.9012\n",
      "2024-01-03 23:05:46.272713: val_loss -0.8133\n",
      "2024-01-03 23:05:46.280713: Pseudo dice [0.9218, 0.9267, 0.9375]\n",
      "2024-01-03 23:05:46.285712: Epoch time: 125.8 s\n",
      "2024-01-03 23:05:47.426597: \n",
      "2024-01-03 23:05:47.432403: Epoch 196\n",
      "2024-01-03 23:05:47.441418: Current learning rate: 0.00822\n",
      "2024-01-03 23:07:53.048311: train_loss -0.8987\n",
      "2024-01-03 23:07:53.055310: val_loss -0.8109\n",
      "2024-01-03 23:07:53.061311: Pseudo dice [0.9256, 0.9287, 0.937]\n",
      "2024-01-03 23:07:53.067310: Epoch time: 125.62 s\n",
      "2024-01-03 23:07:54.228866: \n",
      "2024-01-03 23:07:54.234847: Epoch 197\n",
      "2024-01-03 23:07:54.242874: Current learning rate: 0.00821\n",
      "2024-01-03 23:09:59.576583: train_loss -0.9037\n",
      "2024-01-03 23:09:59.585094: val_loss -0.8125\n",
      "2024-01-03 23:09:59.590095: Pseudo dice [0.9257, 0.9272, 0.9363]\n",
      "2024-01-03 23:09:59.594093: Epoch time: 125.35 s\n",
      "2024-01-03 23:09:59.598092: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2024-01-03 23:10:01.018293: \n",
      "2024-01-03 23:10:01.024291: Epoch 198\n",
      "2024-01-03 23:10:01.029289: Current learning rate: 0.0082\n",
      "2024-01-03 23:12:06.733423: train_loss -0.9021\n",
      "2024-01-03 23:12:06.741423: val_loss -0.8109\n",
      "2024-01-03 23:12:06.748438: Pseudo dice [0.9243, 0.9276, 0.9365]\n",
      "2024-01-03 23:12:06.756435: Epoch time: 125.72 s\n",
      "2024-01-03 23:12:06.760426: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2024-01-03 23:12:08.235497: \n",
      "2024-01-03 23:12:08.246590: Epoch 199\n",
      "2024-01-03 23:12:08.252741: Current learning rate: 0.00819\n",
      "2024-01-03 23:14:14.199877: train_loss -0.9017\n",
      "2024-01-03 23:14:14.206875: val_loss -0.8086\n",
      "2024-01-03 23:14:14.212873: Pseudo dice [0.9229, 0.9248, 0.9358]\n",
      "2024-01-03 23:14:14.218873: Epoch time: 125.97 s\n",
      "2024-01-03 23:14:16.013830: \n",
      "2024-01-03 23:14:16.019830: Epoch 200\n",
      "2024-01-03 23:14:16.024829: Current learning rate: 0.00818\n",
      "2024-01-03 23:16:21.931294: train_loss -0.901\n",
      "2024-01-03 23:16:21.939295: val_loss -0.807\n",
      "2024-01-03 23:16:21.945298: Pseudo dice [0.9268, 0.9261, 0.9353]\n",
      "2024-01-03 23:16:21.950296: Epoch time: 125.92 s\n",
      "2024-01-03 23:16:23.159005: \n",
      "2024-01-03 23:16:23.165000: Epoch 201\n",
      "2024-01-03 23:16:23.169001: Current learning rate: 0.00817\n",
      "2024-01-03 23:18:29.058157: train_loss -0.9005\n",
      "2024-01-03 23:18:29.069157: val_loss -0.8032\n",
      "2024-01-03 23:18:29.076159: Pseudo dice [0.9244, 0.9259, 0.9376]\n",
      "2024-01-03 23:18:29.083157: Epoch time: 125.9 s\n",
      "2024-01-03 23:18:30.555159: \n",
      "2024-01-03 23:18:30.561167: Epoch 202\n",
      "2024-01-03 23:18:30.565184: Current learning rate: 0.00816\n",
      "2024-01-03 23:20:36.191102: train_loss -0.9023\n",
      "2024-01-03 23:20:36.198104: val_loss -0.8111\n",
      "2024-01-03 23:20:36.203104: Pseudo dice [0.9233, 0.9268, 0.9371]\n",
      "2024-01-03 23:20:36.209104: Epoch time: 125.64 s\n",
      "2024-01-03 23:20:37.549488: \n",
      "2024-01-03 23:20:37.559026: Epoch 203\n",
      "2024-01-03 23:20:37.566025: Current learning rate: 0.00815\n",
      "2024-01-03 23:22:43.143547: train_loss -0.9016\n",
      "2024-01-03 23:22:43.150547: val_loss -0.8004\n",
      "2024-01-03 23:22:43.155546: Pseudo dice [0.9216, 0.9255, 0.9324]\n",
      "2024-01-03 23:22:43.159546: Epoch time: 125.6 s\n",
      "2024-01-03 23:22:44.426820: \n",
      "2024-01-03 23:22:44.432954: Epoch 204\n",
      "2024-01-03 23:22:44.438638: Current learning rate: 0.00814\n",
      "2024-01-03 23:24:49.818203: train_loss -0.9045\n",
      "2024-01-03 23:24:49.825202: val_loss -0.801\n",
      "2024-01-03 23:24:49.830203: Pseudo dice [0.925, 0.9266, 0.9342]\n",
      "2024-01-03 23:24:49.835202: Epoch time: 125.39 s\n",
      "2024-01-03 23:24:51.230438: \n",
      "2024-01-03 23:24:51.236812: Epoch 205\n",
      "2024-01-03 23:24:51.243865: Current learning rate: 0.00813\n",
      "2024-01-03 23:26:56.738522: train_loss -0.9039\n",
      "2024-01-03 23:26:56.745535: val_loss -0.8066\n",
      "2024-01-03 23:26:56.750540: Pseudo dice [0.9214, 0.9254, 0.9343]\n",
      "2024-01-03 23:26:56.756534: Epoch time: 125.51 s\n",
      "2024-01-03 23:26:57.783376: \n",
      "2024-01-03 23:26:57.788380: Epoch 206\n",
      "2024-01-03 23:26:57.796443: Current learning rate: 0.00813\n",
      "2024-01-03 23:29:03.745953: train_loss -0.9011\n",
      "2024-01-03 23:29:03.752954: val_loss -0.8096\n",
      "2024-01-03 23:29:03.757954: Pseudo dice [0.9244, 0.9258, 0.9349]\n",
      "2024-01-03 23:29:03.762955: Epoch time: 125.96 s\n",
      "2024-01-03 23:29:05.050918: \n",
      "2024-01-03 23:29:05.059916: Epoch 207\n",
      "2024-01-03 23:29:05.064882: Current learning rate: 0.00812\n",
      "2024-01-03 23:31:10.821318: train_loss -0.9004\n",
      "2024-01-03 23:31:10.829321: val_loss -0.8146\n",
      "2024-01-03 23:31:10.837320: Pseudo dice [0.9224, 0.9273, 0.9347]\n",
      "2024-01-03 23:31:10.844321: Epoch time: 125.77 s\n",
      "2024-01-03 23:31:12.142271: \n",
      "2024-01-03 23:31:12.149489: Epoch 208\n",
      "2024-01-03 23:31:12.153542: Current learning rate: 0.00811\n",
      "2024-01-03 23:33:18.021080: train_loss -0.9018\n",
      "2024-01-03 23:33:18.032168: val_loss -0.8042\n",
      "2024-01-03 23:33:18.039167: Pseudo dice [0.925, 0.9253, 0.9352]\n",
      "2024-01-03 23:33:18.044676: Epoch time: 125.88 s\n",
      "2024-01-03 23:33:19.122469: \n",
      "2024-01-03 23:33:19.128469: Epoch 209\n",
      "2024-01-03 23:33:19.132587: Current learning rate: 0.0081\n",
      "2024-01-03 23:35:24.864839: train_loss -0.9005\n",
      "2024-01-03 23:35:24.873706: val_loss -0.8116\n",
      "2024-01-03 23:35:24.879707: Pseudo dice [0.9256, 0.9259, 0.9351]\n",
      "2024-01-03 23:35:24.885706: Epoch time: 125.74 s\n",
      "2024-01-03 23:35:26.029312: \n",
      "2024-01-03 23:35:26.039318: Epoch 210\n",
      "2024-01-03 23:35:26.044247: Current learning rate: 0.00809\n",
      "2024-01-03 23:37:32.027850: train_loss -0.9016\n",
      "2024-01-03 23:37:32.036853: val_loss -0.8075\n",
      "2024-01-03 23:37:32.042852: Pseudo dice [0.9245, 0.9231, 0.9331]\n",
      "2024-01-03 23:37:32.048851: Epoch time: 126.0 s\n",
      "2024-01-03 23:37:33.263410: \n",
      "2024-01-03 23:37:33.271424: Epoch 211\n",
      "2024-01-03 23:37:33.277522: Current learning rate: 0.00808\n",
      "2024-01-03 23:39:38.962351: train_loss -0.902\n",
      "2024-01-03 23:39:38.968349: val_loss -0.8089\n",
      "2024-01-03 23:39:38.974348: Pseudo dice [0.9221, 0.9247, 0.933]\n",
      "2024-01-03 23:39:38.979349: Epoch time: 125.7 s\n",
      "2024-01-03 23:39:40.103736: \n",
      "2024-01-03 23:39:40.110729: Epoch 212\n",
      "2024-01-03 23:39:40.115673: Current learning rate: 0.00807\n",
      "2024-01-03 23:41:46.157444: train_loss -0.9034\n",
      "2024-01-03 23:41:46.164445: val_loss -0.8146\n",
      "2024-01-03 23:41:46.169444: Pseudo dice [0.924, 0.9267, 0.933]\n",
      "2024-01-03 23:41:46.174993: Epoch time: 126.05 s\n",
      "2024-01-03 23:41:47.316736: \n",
      "2024-01-03 23:41:47.324756: Epoch 213\n",
      "2024-01-03 23:41:47.328747: Current learning rate: 0.00806\n",
      "2024-01-03 23:43:52.940801: train_loss -0.9042\n",
      "2024-01-03 23:43:52.946795: val_loss -0.8136\n",
      "2024-01-03 23:43:52.952790: Pseudo dice [0.9193, 0.9255, 0.9338]\n",
      "2024-01-03 23:43:52.957793: Epoch time: 125.63 s\n",
      "2024-01-03 23:43:54.149065: \n",
      "2024-01-03 23:43:54.157038: Epoch 214\n",
      "2024-01-03 23:43:54.162062: Current learning rate: 0.00805\n",
      "2024-01-03 23:45:59.555095: train_loss -0.9049\n",
      "2024-01-03 23:45:59.562094: val_loss -0.8132\n",
      "2024-01-03 23:45:59.567100: Pseudo dice [0.9199, 0.9302, 0.9363]\n",
      "2024-01-03 23:45:59.572098: Epoch time: 125.41 s\n",
      "2024-01-03 23:46:00.901507: \n",
      "2024-01-03 23:46:00.915019: Epoch 215\n",
      "2024-01-03 23:46:00.920051: Current learning rate: 0.00804\n",
      "2024-01-03 23:48:06.408430: train_loss -0.9045\n",
      "2024-01-03 23:48:06.418421: val_loss -0.8123\n",
      "2024-01-03 23:48:06.427422: Pseudo dice [0.9253, 0.9295, 0.9364]\n",
      "2024-01-03 23:48:06.433422: Epoch time: 125.51 s\n",
      "2024-01-03 23:48:07.510351: \n",
      "2024-01-03 23:48:07.516527: Epoch 216\n",
      "2024-01-03 23:48:07.520711: Current learning rate: 0.00803\n",
      "2024-01-03 23:50:13.112676: train_loss -0.9032\n",
      "2024-01-03 23:50:13.121765: val_loss -0.8106\n",
      "2024-01-03 23:50:13.126765: Pseudo dice [0.9226, 0.9264, 0.9348]\n",
      "2024-01-03 23:50:13.130766: Epoch time: 125.6 s\n",
      "2024-01-03 23:50:14.250994: \n",
      "2024-01-03 23:50:14.256065: Epoch 217\n",
      "2024-01-03 23:50:14.262136: Current learning rate: 0.00802\n",
      "2024-01-03 23:52:19.863778: train_loss -0.905\n",
      "2024-01-03 23:52:19.870299: val_loss -0.8144\n",
      "2024-01-03 23:52:19.875301: Pseudo dice [0.9256, 0.9295, 0.9374]\n",
      "2024-01-03 23:52:19.880300: Epoch time: 125.61 s\n",
      "2024-01-03 23:52:21.047527: \n",
      "2024-01-03 23:52:21.059526: Epoch 218\n",
      "2024-01-03 23:52:21.067600: Current learning rate: 0.00801\n",
      "2024-01-03 23:54:26.925772: train_loss -0.903\n",
      "2024-01-03 23:54:26.932782: val_loss -0.8091\n",
      "2024-01-03 23:54:26.938772: Pseudo dice [0.9256, 0.9269, 0.9351]\n",
      "2024-01-03 23:54:26.943772: Epoch time: 125.88 s\n",
      "2024-01-03 23:54:27.961529: \n",
      "2024-01-03 23:54:27.966591: Epoch 219\n",
      "2024-01-03 23:54:27.971591: Current learning rate: 0.00801\n",
      "2024-01-03 23:56:33.910036: train_loss -0.9008\n",
      "2024-01-03 23:56:33.919041: val_loss -0.8112\n",
      "2024-01-03 23:56:33.924041: Pseudo dice [0.9228, 0.9269, 0.9366]\n",
      "2024-01-03 23:56:33.928550: Epoch time: 125.95 s\n",
      "2024-01-03 23:56:35.006026: \n",
      "2024-01-03 23:56:35.011787: Epoch 220\n",
      "2024-01-03 23:56:35.015424: Current learning rate: 0.008\n",
      "2024-01-03 23:58:40.610034: train_loss -0.905\n",
      "2024-01-03 23:58:40.619042: val_loss -0.805\n",
      "2024-01-03 23:58:40.628052: Pseudo dice [0.9244, 0.9272, 0.9353]\n",
      "2024-01-03 23:58:40.638032: Epoch time: 125.61 s\n",
      "2024-01-03 23:58:41.790732: \n",
      "2024-01-03 23:58:41.796731: Epoch 221\n",
      "2024-01-03 23:58:41.800268: Current learning rate: 0.00799\n",
      "2024-01-04 00:00:47.424842: train_loss -0.903\n",
      "2024-01-04 00:00:47.430845: val_loss -0.8043\n",
      "2024-01-04 00:00:47.436849: Pseudo dice [0.9243, 0.9239, 0.9326]\n",
      "2024-01-04 00:00:47.442848: Epoch time: 125.63 s\n",
      "2024-01-04 00:00:48.509291: \n",
      "2024-01-04 00:00:48.515481: Epoch 222\n",
      "2024-01-04 00:00:48.520412: Current learning rate: 0.00798\n",
      "2024-01-04 00:02:54.123990: train_loss -0.9051\n",
      "2024-01-04 00:02:54.130982: val_loss -0.8008\n",
      "2024-01-04 00:02:54.135989: Pseudo dice [0.922, 0.9237, 0.9336]\n",
      "2024-01-04 00:02:54.139980: Epoch time: 125.62 s\n",
      "2024-01-04 00:02:55.503552: \n",
      "2024-01-04 00:02:55.512643: Epoch 223\n",
      "2024-01-04 00:02:55.516640: Current learning rate: 0.00797\n",
      "2024-01-04 00:05:00.870479: train_loss -0.9064\n",
      "2024-01-04 00:05:00.877479: val_loss -0.8092\n",
      "2024-01-04 00:05:00.882479: Pseudo dice [0.927, 0.9286, 0.9361]\n",
      "2024-01-04 00:05:00.887488: Epoch time: 125.37 s\n",
      "2024-01-04 00:05:02.166589: \n",
      "2024-01-04 00:05:02.171938: Epoch 224\n",
      "2024-01-04 00:05:02.176926: Current learning rate: 0.00796\n",
      "2024-01-04 00:07:07.759102: train_loss -0.9029\n",
      "2024-01-04 00:07:07.766102: val_loss -0.8094\n",
      "2024-01-04 00:07:07.771102: Pseudo dice [0.9263, 0.9266, 0.9363]\n",
      "2024-01-04 00:07:07.776102: Epoch time: 125.59 s\n",
      "2024-01-04 00:07:08.982492: \n",
      "2024-01-04 00:07:08.988280: Epoch 225\n",
      "2024-01-04 00:07:08.992272: Current learning rate: 0.00795\n",
      "2024-01-04 00:09:14.710886: train_loss -0.9034\n",
      "2024-01-04 00:09:14.717886: val_loss -0.8042\n",
      "2024-01-04 00:09:14.723886: Pseudo dice [0.9251, 0.9257, 0.9345]\n",
      "2024-01-04 00:09:14.730885: Epoch time: 125.73 s\n",
      "2024-01-04 00:09:15.928473: \n",
      "2024-01-04 00:09:15.934454: Epoch 226\n",
      "2024-01-04 00:09:15.939389: Current learning rate: 0.00794\n",
      "2024-01-04 00:11:21.513010: train_loss -0.9046\n",
      "2024-01-04 00:11:21.520012: val_loss -0.8086\n",
      "2024-01-04 00:11:21.527014: Pseudo dice [0.9249, 0.9251, 0.9338]\n",
      "2024-01-04 00:11:21.532012: Epoch time: 125.59 s\n",
      "2024-01-04 00:11:22.617275: \n",
      "2024-01-04 00:11:22.628129: Epoch 227\n",
      "2024-01-04 00:11:22.639119: Current learning rate: 0.00793\n",
      "2024-01-04 00:13:28.321356: train_loss -0.9015\n",
      "2024-01-04 00:13:28.327356: val_loss -0.8097\n",
      "2024-01-04 00:13:28.333357: Pseudo dice [0.9212, 0.9282, 0.9354]\n",
      "2024-01-04 00:13:28.338362: Epoch time: 125.71 s\n",
      "2024-01-04 00:13:29.451949: \n",
      "2024-01-04 00:13:29.460693: Epoch 228\n",
      "2024-01-04 00:13:29.466917: Current learning rate: 0.00792\n",
      "2024-01-04 00:15:34.977274: train_loss -0.905\n",
      "2024-01-04 00:15:34.983274: val_loss -0.8113\n",
      "2024-01-04 00:15:34.988274: Pseudo dice [0.9232, 0.9273, 0.9358]\n",
      "2024-01-04 00:15:34.996274: Epoch time: 125.53 s\n",
      "2024-01-04 00:15:36.155448: \n",
      "2024-01-04 00:15:36.160455: Epoch 229\n",
      "2024-01-04 00:15:36.165381: Current learning rate: 0.00791\n",
      "2024-01-04 00:17:41.930389: train_loss -0.9029\n",
      "2024-01-04 00:17:41.936417: val_loss -0.8062\n",
      "2024-01-04 00:17:41.941410: Pseudo dice [0.9234, 0.9264, 0.9341]\n",
      "2024-01-04 00:17:41.946405: Epoch time: 125.78 s\n",
      "2024-01-04 00:17:43.047250: \n",
      "2024-01-04 00:17:43.057198: Epoch 230\n",
      "2024-01-04 00:17:43.062201: Current learning rate: 0.0079\n",
      "2024-01-04 00:19:48.782140: train_loss -0.9055\n",
      "2024-01-04 00:19:48.790145: val_loss -0.8098\n",
      "2024-01-04 00:19:48.796144: Pseudo dice [0.9218, 0.926, 0.9338]\n",
      "2024-01-04 00:19:48.801662: Epoch time: 125.74 s\n",
      "2024-01-04 00:19:50.189880: \n",
      "2024-01-04 00:19:50.197922: Epoch 231\n",
      "2024-01-04 00:19:50.206594: Current learning rate: 0.00789\n",
      "2024-01-04 00:21:56.001736: train_loss -0.9032\n",
      "2024-01-04 00:21:56.010736: val_loss -0.8152\n",
      "2024-01-04 00:21:56.017738: Pseudo dice [0.922, 0.9277, 0.936]\n",
      "2024-01-04 00:21:56.022736: Epoch time: 125.81 s\n",
      "2024-01-04 00:21:57.348729: \n",
      "2024-01-04 00:21:57.353737: Epoch 232\n",
      "2024-01-04 00:21:57.357737: Current learning rate: 0.00789\n",
      "2024-01-04 00:24:03.012496: train_loss -0.9027\n",
      "2024-01-04 00:24:03.022496: val_loss -0.811\n",
      "2024-01-04 00:24:03.029497: Pseudo dice [0.9208, 0.924, 0.9317]\n",
      "2024-01-04 00:24:03.035496: Epoch time: 125.67 s\n",
      "2024-01-04 00:24:04.204708: \n",
      "2024-01-04 00:24:04.213639: Epoch 233\n",
      "2024-01-04 00:24:04.218690: Current learning rate: 0.00788\n",
      "2024-01-04 00:26:09.751416: train_loss -0.9064\n",
      "2024-01-04 00:26:09.757421: val_loss -0.8112\n",
      "2024-01-04 00:26:09.763939: Pseudo dice [0.9241, 0.9277, 0.9364]\n",
      "2024-01-04 00:26:09.769939: Epoch time: 125.55 s\n",
      "2024-01-04 00:26:10.889308: \n",
      "2024-01-04 00:26:10.895554: Epoch 234\n",
      "2024-01-04 00:26:10.901642: Current learning rate: 0.00787\n",
      "2024-01-04 00:28:16.361506: train_loss -0.9039\n",
      "2024-01-04 00:28:16.367511: val_loss -0.8074\n",
      "2024-01-04 00:28:16.373522: Pseudo dice [0.9242, 0.9283, 0.9353]\n",
      "2024-01-04 00:28:16.378508: Epoch time: 125.47 s\n",
      "2024-01-04 00:28:17.424650: \n",
      "2024-01-04 00:28:17.433591: Epoch 235\n",
      "2024-01-04 00:28:17.438162: Current learning rate: 0.00786\n",
      "2024-01-04 00:30:22.805551: train_loss -0.9074\n",
      "2024-01-04 00:30:22.812070: val_loss -0.807\n",
      "2024-01-04 00:30:22.818068: Pseudo dice [0.9241, 0.9246, 0.9348]\n",
      "2024-01-04 00:30:22.824589: Epoch time: 125.38 s\n",
      "2024-01-04 00:30:24.028785: \n",
      "2024-01-04 00:30:24.037786: Epoch 236\n",
      "2024-01-04 00:30:24.042727: Current learning rate: 0.00785\n",
      "2024-01-04 00:32:29.711343: train_loss -0.9047\n",
      "2024-01-04 00:32:29.716357: val_loss -0.8097\n",
      "2024-01-04 00:32:29.721341: Pseudo dice [0.9193, 0.9284, 0.939]\n",
      "2024-01-04 00:32:29.726341: Epoch time: 125.68 s\n",
      "2024-01-04 00:32:30.832795: \n",
      "2024-01-04 00:32:30.838798: Epoch 237\n",
      "2024-01-04 00:32:30.843862: Current learning rate: 0.00784\n",
      "2024-01-04 00:34:36.334423: train_loss -0.9072\n",
      "2024-01-04 00:34:36.344927: val_loss -0.8051\n",
      "2024-01-04 00:34:36.351933: Pseudo dice [0.9232, 0.9256, 0.9347]\n",
      "2024-01-04 00:34:36.358931: Epoch time: 125.5 s\n",
      "2024-01-04 00:34:37.618394: \n",
      "2024-01-04 00:34:37.628604: Epoch 238\n",
      "2024-01-04 00:34:37.633705: Current learning rate: 0.00783\n",
      "2024-01-04 00:36:43.263132: train_loss -0.9059\n",
      "2024-01-04 00:36:43.269130: val_loss -0.8105\n",
      "2024-01-04 00:36:43.274873: Pseudo dice [0.9237, 0.9279, 0.9361]\n",
      "2024-01-04 00:36:43.279874: Epoch time: 125.65 s\n",
      "2024-01-04 00:36:44.551932: \n",
      "2024-01-04 00:36:44.557912: Epoch 239\n",
      "2024-01-04 00:36:44.561868: Current learning rate: 0.00782\n",
      "2024-01-04 00:38:49.864114: train_loss -0.9075\n",
      "2024-01-04 00:38:49.873842: val_loss -0.8064\n",
      "2024-01-04 00:38:49.881840: Pseudo dice [0.9218, 0.9265, 0.9351]\n",
      "2024-01-04 00:38:49.889839: Epoch time: 125.31 s\n",
      "2024-01-04 00:38:51.078681: \n",
      "2024-01-04 00:38:51.084762: Epoch 240\n",
      "2024-01-04 00:38:51.089750: Current learning rate: 0.00781\n",
      "2024-01-04 00:40:56.627480: train_loss -0.9065\n",
      "2024-01-04 00:40:56.633478: val_loss -0.8112\n",
      "2024-01-04 00:40:56.638478: Pseudo dice [0.9217, 0.9306, 0.9392]\n",
      "2024-01-04 00:40:56.644478: Epoch time: 125.55 s\n",
      "2024-01-04 00:40:57.781850: \n",
      "2024-01-04 00:40:57.787570: Epoch 241\n",
      "2024-01-04 00:40:57.794574: Current learning rate: 0.0078\n",
      "2024-01-04 00:43:03.321061: train_loss -0.9081\n",
      "2024-01-04 00:43:03.328052: val_loss -0.7997\n",
      "2024-01-04 00:43:03.333050: Pseudo dice [0.9255, 0.9231, 0.934]\n",
      "2024-01-04 00:43:03.337058: Epoch time: 125.54 s\n",
      "2024-01-04 00:43:04.442702: \n",
      "2024-01-04 00:43:04.448982: Epoch 242\n",
      "2024-01-04 00:43:04.457050: Current learning rate: 0.00779\n",
      "2024-01-04 00:45:10.223141: train_loss -0.905\n",
      "2024-01-04 00:45:10.231133: val_loss -0.8081\n",
      "2024-01-04 00:45:10.236135: Pseudo dice [0.9208, 0.9262, 0.9349]\n",
      "2024-01-04 00:45:10.241134: Epoch time: 125.78 s\n",
      "2024-01-04 00:45:11.329466: \n",
      "2024-01-04 00:45:11.335469: Epoch 243\n",
      "2024-01-04 00:45:11.342827: Current learning rate: 0.00778\n",
      "2024-01-04 00:47:17.474460: train_loss -0.904\n",
      "2024-01-04 00:47:17.484461: val_loss -0.8102\n",
      "2024-01-04 00:47:17.494460: Pseudo dice [0.9243, 0.9279, 0.9361]\n",
      "2024-01-04 00:47:17.503462: Epoch time: 126.15 s\n",
      "2024-01-04 00:47:18.743930: \n",
      "2024-01-04 00:47:18.749652: Epoch 244\n",
      "2024-01-04 00:47:18.759121: Current learning rate: 0.00777\n",
      "2024-01-04 00:49:24.245430: train_loss -0.9045\n",
      "2024-01-04 00:49:24.252420: val_loss -0.8041\n",
      "2024-01-04 00:49:24.258421: Pseudo dice [0.9238, 0.928, 0.9372]\n",
      "2024-01-04 00:49:24.265420: Epoch time: 125.5 s\n",
      "2024-01-04 00:49:25.296325: \n",
      "2024-01-04 00:49:25.302080: Epoch 245\n",
      "2024-01-04 00:49:25.309159: Current learning rate: 0.00777\n",
      "2024-01-04 00:51:31.263395: train_loss -0.9043\n",
      "2024-01-04 00:51:31.270394: val_loss -0.8166\n",
      "2024-01-04 00:51:31.277910: Pseudo dice [0.9228, 0.9294, 0.937]\n",
      "2024-01-04 00:51:31.283919: Epoch time: 125.97 s\n",
      "2024-01-04 00:51:32.414661: \n",
      "2024-01-04 00:51:32.419719: Epoch 246\n",
      "2024-01-04 00:51:32.424721: Current learning rate: 0.00776\n",
      "2024-01-04 00:53:38.089868: train_loss -0.904\n",
      "2024-01-04 00:53:38.098860: val_loss -0.8086\n",
      "2024-01-04 00:53:38.103860: Pseudo dice [0.925, 0.926, 0.9351]\n",
      "2024-01-04 00:53:38.108866: Epoch time: 125.68 s\n",
      "2024-01-04 00:53:39.452569: \n",
      "2024-01-04 00:53:39.457570: Epoch 247\n",
      "2024-01-04 00:53:39.462563: Current learning rate: 0.00775\n",
      "2024-01-04 00:55:44.974811: train_loss -0.9051\n",
      "2024-01-04 00:55:44.981807: val_loss -0.816\n",
      "2024-01-04 00:55:44.986811: Pseudo dice [0.9265, 0.9308, 0.9383]\n",
      "2024-01-04 00:55:44.990811: Epoch time: 125.53 s\n",
      "2024-01-04 00:55:46.066478: \n",
      "2024-01-04 00:55:46.073538: Epoch 248\n",
      "2024-01-04 00:55:46.082479: Current learning rate: 0.00774\n",
      "2024-01-04 00:57:51.482864: train_loss -0.9039\n",
      "2024-01-04 00:57:51.490861: val_loss -0.8053\n",
      "2024-01-04 00:57:51.497861: Pseudo dice [0.9259, 0.9271, 0.9346]\n",
      "2024-01-04 00:57:51.503862: Epoch time: 125.42 s\n",
      "2024-01-04 00:57:52.616275: \n",
      "2024-01-04 00:57:52.627873: Epoch 249\n",
      "2024-01-04 00:57:52.635859: Current learning rate: 0.00773\n",
      "2024-01-04 00:59:58.180163: train_loss -0.9002\n",
      "2024-01-04 00:59:58.189163: val_loss -0.8076\n",
      "2024-01-04 00:59:58.196165: Pseudo dice [0.9251, 0.9245, 0.9344]\n",
      "2024-01-04 00:59:58.201165: Epoch time: 125.56 s\n",
      "2024-01-04 00:59:59.736961: \n",
      "2024-01-04 00:59:59.742962: Epoch 250\n",
      "2024-01-04 00:59:59.748962: Current learning rate: 0.00772\n",
      "2024-01-04 01:02:05.432925: train_loss -0.9001\n",
      "2024-01-04 01:02:05.440927: val_loss -0.7984\n",
      "2024-01-04 01:02:05.446926: Pseudo dice [0.9183, 0.9234, 0.9327]\n",
      "2024-01-04 01:02:05.454926: Epoch time: 125.7 s\n",
      "2024-01-04 01:02:06.542508: \n",
      "2024-01-04 01:02:06.548732: Epoch 251\n",
      "2024-01-04 01:02:06.553750: Current learning rate: 0.00771\n",
      "2024-01-04 01:04:12.453300: train_loss -0.9011\n",
      "2024-01-04 01:04:12.463300: val_loss -0.8068\n",
      "2024-01-04 01:04:12.470301: Pseudo dice [0.9227, 0.9272, 0.9339]\n",
      "2024-01-04 01:04:12.475299: Epoch time: 125.91 s\n",
      "2024-01-04 01:04:13.858763: \n",
      "2024-01-04 01:04:13.864762: Epoch 252\n",
      "2024-01-04 01:04:13.872763: Current learning rate: 0.0077\n",
      "2024-01-04 01:06:19.278612: train_loss -0.9028\n",
      "2024-01-04 01:06:19.286610: val_loss -0.803\n",
      "2024-01-04 01:06:19.290610: Pseudo dice [0.9265, 0.9247, 0.9329]\n",
      "2024-01-04 01:06:19.295610: Epoch time: 125.42 s\n",
      "2024-01-04 01:06:20.396219: \n",
      "2024-01-04 01:06:20.403577: Epoch 253\n",
      "2024-01-04 01:06:20.408577: Current learning rate: 0.00769\n",
      "2024-01-04 01:08:26.195744: train_loss -0.9039\n",
      "2024-01-04 01:08:26.202744: val_loss -0.7631\n",
      "2024-01-04 01:08:26.207743: Pseudo dice [0.9112, 0.9156, 0.9222]\n",
      "2024-01-04 01:08:26.213744: Epoch time: 125.8 s\n",
      "2024-01-04 01:08:27.365423: \n",
      "2024-01-04 01:08:27.373429: Epoch 254\n",
      "2024-01-04 01:08:27.378407: Current learning rate: 0.00768\n",
      "2024-01-04 01:10:32.677862: train_loss -0.9018\n",
      "2024-01-04 01:10:32.683862: val_loss -0.8146\n",
      "2024-01-04 01:10:32.689870: Pseudo dice [0.9236, 0.9263, 0.9349]\n",
      "2024-01-04 01:10:32.695864: Epoch time: 125.31 s\n",
      "2024-01-04 01:10:34.041167: \n",
      "2024-01-04 01:10:34.048106: Epoch 255\n",
      "2024-01-04 01:10:34.052761: Current learning rate: 0.00767\n",
      "2024-01-04 01:12:39.605559: train_loss -0.9067\n",
      "2024-01-04 01:12:39.613564: val_loss -0.8097\n",
      "2024-01-04 01:12:39.620560: Pseudo dice [0.924, 0.9277, 0.9379]\n",
      "2024-01-04 01:12:39.627562: Epoch time: 125.57 s\n",
      "2024-01-04 01:12:40.851199: \n",
      "2024-01-04 01:12:40.856269: Epoch 256\n",
      "2024-01-04 01:12:40.864275: Current learning rate: 0.00766\n",
      "2024-01-04 01:14:46.361887: train_loss -0.9043\n",
      "2024-01-04 01:14:46.368886: val_loss -0.81\n",
      "2024-01-04 01:14:46.373967: Pseudo dice [0.924, 0.9286, 0.938]\n",
      "2024-01-04 01:14:46.379894: Epoch time: 125.51 s\n",
      "2024-01-04 01:14:47.518746: \n",
      "2024-01-04 01:14:47.525812: Epoch 257\n",
      "2024-01-04 01:14:47.534473: Current learning rate: 0.00765\n",
      "2024-01-04 01:16:53.525271: train_loss -0.903\n",
      "2024-01-04 01:16:53.531269: val_loss -0.8015\n",
      "2024-01-04 01:16:53.538266: Pseudo dice [0.9211, 0.9237, 0.9316]\n",
      "2024-01-04 01:16:53.543266: Epoch time: 126.01 s\n",
      "2024-01-04 01:16:54.636506: \n",
      "2024-01-04 01:16:54.644491: Epoch 258\n",
      "2024-01-04 01:16:54.648501: Current learning rate: 0.00764\n",
      "2024-01-04 01:19:00.113394: train_loss -0.9066\n",
      "2024-01-04 01:19:00.122397: val_loss -0.8107\n",
      "2024-01-04 01:19:00.129396: Pseudo dice [0.9239, 0.9267, 0.9338]\n",
      "2024-01-04 01:19:00.138394: Epoch time: 125.48 s\n",
      "2024-01-04 01:19:01.267056: \n",
      "2024-01-04 01:19:01.273056: Epoch 259\n",
      "2024-01-04 01:19:01.278056: Current learning rate: 0.00764\n",
      "2024-01-04 01:21:06.861737: train_loss -0.9042\n",
      "2024-01-04 01:21:06.868737: val_loss -0.8079\n",
      "2024-01-04 01:21:06.873737: Pseudo dice [0.9213, 0.9276, 0.936]\n",
      "2024-01-04 01:21:06.878737: Epoch time: 125.6 s\n",
      "2024-01-04 01:21:07.963979: \n",
      "2024-01-04 01:21:07.972716: Epoch 260\n",
      "2024-01-04 01:21:07.978715: Current learning rate: 0.00763\n",
      "2024-01-04 01:23:13.590808: train_loss -0.9072\n",
      "2024-01-04 01:23:13.596802: val_loss -0.8139\n",
      "2024-01-04 01:23:13.601811: Pseudo dice [0.9205, 0.9286, 0.9359]\n",
      "2024-01-04 01:23:13.608810: Epoch time: 125.63 s\n",
      "2024-01-04 01:23:14.688509: \n",
      "2024-01-04 01:23:14.700514: Epoch 261\n",
      "2024-01-04 01:23:14.705570: Current learning rate: 0.00762\n",
      "2024-01-04 01:25:20.439560: train_loss -0.9061\n",
      "2024-01-04 01:25:20.445551: val_loss -0.8025\n",
      "2024-01-04 01:25:20.452551: Pseudo dice [0.9242, 0.9264, 0.9346]\n",
      "2024-01-04 01:25:20.459551: Epoch time: 125.75 s\n",
      "2024-01-04 01:25:21.658235: \n",
      "2024-01-04 01:25:21.666594: Epoch 262\n",
      "2024-01-04 01:25:21.671019: Current learning rate: 0.00761\n",
      "2024-01-04 01:27:27.332884: train_loss -0.9044\n",
      "2024-01-04 01:27:27.338883: val_loss -0.8024\n",
      "2024-01-04 01:27:27.344512: Pseudo dice [0.9228, 0.9244, 0.9336]\n",
      "2024-01-04 01:27:27.350512: Epoch time: 125.68 s\n",
      "2024-01-04 01:27:28.665258: \n",
      "2024-01-04 01:27:28.672412: Epoch 263\n",
      "2024-01-04 01:27:28.676850: Current learning rate: 0.0076\n",
      "2024-01-04 01:29:34.494707: train_loss -0.9063\n",
      "2024-01-04 01:29:34.503708: val_loss -0.8087\n",
      "2024-01-04 01:29:34.508708: Pseudo dice [0.9278, 0.9275, 0.936]\n",
      "2024-01-04 01:29:34.513710: Epoch time: 125.83 s\n",
      "2024-01-04 01:29:35.613894: \n",
      "2024-01-04 01:29:35.619894: Epoch 264\n",
      "2024-01-04 01:29:35.624902: Current learning rate: 0.00759\n",
      "2024-01-04 01:31:41.059193: train_loss -0.9077\n",
      "2024-01-04 01:31:41.072192: val_loss -0.7956\n",
      "2024-01-04 01:31:41.082194: Pseudo dice [0.922, 0.9259, 0.9353]\n",
      "2024-01-04 01:31:41.090195: Epoch time: 125.45 s\n",
      "2024-01-04 01:31:42.561629: \n",
      "2024-01-04 01:31:42.568568: Epoch 265\n",
      "2024-01-04 01:31:42.574576: Current learning rate: 0.00758\n",
      "2024-01-04 01:33:48.149471: train_loss -0.9044\n",
      "2024-01-04 01:33:48.155475: val_loss -0.8182\n",
      "2024-01-04 01:33:48.162477: Pseudo dice [0.9199, 0.9324, 0.9409]\n",
      "2024-01-04 01:33:48.167468: Epoch time: 125.59 s\n",
      "2024-01-04 01:33:49.378345: \n",
      "2024-01-04 01:33:49.385420: Epoch 266\n",
      "2024-01-04 01:33:49.396880: Current learning rate: 0.00757\n",
      "2024-01-04 01:35:54.990707: train_loss -0.9061\n",
      "2024-01-04 01:35:54.996696: val_loss -0.8035\n",
      "2024-01-04 01:35:55.001696: Pseudo dice [0.922, 0.929, 0.9372]\n",
      "2024-01-04 01:35:55.006785: Epoch time: 125.61 s\n",
      "2024-01-04 01:35:56.142469: \n",
      "2024-01-04 01:35:56.150100: Epoch 267\n",
      "2024-01-04 01:35:56.155415: Current learning rate: 0.00756\n",
      "2024-01-04 01:38:02.239993: train_loss -0.9079\n",
      "2024-01-04 01:38:02.250502: val_loss -0.8141\n",
      "2024-01-04 01:38:02.257503: Pseudo dice [0.9255, 0.9283, 0.9365]\n",
      "2024-01-04 01:38:02.265503: Epoch time: 126.1 s\n",
      "2024-01-04 01:38:03.568283: \n",
      "2024-01-04 01:38:03.575346: Epoch 268\n",
      "2024-01-04 01:38:03.579345: Current learning rate: 0.00755\n",
      "2024-01-04 01:40:09.391475: train_loss -0.9064\n",
      "2024-01-04 01:40:09.397473: val_loss -0.8047\n",
      "2024-01-04 01:40:09.402483: Pseudo dice [0.9247, 0.9263, 0.9347]\n",
      "2024-01-04 01:40:09.407476: Epoch time: 125.82 s\n",
      "2024-01-04 01:40:10.485549: \n",
      "2024-01-04 01:40:10.496522: Epoch 269\n",
      "2024-01-04 01:40:10.501452: Current learning rate: 0.00754\n",
      "2024-01-04 01:42:16.222021: train_loss -0.9052\n",
      "2024-01-04 01:42:16.230022: val_loss -0.8155\n",
      "2024-01-04 01:42:16.237533: Pseudo dice [0.924, 0.9314, 0.939]\n",
      "2024-01-04 01:42:16.241532: Epoch time: 125.74 s\n",
      "2024-01-04 01:42:17.478590: \n",
      "2024-01-04 01:42:17.484592: Epoch 270\n",
      "2024-01-04 01:42:17.490596: Current learning rate: 0.00753\n",
      "2024-01-04 01:44:23.108289: train_loss -0.9066\n",
      "2024-01-04 01:44:23.116292: val_loss -0.805\n",
      "2024-01-04 01:44:23.121290: Pseudo dice [0.9234, 0.9255, 0.9337]\n",
      "2024-01-04 01:44:23.127290: Epoch time: 125.63 s\n",
      "2024-01-04 01:44:24.553365: \n",
      "2024-01-04 01:44:24.559520: Epoch 271\n",
      "2024-01-04 01:44:24.564503: Current learning rate: 0.00752\n",
      "2024-01-04 01:46:30.274356: train_loss -0.9079\n",
      "2024-01-04 01:46:30.280358: val_loss -0.8131\n",
      "2024-01-04 01:46:30.285357: Pseudo dice [0.9254, 0.9294, 0.9357]\n",
      "2024-01-04 01:46:30.291356: Epoch time: 125.72 s\n",
      "2024-01-04 01:46:31.481045: \n",
      "2024-01-04 01:46:31.487024: Epoch 272\n",
      "2024-01-04 01:46:31.496022: Current learning rate: 0.00751\n",
      "2024-01-04 01:48:37.166468: train_loss -0.9066\n",
      "2024-01-04 01:48:37.174982: val_loss -0.8031\n",
      "2024-01-04 01:48:37.181983: Pseudo dice [0.9184, 0.9275, 0.9377]\n",
      "2024-01-04 01:48:37.190983: Epoch time: 125.69 s\n",
      "2024-01-04 01:48:38.338123: \n",
      "2024-01-04 01:48:38.348165: Epoch 273\n",
      "2024-01-04 01:48:38.357800: Current learning rate: 0.00751\n",
      "2024-01-04 01:50:44.004188: train_loss -0.9082\n",
      "2024-01-04 01:50:44.012206: val_loss -0.8058\n",
      "2024-01-04 01:50:44.019205: Pseudo dice [0.9249, 0.9275, 0.9349]\n",
      "2024-01-04 01:50:44.024204: Epoch time: 125.67 s\n",
      "2024-01-04 01:50:45.163297: \n",
      "2024-01-04 01:50:45.173625: Epoch 274\n",
      "2024-01-04 01:50:45.178661: Current learning rate: 0.0075\n",
      "2024-01-04 01:52:50.712730: train_loss -0.9082\n",
      "2024-01-04 01:52:50.722728: val_loss -0.805\n",
      "2024-01-04 01:52:50.728728: Pseudo dice [0.9233, 0.9275, 0.936]\n",
      "2024-01-04 01:52:50.736728: Epoch time: 125.55 s\n",
      "2024-01-04 01:52:51.925033: \n",
      "2024-01-04 01:52:51.931491: Epoch 275\n",
      "2024-01-04 01:52:51.937572: Current learning rate: 0.00749\n",
      "2024-01-04 01:54:57.505509: train_loss -0.907\n",
      "2024-01-04 01:54:57.512510: val_loss -0.8134\n",
      "2024-01-04 01:54:57.517509: Pseudo dice [0.9233, 0.9276, 0.9381]\n",
      "2024-01-04 01:54:57.523523: Epoch time: 125.58 s\n",
      "2024-01-04 01:54:58.712389: \n",
      "2024-01-04 01:54:58.717389: Epoch 276\n",
      "2024-01-04 01:54:58.722324: Current learning rate: 0.00748\n",
      "2024-01-04 01:57:04.262879: train_loss -0.9086\n",
      "2024-01-04 01:57:04.275875: val_loss -0.8089\n",
      "2024-01-04 01:57:04.284381: Pseudo dice [0.9215, 0.9273, 0.9362]\n",
      "2024-01-04 01:57:04.289381: Epoch time: 125.55 s\n",
      "2024-01-04 01:57:05.495772: \n",
      "2024-01-04 01:57:05.503766: Epoch 277\n",
      "2024-01-04 01:57:05.508781: Current learning rate: 0.00747\n",
      "2024-01-04 01:59:11.246332: train_loss -0.9092\n",
      "2024-01-04 01:59:11.255333: val_loss -0.8125\n",
      "2024-01-04 01:59:11.262342: Pseudo dice [0.9208, 0.9309, 0.9377]\n",
      "2024-01-04 01:59:11.268339: Epoch time: 125.75 s\n",
      "2024-01-04 01:59:12.527079: \n",
      "2024-01-04 01:59:12.538290: Epoch 278\n",
      "2024-01-04 01:59:12.547223: Current learning rate: 0.00746\n",
      "2024-01-04 02:01:18.042772: train_loss -0.9111\n",
      "2024-01-04 02:01:18.051771: val_loss -0.8057\n",
      "2024-01-04 02:01:18.057775: Pseudo dice [0.924, 0.9281, 0.9373]\n",
      "2024-01-04 02:01:18.062770: Epoch time: 125.52 s\n",
      "2024-01-04 02:01:19.387551: \n",
      "2024-01-04 02:01:19.392546: Epoch 279\n",
      "2024-01-04 02:01:19.397313: Current learning rate: 0.00745\n",
      "2024-01-04 02:03:25.080732: train_loss -0.9067\n",
      "2024-01-04 02:03:25.086732: val_loss -0.8095\n",
      "2024-01-04 02:03:25.091732: Pseudo dice [0.9239, 0.9291, 0.9379]\n",
      "2024-01-04 02:03:25.097734: Epoch time: 125.7 s\n",
      "2024-01-04 02:03:25.102738: Yayy! New best EMA pseudo Dice: 0.9292\n",
      "2024-01-04 02:03:26.618914: \n",
      "2024-01-04 02:03:26.628371: Epoch 280\n",
      "2024-01-04 02:03:26.632947: Current learning rate: 0.00744\n",
      "2024-01-04 02:05:32.113345: train_loss -0.908\n",
      "2024-01-04 02:05:32.119337: val_loss -0.813\n",
      "2024-01-04 02:05:32.124338: Pseudo dice [0.9223, 0.9292, 0.9378]\n",
      "2024-01-04 02:05:32.128338: Epoch time: 125.5 s\n",
      "2024-01-04 02:05:32.132338: Yayy! New best EMA pseudo Dice: 0.9292\n",
      "2024-01-04 02:05:33.537322: \n",
      "2024-01-04 02:05:33.542861: Epoch 281\n",
      "2024-01-04 02:05:33.547811: Current learning rate: 0.00743\n",
      "2024-01-04 02:07:39.056233: train_loss -0.9068\n",
      "2024-01-04 02:07:39.064241: val_loss -0.8068\n",
      "2024-01-04 02:07:39.072233: Pseudo dice [0.9243, 0.9278, 0.9357]\n",
      "2024-01-04 02:07:39.078240: Epoch time: 125.52 s\n",
      "2024-01-04 02:07:39.084232: Yayy! New best EMA pseudo Dice: 0.9292\n",
      "2024-01-04 02:07:40.504551: \n",
      "2024-01-04 02:07:40.512564: Epoch 282\n",
      "2024-01-04 02:07:40.517556: Current learning rate: 0.00742\n",
      "2024-01-04 02:09:46.252120: train_loss -0.9091\n",
      "2024-01-04 02:09:46.258629: val_loss -0.8095\n",
      "2024-01-04 02:09:46.265630: Pseudo dice [0.9241, 0.9281, 0.9364]\n",
      "2024-01-04 02:09:46.271629: Epoch time: 125.75 s\n",
      "2024-01-04 02:09:46.276632: Yayy! New best EMA pseudo Dice: 0.9293\n",
      "2024-01-04 02:09:47.670822: \n",
      "2024-01-04 02:09:47.676451: Epoch 283\n",
      "2024-01-04 02:09:47.681474: Current learning rate: 0.00741\n",
      "2024-01-04 02:11:53.605355: train_loss -0.9053\n",
      "2024-01-04 02:11:53.612346: val_loss -0.8019\n",
      "2024-01-04 02:11:53.618353: Pseudo dice [0.9242, 0.9292, 0.937]\n",
      "2024-01-04 02:11:53.623351: Epoch time: 125.94 s\n",
      "2024-01-04 02:11:53.628359: Yayy! New best EMA pseudo Dice: 0.9293\n",
      "2024-01-04 02:11:55.162000: \n",
      "2024-01-04 02:11:55.174943: Epoch 284\n",
      "2024-01-04 02:11:55.179948: Current learning rate: 0.0074\n",
      "2024-01-04 02:14:00.878572: train_loss -0.9098\n",
      "2024-01-04 02:14:00.888572: val_loss -0.8151\n",
      "2024-01-04 02:14:00.894563: Pseudo dice [0.9237, 0.931, 0.9394]\n",
      "2024-01-04 02:14:00.899564: Epoch time: 125.72 s\n",
      "2024-01-04 02:14:00.904563: Yayy! New best EMA pseudo Dice: 0.9295\n",
      "2024-01-04 02:14:02.327841: \n",
      "2024-01-04 02:14:02.340403: Epoch 285\n",
      "2024-01-04 02:14:02.345314: Current learning rate: 0.00739\n",
      "2024-01-04 02:16:07.949803: train_loss -0.9072\n",
      "2024-01-04 02:16:07.955803: val_loss -0.8158\n",
      "2024-01-04 02:16:07.960803: Pseudo dice [0.9198, 0.928, 0.9354]\n",
      "2024-01-04 02:16:07.964810: Epoch time: 125.62 s\n",
      "2024-01-04 02:16:09.137564: \n",
      "2024-01-04 02:16:09.141978: Epoch 286\n",
      "2024-01-04 02:16:09.147036: Current learning rate: 0.00738\n",
      "2024-01-04 02:18:14.916003: train_loss -0.9096\n",
      "2024-01-04 02:18:14.926008: val_loss -0.8065\n",
      "2024-01-04 02:18:14.936003: Pseudo dice [0.9234, 0.928, 0.9364]\n",
      "2024-01-04 02:18:14.943014: Epoch time: 125.78 s\n",
      "2024-01-04 02:18:16.159316: \n",
      "2024-01-04 02:18:16.168229: Epoch 287\n",
      "2024-01-04 02:18:16.182997: Current learning rate: 0.00738\n",
      "2024-01-04 02:20:22.360003: train_loss -0.9067\n",
      "2024-01-04 02:20:22.369514: val_loss -0.8038\n",
      "2024-01-04 02:20:22.377546: Pseudo dice [0.9217, 0.9265, 0.9355]\n",
      "2024-01-04 02:20:22.382509: Epoch time: 126.2 s\n",
      "2024-01-04 02:20:23.475160: \n",
      "2024-01-04 02:20:23.481147: Epoch 288\n",
      "2024-01-04 02:20:23.487660: Current learning rate: 0.00737\n",
      "2024-01-04 02:22:29.254415: train_loss -0.9082\n",
      "2024-01-04 02:22:29.264577: val_loss -0.8033\n",
      "2024-01-04 02:22:29.272577: Pseudo dice [0.9219, 0.9262, 0.9359]\n",
      "2024-01-04 02:22:29.279575: Epoch time: 125.78 s\n",
      "2024-01-04 02:22:30.461118: \n",
      "2024-01-04 02:22:30.466702: Epoch 289\n",
      "2024-01-04 02:22:30.471781: Current learning rate: 0.00736\n",
      "2024-01-04 02:24:36.594815: train_loss -0.9068\n",
      "2024-01-04 02:24:36.604847: val_loss -0.8015\n",
      "2024-01-04 02:24:36.609813: Pseudo dice [0.9262, 0.9269, 0.9366]\n",
      "2024-01-04 02:24:36.614815: Epoch time: 126.14 s\n",
      "2024-01-04 02:24:37.714072: \n",
      "2024-01-04 02:24:37.725072: Epoch 290\n",
      "2024-01-04 02:24:37.729080: Current learning rate: 0.00735\n",
      "2024-01-04 02:26:43.487629: train_loss -0.9086\n",
      "2024-01-04 02:26:43.496629: val_loss -0.8106\n",
      "2024-01-04 02:26:43.503630: Pseudo dice [0.9265, 0.9299, 0.9376]\n",
      "2024-01-04 02:26:43.508628: Epoch time: 125.77 s\n",
      "2024-01-04 02:26:44.670650: \n",
      "2024-01-04 02:26:44.676768: Epoch 291\n",
      "2024-01-04 02:26:44.681854: Current learning rate: 0.00734\n",
      "2024-01-04 02:28:50.867074: train_loss -0.9063\n",
      "2024-01-04 02:28:50.875063: val_loss -0.8017\n",
      "2024-01-04 02:28:50.879059: Pseudo dice [0.9245, 0.9268, 0.9341]\n",
      "2024-01-04 02:28:50.884060: Epoch time: 126.2 s\n",
      "2024-01-04 02:28:52.076746: \n",
      "2024-01-04 02:28:52.086746: Epoch 292\n",
      "2024-01-04 02:28:52.095815: Current learning rate: 0.00733\n",
      "2024-01-04 02:30:57.664888: train_loss -0.9085\n",
      "2024-01-04 02:30:57.672888: val_loss -0.8111\n",
      "2024-01-04 02:30:57.680888: Pseudo dice [0.9244, 0.9306, 0.9392]\n",
      "2024-01-04 02:30:57.687888: Epoch time: 125.59 s\n",
      "2024-01-04 02:30:58.860554: \n",
      "2024-01-04 02:30:58.865858: Epoch 293\n",
      "2024-01-04 02:30:58.873869: Current learning rate: 0.00732\n",
      "2024-01-04 02:33:04.714115: train_loss -0.9092\n",
      "2024-01-04 02:33:04.723616: val_loss -0.8113\n",
      "2024-01-04 02:33:04.728622: Pseudo dice [0.92, 0.9283, 0.9376]\n",
      "2024-01-04 02:33:04.733620: Epoch time: 125.85 s\n",
      "2024-01-04 02:33:05.842932: \n",
      "2024-01-04 02:33:05.855446: Epoch 294\n",
      "2024-01-04 02:33:05.860425: Current learning rate: 0.00731\n",
      "2024-01-04 02:35:11.765808: train_loss -0.909\n",
      "2024-01-04 02:35:11.773824: val_loss -0.8025\n",
      "2024-01-04 02:35:11.782353: Pseudo dice [0.9243, 0.9273, 0.9364]\n",
      "2024-01-04 02:35:11.787346: Epoch time: 125.93 s\n",
      "2024-01-04 02:35:12.964590: \n",
      "2024-01-04 02:35:12.980482: Epoch 295\n",
      "2024-01-04 02:35:12.985031: Current learning rate: 0.0073\n",
      "2024-01-04 02:37:19.446566: train_loss -0.9068\n",
      "2024-01-04 02:37:19.454567: val_loss -0.8017\n",
      "2024-01-04 02:37:19.466568: Pseudo dice [0.924, 0.9291, 0.9376]\n",
      "2024-01-04 02:37:19.476080: Epoch time: 126.48 s\n",
      "2024-01-04 02:37:21.098867: \n",
      "2024-01-04 02:37:21.105814: Epoch 296\n",
      "2024-01-04 02:37:21.110808: Current learning rate: 0.00729\n",
      "2024-01-04 02:39:26.766004: train_loss -0.9047\n",
      "2024-01-04 02:39:26.775005: val_loss -0.8005\n",
      "2024-01-04 02:39:26.780995: Pseudo dice [0.9147, 0.9256, 0.9333]\n",
      "2024-01-04 02:39:26.784997: Epoch time: 125.67 s\n",
      "2024-01-04 02:39:27.880539: \n",
      "2024-01-04 02:39:27.885597: Epoch 297\n",
      "2024-01-04 02:39:27.890595: Current learning rate: 0.00728\n",
      "2024-01-04 02:41:33.783437: train_loss -0.9074\n",
      "2024-01-04 02:41:33.792450: val_loss -0.8056\n",
      "2024-01-04 02:41:33.799437: Pseudo dice [0.9215, 0.926, 0.9344]\n",
      "2024-01-04 02:41:33.806437: Epoch time: 125.9 s\n",
      "2024-01-04 02:41:34.966288: \n",
      "2024-01-04 02:41:34.971837: Epoch 298\n",
      "2024-01-04 02:41:34.976241: Current learning rate: 0.00727\n",
      "2024-01-04 02:43:40.789494: train_loss -0.9074\n",
      "2024-01-04 02:43:40.797002: val_loss -0.8118\n",
      "2024-01-04 02:43:40.805005: Pseudo dice [0.9269, 0.9297, 0.9356]\n",
      "2024-01-04 02:43:40.814003: Epoch time: 125.82 s\n",
      "2024-01-04 02:43:42.088303: \n",
      "2024-01-04 02:43:42.100499: Epoch 299\n",
      "2024-01-04 02:43:42.108451: Current learning rate: 0.00726\n",
      "2024-01-04 02:45:47.811425: train_loss -0.9082\n",
      "2024-01-04 02:45:47.818443: val_loss -0.804\n",
      "2024-01-04 02:45:47.825425: Pseudo dice [0.924, 0.9269, 0.9349]\n",
      "2024-01-04 02:45:47.830424: Epoch time: 125.72 s\n",
      "2024-01-04 02:45:49.279355: \n",
      "2024-01-04 02:45:49.284844: Epoch 300\n",
      "2024-01-04 02:45:49.291845: Current learning rate: 0.00725\n",
      "2024-01-04 02:47:55.164106: train_loss -0.9068\n",
      "2024-01-04 02:47:55.170107: val_loss -0.8082\n",
      "2024-01-04 02:47:55.176108: Pseudo dice [0.9246, 0.9285, 0.937]\n",
      "2024-01-04 02:47:55.184109: Epoch time: 125.89 s\n",
      "2024-01-04 02:47:56.293972: \n",
      "2024-01-04 02:47:56.301437: Epoch 301\n",
      "2024-01-04 02:47:56.305524: Current learning rate: 0.00724\n",
      "2024-01-04 02:50:01.785912: train_loss -0.9092\n",
      "2024-01-04 02:50:01.792938: val_loss -0.8112\n",
      "2024-01-04 02:50:01.798933: Pseudo dice [0.9224, 0.9289, 0.9368]\n",
      "2024-01-04 02:50:01.804934: Epoch time: 125.49 s\n",
      "2024-01-04 02:50:03.222909: \n",
      "2024-01-04 02:50:03.228618: Epoch 302\n",
      "2024-01-04 02:50:03.232311: Current learning rate: 0.00724\n",
      "2024-01-04 02:52:09.110189: train_loss -0.9079\n",
      "2024-01-04 02:52:09.117190: val_loss -0.8058\n",
      "2024-01-04 02:52:09.122190: Pseudo dice [0.9206, 0.9299, 0.9378]\n",
      "2024-01-04 02:52:09.129190: Epoch time: 125.89 s\n",
      "2024-01-04 02:52:10.289537: \n",
      "2024-01-04 02:52:10.294882: Epoch 303\n",
      "2024-01-04 02:52:10.299221: Current learning rate: 0.00723\n",
      "2024-01-04 02:54:15.724886: train_loss -0.9111\n",
      "2024-01-04 02:54:15.732888: val_loss -0.8075\n",
      "2024-01-04 02:54:15.737886: Pseudo dice [0.9233, 0.9292, 0.9378]\n",
      "2024-01-04 02:54:15.743887: Epoch time: 125.44 s\n",
      "2024-01-04 02:54:16.912860: \n",
      "2024-01-04 02:54:16.921859: Epoch 304\n",
      "2024-01-04 02:54:16.925859: Current learning rate: 0.00722\n",
      "2024-01-04 02:56:22.617810: train_loss -0.9107\n",
      "2024-01-04 02:56:22.624815: val_loss -0.8102\n",
      "2024-01-04 02:56:22.632816: Pseudo dice [0.9234, 0.9286, 0.9378]\n",
      "2024-01-04 02:56:22.639536: Epoch time: 125.71 s\n",
      "2024-01-04 02:56:23.744870: \n",
      "2024-01-04 02:56:23.754811: Epoch 305\n",
      "2024-01-04 02:56:23.760354: Current learning rate: 0.00721\n",
      "2024-01-04 02:58:29.630608: train_loss -0.9079\n",
      "2024-01-04 02:58:29.637611: val_loss -0.8083\n",
      "2024-01-04 02:58:29.644608: Pseudo dice [0.9253, 0.928, 0.937]\n",
      "2024-01-04 02:58:29.653610: Epoch time: 125.89 s\n",
      "2024-01-04 02:58:31.028687: \n",
      "2024-01-04 02:58:31.033812: Epoch 306\n",
      "2024-01-04 02:58:31.038882: Current learning rate: 0.0072\n",
      "2024-01-04 03:00:36.827713: train_loss -0.9087\n",
      "2024-01-04 03:00:36.835715: val_loss -0.8162\n",
      "2024-01-04 03:00:36.843714: Pseudo dice [0.9178, 0.9298, 0.9379]\n",
      "2024-01-04 03:00:36.848715: Epoch time: 125.8 s\n",
      "2024-01-04 03:00:38.101467: \n",
      "2024-01-04 03:00:38.108464: Epoch 307\n",
      "2024-01-04 03:00:38.116986: Current learning rate: 0.00719\n",
      "2024-01-04 03:02:44.124681: train_loss -0.909\n",
      "2024-01-04 03:02:44.134682: val_loss -0.8067\n",
      "2024-01-04 03:02:44.140682: Pseudo dice [0.9217, 0.9262, 0.9345]\n",
      "2024-01-04 03:02:44.144681: Epoch time: 126.03 s\n",
      "2024-01-04 03:02:45.250774: \n",
      "2024-01-04 03:02:45.256715: Epoch 308\n",
      "2024-01-04 03:02:45.260715: Current learning rate: 0.00718\n",
      "2024-01-04 03:04:51.218403: train_loss -0.9054\n",
      "2024-01-04 03:04:51.228403: val_loss -0.8042\n",
      "2024-01-04 03:04:51.235913: Pseudo dice [0.9196, 0.93, 0.9385]\n",
      "2024-01-04 03:04:51.241913: Epoch time: 125.97 s\n",
      "2024-01-04 03:04:52.317427: \n",
      "2024-01-04 03:04:52.323367: Epoch 309\n",
      "2024-01-04 03:04:52.327375: Current learning rate: 0.00717\n",
      "2024-01-04 03:06:58.165057: train_loss -0.9105\n",
      "2024-01-04 03:06:58.173060: val_loss -0.8018\n",
      "2024-01-04 03:06:58.178060: Pseudo dice [0.9231, 0.9264, 0.9363]\n",
      "2024-01-04 03:06:58.182060: Epoch time: 125.85 s\n",
      "2024-01-04 03:06:59.313468: \n",
      "2024-01-04 03:06:59.320474: Epoch 310\n",
      "2024-01-04 03:06:59.328540: Current learning rate: 0.00716\n",
      "2024-01-04 03:09:05.224630: train_loss -0.9085\n",
      "2024-01-04 03:09:05.230645: val_loss -0.7979\n",
      "2024-01-04 03:09:05.235648: Pseudo dice [0.9226, 0.9288, 0.9359]\n",
      "2024-01-04 03:09:05.239647: Epoch time: 125.91 s\n",
      "2024-01-04 03:09:06.574051: \n",
      "2024-01-04 03:09:06.584055: Epoch 311\n",
      "2024-01-04 03:09:06.594132: Current learning rate: 0.00715\n",
      "2024-01-04 03:11:12.403146: train_loss -0.9073\n",
      "2024-01-04 03:11:12.414146: val_loss -0.8048\n",
      "2024-01-04 03:11:12.422148: Pseudo dice [0.9233, 0.9277, 0.9387]\n",
      "2024-01-04 03:11:12.428149: Epoch time: 125.83 s\n",
      "2024-01-04 03:11:13.587450: \n",
      "2024-01-04 03:11:13.600476: Epoch 312\n",
      "2024-01-04 03:11:13.605738: Current learning rate: 0.00714\n",
      "2024-01-04 03:13:19.260473: train_loss -0.9092\n",
      "2024-01-04 03:13:19.268971: val_loss -0.8045\n",
      "2024-01-04 03:13:19.274972: Pseudo dice [0.9254, 0.9274, 0.9375]\n",
      "2024-01-04 03:13:19.279980: Epoch time: 125.68 s\n",
      "2024-01-04 03:13:20.455481: \n",
      "2024-01-04 03:13:20.461480: Epoch 313\n",
      "2024-01-04 03:13:20.466994: Current learning rate: 0.00713\n",
      "2024-01-04 03:15:26.445569: train_loss -0.9087\n",
      "2024-01-04 03:15:26.457568: val_loss -0.8096\n",
      "2024-01-04 03:15:26.468569: Pseudo dice [0.9207, 0.9291, 0.9366]\n",
      "2024-01-04 03:15:26.478568: Epoch time: 125.99 s\n",
      "2024-01-04 03:15:27.499148: \n",
      "2024-01-04 03:15:27.506927: Epoch 314\n",
      "2024-01-04 03:15:27.510997: Current learning rate: 0.00712\n",
      "2024-01-04 03:17:33.125802: train_loss -0.9101\n",
      "2024-01-04 03:17:33.133801: val_loss -0.8057\n",
      "2024-01-04 03:17:33.139810: Pseudo dice [0.9257, 0.9269, 0.936]\n",
      "2024-01-04 03:17:33.147812: Epoch time: 125.63 s\n",
      "2024-01-04 03:17:34.415706: \n",
      "2024-01-04 03:17:34.421703: Epoch 315\n",
      "2024-01-04 03:17:34.426735: Current learning rate: 0.00711\n",
      "2024-01-04 03:19:39.988320: train_loss -0.9109\n",
      "2024-01-04 03:19:39.995319: val_loss -0.8073\n",
      "2024-01-04 03:19:40.000319: Pseudo dice [0.9262, 0.9278, 0.937]\n",
      "2024-01-04 03:19:40.004319: Epoch time: 125.57 s\n",
      "2024-01-04 03:19:41.192397: \n",
      "2024-01-04 03:19:41.198431: Epoch 316\n",
      "2024-01-04 03:19:41.206477: Current learning rate: 0.0071\n",
      "2024-01-04 03:21:46.577539: train_loss -0.9108\n",
      "2024-01-04 03:21:46.584527: val_loss -0.8089\n",
      "2024-01-04 03:21:46.589534: Pseudo dice [0.9265, 0.9286, 0.9366]\n",
      "2024-01-04 03:21:46.594529: Epoch time: 125.39 s\n",
      "2024-01-04 03:21:47.940809: \n",
      "2024-01-04 03:21:47.945812: Epoch 317\n",
      "2024-01-04 03:21:47.950825: Current learning rate: 0.0071\n",
      "2024-01-04 03:23:53.710286: train_loss -0.9125\n",
      "2024-01-04 03:23:53.716288: val_loss -0.8084\n",
      "2024-01-04 03:23:53.722288: Pseudo dice [0.9203, 0.928, 0.9375]\n",
      "2024-01-04 03:23:53.729296: Epoch time: 125.77 s\n",
      "2024-01-04 03:23:54.971819: \n",
      "2024-01-04 03:23:54.980425: Epoch 318\n",
      "2024-01-04 03:23:54.987480: Current learning rate: 0.00709\n",
      "2024-01-04 03:26:00.669193: train_loss -0.9073\n",
      "2024-01-04 03:26:00.675702: val_loss -0.8018\n",
      "2024-01-04 03:26:00.680699: Pseudo dice [0.9235, 0.9261, 0.9359]\n",
      "2024-01-04 03:26:00.685707: Epoch time: 125.7 s\n",
      "2024-01-04 03:26:01.892753: \n",
      "2024-01-04 03:26:01.905849: Epoch 319\n",
      "2024-01-04 03:26:01.910900: Current learning rate: 0.00708\n",
      "2024-01-04 03:28:07.669305: train_loss -0.9101\n",
      "2024-01-04 03:28:07.678314: val_loss -0.8099\n",
      "2024-01-04 03:28:07.686315: Pseudo dice [0.9243, 0.9271, 0.935]\n",
      "2024-01-04 03:28:07.694314: Epoch time: 125.78 s\n",
      "2024-01-04 03:28:08.876507: \n",
      "2024-01-04 03:28:08.890591: Epoch 320\n",
      "2024-01-04 03:28:08.897578: Current learning rate: 0.00707\n",
      "2024-01-04 03:30:14.868230: train_loss -0.908\n",
      "2024-01-04 03:30:14.877228: val_loss -0.8059\n",
      "2024-01-04 03:30:14.884738: Pseudo dice [0.9231, 0.9284, 0.9367]\n",
      "2024-01-04 03:30:14.889737: Epoch time: 125.99 s\n",
      "2024-01-04 03:30:16.099941: \n",
      "2024-01-04 03:30:16.108169: Epoch 321\n",
      "2024-01-04 03:30:16.114100: Current learning rate: 0.00706\n",
      "2024-01-04 03:32:22.004502: train_loss -0.9074\n",
      "2024-01-04 03:32:22.011500: val_loss -0.8023\n",
      "2024-01-04 03:32:22.016500: Pseudo dice [0.9282, 0.9299, 0.938]\n",
      "2024-01-04 03:32:22.024501: Epoch time: 125.91 s\n",
      "2024-01-04 03:32:23.124012: \n",
      "2024-01-04 03:32:23.132080: Epoch 322\n",
      "2024-01-04 03:32:23.137083: Current learning rate: 0.00705\n",
      "2024-01-04 03:34:28.630024: train_loss -0.9114\n",
      "2024-01-04 03:34:28.639024: val_loss -0.8096\n",
      "2024-01-04 03:34:28.645024: Pseudo dice [0.9254, 0.9288, 0.9372]\n",
      "2024-01-04 03:34:28.651025: Epoch time: 125.51 s\n",
      "2024-01-04 03:34:28.657024: Yayy! New best EMA pseudo Dice: 0.9296\n",
      "2024-01-04 03:34:30.126650: \n",
      "2024-01-04 03:34:30.131330: Epoch 323\n",
      "2024-01-04 03:34:30.136422: Current learning rate: 0.00704\n",
      "2024-01-04 03:36:35.733639: train_loss -0.9114\n",
      "2024-01-04 03:36:35.740639: val_loss -0.8083\n",
      "2024-01-04 03:36:35.745636: Pseudo dice [0.9217, 0.9291, 0.9367]\n",
      "2024-01-04 03:36:35.755634: Epoch time: 125.61 s\n",
      "2024-01-04 03:36:36.886457: \n",
      "2024-01-04 03:36:36.892457: Epoch 324\n",
      "2024-01-04 03:36:36.902457: Current learning rate: 0.00703\n",
      "2024-01-04 03:38:42.682013: train_loss -0.9104\n",
      "2024-01-04 03:38:42.691014: val_loss -0.808\n",
      "2024-01-04 03:38:42.697016: Pseudo dice [0.9229, 0.9295, 0.9366]\n",
      "2024-01-04 03:38:42.703014: Epoch time: 125.8 s\n",
      "2024-01-04 03:38:44.139365: \n",
      "2024-01-04 03:38:44.145772: Epoch 325\n",
      "2024-01-04 03:38:44.150787: Current learning rate: 0.00702\n",
      "2024-01-04 03:40:49.674853: train_loss -0.9085\n",
      "2024-01-04 03:40:49.680929: val_loss -0.8049\n",
      "2024-01-04 03:40:49.685860: Pseudo dice [0.9182, 0.9277, 0.9356]\n",
      "2024-01-04 03:40:49.690857: Epoch time: 125.54 s\n",
      "2024-01-04 03:40:50.877226: \n",
      "2024-01-04 03:40:50.883349: Epoch 326\n",
      "2024-01-04 03:40:50.889287: Current learning rate: 0.00701\n",
      "2024-01-04 03:42:56.516704: train_loss -0.9061\n",
      "2024-01-04 03:42:56.522720: val_loss -0.7931\n",
      "2024-01-04 03:42:56.528716: Pseudo dice [0.9198, 0.9246, 0.9307]\n",
      "2024-01-04 03:42:56.532781: Epoch time: 125.64 s\n",
      "2024-01-04 03:42:57.771056: \n",
      "2024-01-04 03:42:57.780121: Epoch 327\n",
      "2024-01-04 03:42:57.791131: Current learning rate: 0.007\n",
      "2024-01-04 03:45:03.494530: train_loss -0.9017\n",
      "2024-01-04 03:45:03.504530: val_loss -0.8124\n",
      "2024-01-04 03:45:03.512049: Pseudo dice [0.9241, 0.9306, 0.9376]\n",
      "2024-01-04 03:45:03.519056: Epoch time: 125.72 s\n",
      "2024-01-04 03:45:04.755585: \n",
      "2024-01-04 03:45:04.761568: Epoch 328\n",
      "2024-01-04 03:45:04.767156: Current learning rate: 0.00699\n",
      "2024-01-04 03:47:10.211597: train_loss -0.9061\n",
      "2024-01-04 03:47:10.217597: val_loss -0.8073\n",
      "2024-01-04 03:47:10.223595: Pseudo dice [0.9181, 0.928, 0.9379]\n",
      "2024-01-04 03:47:10.228594: Epoch time: 125.46 s\n",
      "2024-01-04 03:47:11.392991: \n",
      "2024-01-04 03:47:11.401143: Epoch 329\n",
      "2024-01-04 03:47:11.405792: Current learning rate: 0.00698\n",
      "2024-01-04 03:49:16.881250: train_loss -0.9002\n",
      "2024-01-04 03:49:16.893253: val_loss -0.8057\n",
      "2024-01-04 03:49:16.900257: Pseudo dice [0.9172, 0.9299, 0.9396]\n",
      "2024-01-04 03:49:16.910333: Epoch time: 125.49 s\n",
      "2024-01-04 03:49:18.041030: \n",
      "2024-01-04 03:49:18.050019: Epoch 330\n",
      "2024-01-04 03:49:18.054101: Current learning rate: 0.00697\n",
      "2024-01-04 03:51:23.602157: train_loss -0.8924\n",
      "2024-01-04 03:51:23.608156: val_loss -0.8098\n",
      "2024-01-04 03:51:23.614157: Pseudo dice [0.9228, 0.9288, 0.9349]\n",
      "2024-01-04 03:51:23.619157: Epoch time: 125.56 s\n",
      "2024-01-04 03:51:24.740700: \n",
      "2024-01-04 03:51:24.748888: Epoch 331\n",
      "2024-01-04 03:51:24.757271: Current learning rate: 0.00696\n",
      "2024-01-04 03:53:30.348046: train_loss -0.8999\n",
      "2024-01-04 03:53:30.355047: val_loss -0.801\n",
      "2024-01-04 03:53:30.361047: Pseudo dice [0.9258, 0.9252, 0.9352]\n",
      "2024-01-04 03:53:30.366136: Epoch time: 125.61 s\n",
      "2024-01-04 03:53:31.820783: \n",
      "2024-01-04 03:53:31.826788: Epoch 332\n",
      "2024-01-04 03:53:31.831806: Current learning rate: 0.00696\n",
      "2024-01-04 03:55:37.230314: train_loss -0.9059\n",
      "2024-01-04 03:55:37.237826: val_loss -0.7986\n",
      "2024-01-04 03:55:37.244825: Pseudo dice [0.9231, 0.9241, 0.9329]\n",
      "2024-01-04 03:55:37.255825: Epoch time: 125.41 s\n",
      "2024-01-04 03:55:38.357340: \n",
      "2024-01-04 03:55:38.366344: Epoch 333\n",
      "2024-01-04 03:55:38.370270: Current learning rate: 0.00695\n",
      "2024-01-04 03:57:44.444575: train_loss -0.9067\n",
      "2024-01-04 03:57:44.454577: val_loss -0.7978\n",
      "2024-01-04 03:57:44.461584: Pseudo dice [0.9216, 0.927, 0.9348]\n",
      "2024-01-04 03:57:44.467583: Epoch time: 126.09 s\n",
      "2024-01-04 03:57:45.664557: \n",
      "2024-01-04 03:57:45.671562: Epoch 334\n",
      "2024-01-04 03:57:45.676208: Current learning rate: 0.00694\n",
      "2024-01-04 03:59:51.323491: train_loss -0.9065\n",
      "2024-01-04 03:59:51.334777: val_loss -0.8065\n",
      "2024-01-04 03:59:51.343718: Pseudo dice [0.9226, 0.929, 0.9376]\n",
      "2024-01-04 03:59:51.351718: Epoch time: 125.66 s\n",
      "2024-01-04 03:59:52.470744: \n",
      "2024-01-04 03:59:52.478544: Epoch 335\n",
      "2024-01-04 03:59:52.484535: Current learning rate: 0.00693\n",
      "2024-01-04 04:01:58.051857: train_loss -0.9065\n",
      "2024-01-04 04:01:58.057857: val_loss -0.8017\n",
      "2024-01-04 04:01:58.063857: Pseudo dice [0.925, 0.9262, 0.9359]\n",
      "2024-01-04 04:01:58.068851: Epoch time: 125.58 s\n",
      "2024-01-04 04:01:59.358645: \n",
      "2024-01-04 04:01:59.364736: Epoch 336\n",
      "2024-01-04 04:01:59.368648: Current learning rate: 0.00692\n",
      "2024-01-04 04:04:04.965388: train_loss -0.9075\n",
      "2024-01-04 04:04:04.974381: val_loss -0.8085\n",
      "2024-01-04 04:04:04.979379: Pseudo dice [0.9224, 0.9278, 0.9366]\n",
      "2024-01-04 04:04:04.983382: Epoch time: 125.61 s\n",
      "2024-01-04 04:04:06.143467: \n",
      "2024-01-04 04:04:06.149741: Epoch 337\n",
      "2024-01-04 04:04:06.154736: Current learning rate: 0.00691\n",
      "2024-01-04 04:06:11.932565: train_loss -0.91\n",
      "2024-01-04 04:06:11.939467: val_loss -0.809\n",
      "2024-01-04 04:06:11.944543: Pseudo dice [0.9215, 0.9304, 0.9387]\n",
      "2024-01-04 04:06:11.950464: Epoch time: 125.79 s\n",
      "2024-01-04 04:06:13.195341: \n",
      "2024-01-04 04:06:13.202340: Epoch 338\n",
      "2024-01-04 04:06:13.206973: Current learning rate: 0.0069\n",
      "2024-01-04 04:08:19.058490: train_loss -0.9098\n",
      "2024-01-04 04:08:19.065490: val_loss -0.8014\n",
      "2024-01-04 04:08:19.074489: Pseudo dice [0.9201, 0.9245, 0.932]\n",
      "2024-01-04 04:08:19.079490: Epoch time: 125.86 s\n",
      "2024-01-04 04:08:20.317198: \n",
      "2024-01-04 04:08:20.326370: Epoch 339\n",
      "2024-01-04 04:08:20.334537: Current learning rate: 0.00689\n",
      "2024-01-04 04:10:26.048159: train_loss -0.9059\n",
      "2024-01-04 04:10:26.055168: val_loss -0.7986\n",
      "2024-01-04 04:10:26.061159: Pseudo dice [0.9183, 0.9254, 0.9354]\n",
      "2024-01-04 04:10:26.066158: Epoch time: 125.73 s\n",
      "2024-01-04 04:10:27.440889: \n",
      "2024-01-04 04:10:27.448887: Epoch 340\n",
      "2024-01-04 04:10:27.453887: Current learning rate: 0.00688\n",
      "2024-01-04 04:12:33.349573: train_loss -0.9026\n",
      "2024-01-04 04:12:33.355572: val_loss -0.8054\n",
      "2024-01-04 04:12:33.363573: Pseudo dice [0.9211, 0.9286, 0.9355]\n",
      "2024-01-04 04:12:33.369574: Epoch time: 125.91 s\n",
      "2024-01-04 04:12:34.469142: \n",
      "2024-01-04 04:12:34.475212: Epoch 341\n",
      "2024-01-04 04:12:34.479210: Current learning rate: 0.00687\n",
      "2024-01-04 04:14:40.189223: train_loss -0.9076\n",
      "2024-01-04 04:14:40.195223: val_loss -0.8146\n",
      "2024-01-04 04:14:40.201233: Pseudo dice [0.9248, 0.9283, 0.9367]\n",
      "2024-01-04 04:14:40.208235: Epoch time: 125.72 s\n",
      "2024-01-04 04:14:41.606079: \n",
      "2024-01-04 04:14:41.615377: Epoch 342\n",
      "2024-01-04 04:14:41.620380: Current learning rate: 0.00686\n",
      "2024-01-04 04:16:47.082617: train_loss -0.9079\n",
      "2024-01-04 04:16:47.089628: val_loss -0.8015\n",
      "2024-01-04 04:16:47.096617: Pseudo dice [0.9254, 0.9268, 0.935]\n",
      "2024-01-04 04:16:47.102615: Epoch time: 125.48 s\n",
      "2024-01-04 04:16:48.315029: \n",
      "2024-01-04 04:16:48.323105: Epoch 343\n",
      "2024-01-04 04:16:48.328104: Current learning rate: 0.00685\n",
      "2024-01-04 04:18:53.952832: train_loss -0.9051\n",
      "2024-01-04 04:18:53.958844: val_loss -0.8074\n",
      "2024-01-04 04:18:53.965842: Pseudo dice [0.9204, 0.9282, 0.9359]\n",
      "2024-01-04 04:18:53.970841: Epoch time: 125.64 s\n",
      "2024-01-04 04:18:55.123110: \n",
      "2024-01-04 04:18:55.129102: Epoch 344\n",
      "2024-01-04 04:18:55.134097: Current learning rate: 0.00684\n",
      "2024-01-04 04:21:00.752764: train_loss -0.9091\n",
      "2024-01-04 04:21:00.761276: val_loss -0.8114\n",
      "2024-01-04 04:21:00.771276: Pseudo dice [0.9197, 0.9299, 0.9377]\n",
      "2024-01-04 04:21:00.777278: Epoch time: 125.63 s\n",
      "2024-01-04 04:21:02.142651: \n",
      "2024-01-04 04:21:02.148646: Epoch 345\n",
      "2024-01-04 04:21:02.152652: Current learning rate: 0.00683\n",
      "2024-01-04 04:23:07.602398: train_loss -0.91\n",
      "2024-01-04 04:23:07.610390: val_loss -0.8058\n",
      "2024-01-04 04:23:07.615390: Pseudo dice [0.9255, 0.9289, 0.9352]\n",
      "2024-01-04 04:23:07.620391: Epoch time: 125.46 s\n",
      "2024-01-04 04:23:08.827074: \n",
      "2024-01-04 04:23:08.832052: Epoch 346\n",
      "2024-01-04 04:23:08.837051: Current learning rate: 0.00682\n",
      "2024-01-04 04:25:14.211172: train_loss -0.9109\n",
      "2024-01-04 04:25:14.220171: val_loss -0.8037\n",
      "2024-01-04 04:25:14.229194: Pseudo dice [0.9255, 0.9264, 0.9358]\n",
      "2024-01-04 04:25:14.236182: Epoch time: 125.39 s\n",
      "2024-01-04 04:25:15.439246: \n",
      "2024-01-04 04:25:15.448236: Epoch 347\n",
      "2024-01-04 04:25:15.458268: Current learning rate: 0.00681\n",
      "2024-01-04 04:27:20.844800: train_loss -0.9107\n",
      "2024-01-04 04:27:20.852799: val_loss -0.8058\n",
      "2024-01-04 04:27:20.860801: Pseudo dice [0.9236, 0.9288, 0.936]\n",
      "2024-01-04 04:27:20.868799: Epoch time: 125.41 s\n",
      "2024-01-04 04:27:22.212172: \n",
      "2024-01-04 04:27:22.219998: Epoch 348\n",
      "2024-01-04 04:27:22.227086: Current learning rate: 0.0068\n",
      "2024-01-04 04:29:27.750332: train_loss -0.9106\n",
      "2024-01-04 04:29:27.758836: val_loss -0.8061\n",
      "2024-01-04 04:29:27.763840: Pseudo dice [0.9239, 0.9269, 0.9333]\n",
      "2024-01-04 04:29:27.768840: Epoch time: 125.54 s\n",
      "2024-01-04 04:29:28.912873: \n",
      "2024-01-04 04:29:28.918874: Epoch 349\n",
      "2024-01-04 04:29:28.926924: Current learning rate: 0.0068\n",
      "2024-01-04 04:31:34.537360: train_loss -0.91\n",
      "2024-01-04 04:31:34.544362: val_loss -0.8019\n",
      "2024-01-04 04:31:34.548359: Pseudo dice [0.9179, 0.9267, 0.9342]\n",
      "2024-01-04 04:31:34.553360: Epoch time: 125.62 s\n",
      "2024-01-04 04:31:36.094959: \n",
      "2024-01-04 04:31:36.100930: Epoch 350\n",
      "2024-01-04 04:31:36.105917: Current learning rate: 0.00679\n",
      "2024-01-04 04:33:41.923734: train_loss -0.912\n",
      "2024-01-04 04:33:41.929742: val_loss -0.8048\n",
      "2024-01-04 04:33:41.936758: Pseudo dice [0.9186, 0.9268, 0.9354]\n",
      "2024-01-04 04:33:41.943734: Epoch time: 125.83 s\n",
      "2024-01-04 04:33:43.281350: \n",
      "2024-01-04 04:33:43.288096: Epoch 351\n",
      "2024-01-04 04:33:43.296033: Current learning rate: 0.00678\n",
      "2024-01-04 04:35:49.091607: train_loss -0.9103\n",
      "2024-01-04 04:35:49.102607: val_loss -0.8055\n",
      "2024-01-04 04:35:49.107607: Pseudo dice [0.9204, 0.9285, 0.9366]\n",
      "2024-01-04 04:35:49.112607: Epoch time: 125.81 s\n",
      "2024-01-04 04:35:50.274480: \n",
      "2024-01-04 04:35:50.280502: Epoch 352\n",
      "2024-01-04 04:35:50.284510: Current learning rate: 0.00677\n",
      "2024-01-04 04:37:55.959999: train_loss -0.9088\n",
      "2024-01-04 04:37:55.966000: val_loss -0.8132\n",
      "2024-01-04 04:37:55.973000: Pseudo dice [0.9238, 0.931, 0.9394]\n",
      "2024-01-04 04:37:55.979002: Epoch time: 125.69 s\n",
      "2024-01-04 04:37:57.171506: \n",
      "2024-01-04 04:37:57.177093: Epoch 353\n",
      "2024-01-04 04:37:57.181093: Current learning rate: 0.00676\n",
      "2024-01-04 04:40:03.027928: train_loss -0.9109\n",
      "2024-01-04 04:40:03.034928: val_loss -0.8075\n",
      "2024-01-04 04:40:03.039928: Pseudo dice [0.9214, 0.9277, 0.9357]\n",
      "2024-01-04 04:40:03.043928: Epoch time: 125.86 s\n",
      "2024-01-04 04:40:04.198919: \n",
      "2024-01-04 04:40:04.207102: Epoch 354\n",
      "2024-01-04 04:40:04.214175: Current learning rate: 0.00675\n",
      "2024-01-04 04:42:10.023335: train_loss -0.9122\n",
      "2024-01-04 04:42:10.029337: val_loss -0.812\n",
      "2024-01-04 04:42:10.037337: Pseudo dice [0.9249, 0.929, 0.9352]\n",
      "2024-01-04 04:42:10.042843: Epoch time: 125.83 s\n",
      "2024-01-04 04:42:11.427193: \n",
      "2024-01-04 04:42:11.433116: Epoch 355\n",
      "2024-01-04 04:42:11.441188: Current learning rate: 0.00674\n",
      "2024-01-04 04:44:16.995038: train_loss -0.9118\n",
      "2024-01-04 04:44:17.002036: val_loss -0.7987\n",
      "2024-01-04 04:44:17.008622: Pseudo dice [0.9194, 0.9296, 0.9375]\n",
      "2024-01-04 04:44:17.013631: Epoch time: 125.57 s\n",
      "2024-01-04 04:44:18.166083: \n",
      "2024-01-04 04:44:18.176148: Epoch 356\n",
      "2024-01-04 04:44:18.180161: Current learning rate: 0.00673\n",
      "2024-01-04 04:46:23.504985: train_loss -0.9127\n",
      "2024-01-04 04:46:23.513986: val_loss -0.8048\n",
      "2024-01-04 04:46:23.519987: Pseudo dice [0.9237, 0.9266, 0.9359]\n",
      "2024-01-04 04:46:23.525986: Epoch time: 125.34 s\n",
      "2024-01-04 04:46:24.739246: \n",
      "2024-01-04 04:46:24.746316: Epoch 357\n",
      "2024-01-04 04:46:24.751363: Current learning rate: 0.00672\n",
      "2024-01-04 04:48:30.641008: train_loss -0.9101\n",
      "2024-01-04 04:48:30.650018: val_loss -0.8086\n",
      "2024-01-04 04:48:30.656011: Pseudo dice [0.9183, 0.9284, 0.9378]\n",
      "2024-01-04 04:48:30.661008: Epoch time: 125.9 s\n",
      "2024-01-04 04:48:31.846101: \n",
      "2024-01-04 04:48:31.852114: Epoch 358\n",
      "2024-01-04 04:48:31.857179: Current learning rate: 0.00671\n",
      "2024-01-04 04:50:37.723458: train_loss -0.9086\n",
      "2024-01-04 04:50:37.730467: val_loss -0.8053\n",
      "2024-01-04 04:50:37.735982: Pseudo dice [0.9183, 0.9275, 0.9333]\n",
      "2024-01-04 04:50:37.741976: Epoch time: 125.88 s\n",
      "2024-01-04 04:50:38.921649: \n",
      "2024-01-04 04:50:38.927649: Epoch 359\n",
      "2024-01-04 04:50:38.931649: Current learning rate: 0.0067\n",
      "2024-01-04 04:52:44.706506: train_loss -0.9098\n",
      "2024-01-04 04:52:44.712494: val_loss -0.7991\n",
      "2024-01-04 04:52:44.718495: Pseudo dice [0.9223, 0.9289, 0.9381]\n",
      "2024-01-04 04:52:44.722999: Epoch time: 125.79 s\n",
      "2024-01-04 04:52:46.197072: \n",
      "2024-01-04 04:52:46.205138: Epoch 360\n",
      "2024-01-04 04:52:46.210142: Current learning rate: 0.00669\n",
      "2024-01-04 04:54:51.771191: train_loss -0.9111\n",
      "2024-01-04 04:54:51.779190: val_loss -0.8043\n",
      "2024-01-04 04:54:51.785199: Pseudo dice [0.9232, 0.9288, 0.9359]\n",
      "2024-01-04 04:54:51.791199: Epoch time: 125.58 s\n",
      "2024-01-04 04:54:52.957196: \n",
      "2024-01-04 04:54:52.962265: Epoch 361\n",
      "2024-01-04 04:54:52.967835: Current learning rate: 0.00668\n",
      "2024-01-04 04:56:58.611979: train_loss -0.91\n",
      "2024-01-04 04:56:58.621486: val_loss -0.7995\n",
      "2024-01-04 04:56:58.628485: Pseudo dice [0.9233, 0.927, 0.9352]\n",
      "2024-01-04 04:56:58.635487: Epoch time: 125.66 s\n",
      "2024-01-04 04:56:59.725165: \n",
      "2024-01-04 04:56:59.737167: Epoch 362\n",
      "2024-01-04 04:56:59.742169: Current learning rate: 0.00667\n",
      "2024-01-04 04:59:05.455833: train_loss -0.911\n",
      "2024-01-04 04:59:05.464837: val_loss -0.8071\n",
      "2024-01-04 04:59:05.471837: Pseudo dice [0.9245, 0.9275, 0.9352]\n",
      "2024-01-04 04:59:05.479350: Epoch time: 125.73 s\n",
      "2024-01-04 04:59:06.946275: \n",
      "2024-01-04 04:59:06.953251: Epoch 363\n",
      "2024-01-04 04:59:06.958027: Current learning rate: 0.00666\n",
      "2024-01-04 05:01:12.942389: train_loss -0.9087\n",
      "2024-01-04 05:01:12.949392: val_loss -0.8053\n",
      "2024-01-04 05:01:12.956392: Pseudo dice [0.9255, 0.9272, 0.9355]\n",
      "2024-01-04 05:01:12.961392: Epoch time: 126.0 s\n",
      "2024-01-04 05:01:14.186192: \n",
      "2024-01-04 05:01:14.192232: Epoch 364\n",
      "2024-01-04 05:01:14.196763: Current learning rate: 0.00665\n",
      "2024-01-04 05:03:20.037720: train_loss -0.912\n",
      "2024-01-04 05:03:20.045730: val_loss -0.8085\n",
      "2024-01-04 05:03:20.051717: Pseudo dice [0.9222, 0.9293, 0.937]\n",
      "2024-01-04 05:03:20.056718: Epoch time: 125.85 s\n",
      "2024-01-04 05:03:21.285180: \n",
      "2024-01-04 05:03:21.290180: Epoch 365\n",
      "2024-01-04 05:03:21.295114: Current learning rate: 0.00665\n",
      "2024-01-04 05:05:26.755048: train_loss -0.9111\n",
      "2024-01-04 05:05:26.761047: val_loss -0.803\n",
      "2024-01-04 05:05:26.766047: Pseudo dice [0.9189, 0.9281, 0.9356]\n",
      "2024-01-04 05:05:26.770051: Epoch time: 125.47 s\n",
      "2024-01-04 05:05:27.967850: \n",
      "2024-01-04 05:05:27.976504: Epoch 366\n",
      "2024-01-04 05:05:27.988179: Current learning rate: 0.00664\n",
      "2024-01-04 05:07:33.513285: train_loss -0.9144\n",
      "2024-01-04 05:07:33.521283: val_loss -0.8018\n",
      "2024-01-04 05:07:33.526283: Pseudo dice [0.9192, 0.9274, 0.936]\n",
      "2024-01-04 05:07:33.532283: Epoch time: 125.55 s\n",
      "2024-01-04 05:07:34.671743: \n",
      "2024-01-04 05:07:34.683160: Epoch 367\n",
      "2024-01-04 05:07:34.689178: Current learning rate: 0.00663\n",
      "2024-01-04 05:09:40.438457: train_loss -0.9113\n",
      "2024-01-04 05:09:40.446459: val_loss -0.8073\n",
      "2024-01-04 05:09:40.453461: Pseudo dice [0.9199, 0.9293, 0.9378]\n",
      "2024-01-04 05:09:40.459462: Epoch time: 125.77 s\n",
      "2024-01-04 05:09:41.773404: \n",
      "2024-01-04 05:09:41.779465: Epoch 368\n",
      "2024-01-04 05:09:41.783477: Current learning rate: 0.00662\n",
      "2024-01-04 05:11:47.290225: train_loss -0.9124\n",
      "2024-01-04 05:11:47.297228: val_loss -0.8035\n",
      "2024-01-04 05:11:47.303222: Pseudo dice [0.9223, 0.9276, 0.9361]\n",
      "2024-01-04 05:11:47.307222: Epoch time: 125.52 s\n",
      "2024-01-04 05:11:48.521383: \n",
      "2024-01-04 05:11:48.527489: Epoch 369\n",
      "2024-01-04 05:11:48.537244: Current learning rate: 0.00661\n",
      "2024-01-04 05:13:54.063986: train_loss -0.9128\n",
      "2024-01-04 05:13:54.071991: val_loss -0.8085\n",
      "2024-01-04 05:13:54.079981: Pseudo dice [0.9227, 0.9315, 0.9396]\n",
      "2024-01-04 05:13:54.084981: Epoch time: 125.54 s\n",
      "2024-01-04 05:13:55.344506: \n",
      "2024-01-04 05:13:55.351718: Epoch 370\n",
      "2024-01-04 05:13:55.357732: Current learning rate: 0.0066\n",
      "2024-01-04 05:16:00.881603: train_loss -0.9152\n",
      "2024-01-04 05:16:00.888605: val_loss -0.8096\n",
      "2024-01-04 05:16:00.893602: Pseudo dice [0.9234, 0.932, 0.9406]\n",
      "2024-01-04 05:16:00.898605: Epoch time: 125.54 s\n",
      "2024-01-04 05:16:02.336943: \n",
      "2024-01-04 05:16:02.342697: Epoch 371\n",
      "2024-01-04 05:16:02.348773: Current learning rate: 0.00659\n",
      "2024-01-04 05:18:08.055133: train_loss -0.9093\n",
      "2024-01-04 05:18:08.065052: val_loss -0.8039\n",
      "2024-01-04 05:18:08.071051: Pseudo dice [0.9203, 0.9265, 0.9349]\n",
      "2024-01-04 05:18:08.079051: Epoch time: 125.72 s\n",
      "2024-01-04 05:18:09.254855: \n",
      "2024-01-04 05:18:09.263004: Epoch 372\n",
      "2024-01-04 05:18:09.268007: Current learning rate: 0.00658\n",
      "2024-01-04 05:20:14.705543: train_loss -0.9142\n",
      "2024-01-04 05:20:14.713539: val_loss -0.805\n",
      "2024-01-04 05:20:14.720539: Pseudo dice [0.9227, 0.9276, 0.9365]\n",
      "2024-01-04 05:20:14.725537: Epoch time: 125.45 s\n",
      "2024-01-04 05:20:15.919276: \n",
      "2024-01-04 05:20:15.924286: Epoch 373\n",
      "2024-01-04 05:20:15.929286: Current learning rate: 0.00657\n",
      "2024-01-04 05:22:21.565051: train_loss -0.9114\n",
      "2024-01-04 05:22:21.571546: val_loss -0.8091\n",
      "2024-01-04 05:22:21.576545: Pseudo dice [0.9219, 0.9265, 0.9354]\n",
      "2024-01-04 05:22:21.581545: Epoch time: 125.65 s\n",
      "2024-01-04 05:22:22.769592: \n",
      "2024-01-04 05:22:22.775609: Epoch 374\n",
      "2024-01-04 05:22:22.780612: Current learning rate: 0.00656\n",
      "2024-01-04 05:24:28.280901: train_loss -0.9108\n",
      "2024-01-04 05:24:28.286902: val_loss -0.7985\n",
      "2024-01-04 05:24:28.292901: Pseudo dice [0.9247, 0.9276, 0.9358]\n",
      "2024-01-04 05:24:28.298905: Epoch time: 125.51 s\n",
      "2024-01-04 05:24:29.406938: \n",
      "2024-01-04 05:24:29.413528: Epoch 375\n",
      "2024-01-04 05:24:29.417579: Current learning rate: 0.00655\n",
      "2024-01-04 05:26:35.844072: train_loss -0.9115\n",
      "2024-01-04 05:26:35.856079: val_loss -0.8102\n",
      "2024-01-04 05:26:35.867589: Pseudo dice [0.9217, 0.9289, 0.9379]\n",
      "2024-01-04 05:26:35.872589: Epoch time: 126.44 s\n",
      "2024-01-04 05:26:37.078966: \n",
      "2024-01-04 05:26:37.084485: Epoch 376\n",
      "2024-01-04 05:26:37.093551: Current learning rate: 0.00654\n",
      "2024-01-04 05:28:42.746679: train_loss -0.9089\n",
      "2024-01-04 05:28:42.756691: val_loss -0.8022\n",
      "2024-01-04 05:28:42.764680: Pseudo dice [0.9222, 0.9292, 0.9382]\n",
      "2024-01-04 05:28:42.770682: Epoch time: 125.67 s\n",
      "2024-01-04 05:28:43.938299: \n",
      "2024-01-04 05:28:43.945359: Epoch 377\n",
      "2024-01-04 05:28:43.949359: Current learning rate: 0.00653\n",
      "2024-01-04 05:30:49.466378: train_loss -0.9114\n",
      "2024-01-04 05:30:49.475377: val_loss -0.8037\n",
      "2024-01-04 05:30:49.480377: Pseudo dice [0.9234, 0.9274, 0.9337]\n",
      "2024-01-04 05:30:49.486378: Epoch time: 125.53 s\n",
      "2024-01-04 05:30:50.621578: \n",
      "2024-01-04 05:30:50.628114: Epoch 378\n",
      "2024-01-04 05:30:50.632124: Current learning rate: 0.00652\n",
      "2024-01-04 05:32:56.130032: train_loss -0.9112\n",
      "2024-01-04 05:32:56.138017: val_loss -0.805\n",
      "2024-01-04 05:32:56.145017: Pseudo dice [0.9197, 0.9313, 0.9383]\n",
      "2024-01-04 05:32:56.152017: Epoch time: 125.51 s\n",
      "2024-01-04 05:32:57.332052: \n",
      "2024-01-04 05:32:57.337043: Epoch 379\n",
      "2024-01-04 05:32:57.343059: Current learning rate: 0.00651\n",
      "2024-01-04 05:35:02.786529: train_loss -0.9134\n",
      "2024-01-04 05:35:02.797532: val_loss -0.8048\n",
      "2024-01-04 05:35:02.802541: Pseudo dice [0.9164, 0.9276, 0.9343]\n",
      "2024-01-04 05:35:02.807541: Epoch time: 125.46 s\n",
      "2024-01-04 05:35:03.889802: \n",
      "2024-01-04 05:35:03.897486: Epoch 380\n",
      "2024-01-04 05:35:03.908504: Current learning rate: 0.0065\n",
      "2024-01-04 05:37:09.731091: train_loss -0.9101\n",
      "2024-01-04 05:37:09.743093: val_loss -0.8019\n",
      "2024-01-04 05:37:09.751620: Pseudo dice [0.918, 0.9279, 0.9364]\n",
      "2024-01-04 05:37:09.759630: Epoch time: 125.84 s\n",
      "2024-01-04 05:37:10.818916: \n",
      "2024-01-04 05:37:10.825140: Epoch 381\n",
      "2024-01-04 05:37:10.832294: Current learning rate: 0.00649\n",
      "2024-01-04 05:39:16.733658: train_loss -0.9119\n",
      "2024-01-04 05:39:16.740657: val_loss -0.8032\n",
      "2024-01-04 05:39:16.746658: Pseudo dice [0.9221, 0.9292, 0.9381]\n",
      "2024-01-04 05:39:16.751658: Epoch time: 125.92 s\n",
      "2024-01-04 05:39:18.020733: \n",
      "2024-01-04 05:39:18.025881: Epoch 382\n",
      "2024-01-04 05:39:18.030885: Current learning rate: 0.00648\n",
      "2024-01-04 05:41:23.544956: train_loss -0.9125\n",
      "2024-01-04 05:41:23.553960: val_loss -0.8069\n",
      "2024-01-04 05:41:23.558958: Pseudo dice [0.9265, 0.9283, 0.9353]\n",
      "2024-01-04 05:41:23.563956: Epoch time: 125.53 s\n",
      "2024-01-04 05:41:24.750290: \n",
      "2024-01-04 05:41:24.755284: Epoch 383\n",
      "2024-01-04 05:41:24.760284: Current learning rate: 0.00648\n",
      "2024-01-04 05:43:30.395722: train_loss -0.9134\n",
      "2024-01-04 05:43:30.402721: val_loss -0.8047\n",
      "2024-01-04 05:43:30.407721: Pseudo dice [0.922, 0.9278, 0.9367]\n",
      "2024-01-04 05:43:30.411721: Epoch time: 125.65 s\n",
      "2024-01-04 05:43:31.594743: \n",
      "2024-01-04 05:43:31.604161: Epoch 384\n",
      "2024-01-04 05:43:31.608192: Current learning rate: 0.00647\n",
      "2024-01-04 05:45:37.093190: train_loss -0.9152\n",
      "2024-01-04 05:45:37.100192: val_loss -0.8009\n",
      "2024-01-04 05:45:37.105191: Pseudo dice [0.9217, 0.9264, 0.935]\n",
      "2024-01-04 05:45:37.110192: Epoch time: 125.5 s\n",
      "2024-01-04 05:45:38.261729: \n",
      "2024-01-04 05:45:38.266729: Epoch 385\n",
      "2024-01-04 05:45:38.274663: Current learning rate: 0.00646\n",
      "2024-01-04 05:47:43.782984: train_loss -0.9145\n",
      "2024-01-04 05:47:43.788981: val_loss -0.808\n",
      "2024-01-04 05:47:43.797980: Pseudo dice [0.924, 0.931, 0.9396]\n",
      "2024-01-04 05:47:43.803982: Epoch time: 125.52 s\n",
      "2024-01-04 05:47:45.201813: \n",
      "2024-01-04 05:47:45.212503: Epoch 386\n",
      "2024-01-04 05:47:45.219461: Current learning rate: 0.00645\n",
      "2024-01-04 05:49:50.805449: train_loss -0.9117\n",
      "2024-01-04 05:49:50.812450: val_loss -0.8084\n",
      "2024-01-04 05:49:50.818449: Pseudo dice [0.9197, 0.9291, 0.937]\n",
      "2024-01-04 05:49:50.823449: Epoch time: 125.6 s\n",
      "2024-01-04 05:49:51.984953: \n",
      "2024-01-04 05:49:51.991183: Epoch 387\n",
      "2024-01-04 05:49:51.999177: Current learning rate: 0.00644\n",
      "2024-01-04 05:51:57.603036: train_loss -0.9137\n",
      "2024-01-04 05:51:57.609036: val_loss -0.8086\n",
      "2024-01-04 05:51:57.615035: Pseudo dice [0.9214, 0.9284, 0.9374]\n",
      "2024-01-04 05:51:57.620548: Epoch time: 125.62 s\n",
      "2024-01-04 05:51:58.957724: \n",
      "2024-01-04 05:51:58.965713: Epoch 388\n",
      "2024-01-04 05:51:58.972719: Current learning rate: 0.00643\n",
      "2024-01-04 05:54:04.572897: train_loss -0.9119\n",
      "2024-01-04 05:54:04.581896: val_loss -0.8034\n",
      "2024-01-04 05:54:04.591407: Pseudo dice [0.9222, 0.9273, 0.9342]\n",
      "2024-01-04 05:54:04.598406: Epoch time: 125.62 s\n",
      "2024-01-04 05:54:05.727395: \n",
      "2024-01-04 05:54:05.732395: Epoch 389\n",
      "2024-01-04 05:54:05.742325: Current learning rate: 0.00642\n",
      "2024-01-04 05:56:11.558363: train_loss -0.9107\n",
      "2024-01-04 05:56:11.569377: val_loss -0.8031\n",
      "2024-01-04 05:56:11.578374: Pseudo dice [0.924, 0.9289, 0.9368]\n",
      "2024-01-04 05:56:11.583367: Epoch time: 125.83 s\n",
      "2024-01-04 05:56:12.894881: \n",
      "2024-01-04 05:56:12.903959: Epoch 390\n",
      "2024-01-04 05:56:12.908948: Current learning rate: 0.00641\n",
      "2024-01-04 05:58:18.692406: train_loss -0.9111\n",
      "2024-01-04 05:58:18.703916: val_loss -0.8063\n",
      "2024-01-04 05:58:18.712915: Pseudo dice [0.9189, 0.9282, 0.9355]\n",
      "2024-01-04 05:58:18.719915: Epoch time: 125.8 s\n",
      "2024-01-04 05:58:19.888423: \n",
      "2024-01-04 05:58:19.894423: Epoch 391\n",
      "2024-01-04 05:58:19.899957: Current learning rate: 0.0064\n",
      "2024-01-04 06:00:26.142377: train_loss -0.9121\n",
      "2024-01-04 06:00:26.150379: val_loss -0.8069\n",
      "2024-01-04 06:00:26.156376: Pseudo dice [0.9188, 0.928, 0.9365]\n",
      "2024-01-04 06:00:26.161376: Epoch time: 126.25 s\n",
      "2024-01-04 06:00:27.360451: \n",
      "2024-01-04 06:00:27.367456: Epoch 392\n",
      "2024-01-04 06:00:27.375239: Current learning rate: 0.00639\n",
      "2024-01-04 06:02:33.386071: train_loss -0.9115\n",
      "2024-01-04 06:02:33.393073: val_loss -0.8056\n",
      "2024-01-04 06:02:33.401573: Pseudo dice [0.9241, 0.9295, 0.9374]\n",
      "2024-01-04 06:02:33.409576: Epoch time: 126.03 s\n",
      "2024-01-04 06:02:34.805862: \n",
      "2024-01-04 06:02:34.811919: Epoch 393\n",
      "2024-01-04 06:02:34.816862: Current learning rate: 0.00638\n",
      "2024-01-04 06:04:40.399894: train_loss -0.9129\n",
      "2024-01-04 06:04:40.406894: val_loss -0.805\n",
      "2024-01-04 06:04:40.413986: Pseudo dice [0.9241, 0.9289, 0.9347]\n",
      "2024-01-04 06:04:40.419903: Epoch time: 125.6 s\n",
      "2024-01-04 06:04:41.597876: \n",
      "2024-01-04 06:04:41.603913: Epoch 394\n",
      "2024-01-04 06:04:41.614506: Current learning rate: 0.00637\n",
      "2024-01-04 06:06:46.896275: train_loss -0.9156\n",
      "2024-01-04 06:06:46.903265: val_loss -0.801\n",
      "2024-01-04 06:06:46.909265: Pseudo dice [0.9242, 0.9274, 0.9365]\n",
      "2024-01-04 06:06:46.914265: Epoch time: 125.3 s\n",
      "2024-01-04 06:06:48.199414: \n",
      "2024-01-04 06:06:48.205355: Epoch 395\n",
      "2024-01-04 06:06:48.210430: Current learning rate: 0.00636\n",
      "2024-01-04 06:08:54.211813: train_loss -0.9113\n",
      "2024-01-04 06:08:54.221815: val_loss -0.7929\n",
      "2024-01-04 06:08:54.230813: Pseudo dice [0.9237, 0.9269, 0.9351]\n",
      "2024-01-04 06:08:54.237814: Epoch time: 126.01 s\n",
      "2024-01-04 06:08:55.757651: \n",
      "2024-01-04 06:08:55.763651: Epoch 396\n",
      "2024-01-04 06:08:55.768651: Current learning rate: 0.00635\n",
      "2024-01-04 06:11:01.710076: train_loss -0.9102\n",
      "2024-01-04 06:11:01.717076: val_loss -0.8076\n",
      "2024-01-04 06:11:01.722078: Pseudo dice [0.9209, 0.9292, 0.9373]\n",
      "2024-01-04 06:11:01.727087: Epoch time: 125.95 s\n",
      "2024-01-04 06:11:02.837125: \n",
      "2024-01-04 06:11:02.843190: Epoch 397\n",
      "2024-01-04 06:11:02.850190: Current learning rate: 0.00634\n",
      "2024-01-04 06:13:08.343257: train_loss -0.9135\n",
      "2024-01-04 06:13:08.350260: val_loss -0.8041\n",
      "2024-01-04 06:13:08.356260: Pseudo dice [0.922, 0.9278, 0.9356]\n",
      "2024-01-04 06:13:08.362258: Epoch time: 125.51 s\n",
      "2024-01-04 06:13:09.583349: \n",
      "2024-01-04 06:13:09.589346: Epoch 398\n",
      "2024-01-04 06:13:09.597440: Current learning rate: 0.00633\n",
      "2024-01-04 06:15:15.363821: train_loss -0.911\n",
      "2024-01-04 06:15:15.370821: val_loss -0.8024\n",
      "2024-01-04 06:15:15.377825: Pseudo dice [0.9198, 0.928, 0.9357]\n",
      "2024-01-04 06:15:15.384830: Epoch time: 125.78 s\n",
      "2024-01-04 06:15:16.624085: \n",
      "2024-01-04 06:15:16.630033: Epoch 399\n",
      "2024-01-04 06:15:16.635019: Current learning rate: 0.00632\n",
      "2024-01-04 06:17:22.301475: train_loss -0.9137\n",
      "2024-01-04 06:17:22.309465: val_loss -0.8069\n",
      "2024-01-04 06:17:22.317464: Pseudo dice [0.9227, 0.9276, 0.9343]\n",
      "2024-01-04 06:17:22.321470: Epoch time: 125.68 s\n",
      "2024-01-04 06:17:23.946306: \n",
      "2024-01-04 06:17:23.951843: Epoch 400\n",
      "2024-01-04 06:17:23.955843: Current learning rate: 0.00631\n",
      "2024-01-04 06:19:29.489591: train_loss -0.9144\n",
      "2024-01-04 06:19:29.495094: val_loss -0.8087\n",
      "2024-01-04 06:19:29.505093: Pseudo dice [0.9248, 0.9291, 0.9372]\n",
      "2024-01-04 06:19:29.511158: Epoch time: 125.54 s\n",
      "2024-01-04 06:19:30.746922: \n",
      "2024-01-04 06:19:30.752998: Epoch 401\n",
      "2024-01-04 06:19:30.757070: Current learning rate: 0.0063\n",
      "2024-01-04 06:21:36.554736: train_loss -0.9128\n",
      "2024-01-04 06:21:36.561737: val_loss -0.8013\n",
      "2024-01-04 06:21:36.567737: Pseudo dice [0.922, 0.9271, 0.9338]\n",
      "2024-01-04 06:21:36.574736: Epoch time: 125.81 s\n",
      "2024-01-04 06:21:37.919666: \n",
      "2024-01-04 06:21:37.927248: Epoch 402\n",
      "2024-01-04 06:21:37.932264: Current learning rate: 0.0063\n",
      "2024-01-04 06:23:43.391769: train_loss -0.912\n",
      "2024-01-04 06:23:43.400776: val_loss -0.8032\n",
      "2024-01-04 06:23:43.406775: Pseudo dice [0.9187, 0.9303, 0.9383]\n",
      "2024-01-04 06:23:43.412784: Epoch time: 125.47 s\n",
      "2024-01-04 06:23:44.665541: \n",
      "2024-01-04 06:23:44.671476: Epoch 403\n",
      "2024-01-04 06:23:44.681501: Current learning rate: 0.00629\n",
      "2024-01-04 06:25:50.564542: train_loss -0.9106\n",
      "2024-01-04 06:25:50.571542: val_loss -0.8059\n",
      "2024-01-04 06:25:50.577839: Pseudo dice [0.919, 0.9289, 0.9362]\n",
      "2024-01-04 06:25:50.582839: Epoch time: 125.9 s\n",
      "2024-01-04 06:25:51.898389: \n",
      "2024-01-04 06:25:51.904412: Epoch 404\n",
      "2024-01-04 06:25:51.912328: Current learning rate: 0.00628\n",
      "2024-01-04 06:27:57.399157: train_loss -0.9145\n",
      "2024-01-04 06:27:57.405149: val_loss -0.8056\n",
      "2024-01-04 06:27:57.411156: Pseudo dice [0.9208, 0.9284, 0.9341]\n",
      "2024-01-04 06:27:57.416153: Epoch time: 125.5 s\n",
      "2024-01-04 06:27:58.582622: \n",
      "2024-01-04 06:27:58.589621: Epoch 405\n",
      "2024-01-04 06:27:58.594612: Current learning rate: 0.00627\n",
      "2024-01-04 06:30:04.312829: train_loss -0.9125\n",
      "2024-01-04 06:30:04.319829: val_loss -0.8009\n",
      "2024-01-04 06:30:04.324829: Pseudo dice [0.9232, 0.9265, 0.9341]\n",
      "2024-01-04 06:30:04.329829: Epoch time: 125.73 s\n",
      "2024-01-04 06:30:05.525713: \n",
      "2024-01-04 06:30:05.530707: Epoch 406\n",
      "2024-01-04 06:30:05.535708: Current learning rate: 0.00626\n",
      "2024-01-04 06:32:10.890312: train_loss -0.9134\n",
      "2024-01-04 06:32:10.898311: val_loss -0.8144\n",
      "2024-01-04 06:32:10.905311: Pseudo dice [0.9227, 0.9301, 0.9382]\n",
      "2024-01-04 06:32:10.912311: Epoch time: 125.37 s\n",
      "2024-01-04 06:32:12.245811: \n",
      "2024-01-04 06:32:12.251233: Epoch 407\n",
      "2024-01-04 06:32:12.259348: Current learning rate: 0.00625\n",
      "2024-01-04 06:34:18.009406: train_loss -0.9131\n",
      "2024-01-04 06:34:18.016407: val_loss -0.7943\n",
      "2024-01-04 06:34:18.024400: Pseudo dice [0.9253, 0.9272, 0.9345]\n",
      "2024-01-04 06:34:18.029402: Epoch time: 125.76 s\n",
      "2024-01-04 06:34:19.538775: \n",
      "2024-01-04 06:34:19.546790: Epoch 408\n",
      "2024-01-04 06:34:19.550790: Current learning rate: 0.00624\n",
      "2024-01-04 06:36:24.995609: train_loss -0.9123\n",
      "2024-01-04 06:36:25.004609: val_loss -0.8094\n",
      "2024-01-04 06:36:25.011609: Pseudo dice [0.9229, 0.9299, 0.9381]\n",
      "2024-01-04 06:36:25.017610: Epoch time: 125.46 s\n",
      "2024-01-04 06:36:26.467984: \n",
      "2024-01-04 06:36:26.474926: Epoch 409\n",
      "2024-01-04 06:36:26.478844: Current learning rate: 0.00623\n",
      "2024-01-04 06:38:32.492262: train_loss -0.9084\n",
      "2024-01-04 06:38:32.499265: val_loss -0.7971\n",
      "2024-01-04 06:38:32.504265: Pseudo dice [0.9188, 0.9264, 0.934]\n",
      "2024-01-04 06:38:32.509265: Epoch time: 126.03 s\n",
      "2024-01-04 06:38:33.714238: \n",
      "2024-01-04 06:38:33.721514: Epoch 410\n",
      "2024-01-04 06:38:33.725583: Current learning rate: 0.00622\n",
      "2024-01-04 06:40:39.404297: train_loss -0.9075\n",
      "2024-01-04 06:40:39.414298: val_loss -0.8024\n",
      "2024-01-04 06:40:39.422297: Pseudo dice [0.9209, 0.9285, 0.9362]\n",
      "2024-01-04 06:40:39.430298: Epoch time: 125.69 s\n",
      "2024-01-04 06:40:40.588883: \n",
      "2024-01-04 06:40:40.593883: Epoch 411\n",
      "2024-01-04 06:40:40.597875: Current learning rate: 0.00621\n",
      "2024-01-04 06:42:46.130902: train_loss -0.9107\n",
      "2024-01-04 06:42:46.138904: val_loss -0.8026\n",
      "2024-01-04 06:42:46.144904: Pseudo dice [0.9202, 0.9271, 0.9342]\n",
      "2024-01-04 06:42:46.149904: Epoch time: 125.54 s\n",
      "2024-01-04 06:42:47.238126: \n",
      "2024-01-04 06:42:47.243305: Epoch 412\n",
      "2024-01-04 06:42:47.248286: Current learning rate: 0.0062\n",
      "2024-01-04 06:44:52.742069: train_loss -0.9115\n",
      "2024-01-04 06:44:52.752579: val_loss -0.8093\n",
      "2024-01-04 06:44:52.759579: Pseudo dice [0.9244, 0.9289, 0.9359]\n",
      "2024-01-04 06:44:52.764579: Epoch time: 125.5 s\n",
      "2024-01-04 06:44:53.996006: \n",
      "2024-01-04 06:44:54.004955: Epoch 413\n",
      "2024-01-04 06:44:54.010018: Current learning rate: 0.00619\n",
      "2024-01-04 06:46:59.388772: train_loss -0.9144\n",
      "2024-01-04 06:46:59.396285: val_loss -0.785\n",
      "2024-01-04 06:46:59.401281: Pseudo dice [0.9111, 0.925, 0.9305]\n",
      "2024-01-04 06:46:59.407281: Epoch time: 125.39 s\n",
      "2024-01-04 06:47:00.461229: \n",
      "2024-01-04 06:47:00.469454: Epoch 414\n",
      "2024-01-04 06:47:00.473824: Current learning rate: 0.00618\n",
      "2024-01-04 06:49:06.304632: train_loss -0.9086\n",
      "2024-01-04 06:49:06.312633: val_loss -0.8142\n",
      "2024-01-04 06:49:06.317632: Pseudo dice [0.9237, 0.9296, 0.9352]\n",
      "2024-01-04 06:49:06.322634: Epoch time: 125.85 s\n",
      "2024-01-04 06:49:07.766378: \n",
      "2024-01-04 06:49:07.773396: Epoch 415\n",
      "2024-01-04 06:49:07.777414: Current learning rate: 0.00617\n",
      "2024-01-04 06:51:13.478964: train_loss -0.9128\n",
      "2024-01-04 06:51:13.488966: val_loss -0.8173\n",
      "2024-01-04 06:51:13.497964: Pseudo dice [0.9217, 0.9317, 0.9399]\n",
      "2024-01-04 06:51:13.503965: Epoch time: 125.71 s\n",
      "2024-01-04 06:51:14.699115: \n",
      "2024-01-04 06:51:14.708256: Epoch 416\n",
      "2024-01-04 06:51:14.712783: Current learning rate: 0.00616\n",
      "2024-01-04 06:53:20.431478: train_loss -0.9142\n",
      "2024-01-04 06:53:20.442480: val_loss -0.8085\n",
      "2024-01-04 06:53:20.450479: Pseudo dice [0.9211, 0.9298, 0.9372]\n",
      "2024-01-04 06:53:20.457479: Epoch time: 125.73 s\n",
      "2024-01-04 06:53:21.670621: \n",
      "2024-01-04 06:53:21.676886: Epoch 417\n",
      "2024-01-04 06:53:21.681953: Current learning rate: 0.00615\n",
      "2024-01-04 06:55:27.513923: train_loss -0.9117\n",
      "2024-01-04 06:55:27.523916: val_loss -0.8065\n",
      "2024-01-04 06:55:27.531914: Pseudo dice [0.9229, 0.9282, 0.9367]\n",
      "2024-01-04 06:55:27.538913: Epoch time: 125.84 s\n",
      "2024-01-04 06:55:28.683622: \n",
      "2024-01-04 06:55:28.689600: Epoch 418\n",
      "2024-01-04 06:55:28.694174: Current learning rate: 0.00614\n",
      "2024-01-04 06:57:34.402843: train_loss -0.9113\n",
      "2024-01-04 06:57:34.408842: val_loss -0.811\n",
      "2024-01-04 06:57:34.413841: Pseudo dice [0.9232, 0.93, 0.9374]\n",
      "2024-01-04 06:57:34.418842: Epoch time: 125.72 s\n",
      "2024-01-04 06:57:35.559782: \n",
      "2024-01-04 06:57:35.566156: Epoch 419\n",
      "2024-01-04 06:57:35.576107: Current learning rate: 0.00613\n",
      "2024-01-04 06:59:41.462197: train_loss -0.9116\n",
      "2024-01-04 06:59:41.472198: val_loss -0.8064\n",
      "2024-01-04 06:59:41.479198: Pseudo dice [0.9241, 0.9277, 0.9353]\n",
      "2024-01-04 06:59:41.484197: Epoch time: 125.9 s\n",
      "2024-01-04 06:59:42.689186: \n",
      "2024-01-04 06:59:42.696801: Epoch 420\n",
      "2024-01-04 06:59:42.701887: Current learning rate: 0.00612\n",
      "2024-01-04 07:01:48.257162: train_loss -0.9157\n",
      "2024-01-04 07:01:48.265162: val_loss -0.8068\n",
      "2024-01-04 07:01:48.270166: Pseudo dice [0.9241, 0.9278, 0.9364]\n",
      "2024-01-04 07:01:48.275166: Epoch time: 125.57 s\n",
      "2024-01-04 07:01:49.476779: \n",
      "2024-01-04 07:01:49.484423: Epoch 421\n",
      "2024-01-04 07:01:49.490411: Current learning rate: 0.00612\n",
      "2024-01-04 07:03:55.368767: train_loss -0.9149\n",
      "2024-01-04 07:03:55.375768: val_loss -0.8045\n",
      "2024-01-04 07:03:55.381768: Pseudo dice [0.9278, 0.9266, 0.9347]\n",
      "2024-01-04 07:03:55.386762: Epoch time: 125.89 s\n",
      "2024-01-04 07:03:56.571199: \n",
      "2024-01-04 07:03:56.577196: Epoch 422\n",
      "2024-01-04 07:03:56.582262: Current learning rate: 0.00611\n",
      "2024-01-04 07:06:02.296887: train_loss -0.9112\n",
      "2024-01-04 07:06:02.307894: val_loss -0.8078\n",
      "2024-01-04 07:06:02.317888: Pseudo dice [0.9215, 0.9271, 0.9342]\n",
      "2024-01-04 07:06:02.324887: Epoch time: 125.73 s\n",
      "2024-01-04 07:06:03.689416: \n",
      "2024-01-04 07:06:03.694937: Epoch 423\n",
      "2024-01-04 07:06:03.698938: Current learning rate: 0.0061\n",
      "2024-01-04 07:08:09.611315: train_loss -0.9107\n",
      "2024-01-04 07:08:09.618255: val_loss -0.8043\n",
      "2024-01-04 07:08:09.622255: Pseudo dice [0.9214, 0.9271, 0.9365]\n",
      "2024-01-04 07:08:09.629256: Epoch time: 125.92 s\n",
      "2024-01-04 07:08:10.765635: \n",
      "2024-01-04 07:08:10.773323: Epoch 424\n",
      "2024-01-04 07:08:10.778250: Current learning rate: 0.00609\n",
      "2024-01-04 07:10:16.296126: train_loss -0.9144\n",
      "2024-01-04 07:10:16.303131: val_loss -0.8033\n",
      "2024-01-04 07:10:16.311142: Pseudo dice [0.9231, 0.9282, 0.9349]\n",
      "2024-01-04 07:10:16.319651: Epoch time: 125.53 s\n",
      "2024-01-04 07:10:17.471861: \n",
      "2024-01-04 07:10:17.478975: Epoch 425\n",
      "2024-01-04 07:10:17.483976: Current learning rate: 0.00608\n",
      "2024-01-04 07:12:23.385008: train_loss -0.9115\n",
      "2024-01-04 07:12:23.392009: val_loss -0.802\n",
      "2024-01-04 07:12:23.398011: Pseudo dice [0.9203, 0.9266, 0.9362]\n",
      "2024-01-04 07:12:23.403010: Epoch time: 125.91 s\n",
      "2024-01-04 07:12:24.452547: \n",
      "2024-01-04 07:12:24.463645: Epoch 426\n",
      "2024-01-04 07:12:24.468645: Current learning rate: 0.00607\n",
      "2024-01-04 07:14:29.738299: train_loss -0.9142\n",
      "2024-01-04 07:14:29.745298: val_loss -0.8008\n",
      "2024-01-04 07:14:29.750299: Pseudo dice [0.92, 0.9268, 0.936]\n",
      "2024-01-04 07:14:29.755299: Epoch time: 125.29 s\n",
      "2024-01-04 07:14:30.925565: \n",
      "2024-01-04 07:14:30.931563: Epoch 427\n",
      "2024-01-04 07:14:30.936155: Current learning rate: 0.00606\n",
      "2024-01-04 07:16:36.677980: train_loss -0.9138\n",
      "2024-01-04 07:16:36.685983: val_loss -0.8073\n",
      "2024-01-04 07:16:36.691984: Pseudo dice [0.9228, 0.9301, 0.939]\n",
      "2024-01-04 07:16:36.696982: Epoch time: 125.75 s\n",
      "2024-01-04 07:16:37.941226: \n",
      "2024-01-04 07:16:37.949293: Epoch 428\n",
      "2024-01-04 07:16:37.954228: Current learning rate: 0.00605\n",
      "2024-01-04 07:18:44.460005: train_loss -0.9136\n",
      "2024-01-04 07:18:44.473199: val_loss -0.805\n",
      "2024-01-04 07:18:44.487846: Pseudo dice [0.9206, 0.9268, 0.9345]\n",
      "2024-01-04 07:18:44.499413: Epoch time: 126.52 s\n",
      "2024-01-04 07:18:45.674904: \n",
      "2024-01-04 07:18:45.683177: Epoch 429\n",
      "2024-01-04 07:18:45.693170: Current learning rate: 0.00604\n",
      "2024-01-04 07:20:53.280560: train_loss -0.9158\n",
      "2024-01-04 07:20:53.287559: val_loss -0.8002\n",
      "2024-01-04 07:20:53.293560: Pseudo dice [0.9229, 0.9283, 0.9348]\n",
      "2024-01-04 07:20:53.300565: Epoch time: 127.61 s\n",
      "2024-01-04 07:20:54.532153: \n",
      "2024-01-04 07:20:54.539153: Epoch 430\n",
      "2024-01-04 07:20:54.543156: Current learning rate: 0.00603\n",
      "2024-01-04 07:23:00.610069: train_loss -0.9153\n",
      "2024-01-04 07:23:00.620069: val_loss -0.8013\n",
      "2024-01-04 07:23:00.628070: Pseudo dice [0.9235, 0.9272, 0.9348]\n",
      "2024-01-04 07:23:00.634417: Epoch time: 126.08 s\n",
      "2024-01-04 07:23:01.819396: \n",
      "2024-01-04 07:23:01.823550: Epoch 431\n",
      "2024-01-04 07:23:01.828622: Current learning rate: 0.00602\n",
      "2024-01-04 07:25:07.722962: train_loss -0.9177\n",
      "2024-01-04 07:25:07.732032: val_loss -0.8035\n",
      "2024-01-04 07:25:07.740006: Pseudo dice [0.922, 0.9298, 0.9363]\n",
      "2024-01-04 07:25:07.744536: Epoch time: 125.91 s\n",
      "2024-01-04 07:25:08.980572: \n",
      "2024-01-04 07:25:08.986492: Epoch 432\n",
      "2024-01-04 07:25:08.994574: Current learning rate: 0.00601\n",
      "2024-01-04 07:27:14.481756: train_loss -0.9155\n",
      "2024-01-04 07:27:14.489756: val_loss -0.7989\n",
      "2024-01-04 07:27:14.495755: Pseudo dice [0.9219, 0.9284, 0.9361]\n",
      "2024-01-04 07:27:14.500784: Epoch time: 125.5 s\n",
      "2024-01-04 07:27:15.618464: \n",
      "2024-01-04 07:27:15.627278: Epoch 433\n",
      "2024-01-04 07:27:15.631363: Current learning rate: 0.006\n",
      "2024-01-04 07:29:21.418934: train_loss -0.913\n",
      "2024-01-04 07:29:21.425935: val_loss -0.808\n",
      "2024-01-04 07:29:21.430936: Pseudo dice [0.9233, 0.9313, 0.9381]\n",
      "2024-01-04 07:29:21.434942: Epoch time: 125.8 s\n",
      "2024-01-04 07:29:22.547167: \n",
      "2024-01-04 07:29:22.552450: Epoch 434\n",
      "2024-01-04 07:29:22.560468: Current learning rate: 0.00599\n",
      "2024-01-04 07:31:28.005279: train_loss -0.9141\n",
      "2024-01-04 07:31:28.012279: val_loss -0.8023\n",
      "2024-01-04 07:31:28.017281: Pseudo dice [0.9218, 0.9271, 0.9342]\n",
      "2024-01-04 07:31:28.022280: Epoch time: 125.46 s\n",
      "2024-01-04 07:31:29.124737: \n",
      "2024-01-04 07:31:29.130953: Epoch 435\n",
      "2024-01-04 07:31:29.136980: Current learning rate: 0.00598\n",
      "2024-01-04 07:33:34.842355: train_loss -0.9139\n",
      "2024-01-04 07:33:34.849355: val_loss -0.804\n",
      "2024-01-04 07:33:34.858020: Pseudo dice [0.9275, 0.9296, 0.9361]\n",
      "2024-01-04 07:33:34.865018: Epoch time: 125.72 s\n",
      "2024-01-04 07:33:35.950772: \n",
      "2024-01-04 07:33:35.959833: Epoch 436\n",
      "2024-01-04 07:33:35.965834: Current learning rate: 0.00597\n",
      "2024-01-04 07:35:41.197539: train_loss -0.9141\n",
      "2024-01-04 07:35:41.206544: val_loss -0.8045\n",
      "2024-01-04 07:35:41.213547: Pseudo dice [0.9212, 0.9266, 0.936]\n",
      "2024-01-04 07:35:41.219543: Epoch time: 125.25 s\n",
      "2024-01-04 07:35:42.310243: \n",
      "2024-01-04 07:35:42.316418: Epoch 437\n",
      "2024-01-04 07:35:42.320515: Current learning rate: 0.00596\n",
      "2024-01-04 07:37:47.481828: train_loss -0.9183\n",
      "2024-01-04 07:37:47.490333: val_loss -0.8101\n",
      "2024-01-04 07:37:47.496343: Pseudo dice [0.9225, 0.9297, 0.9382]\n",
      "2024-01-04 07:37:47.501336: Epoch time: 125.17 s\n",
      "2024-01-04 07:37:48.578648: \n",
      "2024-01-04 07:37:48.585670: Epoch 438\n",
      "2024-01-04 07:37:48.591374: Current learning rate: 0.00595\n",
      "2024-01-04 07:39:54.245467: train_loss -0.9147\n",
      "2024-01-04 07:39:54.255539: val_loss -0.8112\n",
      "2024-01-04 07:39:54.260548: Pseudo dice [0.9212, 0.9311, 0.9387]\n",
      "2024-01-04 07:39:54.265548: Epoch time: 125.67 s\n",
      "2024-01-04 07:39:55.575189: \n",
      "2024-01-04 07:39:55.583199: Epoch 439\n",
      "2024-01-04 07:39:55.588198: Current learning rate: 0.00594\n",
      "2024-01-04 07:42:01.399067: train_loss -0.9146\n",
      "2024-01-04 07:42:01.408576: val_loss -0.808\n",
      "2024-01-04 07:42:01.416576: Pseudo dice [0.925, 0.9272, 0.9347]\n",
      "2024-01-04 07:42:01.424577: Epoch time: 125.82 s\n",
      "2024-01-04 07:42:02.691381: \n",
      "2024-01-04 07:42:02.696373: Epoch 440\n",
      "2024-01-04 07:42:02.700382: Current learning rate: 0.00593\n",
      "2024-01-04 07:44:08.703958: train_loss -0.913\n",
      "2024-01-04 07:44:08.711959: val_loss -0.7922\n",
      "2024-01-04 07:44:08.716959: Pseudo dice [0.9229, 0.9278, 0.9355]\n",
      "2024-01-04 07:44:08.722959: Epoch time: 126.01 s\n",
      "2024-01-04 07:44:09.989958: \n",
      "2024-01-04 07:44:09.997936: Epoch 441\n",
      "2024-01-04 07:44:10.001968: Current learning rate: 0.00592\n",
      "2024-01-04 07:46:15.530732: train_loss -0.9118\n",
      "2024-01-04 07:46:15.537729: val_loss -0.8034\n",
      "2024-01-04 07:46:15.543738: Pseudo dice [0.9222, 0.927, 0.9369]\n",
      "2024-01-04 07:46:15.548729: Epoch time: 125.54 s\n",
      "2024-01-04 07:46:16.690657: \n",
      "2024-01-04 07:46:16.696656: Epoch 442\n",
      "2024-01-04 07:46:16.700656: Current learning rate: 0.00592\n",
      "2024-01-04 07:48:22.379484: train_loss -0.9153\n",
      "2024-01-04 07:48:22.387484: val_loss -0.8095\n",
      "2024-01-04 07:48:22.393484: Pseudo dice [0.926, 0.9309, 0.9382]\n",
      "2024-01-04 07:48:22.401483: Epoch time: 125.69 s\n",
      "2024-01-04 07:48:23.658883: \n",
      "2024-01-04 07:48:23.671819: Epoch 443\n",
      "2024-01-04 07:48:23.679790: Current learning rate: 0.00591\n",
      "2024-01-04 07:50:29.054726: train_loss -0.9138\n",
      "2024-01-04 07:50:29.063722: val_loss -0.8041\n",
      "2024-01-04 07:50:29.072230: Pseudo dice [0.9236, 0.928, 0.9348]\n",
      "2024-01-04 07:50:29.076230: Epoch time: 125.4 s\n",
      "2024-01-04 07:50:30.243280: \n",
      "2024-01-04 07:50:30.249065: Epoch 444\n",
      "2024-01-04 07:50:30.257218: Current learning rate: 0.0059\n",
      "2024-01-04 07:52:35.847362: train_loss -0.9154\n",
      "2024-01-04 07:52:35.853362: val_loss -0.8038\n",
      "2024-01-04 07:52:35.860362: Pseudo dice [0.9171, 0.9281, 0.9357]\n",
      "2024-01-04 07:52:35.865362: Epoch time: 125.61 s\n",
      "2024-01-04 07:52:37.075170: \n",
      "2024-01-04 07:52:37.084161: Epoch 445\n",
      "2024-01-04 07:52:37.088169: Current learning rate: 0.00589\n",
      "2024-01-04 07:54:42.818468: train_loss -0.9113\n",
      "2024-01-04 07:54:42.827470: val_loss -0.7899\n",
      "2024-01-04 07:54:42.833467: Pseudo dice [0.9215, 0.926, 0.9332]\n",
      "2024-01-04 07:54:42.839305: Epoch time: 125.74 s\n",
      "2024-01-04 07:54:43.991273: \n",
      "2024-01-04 07:54:43.998335: Epoch 446\n",
      "2024-01-04 07:54:44.005341: Current learning rate: 0.00588\n",
      "2024-01-04 07:56:49.528347: train_loss -0.9153\n",
      "2024-01-04 07:56:49.534350: val_loss -0.7978\n",
      "2024-01-04 07:56:49.539347: Pseudo dice [0.9239, 0.9263, 0.9344]\n",
      "2024-01-04 07:56:49.549349: Epoch time: 125.54 s\n",
      "2024-01-04 07:56:50.631142: \n",
      "2024-01-04 07:56:50.637405: Epoch 447\n",
      "2024-01-04 07:56:50.645414: Current learning rate: 0.00587\n",
      "2024-01-04 07:58:56.495876: train_loss -0.9133\n",
      "2024-01-04 07:58:56.505880: val_loss -0.807\n",
      "2024-01-04 07:58:56.513876: Pseudo dice [0.9214, 0.929, 0.9365]\n",
      "2024-01-04 07:58:56.522876: Epoch time: 125.87 s\n",
      "2024-01-04 07:58:57.896698: \n",
      "2024-01-04 07:58:57.904638: Epoch 448\n",
      "2024-01-04 07:58:57.908691: Current learning rate: 0.00586\n",
      "2024-01-04 08:01:03.382678: train_loss -0.9159\n",
      "2024-01-04 08:01:03.390677: val_loss -0.8172\n",
      "2024-01-04 08:01:03.396676: Pseudo dice [0.9219, 0.9294, 0.9368]\n",
      "2024-01-04 08:01:03.401675: Epoch time: 125.49 s\n",
      "2024-01-04 08:01:04.564936: \n",
      "2024-01-04 08:01:04.570944: Epoch 449\n",
      "2024-01-04 08:01:04.577999: Current learning rate: 0.00585\n",
      "2024-01-04 08:03:10.196410: train_loss -0.9128\n",
      "2024-01-04 08:03:10.205410: val_loss -0.7994\n",
      "2024-01-04 08:03:10.212917: Pseudo dice [0.9225, 0.9263, 0.9352]\n",
      "2024-01-04 08:03:10.217917: Epoch time: 125.63 s\n",
      "2024-01-04 08:03:11.761589: \n",
      "2024-01-04 08:03:11.770587: Epoch 450\n",
      "2024-01-04 08:03:11.775647: Current learning rate: 0.00584\n",
      "2024-01-04 08:05:16.972872: train_loss -0.9164\n",
      "2024-01-04 08:05:16.979873: val_loss -0.8033\n",
      "2024-01-04 08:05:16.986873: Pseudo dice [0.9252, 0.9283, 0.9356]\n",
      "2024-01-04 08:05:16.991873: Epoch time: 125.21 s\n",
      "2024-01-04 08:05:18.213801: \n",
      "2024-01-04 08:05:18.219702: Epoch 451\n",
      "2024-01-04 08:05:18.224694: Current learning rate: 0.00583\n",
      "2024-01-04 08:07:23.817695: train_loss -0.9155\n",
      "2024-01-04 08:07:23.826762: val_loss -0.801\n",
      "2024-01-04 08:07:23.835240: Pseudo dice [0.9208, 0.9282, 0.9358]\n",
      "2024-01-04 08:07:23.840239: Epoch time: 125.6 s\n",
      "2024-01-04 08:07:24.974610: \n",
      "2024-01-04 08:07:24.980614: Epoch 452\n",
      "2024-01-04 08:07:24.986674: Current learning rate: 0.00582\n",
      "2024-01-04 08:09:30.678932: train_loss -0.9124\n",
      "2024-01-04 08:09:30.685932: val_loss -0.8089\n",
      "2024-01-04 08:09:30.690932: Pseudo dice [0.9231, 0.9292, 0.9373]\n",
      "2024-01-04 08:09:30.695932: Epoch time: 125.71 s\n",
      "2024-01-04 08:09:31.848903: \n",
      "2024-01-04 08:09:31.855066: Epoch 453\n",
      "2024-01-04 08:09:31.861164: Current learning rate: 0.00581\n",
      "2024-01-04 08:11:37.298382: train_loss -0.9144\n",
      "2024-01-04 08:11:37.307383: val_loss -0.7966\n",
      "2024-01-04 08:11:37.313381: Pseudo dice [0.9219, 0.9259, 0.9343]\n",
      "2024-01-04 08:11:37.317898: Epoch time: 125.45 s\n",
      "2024-01-04 08:11:38.353137: \n",
      "2024-01-04 08:11:38.359141: Epoch 454\n",
      "2024-01-04 08:11:38.363144: Current learning rate: 0.0058\n",
      "2024-01-04 08:13:43.815656: train_loss -0.9144\n",
      "2024-01-04 08:13:43.822654: val_loss -0.8075\n",
      "2024-01-04 08:13:43.827654: Pseudo dice [0.9217, 0.9281, 0.9358]\n",
      "2024-01-04 08:13:43.833663: Epoch time: 125.46 s\n",
      "2024-01-04 08:13:44.975271: \n",
      "2024-01-04 08:13:44.980270: Epoch 455\n",
      "2024-01-04 08:13:44.985283: Current learning rate: 0.00579\n",
      "2024-01-04 08:15:50.252356: train_loss -0.9143\n",
      "2024-01-04 08:15:50.260356: val_loss -0.8054\n",
      "2024-01-04 08:15:50.266362: Pseudo dice [0.9219, 0.9274, 0.9355]\n",
      "2024-01-04 08:15:50.271363: Epoch time: 125.28 s\n",
      "2024-01-04 08:15:51.588461: \n",
      "2024-01-04 08:15:51.596536: Epoch 456\n",
      "2024-01-04 08:15:51.600524: Current learning rate: 0.00578\n",
      "2024-01-04 08:17:56.891519: train_loss -0.9167\n",
      "2024-01-04 08:17:56.902524: val_loss -0.8011\n",
      "2024-01-04 08:17:56.921251: Pseudo dice [0.9196, 0.9263, 0.9335]\n",
      "2024-01-04 08:17:56.927247: Epoch time: 125.3 s\n",
      "2024-01-04 08:17:58.101525: \n",
      "2024-01-04 08:17:58.109464: Epoch 457\n",
      "2024-01-04 08:17:58.117464: Current learning rate: 0.00577\n",
      "2024-01-04 08:20:03.760088: train_loss -0.9144\n",
      "2024-01-04 08:20:03.767087: val_loss -0.8094\n",
      "2024-01-04 08:20:03.774086: Pseudo dice [0.9192, 0.9293, 0.9365]\n",
      "2024-01-04 08:20:03.780090: Epoch time: 125.66 s\n",
      "2024-01-04 08:20:04.974241: \n",
      "2024-01-04 08:20:04.982289: Epoch 458\n",
      "2024-01-04 08:20:04.987319: Current learning rate: 0.00576\n",
      "2024-01-04 08:22:10.388703: train_loss -0.9139\n",
      "2024-01-04 08:22:10.397701: val_loss -0.8102\n",
      "2024-01-04 08:22:10.405210: Pseudo dice [0.9231, 0.9314, 0.9387]\n",
      "2024-01-04 08:22:10.410210: Epoch time: 125.42 s\n",
      "2024-01-04 08:22:11.694109: \n",
      "2024-01-04 08:22:11.703950: Epoch 459\n",
      "2024-01-04 08:22:11.709016: Current learning rate: 0.00575\n",
      "2024-01-04 08:24:17.112830: train_loss -0.9138\n",
      "2024-01-04 08:24:17.120828: val_loss -0.7946\n",
      "2024-01-04 08:24:17.127829: Pseudo dice [0.9219, 0.9272, 0.9355]\n",
      "2024-01-04 08:24:17.136874: Epoch time: 125.42 s\n",
      "2024-01-04 08:24:18.345691: \n",
      "2024-01-04 08:24:18.356699: Epoch 460\n",
      "2024-01-04 08:24:18.362688: Current learning rate: 0.00574\n",
      "2024-01-04 08:26:23.702741: train_loss -0.9152\n",
      "2024-01-04 08:26:23.708741: val_loss -0.8104\n",
      "2024-01-04 08:26:23.713741: Pseudo dice [0.9203, 0.9271, 0.9345]\n",
      "2024-01-04 08:26:23.722741: Epoch time: 125.36 s\n",
      "2024-01-04 08:26:24.766300: \n",
      "2024-01-04 08:26:24.773057: Epoch 461\n",
      "2024-01-04 08:26:24.778996: Current learning rate: 0.00573\n",
      "2024-01-04 08:28:30.308925: train_loss -0.9158\n",
      "2024-01-04 08:28:30.314938: val_loss -0.7972\n",
      "2024-01-04 08:28:30.320927: Pseudo dice [0.9221, 0.9265, 0.9337]\n",
      "2024-01-04 08:28:30.325925: Epoch time: 125.54 s\n",
      "2024-01-04 08:28:31.528051: \n",
      "2024-01-04 08:28:31.535195: Epoch 462\n",
      "2024-01-04 08:28:31.542137: Current learning rate: 0.00572\n",
      "2024-01-04 08:30:37.063286: train_loss -0.913\n",
      "2024-01-04 08:30:37.073820: val_loss -0.8036\n",
      "2024-01-04 08:30:37.079825: Pseudo dice [0.9234, 0.9281, 0.9368]\n",
      "2024-01-04 08:30:37.087818: Epoch time: 125.54 s\n",
      "2024-01-04 08:30:38.309954: \n",
      "2024-01-04 08:30:38.317026: Epoch 463\n",
      "2024-01-04 08:30:38.323965: Current learning rate: 0.00571\n",
      "2024-01-04 08:32:43.861537: train_loss -0.9142\n",
      "2024-01-04 08:32:43.868545: val_loss -0.7987\n",
      "2024-01-04 08:32:43.874547: Pseudo dice [0.9226, 0.9284, 0.9389]\n",
      "2024-01-04 08:32:43.878545: Epoch time: 125.55 s\n",
      "2024-01-04 08:32:45.103789: \n",
      "2024-01-04 08:32:45.115081: Epoch 464\n",
      "2024-01-04 08:32:45.120103: Current learning rate: 0.0057\n",
      "2024-01-04 08:34:50.563425: train_loss -0.9164\n",
      "2024-01-04 08:34:50.570424: val_loss -0.8017\n",
      "2024-01-04 08:34:50.577425: Pseudo dice [0.9212, 0.927, 0.9334]\n",
      "2024-01-04 08:34:50.583426: Epoch time: 125.46 s\n",
      "2024-01-04 08:34:51.769653: \n",
      "2024-01-04 08:34:51.775645: Epoch 465\n",
      "2024-01-04 08:34:51.780655: Current learning rate: 0.0057\n",
      "2024-01-04 08:36:57.026883: train_loss -0.916\n",
      "2024-01-04 08:36:57.034883: val_loss -0.7899\n",
      "2024-01-04 08:36:57.039886: Pseudo dice [0.9251, 0.9256, 0.9338]\n",
      "2024-01-04 08:36:57.047886: Epoch time: 125.26 s\n",
      "2024-01-04 08:36:58.183811: \n",
      "2024-01-04 08:36:58.189668: Epoch 466\n",
      "2024-01-04 08:36:58.195728: Current learning rate: 0.00569\n",
      "2024-01-04 08:39:03.876521: train_loss -0.9159\n",
      "2024-01-04 08:39:03.882525: val_loss -0.8059\n",
      "2024-01-04 08:39:03.888525: Pseudo dice [0.9263, 0.9281, 0.9357]\n",
      "2024-01-04 08:39:03.892526: Epoch time: 125.69 s\n",
      "2024-01-04 08:39:05.020702: \n",
      "2024-01-04 08:39:05.026177: Epoch 467\n",
      "2024-01-04 08:39:05.033245: Current learning rate: 0.00568\n",
      "2024-01-04 08:41:10.194080: train_loss -0.92\n",
      "2024-01-04 08:41:10.203080: val_loss -0.802\n",
      "2024-01-04 08:41:10.209080: Pseudo dice [0.9241, 0.928, 0.9363]\n",
      "2024-01-04 08:41:10.213079: Epoch time: 125.17 s\n",
      "2024-01-04 08:41:11.429587: \n",
      "2024-01-04 08:41:11.436582: Epoch 468\n",
      "2024-01-04 08:41:11.441588: Current learning rate: 0.00567\n",
      "2024-01-04 08:43:16.859981: train_loss -0.9145\n",
      "2024-01-04 08:43:16.865991: val_loss -0.8027\n",
      "2024-01-04 08:43:16.871982: Pseudo dice [0.9236, 0.9277, 0.9372]\n",
      "2024-01-04 08:43:16.878489: Epoch time: 125.43 s\n",
      "2024-01-04 08:43:18.041608: \n",
      "2024-01-04 08:43:18.049199: Epoch 469\n",
      "2024-01-04 08:43:18.054258: Current learning rate: 0.00566\n",
      "2024-01-04 08:45:23.693505: train_loss -0.9144\n",
      "2024-01-04 08:45:23.702513: val_loss -0.7979\n",
      "2024-01-04 08:45:23.708505: Pseudo dice [0.9225, 0.9264, 0.9351]\n",
      "2024-01-04 08:45:23.714506: Epoch time: 125.65 s\n",
      "2024-01-04 08:45:24.812459: \n",
      "2024-01-04 08:45:24.823410: Epoch 470\n",
      "2024-01-04 08:45:24.828410: Current learning rate: 0.00565\n",
      "2024-01-04 08:47:30.236414: train_loss -0.9186\n",
      "2024-01-04 08:47:30.245410: val_loss -0.8076\n",
      "2024-01-04 08:47:30.250417: Pseudo dice [0.9208, 0.9294, 0.936]\n",
      "2024-01-04 08:47:30.256417: Epoch time: 125.43 s\n",
      "2024-01-04 08:47:31.287054: \n",
      "2024-01-04 08:47:31.292138: Epoch 471\n",
      "2024-01-04 08:47:31.297221: Current learning rate: 0.00564\n",
      "2024-01-04 08:49:37.361474: train_loss -0.913\n",
      "2024-01-04 08:49:37.367474: val_loss -0.8054\n",
      "2024-01-04 08:49:37.374476: Pseudo dice [0.9224, 0.9288, 0.9369]\n",
      "2024-01-04 08:49:37.380485: Epoch time: 126.08 s\n",
      "2024-01-04 08:49:38.736548: \n",
      "2024-01-04 08:49:38.741700: Epoch 472\n",
      "2024-01-04 08:49:38.749013: Current learning rate: 0.00563\n",
      "2024-01-04 08:51:44.082792: train_loss -0.9159\n",
      "2024-01-04 08:51:44.089806: val_loss -0.8031\n",
      "2024-01-04 08:51:44.096807: Pseudo dice [0.9212, 0.9293, 0.9367]\n",
      "2024-01-04 08:51:44.105792: Epoch time: 125.35 s\n",
      "2024-01-04 08:51:45.126590: \n",
      "2024-01-04 08:51:45.133221: Epoch 473\n",
      "2024-01-04 08:51:45.138200: Current learning rate: 0.00562\n",
      "2024-01-04 08:53:50.760588: train_loss -0.9145\n",
      "2024-01-04 08:53:50.771589: val_loss -0.7986\n",
      "2024-01-04 08:53:50.781591: Pseudo dice [0.9232, 0.9278, 0.9355]\n",
      "2024-01-04 08:53:50.789590: Epoch time: 125.63 s\n",
      "2024-01-04 08:53:51.888374: \n",
      "2024-01-04 08:53:51.899494: Epoch 474\n",
      "2024-01-04 08:53:51.904418: Current learning rate: 0.00561\n",
      "2024-01-04 08:55:57.294882: train_loss -0.9161\n",
      "2024-01-04 08:55:57.302882: val_loss -0.8052\n",
      "2024-01-04 08:55:57.309882: Pseudo dice [0.9227, 0.9288, 0.9366]\n",
      "2024-01-04 08:55:57.314882: Epoch time: 125.41 s\n",
      "2024-01-04 08:55:58.412528: \n",
      "2024-01-04 08:55:58.418444: Epoch 475\n",
      "2024-01-04 08:55:58.423024: Current learning rate: 0.0056\n",
      "2024-01-04 08:58:04.045344: train_loss -0.9145\n",
      "2024-01-04 08:58:04.051858: val_loss -0.8076\n",
      "2024-01-04 08:58:04.059854: Pseudo dice [0.9186, 0.9273, 0.9344]\n",
      "2024-01-04 08:58:04.064854: Epoch time: 125.63 s\n",
      "2024-01-04 08:58:05.192777: \n",
      "2024-01-04 08:58:05.200918: Epoch 476\n",
      "2024-01-04 08:58:05.204932: Current learning rate: 0.00559\n",
      "2024-01-04 09:00:10.554107: train_loss -0.9161\n",
      "2024-01-04 09:00:10.562107: val_loss -0.8013\n",
      "2024-01-04 09:00:10.570108: Pseudo dice [0.9239, 0.9284, 0.9362]\n",
      "2024-01-04 09:00:10.576108: Epoch time: 125.36 s\n",
      "2024-01-04 09:00:11.732583: \n",
      "2024-01-04 09:00:11.740656: Epoch 477\n",
      "2024-01-04 09:00:11.745656: Current learning rate: 0.00558\n",
      "2024-01-04 09:02:17.526912: train_loss -0.9164\n",
      "2024-01-04 09:02:17.532910: val_loss -0.8038\n",
      "2024-01-04 09:02:17.538903: Pseudo dice [0.9247, 0.9305, 0.9374]\n",
      "2024-01-04 09:02:17.543904: Epoch time: 125.8 s\n",
      "2024-01-04 09:02:18.588383: \n",
      "2024-01-04 09:02:18.597375: Epoch 478\n",
      "2024-01-04 09:02:18.601894: Current learning rate: 0.00557\n",
      "2024-01-04 09:04:23.961926: train_loss -0.9157\n",
      "2024-01-04 09:04:23.968919: val_loss -0.8141\n",
      "2024-01-04 09:04:23.973920: Pseudo dice [0.9232, 0.9297, 0.9364]\n",
      "2024-01-04 09:04:23.979512: Epoch time: 125.37 s\n",
      "2024-01-04 09:04:25.074486: \n",
      "2024-01-04 09:04:25.081149: Epoch 479\n",
      "2024-01-04 09:04:25.087169: Current learning rate: 0.00556\n",
      "2024-01-04 09:06:30.457406: train_loss -0.9163\n",
      "2024-01-04 09:06:30.463410: val_loss -0.8036\n",
      "2024-01-04 09:06:30.468410: Pseudo dice [0.9243, 0.9285, 0.9366]\n",
      "2024-01-04 09:06:30.473410: Epoch time: 125.38 s\n",
      "2024-01-04 09:06:31.805746: \n",
      "2024-01-04 09:06:31.813386: Epoch 480\n",
      "2024-01-04 09:06:31.817393: Current learning rate: 0.00555\n",
      "2024-01-04 09:08:37.491523: train_loss -0.9132\n",
      "2024-01-04 09:08:37.498545: val_loss -0.802\n",
      "2024-01-04 09:08:37.508528: Pseudo dice [0.9232, 0.927, 0.9362]\n",
      "2024-01-04 09:08:37.517665: Epoch time: 125.69 s\n",
      "2024-01-04 09:08:38.719931: \n",
      "2024-01-04 09:08:38.726084: Epoch 481\n",
      "2024-01-04 09:08:38.731082: Current learning rate: 0.00554\n",
      "2024-01-04 09:10:43.997399: train_loss -0.9179\n",
      "2024-01-04 09:10:44.004400: val_loss -0.8087\n",
      "2024-01-04 09:10:44.010402: Pseudo dice [0.9231, 0.9282, 0.9376]\n",
      "2024-01-04 09:10:44.015396: Epoch time: 125.28 s\n",
      "2024-01-04 09:10:45.216616: \n",
      "2024-01-04 09:10:45.226975: Epoch 482\n",
      "2024-01-04 09:10:45.232070: Current learning rate: 0.00553\n",
      "2024-01-04 09:12:51.034018: train_loss -0.9141\n",
      "2024-01-04 09:12:51.047027: val_loss -0.8042\n",
      "2024-01-04 09:12:51.056024: Pseudo dice [0.9238, 0.9271, 0.934]\n",
      "2024-01-04 09:12:51.064023: Epoch time: 125.82 s\n",
      "2024-01-04 09:12:52.326060: \n",
      "2024-01-04 09:12:52.332031: Epoch 483\n",
      "2024-01-04 09:12:52.336046: Current learning rate: 0.00552\n",
      "2024-01-04 09:14:57.824930: train_loss -0.9143\n",
      "2024-01-04 09:14:57.834934: val_loss -0.7958\n",
      "2024-01-04 09:14:57.840934: Pseudo dice [0.9199, 0.9293, 0.9373]\n",
      "2024-01-04 09:14:57.849471: Epoch time: 125.5 s\n",
      "2024-01-04 09:14:59.011907: \n",
      "2024-01-04 09:14:59.020888: Epoch 484\n",
      "2024-01-04 09:14:59.031094: Current learning rate: 0.00551\n",
      "2024-01-04 09:17:04.932949: train_loss -0.9123\n",
      "2024-01-04 09:17:04.939950: val_loss -0.8058\n",
      "2024-01-04 09:17:04.946469: Pseudo dice [0.9163, 0.9279, 0.935]\n",
      "2024-01-04 09:17:04.952471: Epoch time: 125.92 s\n",
      "2024-01-04 09:17:06.115502: \n",
      "2024-01-04 09:17:06.123677: Epoch 485\n",
      "2024-01-04 09:17:06.135702: Current learning rate: 0.0055\n",
      "2024-01-04 09:19:11.909435: train_loss -0.9128\n",
      "2024-01-04 09:19:11.915434: val_loss -0.7968\n",
      "2024-01-04 09:19:11.920434: Pseudo dice [0.9205, 0.9292, 0.9377]\n",
      "2024-01-04 09:19:11.925564: Epoch time: 125.8 s\n",
      "2024-01-04 09:19:13.160262: \n",
      "2024-01-04 09:19:13.165951: Epoch 486\n",
      "2024-01-04 09:19:13.170928: Current learning rate: 0.00549\n",
      "2024-01-04 09:21:18.796654: train_loss -0.9149\n",
      "2024-01-04 09:21:18.804656: val_loss -0.8066\n",
      "2024-01-04 09:21:18.810655: Pseudo dice [0.9203, 0.9278, 0.9377]\n",
      "2024-01-04 09:21:18.815655: Epoch time: 125.64 s\n",
      "2024-01-04 09:21:20.237034: \n",
      "2024-01-04 09:21:20.243018: Epoch 487\n",
      "2024-01-04 09:21:20.247963: Current learning rate: 0.00548\n",
      "2024-01-04 09:23:25.891317: train_loss -0.9159\n",
      "2024-01-04 09:23:25.897835: val_loss -0.8039\n",
      "2024-01-04 09:23:25.903827: Pseudo dice [0.9232, 0.9286, 0.9383]\n",
      "2024-01-04 09:23:25.908826: Epoch time: 125.66 s\n",
      "2024-01-04 09:23:27.065129: \n",
      "2024-01-04 09:23:27.070189: Epoch 488\n",
      "2024-01-04 09:23:27.075201: Current learning rate: 0.00547\n",
      "2024-01-04 09:25:32.407843: train_loss -0.9178\n",
      "2024-01-04 09:25:32.415842: val_loss -0.8\n",
      "2024-01-04 09:25:32.424843: Pseudo dice [0.9224, 0.9277, 0.9356]\n",
      "2024-01-04 09:25:32.446912: Epoch time: 125.34 s\n",
      "2024-01-04 09:25:33.547114: \n",
      "2024-01-04 09:25:33.553153: Epoch 489\n",
      "2024-01-04 09:25:33.557173: Current learning rate: 0.00546\n",
      "2024-01-04 09:27:38.979586: train_loss -0.9178\n",
      "2024-01-04 09:27:38.986577: val_loss -0.8107\n",
      "2024-01-04 09:27:38.992577: Pseudo dice [0.9216, 0.9302, 0.9411]\n",
      "2024-01-04 09:27:38.997577: Epoch time: 125.44 s\n",
      "2024-01-04 09:27:40.203224: \n",
      "2024-01-04 09:27:40.210784: Epoch 490\n",
      "2024-01-04 09:27:40.222800: Current learning rate: 0.00546\n",
      "2024-01-04 09:29:46.065787: train_loss -0.9132\n",
      "2024-01-04 09:29:46.073793: val_loss -0.805\n",
      "2024-01-04 09:29:46.083226: Pseudo dice [0.92, 0.9291, 0.9372]\n",
      "2024-01-04 09:29:46.088226: Epoch time: 125.86 s\n",
      "2024-01-04 09:29:47.211901: \n",
      "2024-01-04 09:29:47.217887: Epoch 491\n",
      "2024-01-04 09:29:47.222965: Current learning rate: 0.00545\n",
      "2024-01-04 09:31:52.686659: train_loss -0.9165\n",
      "2024-01-04 09:31:52.692658: val_loss -0.8061\n",
      "2024-01-04 09:31:52.702658: Pseudo dice [0.9209, 0.9289, 0.9366]\n",
      "2024-01-04 09:31:52.708658: Epoch time: 125.48 s\n",
      "2024-01-04 09:31:53.922816: \n",
      "2024-01-04 09:31:53.929812: Epoch 492\n",
      "2024-01-04 09:31:53.940752: Current learning rate: 0.00544\n",
      "2024-01-04 09:33:59.607263: train_loss -0.916\n",
      "2024-01-04 09:33:59.616274: val_loss -0.8097\n",
      "2024-01-04 09:33:59.623271: Pseudo dice [0.9216, 0.9302, 0.9374]\n",
      "2024-01-04 09:33:59.628271: Epoch time: 125.69 s\n",
      "2024-01-04 09:34:00.990097: \n",
      "2024-01-04 09:34:00.999240: Epoch 493\n",
      "2024-01-04 09:34:01.004185: Current learning rate: 0.00543\n",
      "2024-01-04 09:36:06.645634: train_loss -0.913\n",
      "2024-01-04 09:36:06.652626: val_loss -0.8063\n",
      "2024-01-04 09:36:06.657623: Pseudo dice [0.9251, 0.9278, 0.9353]\n",
      "2024-01-04 09:36:06.663623: Epoch time: 125.66 s\n",
      "2024-01-04 09:36:07.932859: \n",
      "2024-01-04 09:36:07.943720: Epoch 494\n",
      "2024-01-04 09:36:07.952694: Current learning rate: 0.00542\n",
      "2024-01-04 09:38:13.543310: train_loss -0.9148\n",
      "2024-01-04 09:38:13.553310: val_loss -0.8074\n",
      "2024-01-04 09:38:13.560315: Pseudo dice [0.9199, 0.9317, 0.9389]\n",
      "2024-01-04 09:38:13.567315: Epoch time: 125.61 s\n",
      "2024-01-04 09:38:14.859755: \n",
      "2024-01-04 09:38:14.865363: Epoch 495\n",
      "2024-01-04 09:38:14.869952: Current learning rate: 0.00541\n",
      "2024-01-04 09:40:20.467470: train_loss -0.9179\n",
      "2024-01-04 09:40:20.475468: val_loss -0.8065\n",
      "2024-01-04 09:40:20.482468: Pseudo dice [0.9201, 0.9288, 0.9367]\n",
      "2024-01-04 09:40:20.487469: Epoch time: 125.61 s\n",
      "2024-01-04 09:40:21.791935: \n",
      "2024-01-04 09:40:21.798936: Epoch 496\n",
      "2024-01-04 09:40:21.804936: Current learning rate: 0.0054\n",
      "2024-01-04 09:42:27.589394: train_loss -0.9136\n",
      "2024-01-04 09:42:27.596399: val_loss -0.8131\n",
      "2024-01-04 09:42:27.601392: Pseudo dice [0.9225, 0.9293, 0.9362]\n",
      "2024-01-04 09:42:27.605404: Epoch time: 125.8 s\n",
      "2024-01-04 09:42:28.739628: \n",
      "2024-01-04 09:42:28.750144: Epoch 497\n",
      "2024-01-04 09:42:28.755146: Current learning rate: 0.00539\n",
      "2024-01-04 09:44:34.139617: train_loss -0.9174\n",
      "2024-01-04 09:44:34.150617: val_loss -0.8045\n",
      "2024-01-04 09:44:34.160619: Pseudo dice [0.9192, 0.9274, 0.9359]\n",
      "2024-01-04 09:44:34.166617: Epoch time: 125.4 s\n",
      "2024-01-04 09:44:35.374154: \n",
      "2024-01-04 09:44:35.383146: Epoch 498\n",
      "2024-01-04 09:44:35.391087: Current learning rate: 0.00538\n",
      "2024-01-04 09:46:40.870358: train_loss -0.9179\n",
      "2024-01-04 09:46:40.876358: val_loss -0.7919\n",
      "2024-01-04 09:46:40.883357: Pseudo dice [0.9191, 0.9275, 0.9352]\n",
      "2024-01-04 09:46:40.888358: Epoch time: 125.5 s\n",
      "2024-01-04 09:46:42.100307: \n",
      "2024-01-04 09:46:42.108419: Epoch 499\n",
      "2024-01-04 09:46:42.113491: Current learning rate: 0.00537\n",
      "2024-01-04 09:48:47.893341: train_loss -0.9141\n",
      "2024-01-04 09:48:47.900419: val_loss -0.8026\n",
      "2024-01-04 09:48:47.908413: Pseudo dice [0.9202, 0.9271, 0.9358]\n",
      "2024-01-04 09:48:47.913345: Epoch time: 125.79 s\n",
      "2024-01-04 09:48:49.419927: \n",
      "2024-01-04 09:48:49.428000: Epoch 500\n",
      "2024-01-04 09:48:49.432986: Current learning rate: 0.00536\n",
      "2024-01-04 09:50:54.967530: train_loss -0.9132\n",
      "2024-01-04 09:50:54.974531: val_loss -0.8045\n",
      "2024-01-04 09:50:54.979533: Pseudo dice [0.919, 0.9292, 0.9374]\n",
      "2024-01-04 09:50:54.984533: Epoch time: 125.55 s\n",
      "2024-01-04 09:50:56.156064: \n",
      "2024-01-04 09:50:56.162436: Epoch 501\n",
      "2024-01-04 09:50:56.167430: Current learning rate: 0.00535\n",
      "2024-01-04 09:53:01.444830: train_loss -0.9172\n",
      "2024-01-04 09:53:01.451823: val_loss -0.8056\n",
      "2024-01-04 09:53:01.456827: Pseudo dice [0.9202, 0.9299, 0.9378]\n",
      "2024-01-04 09:53:01.461826: Epoch time: 125.29 s\n",
      "2024-01-04 09:53:02.637229: \n",
      "2024-01-04 09:53:02.643283: Epoch 502\n",
      "2024-01-04 09:53:02.649300: Current learning rate: 0.00534\n",
      "2024-01-04 09:55:08.415301: train_loss -0.9139\n",
      "2024-01-04 09:55:08.421231: val_loss -0.7941\n",
      "2024-01-04 09:55:08.426235: Pseudo dice [0.9224, 0.9266, 0.9349]\n",
      "2024-01-04 09:55:08.433313: Epoch time: 125.78 s\n",
      "2024-01-04 09:55:09.749005: \n",
      "2024-01-04 09:55:09.757586: Epoch 503\n",
      "2024-01-04 09:55:09.761585: Current learning rate: 0.00533\n",
      "2024-01-04 09:57:15.431082: train_loss -0.9148\n",
      "2024-01-04 09:57:15.437092: val_loss -0.8052\n",
      "2024-01-04 09:57:15.443637: Pseudo dice [0.9218, 0.9305, 0.9374]\n",
      "2024-01-04 09:57:15.448628: Epoch time: 125.68 s\n",
      "2024-01-04 09:57:16.642303: \n",
      "2024-01-04 09:57:16.706983: Epoch 504\n",
      "2024-01-04 09:57:16.712006: Current learning rate: 0.00532\n",
      "2024-01-04 09:59:22.235679: train_loss -0.9174\n",
      "2024-01-04 09:59:22.242679: val_loss -0.8021\n",
      "2024-01-04 09:59:22.247679: Pseudo dice [0.9224, 0.928, 0.9359]\n",
      "2024-01-04 09:59:22.252683: Epoch time: 125.6 s\n",
      "2024-01-04 09:59:23.453063: \n",
      "2024-01-04 09:59:23.462107: Epoch 505\n",
      "2024-01-04 09:59:23.467704: Current learning rate: 0.00531\n",
      "2024-01-04 10:01:29.209403: train_loss -0.9152\n",
      "2024-01-04 10:01:29.214911: val_loss -0.7967\n",
      "2024-01-04 10:01:29.222913: Pseudo dice [0.9227, 0.9269, 0.9346]\n",
      "2024-01-04 10:01:29.227911: Epoch time: 125.76 s\n",
      "2024-01-04 10:01:30.328016: \n",
      "2024-01-04 10:01:30.333930: Epoch 506\n",
      "2024-01-04 10:01:30.339025: Current learning rate: 0.0053\n",
      "2024-01-04 10:03:35.767467: train_loss -0.9151\n",
      "2024-01-04 10:03:35.775467: val_loss -0.8091\n",
      "2024-01-04 10:03:35.781468: Pseudo dice [0.9208, 0.9295, 0.9378]\n",
      "2024-01-04 10:03:35.786467: Epoch time: 125.44 s\n",
      "2024-01-04 10:03:36.895965: \n",
      "2024-01-04 10:03:36.901900: Epoch 507\n",
      "2024-01-04 10:03:36.906889: Current learning rate: 0.00529\n",
      "2024-01-04 10:05:42.745511: train_loss -0.9159\n",
      "2024-01-04 10:05:42.751503: val_loss -0.7959\n",
      "2024-01-04 10:05:42.756503: Pseudo dice [0.9195, 0.924, 0.9317]\n",
      "2024-01-04 10:05:42.761501: Epoch time: 125.85 s\n",
      "2024-01-04 10:05:43.963219: \n",
      "2024-01-04 10:05:43.968623: Epoch 508\n",
      "2024-01-04 10:05:43.974697: Current learning rate: 0.00528\n",
      "2024-01-04 10:07:49.553692: train_loss -0.9146\n",
      "2024-01-04 10:07:49.559693: val_loss -0.7931\n",
      "2024-01-04 10:07:49.564770: Pseudo dice [0.9239, 0.9244, 0.933]\n",
      "2024-01-04 10:07:49.569774: Epoch time: 125.59 s\n",
      "2024-01-04 10:07:50.741676: \n",
      "2024-01-04 10:07:50.753178: Epoch 509\n",
      "2024-01-04 10:07:50.758182: Current learning rate: 0.00527\n",
      "2024-01-04 10:09:56.515752: train_loss -0.9124\n",
      "2024-01-04 10:09:56.522753: val_loss -0.8078\n",
      "2024-01-04 10:09:56.527752: Pseudo dice [0.9184, 0.9252, 0.9328]\n",
      "2024-01-04 10:09:56.535757: Epoch time: 125.78 s\n",
      "2024-01-04 10:09:57.653126: \n",
      "2024-01-04 10:09:57.661128: Epoch 510\n",
      "2024-01-04 10:09:57.665128: Current learning rate: 0.00526\n",
      "2024-01-04 10:12:03.191212: train_loss -0.9154\n",
      "2024-01-04 10:12:03.202216: val_loss -0.8089\n",
      "2024-01-04 10:12:03.208212: Pseudo dice [0.9224, 0.929, 0.9389]\n",
      "2024-01-04 10:12:03.214217: Epoch time: 125.54 s\n",
      "2024-01-04 10:12:04.574010: \n",
      "2024-01-04 10:12:04.586116: Epoch 511\n",
      "2024-01-04 10:12:04.591041: Current learning rate: 0.00525\n",
      "2024-01-04 10:14:10.440003: train_loss -0.9175\n",
      "2024-01-04 10:14:10.446076: val_loss -0.7978\n",
      "2024-01-04 10:14:10.452068: Pseudo dice [0.9187, 0.9286, 0.9367]\n",
      "2024-01-04 10:14:10.459063: Epoch time: 125.87 s\n",
      "2024-01-04 10:14:11.509250: \n",
      "2024-01-04 10:14:11.515250: Epoch 512\n",
      "2024-01-04 10:14:11.519250: Current learning rate: 0.00524\n",
      "2024-01-04 10:16:17.033288: train_loss -0.9145\n",
      "2024-01-04 10:16:17.041288: val_loss -0.8032\n",
      "2024-01-04 10:16:17.047806: Pseudo dice [0.9218, 0.9275, 0.9361]\n",
      "2024-01-04 10:16:17.054806: Epoch time: 125.53 s\n",
      "2024-01-04 10:16:18.216251: \n",
      "2024-01-04 10:16:18.224477: Epoch 513\n",
      "2024-01-04 10:16:18.228554: Current learning rate: 0.00523\n",
      "2024-01-04 10:18:23.625775: train_loss -0.9159\n",
      "2024-01-04 10:18:23.635284: val_loss -0.8047\n",
      "2024-01-04 10:18:23.642284: Pseudo dice [0.9203, 0.9271, 0.9356]\n",
      "2024-01-04 10:18:23.647284: Epoch time: 125.41 s\n",
      "2024-01-04 10:18:24.745013: \n",
      "2024-01-04 10:18:24.753238: Epoch 514\n",
      "2024-01-04 10:18:24.757302: Current learning rate: 0.00522\n",
      "2024-01-04 10:20:30.283235: train_loss -0.9142\n",
      "2024-01-04 10:20:30.290239: val_loss -0.7924\n",
      "2024-01-04 10:20:30.298224: Pseudo dice [0.9151, 0.9263, 0.9363]\n",
      "2024-01-04 10:20:30.305185: Epoch time: 125.54 s\n",
      "2024-01-04 10:20:31.502838: \n",
      "2024-01-04 10:20:31.512089: Epoch 515\n",
      "2024-01-04 10:20:31.516156: Current learning rate: 0.00521\n",
      "2024-01-04 10:22:36.876714: train_loss -0.918\n",
      "2024-01-04 10:22:36.883713: val_loss -0.8089\n",
      "2024-01-04 10:22:36.888722: Pseudo dice [0.9175, 0.9272, 0.9364]\n",
      "2024-01-04 10:22:36.893722: Epoch time: 125.37 s\n",
      "2024-01-04 10:22:37.958600: \n",
      "2024-01-04 10:22:37.964598: Epoch 516\n",
      "2024-01-04 10:22:37.968606: Current learning rate: 0.0052\n",
      "2024-01-04 10:24:43.646960: train_loss -0.9159\n",
      "2024-01-04 10:24:43.655961: val_loss -0.7921\n",
      "2024-01-04 10:24:43.664472: Pseudo dice [0.9223, 0.9264, 0.9339]\n",
      "2024-01-04 10:24:43.670471: Epoch time: 125.69 s\n",
      "2024-01-04 10:24:44.997172: \n",
      "2024-01-04 10:24:45.003167: Epoch 517\n",
      "2024-01-04 10:24:45.007168: Current learning rate: 0.00519\n",
      "2024-01-04 10:26:50.567572: train_loss -0.9175\n",
      "2024-01-04 10:26:50.573573: val_loss -0.8024\n",
      "2024-01-04 10:26:50.578830: Pseudo dice [0.922, 0.9269, 0.9358]\n",
      "2024-01-04 10:26:50.583831: Epoch time: 125.57 s\n",
      "2024-01-04 10:26:51.867560: \n",
      "2024-01-04 10:26:51.873252: Epoch 518\n",
      "2024-01-04 10:26:51.878240: Current learning rate: 0.00518\n",
      "2024-01-04 10:28:57.710683: train_loss -0.9169\n",
      "2024-01-04 10:28:57.716685: val_loss -0.8069\n",
      "2024-01-04 10:28:57.722685: Pseudo dice [0.9209, 0.9275, 0.9364]\n",
      "2024-01-04 10:28:57.726684: Epoch time: 125.85 s\n",
      "2024-01-04 10:28:59.191483: \n",
      "2024-01-04 10:28:59.199893: Epoch 519\n",
      "2024-01-04 10:28:59.205835: Current learning rate: 0.00518\n",
      "2024-01-04 10:31:04.622291: train_loss -0.9152\n",
      "2024-01-04 10:31:04.630787: val_loss -0.8086\n",
      "2024-01-04 10:31:04.637795: Pseudo dice [0.9236, 0.9303, 0.9388]\n",
      "2024-01-04 10:31:04.642793: Epoch time: 125.43 s\n",
      "2024-01-04 10:31:05.780709: \n",
      "2024-01-04 10:31:05.792848: Epoch 520\n",
      "2024-01-04 10:31:05.797909: Current learning rate: 0.00517\n",
      "2024-01-04 10:33:11.291298: train_loss -0.9142\n",
      "2024-01-04 10:33:11.298299: val_loss -0.8074\n",
      "2024-01-04 10:33:11.305805: Pseudo dice [0.9222, 0.9284, 0.9365]\n",
      "2024-01-04 10:33:11.311810: Epoch time: 125.51 s\n",
      "2024-01-04 10:33:12.483338: \n",
      "2024-01-04 10:33:12.490388: Epoch 521\n",
      "2024-01-04 10:33:12.495473: Current learning rate: 0.00516\n",
      "2024-01-04 10:35:18.107471: train_loss -0.9157\n",
      "2024-01-04 10:35:18.116470: val_loss -0.8016\n",
      "2024-01-04 10:35:18.127470: Pseudo dice [0.9241, 0.9261, 0.9373]\n",
      "2024-01-04 10:35:18.135476: Epoch time: 125.63 s\n",
      "2024-01-04 10:35:19.290213: \n",
      "2024-01-04 10:35:19.297114: Epoch 522\n",
      "2024-01-04 10:35:19.301197: Current learning rate: 0.00515\n",
      "2024-01-04 10:37:24.676643: train_loss -0.9178\n",
      "2024-01-04 10:37:24.683643: val_loss -0.798\n",
      "2024-01-04 10:37:24.689641: Pseudo dice [0.918, 0.9288, 0.9371]\n",
      "2024-01-04 10:37:24.696103: Epoch time: 125.39 s\n",
      "2024-01-04 10:37:25.794623: \n",
      "2024-01-04 10:37:25.800614: Epoch 523\n",
      "2024-01-04 10:37:25.804645: Current learning rate: 0.00514\n",
      "2024-01-04 10:39:31.650785: train_loss -0.9164\n",
      "2024-01-04 10:39:31.661707: val_loss -0.7992\n",
      "2024-01-04 10:39:31.668710: Pseudo dice [0.9221, 0.9302, 0.9369]\n",
      "2024-01-04 10:39:31.674759: Epoch time: 125.86 s\n",
      "2024-01-04 10:39:32.912007: \n",
      "2024-01-04 10:39:32.920595: Epoch 524\n",
      "2024-01-04 10:39:32.924660: Current learning rate: 0.00513\n",
      "2024-01-04 10:41:38.658928: train_loss -0.916\n",
      "2024-01-04 10:41:38.666932: val_loss -0.7996\n",
      "2024-01-04 10:41:38.671931: Pseudo dice [0.9205, 0.9268, 0.9363]\n",
      "2024-01-04 10:41:38.677931: Epoch time: 125.75 s\n",
      "2024-01-04 10:41:39.876964: \n",
      "2024-01-04 10:41:39.882614: Epoch 525\n",
      "2024-01-04 10:41:39.887613: Current learning rate: 0.00512\n",
      "2024-01-04 10:43:45.514468: train_loss -0.9176\n",
      "2024-01-04 10:43:45.521467: val_loss -0.7894\n",
      "2024-01-04 10:43:45.527467: Pseudo dice [0.9182, 0.9297, 0.9385]\n",
      "2024-01-04 10:43:45.532476: Epoch time: 125.64 s\n",
      "2024-01-04 10:43:46.653211: \n",
      "2024-01-04 10:43:46.659292: Epoch 526\n",
      "2024-01-04 10:43:46.666298: Current learning rate: 0.00511\n",
      "2024-01-04 10:45:52.280219: train_loss -0.9165\n",
      "2024-01-04 10:45:52.288216: val_loss -0.8035\n",
      "2024-01-04 10:45:52.293216: Pseudo dice [0.9219, 0.9277, 0.935]\n",
      "2024-01-04 10:45:52.298218: Epoch time: 125.63 s\n",
      "2024-01-04 10:45:53.552516: \n",
      "2024-01-04 10:45:53.562946: Epoch 527\n",
      "2024-01-04 10:45:53.568941: Current learning rate: 0.0051\n",
      "2024-01-04 10:47:59.201941: train_loss -0.9173\n",
      "2024-01-04 10:47:59.209452: val_loss -0.805\n",
      "2024-01-04 10:47:59.214453: Pseudo dice [0.9219, 0.9283, 0.9364]\n",
      "2024-01-04 10:47:59.219451: Epoch time: 125.65 s\n",
      "2024-01-04 10:48:00.403338: \n",
      "2024-01-04 10:48:00.409296: Epoch 528\n",
      "2024-01-04 10:48:00.417369: Current learning rate: 0.00509\n",
      "2024-01-04 10:50:05.895369: train_loss -0.9186\n",
      "2024-01-04 10:50:05.902361: val_loss -0.8058\n",
      "2024-01-04 10:50:05.907365: Pseudo dice [0.9191, 0.9297, 0.9381]\n",
      "2024-01-04 10:50:05.913370: Epoch time: 125.5 s\n",
      "2024-01-04 10:50:07.198480: \n",
      "2024-01-04 10:50:07.206490: Epoch 529\n",
      "2024-01-04 10:50:07.211487: Current learning rate: 0.00508\n",
      "2024-01-04 10:52:13.068532: train_loss -0.9187\n",
      "2024-01-04 10:52:13.077534: val_loss -0.8024\n",
      "2024-01-04 10:52:13.084537: Pseudo dice [0.9232, 0.9292, 0.9368]\n",
      "2024-01-04 10:52:13.091534: Epoch time: 125.87 s\n",
      "2024-01-04 10:52:14.493875: \n",
      "2024-01-04 10:52:14.500875: Epoch 530\n",
      "2024-01-04 10:52:14.505938: Current learning rate: 0.00507\n",
      "2024-01-04 10:54:20.378016: train_loss -0.9151\n",
      "2024-01-04 10:54:20.384528: val_loss -0.7995\n",
      "2024-01-04 10:54:20.390538: Pseudo dice [0.9215, 0.9274, 0.9365]\n",
      "2024-01-04 10:54:20.395529: Epoch time: 125.89 s\n",
      "2024-01-04 10:54:21.590461: \n",
      "2024-01-04 10:54:21.595469: Epoch 531\n",
      "2024-01-04 10:54:21.600550: Current learning rate: 0.00506\n",
      "2024-01-04 10:56:27.636115: train_loss -0.9155\n",
      "2024-01-04 10:56:27.643115: val_loss -0.805\n",
      "2024-01-04 10:56:27.651115: Pseudo dice [0.9191, 0.9314, 0.9394]\n",
      "2024-01-04 10:56:27.658131: Epoch time: 126.05 s\n",
      "2024-01-04 10:56:28.787516: \n",
      "2024-01-04 10:56:28.793752: Epoch 532\n",
      "2024-01-04 10:56:28.802770: Current learning rate: 0.00505\n",
      "2024-01-04 10:58:34.484269: train_loss -0.9162\n",
      "2024-01-04 10:58:34.491269: val_loss -0.8026\n",
      "2024-01-04 10:58:34.496221: Pseudo dice [0.9196, 0.926, 0.933]\n",
      "2024-01-04 10:58:34.502223: Epoch time: 125.7 s\n",
      "2024-01-04 10:58:35.744028: \n",
      "2024-01-04 10:58:35.751021: Epoch 533\n",
      "2024-01-04 10:58:35.757030: Current learning rate: 0.00504\n",
      "2024-01-04 11:00:41.428899: train_loss -0.9169\n",
      "2024-01-04 11:00:41.437900: val_loss -0.801\n",
      "2024-01-04 11:00:41.443900: Pseudo dice [0.9217, 0.9265, 0.9347]\n",
      "2024-01-04 11:00:41.448900: Epoch time: 125.69 s\n",
      "2024-01-04 11:00:42.656068: \n",
      "2024-01-04 11:00:42.667152: Epoch 534\n",
      "2024-01-04 11:00:42.671139: Current learning rate: 0.00503\n",
      "2024-01-04 11:02:48.538405: train_loss -0.9186\n",
      "2024-01-04 11:02:48.548406: val_loss -0.8068\n",
      "2024-01-04 11:02:48.553469: Pseudo dice [0.9243, 0.9281, 0.9369]\n",
      "2024-01-04 11:02:48.558407: Epoch time: 125.88 s\n",
      "2024-01-04 11:02:49.907715: \n",
      "2024-01-04 11:02:49.913862: Epoch 535\n",
      "2024-01-04 11:02:49.918912: Current learning rate: 0.00502\n",
      "2024-01-04 11:04:55.499924: train_loss -0.9165\n",
      "2024-01-04 11:04:55.505925: val_loss -0.8004\n",
      "2024-01-04 11:04:55.510925: Pseudo dice [0.9208, 0.9278, 0.936]\n",
      "2024-01-04 11:04:55.515926: Epoch time: 125.59 s\n",
      "2024-01-04 11:04:56.779284: \n",
      "2024-01-04 11:04:56.785527: Epoch 536\n",
      "2024-01-04 11:04:56.791538: Current learning rate: 0.00501\n",
      "2024-01-04 11:07:02.491127: train_loss -0.9165\n",
      "2024-01-04 11:07:02.500131: val_loss -0.787\n",
      "2024-01-04 11:07:02.505645: Pseudo dice [0.906, 0.9261, 0.9346]\n",
      "2024-01-04 11:07:02.511643: Epoch time: 125.71 s\n",
      "2024-01-04 11:07:03.717058: \n",
      "2024-01-04 11:07:03.724236: Epoch 537\n",
      "2024-01-04 11:07:03.734759: Current learning rate: 0.005\n",
      "2024-01-04 11:09:09.193930: train_loss -0.9121\n",
      "2024-01-04 11:09:09.200939: val_loss -0.8026\n",
      "2024-01-04 11:09:09.204935: Pseudo dice [0.9223, 0.9281, 0.9365]\n",
      "2024-01-04 11:09:09.209930: Epoch time: 125.48 s\n",
      "2024-01-04 11:09:10.310241: \n",
      "2024-01-04 11:09:10.316181: Epoch 538\n",
      "2024-01-04 11:09:10.320647: Current learning rate: 0.00499\n",
      "2024-01-04 11:11:16.221108: train_loss -0.9144\n",
      "2024-01-04 11:11:16.229109: val_loss -0.8044\n",
      "2024-01-04 11:11:16.234108: Pseudo dice [0.9123, 0.9261, 0.9317]\n",
      "2024-01-04 11:11:16.239108: Epoch time: 125.91 s\n",
      "2024-01-04 11:11:17.507611: \n",
      "2024-01-04 11:11:17.517010: Epoch 539\n",
      "2024-01-04 11:11:17.521957: Current learning rate: 0.00498\n",
      "2024-01-04 11:13:23.359070: train_loss -0.9091\n",
      "2024-01-04 11:13:23.369586: val_loss -0.8047\n",
      "2024-01-04 11:13:23.375585: Pseudo dice [0.9224, 0.929, 0.9363]\n",
      "2024-01-04 11:13:23.382586: Epoch time: 125.85 s\n",
      "2024-01-04 11:13:24.579101: \n",
      "2024-01-04 11:13:24.587105: Epoch 540\n",
      "2024-01-04 11:13:24.592046: Current learning rate: 0.00497\n",
      "2024-01-04 11:15:30.291937: train_loss -0.9124\n",
      "2024-01-04 11:15:30.298938: val_loss -0.7955\n",
      "2024-01-04 11:15:30.306937: Pseudo dice [0.9226, 0.9252, 0.9337]\n",
      "2024-01-04 11:15:30.314937: Epoch time: 125.71 s\n",
      "2024-01-04 11:15:31.482019: \n",
      "2024-01-04 11:15:31.489026: Epoch 541\n",
      "2024-01-04 11:15:31.493954: Current learning rate: 0.00496\n",
      "2024-01-04 11:17:37.254076: train_loss -0.9116\n",
      "2024-01-04 11:17:37.260077: val_loss -0.8039\n",
      "2024-01-04 11:17:37.266076: Pseudo dice [0.9234, 0.9285, 0.9364]\n",
      "2024-01-04 11:17:37.272075: Epoch time: 125.77 s\n",
      "2024-01-04 11:17:38.406478: \n",
      "2024-01-04 11:17:38.414667: Epoch 542\n",
      "2024-01-04 11:17:38.424723: Current learning rate: 0.00495\n",
      "2024-01-04 11:19:44.297788: train_loss -0.9139\n",
      "2024-01-04 11:19:44.305789: val_loss -0.7949\n",
      "2024-01-04 11:19:44.312789: Pseudo dice [0.9207, 0.9283, 0.9365]\n",
      "2024-01-04 11:19:44.317791: Epoch time: 125.89 s\n",
      "2024-01-04 11:19:45.549794: \n",
      "2024-01-04 11:19:45.555284: Epoch 543\n",
      "2024-01-04 11:19:45.559930: Current learning rate: 0.00494\n",
      "2024-01-04 11:21:51.057081: train_loss -0.916\n",
      "2024-01-04 11:21:51.063079: val_loss -0.7996\n",
      "2024-01-04 11:21:51.068589: Pseudo dice [0.9219, 0.9274, 0.9364]\n",
      "2024-01-04 11:21:51.073589: Epoch time: 125.51 s\n",
      "2024-01-04 11:21:52.454861: \n",
      "2024-01-04 11:21:52.465273: Epoch 544\n",
      "2024-01-04 11:21:52.469902: Current learning rate: 0.00493\n",
      "2024-01-04 11:23:58.409253: train_loss -0.9152\n",
      "2024-01-04 11:23:58.418252: val_loss -0.8033\n",
      "2024-01-04 11:23:58.423265: Pseudo dice [0.9208, 0.9293, 0.9368]\n",
      "2024-01-04 11:23:58.429266: Epoch time: 125.96 s\n",
      "2024-01-04 11:23:59.542297: \n",
      "2024-01-04 11:23:59.551286: Epoch 545\n",
      "2024-01-04 11:23:59.555369: Current learning rate: 0.00492\n",
      "2024-01-04 11:26:05.018809: train_loss -0.9161\n",
      "2024-01-04 11:26:05.026810: val_loss -0.7831\n",
      "2024-01-04 11:26:05.033817: Pseudo dice [0.9232, 0.9267, 0.9353]\n",
      "2024-01-04 11:26:05.039817: Epoch time: 125.48 s\n",
      "2024-01-04 11:26:06.163221: \n",
      "2024-01-04 11:26:06.169218: Epoch 546\n",
      "2024-01-04 11:26:06.177973: Current learning rate: 0.00491\n",
      "2024-01-04 11:28:12.186596: train_loss -0.9149\n",
      "2024-01-04 11:28:12.195607: val_loss -0.7982\n",
      "2024-01-04 11:28:12.201605: Pseudo dice [0.9182, 0.9278, 0.9356]\n",
      "2024-01-04 11:28:12.209604: Epoch time: 126.02 s\n",
      "2024-01-04 11:28:13.387940: \n",
      "2024-01-04 11:28:13.395403: Epoch 547\n",
      "2024-01-04 11:28:13.402993: Current learning rate: 0.0049\n",
      "2024-01-04 11:30:19.249062: train_loss -0.9133\n",
      "2024-01-04 11:30:19.255063: val_loss -0.8056\n",
      "2024-01-04 11:30:19.264067: Pseudo dice [0.9232, 0.9279, 0.9354]\n",
      "2024-01-04 11:30:19.273068: Epoch time: 125.86 s\n",
      "2024-01-04 11:30:20.491123: \n",
      "2024-01-04 11:30:20.496158: Epoch 548\n",
      "2024-01-04 11:30:20.501130: Current learning rate: 0.00489\n",
      "2024-01-04 11:32:26.197109: train_loss -0.9146\n",
      "2024-01-04 11:32:26.205111: val_loss -0.8042\n",
      "2024-01-04 11:32:26.210109: Pseudo dice [0.92, 0.9268, 0.9379]\n",
      "2024-01-04 11:32:26.214108: Epoch time: 125.71 s\n",
      "2024-01-04 11:32:27.404390: \n",
      "2024-01-04 11:32:27.416691: Epoch 549\n",
      "2024-01-04 11:32:27.429773: Current learning rate: 0.00488\n",
      "2024-01-04 11:34:32.901211: train_loss -0.9172\n",
      "2024-01-04 11:34:32.907211: val_loss -0.8116\n",
      "2024-01-04 11:34:32.912211: Pseudo dice [0.9203, 0.9282, 0.9373]\n",
      "2024-01-04 11:34:32.916214: Epoch time: 125.5 s\n",
      "2024-01-04 11:34:34.457927: \n",
      "2024-01-04 11:34:34.463982: Epoch 550\n",
      "2024-01-04 11:34:34.469004: Current learning rate: 0.00487\n",
      "2024-01-04 11:36:40.199129: train_loss -0.9145\n",
      "2024-01-04 11:36:40.208126: val_loss -0.8027\n",
      "2024-01-04 11:36:40.217126: Pseudo dice [0.9213, 0.9251, 0.9337]\n",
      "2024-01-04 11:36:40.224126: Epoch time: 125.74 s\n",
      "2024-01-04 11:36:41.544317: \n",
      "2024-01-04 11:36:41.550370: Epoch 551\n",
      "2024-01-04 11:36:41.554519: Current learning rate: 0.00486\n",
      "2024-01-04 11:38:47.422778: train_loss -0.9154\n",
      "2024-01-04 11:38:47.430777: val_loss -0.8117\n",
      "2024-01-04 11:38:47.437777: Pseudo dice [0.9172, 0.9299, 0.9396]\n",
      "2024-01-04 11:38:47.442777: Epoch time: 125.88 s\n",
      "2024-01-04 11:38:48.640216: \n",
      "2024-01-04 11:38:48.646224: Epoch 552\n",
      "2024-01-04 11:38:48.651166: Current learning rate: 0.00485\n",
      "2024-01-04 11:40:54.183689: train_loss -0.917\n",
      "2024-01-04 11:40:54.191691: val_loss -0.8122\n",
      "2024-01-04 11:40:54.200660: Pseudo dice [0.921, 0.9277, 0.936]\n",
      "2024-01-04 11:40:54.207098: Epoch time: 125.54 s\n",
      "2024-01-04 11:40:55.385599: \n",
      "2024-01-04 11:40:55.395518: Epoch 553\n",
      "2024-01-04 11:40:55.400519: Current learning rate: 0.00484\n",
      "2024-01-04 11:43:01.062149: train_loss -0.9183\n",
      "2024-01-04 11:43:01.069149: val_loss -0.7995\n",
      "2024-01-04 11:43:01.074150: Pseudo dice [0.9199, 0.9261, 0.9333]\n",
      "2024-01-04 11:43:01.089408: Epoch time: 125.68 s\n",
      "2024-01-04 11:43:02.244783: \n",
      "2024-01-04 11:43:02.250729: Epoch 554\n",
      "2024-01-04 11:43:02.255443: Current learning rate: 0.00484\n",
      "2024-01-04 11:45:07.955075: train_loss -0.9148\n",
      "2024-01-04 11:45:07.962076: val_loss -0.7997\n",
      "2024-01-04 11:45:07.967087: Pseudo dice [0.9228, 0.9291, 0.9395]\n",
      "2024-01-04 11:45:07.972089: Epoch time: 125.71 s\n",
      "2024-01-04 11:45:09.148459: \n",
      "2024-01-04 11:45:09.154533: Epoch 555\n",
      "2024-01-04 11:45:09.162460: Current learning rate: 0.00483\n",
      "2024-01-04 11:47:15.128280: train_loss -0.915\n",
      "2024-01-04 11:47:15.137282: val_loss -0.8105\n",
      "2024-01-04 11:47:15.145283: Pseudo dice [0.9234, 0.9296, 0.9371]\n",
      "2024-01-04 11:47:15.151283: Epoch time: 125.98 s\n",
      "2024-01-04 11:47:16.366258: \n",
      "2024-01-04 11:47:16.374332: Epoch 556\n",
      "2024-01-04 11:47:16.379932: Current learning rate: 0.00482\n",
      "2024-01-04 11:49:22.112401: train_loss -0.9169\n",
      "2024-01-04 11:49:22.119401: val_loss -0.8012\n",
      "2024-01-04 11:49:22.125915: Pseudo dice [0.9216, 0.9298, 0.938]\n",
      "2024-01-04 11:49:22.130914: Epoch time: 125.75 s\n",
      "2024-01-04 11:49:23.333106: \n",
      "2024-01-04 11:49:23.342886: Epoch 557\n",
      "2024-01-04 11:49:23.346946: Current learning rate: 0.00481\n",
      "2024-01-04 11:51:29.495160: train_loss -0.9133\n",
      "2024-01-04 11:51:29.503160: val_loss -0.8063\n",
      "2024-01-04 11:51:29.511159: Pseudo dice [0.9165, 0.9276, 0.9372]\n",
      "2024-01-04 11:51:29.519159: Epoch time: 126.16 s\n",
      "2024-01-04 11:51:30.652462: \n",
      "2024-01-04 11:51:30.657486: Epoch 558\n",
      "2024-01-04 11:51:30.661486: Current learning rate: 0.0048\n",
      "2024-01-04 11:53:36.143430: train_loss -0.9168\n",
      "2024-01-04 11:53:36.151428: val_loss -0.8109\n",
      "2024-01-04 11:53:36.156428: Pseudo dice [0.9218, 0.931, 0.9386]\n",
      "2024-01-04 11:53:36.162430: Epoch time: 125.49 s\n",
      "2024-01-04 11:53:37.292635: \n",
      "2024-01-04 11:53:37.298684: Epoch 559\n",
      "2024-01-04 11:53:37.302689: Current learning rate: 0.00479\n",
      "2024-01-04 11:55:43.269516: train_loss -0.9165\n",
      "2024-01-04 11:55:43.279515: val_loss -0.8004\n",
      "2024-01-04 11:55:43.288024: Pseudo dice [0.9212, 0.9269, 0.9359]\n",
      "2024-01-04 11:55:43.295025: Epoch time: 125.98 s\n",
      "2024-01-04 11:55:44.651681: \n",
      "2024-01-04 11:55:44.660685: Epoch 560\n",
      "2024-01-04 11:55:44.665627: Current learning rate: 0.00478\n",
      "2024-01-04 11:57:50.690385: train_loss -0.9179\n",
      "2024-01-04 11:57:50.697377: val_loss -0.796\n",
      "2024-01-04 11:57:50.704379: Pseudo dice [0.9237, 0.927, 0.935]\n",
      "2024-01-04 11:57:50.711379: Epoch time: 126.04 s\n",
      "2024-01-04 11:57:52.075825: \n",
      "2024-01-04 11:57:52.081826: Epoch 561\n",
      "2024-01-04 11:57:52.086825: Current learning rate: 0.00477\n",
      "2024-01-04 11:59:57.811545: train_loss -0.9163\n",
      "2024-01-04 11:59:57.818549: val_loss -0.8034\n",
      "2024-01-04 11:59:57.824546: Pseudo dice [0.9191, 0.9278, 0.9354]\n",
      "2024-01-04 11:59:57.829545: Epoch time: 125.74 s\n",
      "2024-01-04 11:59:59.049167: \n",
      "2024-01-04 11:59:59.055524: Epoch 562\n",
      "2024-01-04 11:59:59.060537: Current learning rate: 0.00476\n",
      "2024-01-04 12:02:04.610283: train_loss -0.9188\n",
      "2024-01-04 12:02:04.619798: val_loss -0.8047\n",
      "2024-01-04 12:02:04.624797: Pseudo dice [0.9239, 0.9266, 0.9356]\n",
      "2024-01-04 12:02:04.630798: Epoch time: 125.56 s\n",
      "2024-01-04 12:02:05.803775: \n",
      "2024-01-04 12:02:05.815318: Epoch 563\n",
      "2024-01-04 12:02:05.821315: Current learning rate: 0.00475\n",
      "2024-01-04 12:04:11.588468: train_loss -0.9162\n",
      "2024-01-04 12:04:11.598469: val_loss -0.796\n",
      "2024-01-04 12:04:11.604472: Pseudo dice [0.9241, 0.9274, 0.9359]\n",
      "2024-01-04 12:04:11.611546: Epoch time: 125.79 s\n",
      "2024-01-04 12:04:12.726186: \n",
      "2024-01-04 12:04:12.735192: Epoch 564\n",
      "2024-01-04 12:04:12.740132: Current learning rate: 0.00474\n",
      "2024-01-04 12:06:18.907471: train_loss -0.9178\n",
      "2024-01-04 12:06:18.913467: val_loss -0.8047\n",
      "2024-01-04 12:06:18.918467: Pseudo dice [0.9216, 0.9284, 0.9358]\n",
      "2024-01-04 12:06:18.922469: Epoch time: 126.18 s\n",
      "2024-01-04 12:06:20.273271: \n",
      "2024-01-04 12:06:20.283276: Epoch 565\n",
      "2024-01-04 12:06:20.289263: Current learning rate: 0.00473\n",
      "2024-01-04 12:08:25.831609: train_loss -0.9156\n",
      "2024-01-04 12:08:25.838692: val_loss -0.8044\n",
      "2024-01-04 12:08:25.848619: Pseudo dice [0.9226, 0.9296, 0.9373]\n",
      "2024-01-04 12:08:25.855133: Epoch time: 125.56 s\n",
      "2024-01-04 12:08:27.053174: \n",
      "2024-01-04 12:08:27.061174: Epoch 566\n",
      "2024-01-04 12:08:27.066173: Current learning rate: 0.00472\n",
      "2024-01-04 12:10:32.759545: train_loss -0.9187\n",
      "2024-01-04 12:10:32.767546: val_loss -0.7956\n",
      "2024-01-04 12:10:32.773547: Pseudo dice [0.9232, 0.9266, 0.9364]\n",
      "2024-01-04 12:10:32.779546: Epoch time: 125.71 s\n",
      "2024-01-04 12:10:34.205552: \n",
      "2024-01-04 12:10:34.211565: Epoch 567\n",
      "2024-01-04 12:10:34.216576: Current learning rate: 0.00471\n",
      "2024-01-04 12:12:40.387680: train_loss -0.9158\n",
      "2024-01-04 12:12:40.399676: val_loss -0.7996\n",
      "2024-01-04 12:12:40.407677: Pseudo dice [0.9227, 0.9273, 0.9346]\n",
      "2024-01-04 12:12:40.415188: Epoch time: 126.18 s\n",
      "2024-01-04 12:12:41.721321: \n",
      "2024-01-04 12:12:41.728320: Epoch 568\n",
      "2024-01-04 12:12:41.735320: Current learning rate: 0.0047\n",
      "2024-01-04 12:14:47.775441: train_loss -0.9135\n",
      "2024-01-04 12:14:47.781443: val_loss -0.8026\n",
      "2024-01-04 12:14:47.788441: Pseudo dice [0.9217, 0.9282, 0.9367]\n",
      "2024-01-04 12:14:47.794441: Epoch time: 126.06 s\n",
      "2024-01-04 12:14:48.980407: \n",
      "2024-01-04 12:14:48.986470: Epoch 569\n",
      "2024-01-04 12:14:48.990527: Current learning rate: 0.00469\n",
      "2024-01-04 12:16:54.518852: train_loss -0.9179\n",
      "2024-01-04 12:16:54.528852: val_loss -0.7984\n",
      "2024-01-04 12:16:54.537852: Pseudo dice [0.9172, 0.9276, 0.9355]\n",
      "2024-01-04 12:16:54.546853: Epoch time: 125.54 s\n",
      "2024-01-04 12:16:55.711646: \n",
      "2024-01-04 12:16:55.720897: Epoch 570\n",
      "2024-01-04 12:16:55.727924: Current learning rate: 0.00468\n",
      "2024-01-04 12:19:01.684563: train_loss -0.917\n",
      "2024-01-04 12:19:01.694566: val_loss -0.7945\n",
      "2024-01-04 12:19:01.700555: Pseudo dice [0.9218, 0.9275, 0.9357]\n",
      "2024-01-04 12:19:01.705554: Epoch time: 125.97 s\n",
      "2024-01-04 12:19:02.783273: \n",
      "2024-01-04 12:19:02.789273: Epoch 571\n",
      "2024-01-04 12:19:02.796339: Current learning rate: 0.00467\n",
      "2024-01-04 12:21:08.644159: train_loss -0.9173\n",
      "2024-01-04 12:21:08.651159: val_loss -0.8049\n",
      "2024-01-04 12:21:08.655158: Pseudo dice [0.9224, 0.9282, 0.9356]\n",
      "2024-01-04 12:21:08.663667: Epoch time: 125.86 s\n",
      "2024-01-04 12:21:09.770623: \n",
      "2024-01-04 12:21:09.779625: Epoch 572\n",
      "2024-01-04 12:21:09.785616: Current learning rate: 0.00466\n",
      "2024-01-04 12:23:15.416501: train_loss -0.9177\n",
      "2024-01-04 12:23:15.425502: val_loss -0.7956\n",
      "2024-01-04 12:23:15.434022: Pseudo dice [0.9202, 0.9297, 0.9366]\n",
      "2024-01-04 12:23:15.439017: Epoch time: 125.65 s\n",
      "2024-01-04 12:23:16.595272: \n",
      "2024-01-04 12:23:16.601309: Epoch 573\n",
      "2024-01-04 12:23:16.606278: Current learning rate: 0.00465\n",
      "2024-01-04 12:25:22.304297: train_loss -0.9137\n",
      "2024-01-04 12:25:22.312286: val_loss -0.7953\n",
      "2024-01-04 12:25:22.318300: Pseudo dice [0.9194, 0.9288, 0.9394]\n",
      "2024-01-04 12:25:22.323291: Epoch time: 125.71 s\n",
      "2024-01-04 12:25:23.553176: \n",
      "2024-01-04 12:25:23.558967: Epoch 574\n",
      "2024-01-04 12:25:23.563959: Current learning rate: 0.00464\n",
      "2024-01-04 12:27:29.286979: train_loss -0.9159\n",
      "2024-01-04 12:27:29.293979: val_loss -0.8036\n",
      "2024-01-04 12:27:29.299980: Pseudo dice [0.9244, 0.9287, 0.9369]\n",
      "2024-01-04 12:27:29.304983: Epoch time: 125.73 s\n",
      "2024-01-04 12:27:30.604716: \n",
      "2024-01-04 12:27:30.612031: Epoch 575\n",
      "2024-01-04 12:27:30.617048: Current learning rate: 0.00463\n",
      "2024-01-04 12:29:36.625669: train_loss -0.9155\n",
      "2024-01-04 12:29:36.632669: val_loss -0.7899\n",
      "2024-01-04 12:29:36.638670: Pseudo dice [0.9197, 0.925, 0.9323]\n",
      "2024-01-04 12:29:36.643670: Epoch time: 126.02 s\n",
      "2024-01-04 12:29:37.766011: \n",
      "2024-01-04 12:29:37.773015: Epoch 576\n",
      "2024-01-04 12:29:37.777015: Current learning rate: 0.00462\n",
      "2024-01-04 12:31:43.667827: train_loss -0.9159\n",
      "2024-01-04 12:31:43.679835: val_loss -0.8041\n",
      "2024-01-04 12:31:43.685348: Pseudo dice [0.926, 0.9279, 0.9357]\n",
      "2024-01-04 12:31:43.690350: Epoch time: 125.9 s\n",
      "2024-01-04 12:31:44.783265: \n",
      "2024-01-04 12:31:44.792881: Epoch 577\n",
      "2024-01-04 12:31:44.796938: Current learning rate: 0.00461\n",
      "2024-01-04 12:33:50.330216: train_loss -0.9157\n",
      "2024-01-04 12:33:50.341730: val_loss -0.8067\n",
      "2024-01-04 12:33:50.347730: Pseudo dice [0.9208, 0.9283, 0.9364]\n",
      "2024-01-04 12:33:50.354729: Epoch time: 125.55 s\n",
      "2024-01-04 12:33:51.555318: \n",
      "2024-01-04 12:33:51.561302: Epoch 578\n",
      "2024-01-04 12:33:51.565301: Current learning rate: 0.0046\n",
      "2024-01-04 12:35:57.649031: train_loss -0.9164\n",
      "2024-01-04 12:35:57.657029: val_loss -0.7988\n",
      "2024-01-04 12:35:57.664028: Pseudo dice [0.9218, 0.9282, 0.9379]\n",
      "2024-01-04 12:35:57.673029: Epoch time: 126.1 s\n",
      "2024-01-04 12:35:58.845680: \n",
      "2024-01-04 12:35:58.855672: Epoch 579\n",
      "2024-01-04 12:35:58.860681: Current learning rate: 0.00459\n",
      "2024-01-04 12:38:04.670768: train_loss -0.9177\n",
      "2024-01-04 12:38:04.679761: val_loss -0.8162\n",
      "2024-01-04 12:38:04.684768: Pseudo dice [0.9213, 0.9286, 0.9363]\n",
      "2024-01-04 12:38:04.690769: Epoch time: 125.83 s\n",
      "2024-01-04 12:38:05.905640: \n",
      "2024-01-04 12:38:05.915723: Epoch 580\n",
      "2024-01-04 12:38:05.919708: Current learning rate: 0.00458\n",
      "2024-01-04 12:40:11.911392: train_loss -0.9191\n",
      "2024-01-04 12:40:11.918396: val_loss -0.8007\n",
      "2024-01-04 12:40:11.924395: Pseudo dice [0.9199, 0.9273, 0.9344]\n",
      "2024-01-04 12:40:11.929902: Epoch time: 126.01 s\n",
      "2024-01-04 12:40:13.105828: \n",
      "2024-01-04 12:40:13.116851: Epoch 581\n",
      "2024-01-04 12:40:13.121862: Current learning rate: 0.00457\n",
      "2024-01-04 12:42:19.242369: train_loss -0.9195\n",
      "2024-01-04 12:42:19.253365: val_loss -0.8025\n",
      "2024-01-04 12:42:19.261365: Pseudo dice [0.9247, 0.9289, 0.9356]\n",
      "2024-01-04 12:42:19.269366: Epoch time: 126.14 s\n",
      "2024-01-04 12:42:20.777099: \n",
      "2024-01-04 12:42:20.785104: Epoch 582\n",
      "2024-01-04 12:42:20.793103: Current learning rate: 0.00456\n",
      "2024-01-04 12:44:26.335579: train_loss -0.9191\n",
      "2024-01-04 12:44:26.343570: val_loss -0.8053\n",
      "2024-01-04 12:44:26.349569: Pseudo dice [0.921, 0.9293, 0.9401]\n",
      "2024-01-04 12:44:26.354572: Epoch time: 125.56 s\n",
      "2024-01-04 12:44:27.692441: \n",
      "2024-01-04 12:44:27.702257: Epoch 583\n",
      "2024-01-04 12:44:27.707274: Current learning rate: 0.00455\n",
      "2024-01-04 12:46:33.266042: train_loss -0.9194\n",
      "2024-01-04 12:46:33.275038: val_loss -0.788\n",
      "2024-01-04 12:46:33.282857: Pseudo dice [0.9168, 0.923, 0.933]\n",
      "2024-01-04 12:46:33.288372: Epoch time: 125.57 s\n",
      "2024-01-04 12:46:34.467846: \n",
      "2024-01-04 12:46:34.475008: Epoch 584\n",
      "2024-01-04 12:46:34.479592: Current learning rate: 0.00454\n",
      "2024-01-04 12:48:40.473966: train_loss -0.915\n",
      "2024-01-04 12:48:40.480975: val_loss -0.8022\n",
      "2024-01-04 12:48:40.486979: Pseudo dice [0.9201, 0.9251, 0.9342]\n",
      "2024-01-04 12:48:40.492516: Epoch time: 126.01 s\n",
      "2024-01-04 12:48:41.724575: \n",
      "2024-01-04 12:48:41.731508: Epoch 585\n",
      "2024-01-04 12:48:41.741515: Current learning rate: 0.00453\n",
      "2024-01-04 12:50:47.578719: train_loss -0.9173\n",
      "2024-01-04 12:50:47.584721: val_loss -0.8115\n",
      "2024-01-04 12:50:47.590719: Pseudo dice [0.9213, 0.9312, 0.9382]\n",
      "2024-01-04 12:50:47.595719: Epoch time: 125.86 s\n",
      "2024-01-04 12:50:48.788051: \n",
      "2024-01-04 12:50:48.795380: Epoch 586\n",
      "2024-01-04 12:50:48.800157: Current learning rate: 0.00452\n",
      "2024-01-04 12:52:54.815487: train_loss -0.9163\n",
      "2024-01-04 12:52:54.822503: val_loss -0.8117\n",
      "2024-01-04 12:52:54.828493: Pseudo dice [0.9196, 0.9289, 0.9368]\n",
      "2024-01-04 12:52:54.836000: Epoch time: 126.03 s\n",
      "2024-01-04 12:52:56.036278: \n",
      "2024-01-04 12:52:56.042648: Epoch 587\n",
      "2024-01-04 12:52:56.047112: Current learning rate: 0.00451\n",
      "2024-01-04 12:55:01.954435: train_loss -0.9196\n",
      "2024-01-04 12:55:01.963435: val_loss -0.799\n",
      "2024-01-04 12:55:01.971436: Pseudo dice [0.9221, 0.9299, 0.9389]\n",
      "2024-01-04 12:55:01.975441: Epoch time: 125.92 s\n",
      "2024-01-04 12:55:03.244901: \n",
      "2024-01-04 12:55:03.253934: Epoch 588\n",
      "2024-01-04 12:55:03.257922: Current learning rate: 0.0045\n",
      "2024-01-04 12:57:09.430516: train_loss -0.9145\n",
      "2024-01-04 12:57:09.437516: val_loss -0.803\n",
      "2024-01-04 12:57:09.442520: Pseudo dice [0.9202, 0.9273, 0.9364]\n",
      "2024-01-04 12:57:09.448516: Epoch time: 126.19 s\n",
      "2024-01-04 12:57:10.670599: \n",
      "2024-01-04 12:57:10.682765: Epoch 589\n",
      "2024-01-04 12:57:10.687769: Current learning rate: 0.00449\n",
      "2024-01-04 12:59:16.488402: train_loss -0.9177\n",
      "2024-01-04 12:59:16.495400: val_loss -0.8017\n",
      "2024-01-04 12:59:16.501400: Pseudo dice [0.9194, 0.9272, 0.9361]\n",
      "2024-01-04 12:59:16.507400: Epoch time: 125.82 s\n",
      "2024-01-04 12:59:17.647232: \n",
      "2024-01-04 12:59:17.654241: Epoch 590\n",
      "2024-01-04 12:59:17.658255: Current learning rate: 0.00448\n",
      "2024-01-04 13:01:23.288861: train_loss -0.9185\n",
      "2024-01-04 13:01:23.299861: val_loss -0.8068\n",
      "2024-01-04 13:01:23.306877: Pseudo dice [0.921, 0.9307, 0.9393]\n",
      "2024-01-04 13:01:23.312878: Epoch time: 125.64 s\n",
      "2024-01-04 13:01:24.702208: \n",
      "2024-01-04 13:01:24.712374: Epoch 591\n",
      "2024-01-04 13:01:24.717361: Current learning rate: 0.00447\n",
      "2024-01-04 13:03:31.029863: train_loss -0.9168\n",
      "2024-01-04 13:03:31.038863: val_loss -0.8005\n",
      "2024-01-04 13:03:31.046863: Pseudo dice [0.9202, 0.9275, 0.9364]\n",
      "2024-01-04 13:03:31.052863: Epoch time: 126.33 s\n",
      "2024-01-04 13:03:32.324453: \n",
      "2024-01-04 13:03:32.330457: Epoch 592\n",
      "2024-01-04 13:03:32.335390: Current learning rate: 0.00446\n",
      "2024-01-04 13:05:38.163393: train_loss -0.9167\n",
      "2024-01-04 13:05:38.171391: val_loss -0.7996\n",
      "2024-01-04 13:05:38.177391: Pseudo dice [0.9219, 0.93, 0.9374]\n",
      "2024-01-04 13:05:38.181390: Epoch time: 125.84 s\n",
      "2024-01-04 13:05:39.343451: \n",
      "2024-01-04 13:05:39.351529: Epoch 593\n",
      "2024-01-04 13:05:39.356516: Current learning rate: 0.00445\n",
      "2024-01-04 13:07:45.084302: train_loss -0.9187\n",
      "2024-01-04 13:07:45.096306: val_loss -0.8047\n",
      "2024-01-04 13:07:45.102310: Pseudo dice [0.9235, 0.9309, 0.9392]\n",
      "2024-01-04 13:07:45.108310: Epoch time: 125.74 s\n",
      "2024-01-04 13:07:46.232858: \n",
      "2024-01-04 13:07:46.240940: Epoch 594\n",
      "2024-01-04 13:07:46.245868: Current learning rate: 0.00444\n",
      "2024-01-04 13:09:51.905512: train_loss -0.9186\n",
      "2024-01-04 13:09:51.912514: val_loss -0.7776\n",
      "2024-01-04 13:09:51.918513: Pseudo dice [0.922, 0.9262, 0.9353]\n",
      "2024-01-04 13:09:51.923514: Epoch time: 125.67 s\n",
      "2024-01-04 13:09:53.126196: \n",
      "2024-01-04 13:09:53.131790: Epoch 595\n",
      "2024-01-04 13:09:53.136808: Current learning rate: 0.00443\n",
      "2024-01-04 13:11:59.056829: train_loss -0.9157\n",
      "2024-01-04 13:11:59.064829: val_loss -0.8096\n",
      "2024-01-04 13:11:59.070827: Pseudo dice [0.9209, 0.928, 0.9355]\n",
      "2024-01-04 13:11:59.075827: Epoch time: 125.93 s\n",
      "2024-01-04 13:12:00.191239: \n",
      "2024-01-04 13:12:00.202334: Epoch 596\n",
      "2024-01-04 13:12:00.206717: Current learning rate: 0.00442\n",
      "2024-01-04 13:14:06.075999: train_loss -0.9173\n",
      "2024-01-04 13:14:06.083000: val_loss -0.7894\n",
      "2024-01-04 13:14:06.088123: Pseudo dice [0.9199, 0.9281, 0.9355]\n",
      "2024-01-04 13:14:06.095125: Epoch time: 125.89 s\n",
      "2024-01-04 13:14:07.326054: \n",
      "2024-01-04 13:14:07.335125: Epoch 597\n",
      "2024-01-04 13:14:07.340143: Current learning rate: 0.00441\n",
      "2024-01-04 13:16:13.390101: train_loss -0.9179\n",
      "2024-01-04 13:16:13.397110: val_loss -0.8062\n",
      "2024-01-04 13:16:13.403104: Pseudo dice [0.9216, 0.9285, 0.9361]\n",
      "2024-01-04 13:16:13.409104: Epoch time: 126.07 s\n",
      "2024-01-04 13:16:14.808278: \n",
      "2024-01-04 13:16:14.814618: Epoch 598\n",
      "2024-01-04 13:16:14.820652: Current learning rate: 0.0044\n",
      "2024-01-04 13:18:20.436145: train_loss -0.9202\n",
      "2024-01-04 13:18:20.444141: val_loss -0.8074\n",
      "2024-01-04 13:18:20.453145: Pseudo dice [0.9184, 0.9283, 0.9368]\n",
      "2024-01-04 13:18:20.462134: Epoch time: 125.63 s\n",
      "2024-01-04 13:18:21.674025: \n",
      "2024-01-04 13:18:21.685156: Epoch 599\n",
      "2024-01-04 13:18:21.690151: Current learning rate: 0.00439\n",
      "2024-01-04 13:20:27.636910: train_loss -0.917\n",
      "2024-01-04 13:20:27.643922: val_loss -0.8081\n",
      "2024-01-04 13:20:27.649914: Pseudo dice [0.9212, 0.9287, 0.9377]\n",
      "2024-01-04 13:20:27.656911: Epoch time: 125.97 s\n",
      "2024-01-04 13:20:29.184827: \n",
      "2024-01-04 13:20:29.189828: Epoch 600\n",
      "2024-01-04 13:20:29.194799: Current learning rate: 0.00438\n",
      "2024-01-04 13:22:34.922305: train_loss -0.9193\n",
      "2024-01-04 13:22:34.928812: val_loss -0.8025\n",
      "2024-01-04 13:22:34.934821: Pseudo dice [0.9187, 0.9317, 0.9392]\n",
      "2024-01-04 13:22:34.938821: Epoch time: 125.74 s\n",
      "2024-01-04 13:22:36.155579: \n",
      "2024-01-04 13:22:36.162941: Epoch 601\n",
      "2024-01-04 13:22:36.167934: Current learning rate: 0.00437\n",
      "2024-01-04 13:24:41.745451: train_loss -0.9182\n",
      "2024-01-04 13:24:41.752452: val_loss -0.7951\n",
      "2024-01-04 13:24:41.759453: Pseudo dice [0.9198, 0.9272, 0.9343]\n",
      "2024-01-04 13:24:41.765458: Epoch time: 125.59 s\n",
      "2024-01-04 13:24:42.951729: \n",
      "2024-01-04 13:24:42.957974: Epoch 602\n",
      "2024-01-04 13:24:42.962112: Current learning rate: 0.00436\n",
      "2024-01-04 13:26:48.595629: train_loss -0.9205\n",
      "2024-01-04 13:26:48.604631: val_loss -0.7876\n",
      "2024-01-04 13:26:48.609631: Pseudo dice [0.9237, 0.9261, 0.9339]\n",
      "2024-01-04 13:26:48.616629: Epoch time: 125.65 s\n",
      "2024-01-04 13:26:49.781287: \n",
      "2024-01-04 13:26:49.789285: Epoch 603\n",
      "2024-01-04 13:26:49.793364: Current learning rate: 0.00435\n",
      "2024-01-04 13:28:55.479631: train_loss -0.9199\n",
      "2024-01-04 13:28:55.489630: val_loss -0.8117\n",
      "2024-01-04 13:28:55.494633: Pseudo dice [0.9224, 0.9291, 0.937]\n",
      "2024-01-04 13:28:55.499635: Epoch time: 125.7 s\n",
      "2024-01-04 13:28:56.577980: \n",
      "2024-01-04 13:28:56.587910: Epoch 604\n",
      "2024-01-04 13:28:56.604630: Current learning rate: 0.00434\n",
      "2024-01-04 13:31:02.423824: train_loss -0.9186\n",
      "2024-01-04 13:31:02.431838: val_loss -0.8158\n",
      "2024-01-04 13:31:02.439827: Pseudo dice [0.9235, 0.931, 0.9387]\n",
      "2024-01-04 13:31:02.445826: Epoch time: 125.85 s\n",
      "2024-01-04 13:31:03.735979: \n",
      "2024-01-04 13:31:03.742980: Epoch 605\n",
      "2024-01-04 13:31:03.746979: Current learning rate: 0.00433\n",
      "2024-01-04 13:33:09.177369: train_loss -0.9216\n",
      "2024-01-04 13:33:09.191370: val_loss -0.8129\n",
      "2024-01-04 13:33:09.197369: Pseudo dice [0.9199, 0.9299, 0.9369]\n",
      "2024-01-04 13:33:09.202369: Epoch time: 125.44 s\n",
      "2024-01-04 13:33:10.537339: \n",
      "2024-01-04 13:33:10.549389: Epoch 606\n",
      "2024-01-04 13:33:10.558465: Current learning rate: 0.00432\n",
      "2024-01-04 13:35:16.237110: train_loss -0.9172\n",
      "2024-01-04 13:35:16.244110: val_loss -0.8021\n",
      "2024-01-04 13:35:16.249117: Pseudo dice [0.9202, 0.9277, 0.937]\n",
      "2024-01-04 13:35:16.254116: Epoch time: 125.7 s\n",
      "2024-01-04 13:35:17.375175: \n",
      "2024-01-04 13:35:17.386259: Epoch 607\n",
      "2024-01-04 13:35:17.390346: Current learning rate: 0.00431\n",
      "2024-01-04 13:37:22.767871: train_loss -0.9227\n",
      "2024-01-04 13:37:22.774860: val_loss -0.8053\n",
      "2024-01-04 13:37:22.780590: Pseudo dice [0.9219, 0.9265, 0.9345]\n",
      "2024-01-04 13:37:22.787591: Epoch time: 125.39 s\n",
      "2024-01-04 13:37:24.018694: \n",
      "2024-01-04 13:37:24.025051: Epoch 608\n",
      "2024-01-04 13:37:24.029123: Current learning rate: 0.0043\n",
      "2024-01-04 13:39:29.880767: train_loss -0.9184\n",
      "2024-01-04 13:39:29.887770: val_loss -0.8063\n",
      "2024-01-04 13:39:29.892771: Pseudo dice [0.9198, 0.9302, 0.9385]\n",
      "2024-01-04 13:39:29.898278: Epoch time: 125.86 s\n",
      "2024-01-04 13:39:30.999432: \n",
      "2024-01-04 13:39:31.006472: Epoch 609\n",
      "2024-01-04 13:39:31.010495: Current learning rate: 0.00429\n",
      "2024-01-04 13:41:36.931614: train_loss -0.9165\n",
      "2024-01-04 13:41:36.939612: val_loss -0.811\n",
      "2024-01-04 13:41:36.945614: Pseudo dice [0.9219, 0.9286, 0.9379]\n",
      "2024-01-04 13:41:36.953679: Epoch time: 125.93 s\n",
      "2024-01-04 13:41:38.218850: \n",
      "2024-01-04 13:41:38.226787: Epoch 610\n",
      "2024-01-04 13:41:38.231855: Current learning rate: 0.00429\n",
      "2024-01-04 13:43:43.893669: train_loss -0.9182\n",
      "2024-01-04 13:43:43.903673: val_loss -0.8047\n",
      "2024-01-04 13:43:43.911669: Pseudo dice [0.9251, 0.9268, 0.9361]\n",
      "2024-01-04 13:43:43.919682: Epoch time: 125.68 s\n",
      "2024-01-04 13:43:45.102212: \n",
      "2024-01-04 13:43:45.108296: Epoch 611\n",
      "2024-01-04 13:43:45.113242: Current learning rate: 0.00428\n",
      "2024-01-04 13:45:50.765616: train_loss -0.9195\n",
      "2024-01-04 13:45:50.775616: val_loss -0.8019\n",
      "2024-01-04 13:45:50.784616: Pseudo dice [0.9201, 0.9312, 0.9382]\n",
      "2024-01-04 13:45:50.790615: Epoch time: 125.66 s\n",
      "2024-01-04 13:45:51.952858: \n",
      "2024-01-04 13:45:51.959895: Epoch 612\n",
      "2024-01-04 13:45:51.964899: Current learning rate: 0.00427\n",
      "2024-01-04 13:47:57.627728: train_loss -0.92\n",
      "2024-01-04 13:47:57.636726: val_loss -0.8117\n",
      "2024-01-04 13:47:57.641726: Pseudo dice [0.9214, 0.9293, 0.9375]\n",
      "2024-01-04 13:47:57.646735: Epoch time: 125.67 s\n",
      "2024-01-04 13:47:59.044085: \n",
      "2024-01-04 13:47:59.050608: Epoch 613\n",
      "2024-01-04 13:47:59.058586: Current learning rate: 0.00426\n",
      "2024-01-04 13:50:04.616306: train_loss -0.9189\n",
      "2024-01-04 13:50:04.624316: val_loss -0.8131\n",
      "2024-01-04 13:50:04.631352: Pseudo dice [0.9195, 0.9293, 0.9383]\n",
      "2024-01-04 13:50:04.637568: Epoch time: 125.57 s\n",
      "2024-01-04 13:50:05.823850: \n",
      "2024-01-04 13:50:05.830494: Epoch 614\n",
      "2024-01-04 13:50:05.835020: Current learning rate: 0.00425\n",
      "2024-01-04 13:52:11.317932: train_loss -0.9188\n",
      "2024-01-04 13:52:11.324933: val_loss -0.7984\n",
      "2024-01-04 13:52:11.329933: Pseudo dice [0.9194, 0.9289, 0.9369]\n",
      "2024-01-04 13:52:11.334932: Epoch time: 125.5 s\n",
      "2024-01-04 13:52:12.548621: \n",
      "2024-01-04 13:52:12.555621: Epoch 615\n",
      "2024-01-04 13:52:12.560466: Current learning rate: 0.00424\n",
      "2024-01-04 13:54:18.215914: train_loss -0.92\n",
      "2024-01-04 13:54:18.223907: val_loss -0.809\n",
      "2024-01-04 13:54:18.230908: Pseudo dice [0.9206, 0.9313, 0.9394]\n",
      "2024-01-04 13:54:18.234914: Epoch time: 125.67 s\n",
      "2024-01-04 13:54:19.352432: \n",
      "2024-01-04 13:54:19.357551: Epoch 616\n",
      "2024-01-04 13:54:19.362193: Current learning rate: 0.00423\n",
      "2024-01-04 13:56:25.418364: train_loss -0.9185\n",
      "2024-01-04 13:56:25.424366: val_loss -0.7973\n",
      "2024-01-04 13:56:25.432365: Pseudo dice [0.92, 0.9288, 0.9358]\n",
      "2024-01-04 13:56:25.443366: Epoch time: 126.07 s\n",
      "2024-01-04 13:56:26.639871: \n",
      "2024-01-04 13:56:26.646963: Epoch 617\n",
      "2024-01-04 13:56:26.656056: Current learning rate: 0.00422\n",
      "2024-01-04 13:58:32.360596: train_loss -0.9181\n",
      "2024-01-04 13:58:32.368521: val_loss -0.7996\n",
      "2024-01-04 13:58:32.375267: Pseudo dice [0.9197, 0.9282, 0.9367]\n",
      "2024-01-04 13:58:32.381185: Epoch time: 125.72 s\n",
      "2024-01-04 13:58:33.558342: \n",
      "2024-01-04 13:58:33.564377: Epoch 618\n",
      "2024-01-04 13:58:33.576794: Current learning rate: 0.00421\n",
      "2024-01-04 14:00:39.461387: train_loss -0.9172\n",
      "2024-01-04 14:00:39.469386: val_loss -0.7926\n",
      "2024-01-04 14:00:39.478385: Pseudo dice [0.9202, 0.927, 0.9346]\n",
      "2024-01-04 14:00:39.485385: Epoch time: 125.91 s\n",
      "2024-01-04 14:00:40.569540: \n",
      "2024-01-04 14:00:40.580575: Epoch 619\n",
      "2024-01-04 14:00:40.585480: Current learning rate: 0.0042\n",
      "2024-01-04 14:02:46.193352: train_loss -0.9206\n",
      "2024-01-04 14:02:46.200349: val_loss -0.7964\n",
      "2024-01-04 14:02:46.209343: Pseudo dice [0.9238, 0.9262, 0.9351]\n",
      "2024-01-04 14:02:46.217344: Epoch time: 125.62 s\n",
      "2024-01-04 14:02:47.587350: \n",
      "2024-01-04 14:02:47.592571: Epoch 620\n",
      "2024-01-04 14:02:47.597567: Current learning rate: 0.00419\n",
      "2024-01-04 14:04:53.329856: train_loss -0.9147\n",
      "2024-01-04 14:04:53.338873: val_loss -0.8035\n",
      "2024-01-04 14:04:53.345859: Pseudo dice [0.9229, 0.9279, 0.9367]\n",
      "2024-01-04 14:04:53.349873: Epoch time: 125.74 s\n",
      "2024-01-04 14:04:54.624158: \n",
      "2024-01-04 14:04:54.630320: Epoch 621\n",
      "2024-01-04 14:04:54.635551: Current learning rate: 0.00418\n",
      "2024-01-04 14:07:00.295151: train_loss -0.9174\n",
      "2024-01-04 14:07:00.305156: val_loss -0.808\n",
      "2024-01-04 14:07:00.315659: Pseudo dice [0.921, 0.9278, 0.9354]\n",
      "2024-01-04 14:07:00.321663: Epoch time: 125.67 s\n",
      "2024-01-04 14:07:01.532597: \n",
      "2024-01-04 14:07:01.544216: Epoch 622\n",
      "2024-01-04 14:07:01.549293: Current learning rate: 0.00417\n",
      "2024-01-04 14:09:07.533957: train_loss -0.9178\n",
      "2024-01-04 14:09:07.540954: val_loss -0.8049\n",
      "2024-01-04 14:09:07.546962: Pseudo dice [0.9246, 0.9293, 0.9364]\n",
      "2024-01-04 14:09:07.551974: Epoch time: 126.0 s\n",
      "2024-01-04 14:09:08.712208: \n",
      "2024-01-04 14:09:08.719289: Epoch 623\n",
      "2024-01-04 14:09:08.726293: Current learning rate: 0.00416\n",
      "2024-01-04 14:11:14.615595: train_loss -0.917\n",
      "2024-01-04 14:11:14.623117: val_loss -0.8072\n",
      "2024-01-04 14:11:14.630121: Pseudo dice [0.9191, 0.9292, 0.9381]\n",
      "2024-01-04 14:11:14.637193: Epoch time: 125.91 s\n",
      "2024-01-04 14:11:15.932899: \n",
      "2024-01-04 14:11:15.943265: Epoch 624\n",
      "2024-01-04 14:11:15.950188: Current learning rate: 0.00415\n",
      "2024-01-04 14:13:21.635850: train_loss -0.9178\n",
      "2024-01-04 14:13:21.645850: val_loss -0.8143\n",
      "2024-01-04 14:13:21.654850: Pseudo dice [0.923, 0.9257, 0.9356]\n",
      "2024-01-04 14:13:21.661849: Epoch time: 125.71 s\n",
      "2024-01-04 14:13:23.008310: \n",
      "2024-01-04 14:13:23.013408: Epoch 625\n",
      "2024-01-04 14:13:23.022413: Current learning rate: 0.00414\n",
      "2024-01-04 14:15:28.934380: train_loss -0.9202\n",
      "2024-01-04 14:15:28.942804: val_loss -0.8029\n",
      "2024-01-04 14:15:28.951806: Pseudo dice [0.9174, 0.9281, 0.9349]\n",
      "2024-01-04 14:15:28.958806: Epoch time: 125.93 s\n",
      "2024-01-04 14:15:30.254478: \n",
      "2024-01-04 14:15:30.260779: Epoch 626\n",
      "2024-01-04 14:15:30.269799: Current learning rate: 0.00413\n",
      "2024-01-04 14:17:36.149087: train_loss -0.9166\n",
      "2024-01-04 14:17:36.156088: val_loss -0.7974\n",
      "2024-01-04 14:17:36.161088: Pseudo dice [0.9201, 0.927, 0.9356]\n",
      "2024-01-04 14:17:36.166089: Epoch time: 125.9 s\n",
      "2024-01-04 14:17:37.399474: \n",
      "2024-01-04 14:17:37.408559: Epoch 627\n",
      "2024-01-04 14:17:37.413489: Current learning rate: 0.00412\n",
      "2024-01-04 14:19:43.292385: train_loss -0.9214\n",
      "2024-01-04 14:19:43.299381: val_loss -0.8051\n",
      "2024-01-04 14:19:43.304387: Pseudo dice [0.9214, 0.9284, 0.9363]\n",
      "2024-01-04 14:19:43.310381: Epoch time: 125.89 s\n",
      "2024-01-04 14:19:44.756908: \n",
      "2024-01-04 14:19:44.765005: Epoch 628\n",
      "2024-01-04 14:19:44.772965: Current learning rate: 0.00411\n",
      "2024-01-04 14:21:50.369727: train_loss -0.9204\n",
      "2024-01-04 14:21:50.376728: val_loss -0.8006\n",
      "2024-01-04 14:21:50.381740: Pseudo dice [0.9232, 0.9273, 0.9357]\n",
      "2024-01-04 14:21:50.387731: Epoch time: 125.61 s\n",
      "2024-01-04 14:21:51.650962: \n",
      "2024-01-04 14:21:51.660565: Epoch 629\n",
      "2024-01-04 14:21:51.665564: Current learning rate: 0.0041\n",
      "2024-01-04 14:23:57.108446: train_loss -0.9199\n",
      "2024-01-04 14:23:57.115434: val_loss -0.7974\n",
      "2024-01-04 14:23:57.121435: Pseudo dice [0.9177, 0.9271, 0.9354]\n",
      "2024-01-04 14:23:57.125446: Epoch time: 125.46 s\n",
      "2024-01-04 14:23:58.334670: \n",
      "2024-01-04 14:23:58.341425: Epoch 630\n",
      "2024-01-04 14:23:58.346132: Current learning rate: 0.00409\n",
      "2024-01-04 14:26:04.739337: train_loss -0.9218\n",
      "2024-01-04 14:26:04.751338: val_loss -0.8002\n",
      "2024-01-04 14:26:04.761341: Pseudo dice [0.9241, 0.9271, 0.9349]\n",
      "2024-01-04 14:26:04.770337: Epoch time: 126.41 s\n",
      "2024-01-04 14:26:06.379487: \n",
      "2024-01-04 14:26:06.391552: Epoch 631\n",
      "2024-01-04 14:26:06.398486: Current learning rate: 0.00408\n",
      "2024-01-04 14:28:11.897697: train_loss -0.9201\n",
      "2024-01-04 14:28:11.904695: val_loss -0.8032\n",
      "2024-01-04 14:28:11.910096: Pseudo dice [0.9221, 0.9284, 0.9371]\n",
      "2024-01-04 14:28:11.915096: Epoch time: 125.52 s\n",
      "2024-01-04 14:28:13.047883: \n",
      "2024-01-04 14:28:13.053938: Epoch 632\n",
      "2024-01-04 14:28:13.057948: Current learning rate: 0.00407\n",
      "2024-01-04 14:30:18.694957: train_loss -0.9206\n",
      "2024-01-04 14:30:18.702960: val_loss -0.804\n",
      "2024-01-04 14:30:18.707958: Pseudo dice [0.9247, 0.9309, 0.9381]\n",
      "2024-01-04 14:30:18.715957: Epoch time: 125.65 s\n",
      "2024-01-04 14:30:19.859728: \n",
      "2024-01-04 14:30:19.868769: Epoch 633\n",
      "2024-01-04 14:30:19.872786: Current learning rate: 0.00406\n",
      "2024-01-04 14:32:25.485204: train_loss -0.9182\n",
      "2024-01-04 14:32:25.494203: val_loss -0.8115\n",
      "2024-01-04 14:32:25.500216: Pseudo dice [0.9221, 0.9295, 0.9389]\n",
      "2024-01-04 14:32:25.505203: Epoch time: 125.63 s\n",
      "2024-01-04 14:32:26.633993: \n",
      "2024-01-04 14:32:26.639632: Epoch 634\n",
      "2024-01-04 14:32:26.646638: Current learning rate: 0.00405\n",
      "2024-01-04 14:34:32.414277: train_loss -0.9188\n",
      "2024-01-04 14:34:32.421281: val_loss -0.805\n",
      "2024-01-04 14:34:32.430280: Pseudo dice [0.9197, 0.9281, 0.9378]\n",
      "2024-01-04 14:34:32.437287: Epoch time: 125.78 s\n",
      "2024-01-04 14:34:33.696748: \n",
      "2024-01-04 14:34:33.702921: Epoch 635\n",
      "2024-01-04 14:34:33.711980: Current learning rate: 0.00404\n",
      "2024-01-04 14:36:39.546769: train_loss -0.9161\n",
      "2024-01-04 14:36:39.559280: val_loss -0.8072\n",
      "2024-01-04 14:36:39.566280: Pseudo dice [0.9238, 0.9286, 0.9363]\n",
      "2024-01-04 14:36:39.571281: Epoch time: 125.85 s\n",
      "2024-01-04 14:36:40.704755: \n",
      "2024-01-04 14:36:40.710129: Epoch 636\n",
      "2024-01-04 14:36:40.716133: Current learning rate: 0.00403\n",
      "2024-01-04 14:38:46.187604: train_loss -0.9212\n",
      "2024-01-04 14:38:46.195604: val_loss -0.8029\n",
      "2024-01-04 14:38:46.202602: Pseudo dice [0.9212, 0.926, 0.9354]\n",
      "2024-01-04 14:38:46.208602: Epoch time: 125.48 s\n",
      "2024-01-04 14:38:47.544544: \n",
      "2024-01-04 14:38:47.551035: Epoch 637\n",
      "2024-01-04 14:38:47.557048: Current learning rate: 0.00402\n",
      "2024-01-04 14:40:53.226305: train_loss -0.9192\n",
      "2024-01-04 14:40:53.232310: val_loss -0.8039\n",
      "2024-01-04 14:40:53.239965: Pseudo dice [0.9224, 0.9282, 0.937]\n",
      "2024-01-04 14:40:53.248965: Epoch time: 125.68 s\n",
      "2024-01-04 14:40:54.542356: \n",
      "2024-01-04 14:40:54.548423: Epoch 638\n",
      "2024-01-04 14:40:54.553418: Current learning rate: 0.00401\n",
      "2024-01-04 14:42:59.928906: train_loss -0.9231\n",
      "2024-01-04 14:42:59.936905: val_loss -0.8108\n",
      "2024-01-04 14:42:59.943913: Pseudo dice [0.9204, 0.9294, 0.9371]\n",
      "2024-01-04 14:42:59.952909: Epoch time: 125.39 s\n",
      "2024-01-04 14:43:01.174250: \n",
      "2024-01-04 14:43:01.179760: Epoch 639\n",
      "2024-01-04 14:43:01.184846: Current learning rate: 0.004\n",
      "2024-01-04 14:45:07.197002: train_loss -0.9179\n",
      "2024-01-04 14:45:07.204509: val_loss -0.7924\n",
      "2024-01-04 14:45:07.210509: Pseudo dice [0.919, 0.9287, 0.9375]\n",
      "2024-01-04 14:45:07.215509: Epoch time: 126.02 s\n",
      "2024-01-04 14:45:08.446489: \n",
      "2024-01-04 14:45:08.457443: Epoch 640\n",
      "2024-01-04 14:45:08.462498: Current learning rate: 0.00399\n",
      "2024-01-04 14:47:14.709874: train_loss -0.9186\n",
      "2024-01-04 14:47:14.720869: val_loss -0.803\n",
      "2024-01-04 14:47:14.730869: Pseudo dice [0.9228, 0.9278, 0.9367]\n",
      "2024-01-04 14:47:14.737875: Epoch time: 126.27 s\n",
      "2024-01-04 14:47:16.177586: \n",
      "2024-01-04 14:47:16.185580: Epoch 641\n",
      "2024-01-04 14:47:16.190576: Current learning rate: 0.00398\n",
      "2024-01-04 14:49:21.881120: train_loss -0.9169\n",
      "2024-01-04 14:49:21.891134: val_loss -0.7927\n",
      "2024-01-04 14:49:21.897130: Pseudo dice [0.9233, 0.9299, 0.9374]\n",
      "2024-01-04 14:49:21.902652: Epoch time: 125.71 s\n",
      "2024-01-04 14:49:23.040350: \n",
      "2024-01-04 14:49:23.046319: Epoch 642\n",
      "2024-01-04 14:49:23.059325: Current learning rate: 0.00397\n",
      "2024-01-04 14:51:28.556684: train_loss -0.9194\n",
      "2024-01-04 14:51:28.563683: val_loss -0.8027\n",
      "2024-01-04 14:51:28.571691: Pseudo dice [0.9192, 0.9281, 0.9371]\n",
      "2024-01-04 14:51:28.577693: Epoch time: 125.52 s\n",
      "2024-01-04 14:51:29.713296: \n",
      "2024-01-04 14:51:29.719305: Epoch 643\n",
      "2024-01-04 14:51:29.724304: Current learning rate: 0.00396\n",
      "2024-01-04 14:53:35.551019: train_loss -0.9181\n",
      "2024-01-04 14:53:35.561030: val_loss -0.8081\n",
      "2024-01-04 14:53:35.568580: Pseudo dice [0.9181, 0.9287, 0.9356]\n",
      "2024-01-04 14:53:35.575562: Epoch time: 125.84 s\n",
      "2024-01-04 14:53:36.716921: \n",
      "2024-01-04 14:53:36.727222: Epoch 644\n",
      "2024-01-04 14:53:36.733217: Current learning rate: 0.00395\n",
      "2024-01-04 14:55:42.490764: train_loss -0.9188\n",
      "2024-01-04 14:55:42.501761: val_loss -0.8019\n",
      "2024-01-04 14:55:42.506760: Pseudo dice [0.9227, 0.9278, 0.9362]\n",
      "2024-01-04 14:55:42.512761: Epoch time: 125.78 s\n",
      "2024-01-04 14:55:43.907098: \n",
      "2024-01-04 14:55:43.916078: Epoch 645\n",
      "2024-01-04 14:55:43.921094: Current learning rate: 0.00394\n",
      "2024-01-04 14:57:49.463034: train_loss -0.9187\n",
      "2024-01-04 14:57:49.473118: val_loss -0.8025\n",
      "2024-01-04 14:57:49.480105: Pseudo dice [0.9205, 0.9292, 0.9365]\n",
      "2024-01-04 14:57:49.485784: Epoch time: 125.56 s\n",
      "2024-01-04 14:57:50.553821: \n",
      "2024-01-04 14:57:50.559829: Epoch 646\n",
      "2024-01-04 14:57:50.564488: Current learning rate: 0.00393\n",
      "2024-01-04 14:59:56.240082: train_loss -0.9189\n",
      "2024-01-04 14:59:56.246082: val_loss -0.8082\n",
      "2024-01-04 14:59:56.251102: Pseudo dice [0.9176, 0.9314, 0.9381]\n",
      "2024-01-04 14:59:56.256095: Epoch time: 125.69 s\n",
      "2024-01-04 14:59:57.390443: \n",
      "2024-01-04 14:59:57.396849: Epoch 647\n",
      "2024-01-04 14:59:57.405081: Current learning rate: 0.00392\n",
      "2024-01-04 15:02:03.309969: train_loss -0.9202\n",
      "2024-01-04 15:02:03.317483: val_loss -0.8062\n",
      "2024-01-04 15:02:03.322482: Pseudo dice [0.9232, 0.9287, 0.936]\n",
      "2024-01-04 15:02:03.329482: Epoch time: 125.92 s\n",
      "2024-01-04 15:02:04.658930: \n",
      "2024-01-04 15:02:04.672961: Epoch 648\n",
      "2024-01-04 15:02:04.681268: Current learning rate: 0.00391\n",
      "2024-01-04 15:04:10.700819: train_loss -0.9152\n",
      "2024-01-04 15:04:10.708828: val_loss -0.8034\n",
      "2024-01-04 15:04:10.715827: Pseudo dice [0.9205, 0.9299, 0.9389]\n",
      "2024-01-04 15:04:10.720826: Epoch time: 126.04 s\n",
      "2024-01-04 15:04:12.057123: \n",
      "2024-01-04 15:04:12.065383: Epoch 649\n",
      "2024-01-04 15:04:12.069453: Current learning rate: 0.0039\n",
      "2024-01-04 15:06:17.812388: train_loss -0.9183\n",
      "2024-01-04 15:06:17.818379: val_loss -0.7991\n",
      "2024-01-04 15:06:17.823387: Pseudo dice [0.9171, 0.9277, 0.9362]\n",
      "2024-01-04 15:06:17.829388: Epoch time: 125.76 s\n",
      "2024-01-04 15:06:19.395878: \n",
      "2024-01-04 15:06:19.402088: Epoch 650\n",
      "2024-01-04 15:06:19.407079: Current learning rate: 0.00389\n",
      "2024-01-04 15:08:25.179910: train_loss -0.9146\n",
      "2024-01-04 15:08:25.188912: val_loss -0.7948\n",
      "2024-01-04 15:08:25.195911: Pseudo dice [0.9207, 0.9274, 0.9359]\n",
      "2024-01-04 15:08:25.204912: Epoch time: 125.79 s\n",
      "2024-01-04 15:08:26.461288: \n",
      "2024-01-04 15:08:26.469222: Epoch 651\n",
      "2024-01-04 15:08:26.477572: Current learning rate: 0.00388\n",
      "2024-01-04 15:10:32.825577: train_loss -0.9145\n",
      "2024-01-04 15:10:32.833577: val_loss -0.7969\n",
      "2024-01-04 15:10:32.843578: Pseudo dice [0.9178, 0.9286, 0.9374]\n",
      "2024-01-04 15:10:32.853126: Epoch time: 126.37 s\n",
      "2024-01-04 15:10:34.121597: \n",
      "2024-01-04 15:10:34.134325: Epoch 652\n",
      "2024-01-04 15:10:34.139332: Current learning rate: 0.00387\n",
      "2024-01-04 15:12:40.157960: train_loss -0.9046\n",
      "2024-01-04 15:12:40.166983: val_loss -0.796\n",
      "2024-01-04 15:12:40.174983: Pseudo dice [0.9212, 0.9236, 0.9318]\n",
      "2024-01-04 15:12:40.180983: Epoch time: 126.04 s\n",
      "2024-01-04 15:12:41.606731: \n",
      "2024-01-04 15:12:41.612811: Epoch 653\n",
      "2024-01-04 15:12:41.617825: Current learning rate: 0.00386\n",
      "2024-01-04 15:14:47.298764: train_loss -0.9162\n",
      "2024-01-04 15:14:47.305765: val_loss -0.7886\n",
      "2024-01-04 15:14:47.312765: Pseudo dice [0.9223, 0.9238, 0.9323]\n",
      "2024-01-04 15:14:47.318765: Epoch time: 125.69 s\n",
      "2024-01-04 15:14:48.521157: \n",
      "2024-01-04 15:14:48.526158: Epoch 654\n",
      "2024-01-04 15:14:48.531507: Current learning rate: 0.00385\n",
      "2024-01-04 15:16:53.932325: train_loss -0.9185\n",
      "2024-01-04 15:16:53.939325: val_loss -0.7972\n",
      "2024-01-04 15:16:53.947325: Pseudo dice [0.9227, 0.9272, 0.9359]\n",
      "2024-01-04 15:16:53.952333: Epoch time: 125.41 s\n",
      "2024-01-04 15:16:55.181438: \n",
      "2024-01-04 15:16:55.187555: Epoch 655\n",
      "2024-01-04 15:16:55.191192: Current learning rate: 0.00384\n",
      "2024-01-04 15:19:01.023801: train_loss -0.9175\n",
      "2024-01-04 15:19:01.031801: val_loss -0.7923\n",
      "2024-01-04 15:19:01.041799: Pseudo dice [0.9164, 0.9268, 0.9355]\n",
      "2024-01-04 15:19:01.048800: Epoch time: 125.84 s\n",
      "2024-01-04 15:19:02.511922: \n",
      "2024-01-04 15:19:02.518916: Epoch 656\n",
      "2024-01-04 15:19:02.523909: Current learning rate: 0.00383\n",
      "2024-01-04 15:21:08.258261: train_loss -0.9175\n",
      "2024-01-04 15:21:08.264769: val_loss -0.7877\n",
      "2024-01-04 15:21:08.273774: Pseudo dice [0.9225, 0.9266, 0.9343]\n",
      "2024-01-04 15:21:08.279773: Epoch time: 125.75 s\n",
      "2024-01-04 15:21:09.409815: \n",
      "2024-01-04 15:21:09.415815: Epoch 657\n",
      "2024-01-04 15:21:09.420806: Current learning rate: 0.00382\n",
      "2024-01-04 15:23:15.414310: train_loss -0.9186\n",
      "2024-01-04 15:23:15.421820: val_loss -0.7886\n",
      "2024-01-04 15:23:15.427896: Pseudo dice [0.923, 0.9276, 0.9372]\n",
      "2024-01-04 15:23:15.432826: Epoch time: 126.01 s\n",
      "2024-01-04 15:23:16.578399: \n",
      "2024-01-04 15:23:16.587465: Epoch 658\n",
      "2024-01-04 15:23:16.592488: Current learning rate: 0.00381\n",
      "2024-01-04 15:25:22.183882: train_loss -0.9183\n",
      "2024-01-04 15:25:22.192874: val_loss -0.799\n",
      "2024-01-04 15:25:22.197874: Pseudo dice [0.9234, 0.9281, 0.9351]\n",
      "2024-01-04 15:25:22.203379: Epoch time: 125.61 s\n",
      "2024-01-04 15:25:23.268559: \n",
      "2024-01-04 15:25:23.275532: Epoch 659\n",
      "2024-01-04 15:25:23.279553: Current learning rate: 0.0038\n",
      "2024-01-04 15:27:28.987420: train_loss -0.9185\n",
      "2024-01-04 15:27:28.994426: val_loss -0.8047\n",
      "2024-01-04 15:27:29.000420: Pseudo dice [0.9235, 0.9283, 0.9377]\n",
      "2024-01-04 15:27:29.008422: Epoch time: 125.72 s\n",
      "2024-01-04 15:27:30.346741: \n",
      "2024-01-04 15:27:30.351813: Epoch 660\n",
      "2024-01-04 15:27:30.356743: Current learning rate: 0.00379\n",
      "2024-01-04 15:29:35.975774: train_loss -0.919\n",
      "2024-01-04 15:29:35.985774: val_loss -0.7996\n",
      "2024-01-04 15:29:35.993774: Pseudo dice [0.9223, 0.9301, 0.9372]\n",
      "2024-01-04 15:29:35.999772: Epoch time: 125.63 s\n",
      "2024-01-04 15:29:37.251374: \n",
      "2024-01-04 15:29:37.257562: Epoch 661\n",
      "2024-01-04 15:29:37.261547: Current learning rate: 0.00378\n",
      "2024-01-04 15:31:42.752342: train_loss -0.92\n",
      "2024-01-04 15:31:42.762355: val_loss -0.8054\n",
      "2024-01-04 15:31:42.769352: Pseudo dice [0.9201, 0.9286, 0.9374]\n",
      "2024-01-04 15:31:42.775347: Epoch time: 125.5 s\n",
      "2024-01-04 15:31:43.989154: \n",
      "2024-01-04 15:31:43.998139: Epoch 662\n",
      "2024-01-04 15:31:44.006050: Current learning rate: 0.00377\n",
      "2024-01-04 15:33:49.724353: train_loss -0.9183\n",
      "2024-01-04 15:33:49.731352: val_loss -0.7946\n",
      "2024-01-04 15:33:49.736359: Pseudo dice [0.9187, 0.9265, 0.9342]\n",
      "2024-01-04 15:33:49.740362: Epoch time: 125.74 s\n",
      "2024-01-04 15:33:50.866552: \n",
      "2024-01-04 15:33:50.872409: Epoch 663\n",
      "2024-01-04 15:33:50.876493: Current learning rate: 0.00376\n",
      "2024-01-04 15:35:56.553752: train_loss -0.9178\n",
      "2024-01-04 15:35:56.560744: val_loss -0.7996\n",
      "2024-01-04 15:35:56.566742: Pseudo dice [0.9191, 0.9277, 0.9369]\n",
      "2024-01-04 15:35:56.571939: Epoch time: 125.69 s\n",
      "2024-01-04 15:35:57.828860: \n",
      "2024-01-04 15:35:57.835567: Epoch 664\n",
      "2024-01-04 15:35:57.846335: Current learning rate: 0.00375\n",
      "2024-01-04 15:38:03.473628: train_loss -0.9186\n",
      "2024-01-04 15:38:03.480628: val_loss -0.7959\n",
      "2024-01-04 15:38:03.485628: Pseudo dice [0.9207, 0.9285, 0.9372]\n",
      "2024-01-04 15:38:03.489628: Epoch time: 125.65 s\n",
      "2024-01-04 15:38:04.647023: \n",
      "2024-01-04 15:38:04.659362: Epoch 665\n",
      "2024-01-04 15:38:04.664417: Current learning rate: 0.00374\n",
      "2024-01-04 15:40:10.193882: train_loss -0.92\n",
      "2024-01-04 15:40:10.200957: val_loss -0.7858\n",
      "2024-01-04 15:40:10.206898: Pseudo dice [0.9188, 0.9281, 0.9362]\n",
      "2024-01-04 15:40:10.211908: Epoch time: 125.55 s\n",
      "2024-01-04 15:40:11.298589: \n",
      "2024-01-04 15:40:11.304884: Epoch 666\n",
      "2024-01-04 15:40:11.313038: Current learning rate: 0.00373\n",
      "2024-01-04 15:42:17.237565: train_loss -0.9186\n",
      "2024-01-04 15:42:17.250565: val_loss -0.8067\n",
      "2024-01-04 15:42:17.256571: Pseudo dice [0.9192, 0.9285, 0.936]\n",
      "2024-01-04 15:42:17.262568: Epoch time: 125.94 s\n",
      "2024-01-04 15:42:18.395387: \n",
      "2024-01-04 15:42:18.403046: Epoch 667\n",
      "2024-01-04 15:42:18.409134: Current learning rate: 0.00372\n",
      "2024-01-04 15:44:24.314072: train_loss -0.9157\n",
      "2024-01-04 15:44:24.325074: val_loss -0.8035\n",
      "2024-01-04 15:44:24.334073: Pseudo dice [0.9225, 0.9286, 0.9381]\n",
      "2024-01-04 15:44:24.339072: Epoch time: 125.92 s\n",
      "2024-01-04 15:44:25.659100: \n",
      "2024-01-04 15:44:25.667308: Epoch 668\n",
      "2024-01-04 15:44:25.672327: Current learning rate: 0.00371\n",
      "2024-01-04 15:46:31.406932: train_loss -0.9196\n",
      "2024-01-04 15:46:31.414924: val_loss -0.799\n",
      "2024-01-04 15:46:31.420922: Pseudo dice [0.9218, 0.9287, 0.9374]\n",
      "2024-01-04 15:46:31.426921: Epoch time: 125.75 s\n",
      "2024-01-04 15:46:32.664971: \n",
      "2024-01-04 15:46:32.673545: Epoch 669\n",
      "2024-01-04 15:46:32.677607: Current learning rate: 0.0037\n",
      "2024-01-04 15:48:37.841017: train_loss -0.922\n",
      "2024-01-04 15:48:37.849012: val_loss -0.7975\n",
      "2024-01-04 15:48:37.854013: Pseudo dice [0.9215, 0.9273, 0.9363]\n",
      "2024-01-04 15:48:37.859010: Epoch time: 125.18 s\n",
      "2024-01-04 15:48:39.170415: \n",
      "2024-01-04 15:48:39.180842: Epoch 670\n",
      "2024-01-04 15:48:39.186882: Current learning rate: 0.00369\n",
      "2024-01-04 15:50:44.970951: train_loss -0.9203\n",
      "2024-01-04 15:50:44.981952: val_loss -0.8034\n",
      "2024-01-04 15:50:44.987952: Pseudo dice [0.9205, 0.9277, 0.9367]\n",
      "2024-01-04 15:50:44.992970: Epoch time: 125.8 s\n",
      "2024-01-04 15:50:46.144185: \n",
      "2024-01-04 15:50:46.150195: Epoch 671\n",
      "2024-01-04 15:50:46.154204: Current learning rate: 0.00368\n",
      "2024-01-04 15:52:51.778469: train_loss -0.9204\n",
      "2024-01-04 15:52:51.790469: val_loss -0.8046\n",
      "2024-01-04 15:52:51.795476: Pseudo dice [0.922, 0.9283, 0.9361]\n",
      "2024-01-04 15:52:51.800475: Epoch time: 125.64 s\n",
      "2024-01-04 15:52:53.003627: \n",
      "2024-01-04 15:52:53.042491: Epoch 672\n",
      "2024-01-04 15:52:53.051466: Current learning rate: 0.00367\n",
      "2024-01-04 15:54:59.138922: train_loss -0.9172\n",
      "2024-01-04 15:54:59.146923: val_loss -0.8003\n",
      "2024-01-04 15:54:59.152922: Pseudo dice [0.9217, 0.9279, 0.9368]\n",
      "2024-01-04 15:54:59.158433: Epoch time: 126.14 s\n",
      "2024-01-04 15:55:00.493586: \n",
      "2024-01-04 15:55:00.500660: Epoch 673\n",
      "2024-01-04 15:55:00.504659: Current learning rate: 0.00366\n",
      "2024-01-04 15:57:06.159954: train_loss -0.9193\n",
      "2024-01-04 15:57:06.170954: val_loss -0.7998\n",
      "2024-01-04 15:57:06.175947: Pseudo dice [0.922, 0.9291, 0.937]\n",
      "2024-01-04 15:57:06.179948: Epoch time: 125.67 s\n",
      "2024-01-04 15:57:07.307300: \n",
      "2024-01-04 15:57:07.315024: Epoch 674\n",
      "2024-01-04 15:57:07.319976: Current learning rate: 0.00365\n",
      "2024-01-04 15:59:13.005402: train_loss -0.9212\n",
      "2024-01-04 15:59:13.013408: val_loss -0.7788\n",
      "2024-01-04 15:59:13.021405: Pseudo dice [0.9203, 0.9266, 0.9342]\n",
      "2024-01-04 15:59:13.028918: Epoch time: 125.7 s\n",
      "2024-01-04 15:59:14.405975: \n",
      "2024-01-04 15:59:14.414975: Epoch 675\n",
      "2024-01-04 15:59:14.419978: Current learning rate: 0.00364\n",
      "2024-01-04 16:01:20.092038: train_loss -0.9162\n",
      "2024-01-04 16:01:20.099037: val_loss -0.8041\n",
      "2024-01-04 16:01:20.106040: Pseudo dice [0.9219, 0.926, 0.9346]\n",
      "2024-01-04 16:01:20.113045: Epoch time: 125.69 s\n",
      "2024-01-04 16:01:21.563287: \n",
      "2024-01-04 16:01:21.569376: Epoch 676\n",
      "2024-01-04 16:01:21.573366: Current learning rate: 0.00363\n",
      "2024-01-04 16:03:27.174690: train_loss -0.9223\n",
      "2024-01-04 16:03:27.183689: val_loss -0.8022\n",
      "2024-01-04 16:03:27.191695: Pseudo dice [0.9222, 0.9307, 0.9395]\n",
      "2024-01-04 16:03:27.197694: Epoch time: 125.61 s\n",
      "2024-01-04 16:03:28.582338: \n",
      "2024-01-04 16:03:28.595292: Epoch 677\n",
      "2024-01-04 16:03:28.600590: Current learning rate: 0.00362\n",
      "2024-01-04 16:05:34.611044: train_loss -0.9203\n",
      "2024-01-04 16:05:34.618043: val_loss -0.7969\n",
      "2024-01-04 16:05:34.623043: Pseudo dice [0.9211, 0.9309, 0.938]\n",
      "2024-01-04 16:05:34.629045: Epoch time: 126.03 s\n",
      "2024-01-04 16:05:35.724348: \n",
      "2024-01-04 16:05:35.730428: Epoch 678\n",
      "2024-01-04 16:05:35.735483: Current learning rate: 0.00361\n",
      "2024-01-04 16:07:41.071512: train_loss -0.9231\n",
      "2024-01-04 16:07:41.081021: val_loss -0.8016\n",
      "2024-01-04 16:07:41.086025: Pseudo dice [0.9202, 0.9284, 0.9399]\n",
      "2024-01-04 16:07:41.092025: Epoch time: 125.35 s\n",
      "2024-01-04 16:07:42.274338: \n",
      "2024-01-04 16:07:42.280961: Epoch 679\n",
      "2024-01-04 16:07:42.289036: Current learning rate: 0.0036\n",
      "2024-01-04 16:09:48.236492: train_loss -0.9179\n",
      "2024-01-04 16:09:48.243491: val_loss -0.7905\n",
      "2024-01-04 16:09:48.249496: Pseudo dice [0.9179, 0.9285, 0.9364]\n",
      "2024-01-04 16:09:48.255506: Epoch time: 125.96 s\n",
      "2024-01-04 16:09:49.456965: \n",
      "2024-01-04 16:09:49.465281: Epoch 680\n",
      "2024-01-04 16:09:49.472282: Current learning rate: 0.00359\n",
      "2024-01-04 16:11:55.347937: train_loss -0.919\n",
      "2024-01-04 16:11:55.354936: val_loss -0.8044\n",
      "2024-01-04 16:11:55.362938: Pseudo dice [0.9222, 0.9309, 0.9382]\n",
      "2024-01-04 16:11:55.369935: Epoch time: 125.89 s\n",
      "2024-01-04 16:11:56.649659: \n",
      "2024-01-04 16:11:56.658961: Epoch 681\n",
      "2024-01-04 16:11:56.669048: Current learning rate: 0.00358\n",
      "2024-01-04 16:14:02.493792: train_loss -0.919\n",
      "2024-01-04 16:14:02.500794: val_loss -0.8071\n",
      "2024-01-04 16:14:02.509793: Pseudo dice [0.9213, 0.9295, 0.9378]\n",
      "2024-01-04 16:14:02.517793: Epoch time: 125.85 s\n",
      "2024-01-04 16:14:03.952792: \n",
      "2024-01-04 16:14:03.960792: Epoch 682\n",
      "2024-01-04 16:14:03.965791: Current learning rate: 0.00357\n",
      "2024-01-04 16:16:09.388492: train_loss -0.919\n",
      "2024-01-04 16:16:09.396491: val_loss -0.7991\n",
      "2024-01-04 16:16:09.411015: Pseudo dice [0.9222, 0.9257, 0.9352]\n",
      "2024-01-04 16:16:09.418036: Epoch time: 125.44 s\n",
      "2024-01-04 16:16:10.505002: \n",
      "2024-01-04 16:16:10.512936: Epoch 683\n",
      "2024-01-04 16:16:10.517004: Current learning rate: 0.00356\n",
      "2024-01-04 16:18:16.251454: train_loss -0.9191\n",
      "2024-01-04 16:18:16.257455: val_loss -0.7953\n",
      "2024-01-04 16:18:16.264454: Pseudo dice [0.9221, 0.9289, 0.9374]\n",
      "2024-01-04 16:18:16.271455: Epoch time: 125.75 s\n",
      "2024-01-04 16:18:17.695716: \n",
      "2024-01-04 16:18:17.701791: Epoch 684\n",
      "2024-01-04 16:18:17.706855: Current learning rate: 0.00355\n",
      "2024-01-04 16:20:23.664846: train_loss -0.919\n",
      "2024-01-04 16:20:23.673854: val_loss -0.8072\n",
      "2024-01-04 16:20:23.681362: Pseudo dice [0.9218, 0.9282, 0.9364]\n",
      "2024-01-04 16:20:23.689367: Epoch time: 125.97 s\n",
      "2024-01-04 16:20:24.951457: \n",
      "2024-01-04 16:20:24.960412: Epoch 685\n",
      "2024-01-04 16:20:24.965445: Current learning rate: 0.00354\n",
      "2024-01-04 16:22:30.349135: train_loss -0.9224\n",
      "2024-01-04 16:22:30.357139: val_loss -0.7992\n",
      "2024-01-04 16:22:30.363138: Pseudo dice [0.9222, 0.93, 0.9379]\n",
      "2024-01-04 16:22:30.367646: Epoch time: 125.4 s\n",
      "2024-01-04 16:22:31.494426: \n",
      "2024-01-04 16:22:31.500556: Epoch 686\n",
      "2024-01-04 16:22:31.505631: Current learning rate: 0.00353\n",
      "2024-01-04 16:24:37.449360: train_loss -0.9204\n",
      "2024-01-04 16:24:37.457360: val_loss -0.8067\n",
      "2024-01-04 16:24:37.464359: Pseudo dice [0.9211, 0.9271, 0.9383]\n",
      "2024-01-04 16:24:37.470361: Epoch time: 125.96 s\n",
      "2024-01-04 16:24:38.704573: \n",
      "2024-01-04 16:24:38.715269: Epoch 687\n",
      "2024-01-04 16:24:38.719355: Current learning rate: 0.00352\n",
      "2024-01-04 16:26:44.424137: train_loss -0.9205\n",
      "2024-01-04 16:26:44.430137: val_loss -0.7948\n",
      "2024-01-04 16:26:44.436137: Pseudo dice [0.9221, 0.9264, 0.9362]\n",
      "2024-01-04 16:26:44.441136: Epoch time: 125.72 s\n",
      "2024-01-04 16:26:45.569307: \n",
      "2024-01-04 16:26:45.576327: Epoch 688\n",
      "2024-01-04 16:26:45.581391: Current learning rate: 0.00351\n",
      "2024-01-04 16:28:50.977446: train_loss -0.9228\n",
      "2024-01-04 16:28:50.983440: val_loss -0.8054\n",
      "2024-01-04 16:28:50.988438: Pseudo dice [0.9163, 0.9274, 0.9362]\n",
      "2024-01-04 16:28:50.993437: Epoch time: 125.41 s\n",
      "2024-01-04 16:28:52.214061: \n",
      "2024-01-04 16:28:52.221056: Epoch 689\n",
      "2024-01-04 16:28:52.226007: Current learning rate: 0.0035\n",
      "2024-01-04 16:30:58.019074: train_loss -0.918\n",
      "2024-01-04 16:30:58.026066: val_loss -0.8073\n",
      "2024-01-04 16:30:58.031080: Pseudo dice [0.9218, 0.9285, 0.9365]\n",
      "2024-01-04 16:30:58.036079: Epoch time: 125.81 s\n",
      "2024-01-04 16:30:59.337608: \n",
      "2024-01-04 16:30:59.343608: Epoch 690\n",
      "2024-01-04 16:30:59.349870: Current learning rate: 0.00349\n",
      "2024-01-04 16:33:04.873990: train_loss -0.919\n",
      "2024-01-04 16:33:04.879990: val_loss -0.7939\n",
      "2024-01-04 16:33:04.886990: Pseudo dice [0.921, 0.9266, 0.9356]\n",
      "2024-01-04 16:33:04.892990: Epoch time: 125.54 s\n",
      "2024-01-04 16:33:06.403296: \n",
      "2024-01-04 16:33:06.412302: Epoch 691\n",
      "2024-01-04 16:33:06.417311: Current learning rate: 0.00348\n",
      "2024-01-04 16:35:12.133901: train_loss -0.919\n",
      "2024-01-04 16:35:12.140418: val_loss -0.8103\n",
      "2024-01-04 16:35:12.148425: Pseudo dice [0.9169, 0.9313, 0.9402]\n",
      "2024-01-04 16:35:12.153422: Epoch time: 125.73 s\n",
      "2024-01-04 16:35:13.311824: \n",
      "2024-01-04 16:35:13.316823: Epoch 692\n",
      "2024-01-04 16:35:13.322253: Current learning rate: 0.00346\n",
      "2024-01-04 16:37:18.841904: train_loss -0.9207\n",
      "2024-01-04 16:37:18.850906: val_loss -0.8068\n",
      "2024-01-04 16:37:18.857905: Pseudo dice [0.918, 0.9283, 0.9376]\n",
      "2024-01-04 16:37:18.863904: Epoch time: 125.53 s\n",
      "2024-01-04 16:37:20.038344: \n",
      "2024-01-04 16:37:20.046747: Epoch 693\n",
      "2024-01-04 16:37:20.055403: Current learning rate: 0.00345\n",
      "2024-01-04 16:39:33.898509: train_loss -0.9207\n",
      "2024-01-04 16:39:33.910509: val_loss -0.8009\n",
      "2024-01-04 16:39:33.920027: Pseudo dice [0.9225, 0.93, 0.9381]\n",
      "2024-01-04 16:39:33.928026: Epoch time: 133.86 s\n",
      "2024-01-04 16:39:35.639537: \n",
      "2024-01-04 16:39:35.647477: Epoch 694\n",
      "2024-01-04 16:39:35.654477: Current learning rate: 0.00344\n",
      "2024-01-04 16:41:42.230957: train_loss -0.9201\n",
      "2024-01-04 16:41:42.239959: val_loss -0.8015\n",
      "2024-01-04 16:41:42.249468: Pseudo dice [0.9191, 0.9283, 0.9367]\n",
      "2024-01-04 16:41:42.255477: Epoch time: 126.59 s\n",
      "2024-01-04 16:41:43.499339: \n",
      "2024-01-04 16:41:43.505584: Epoch 695\n",
      "2024-01-04 16:41:43.510755: Current learning rate: 0.00343\n",
      "2024-01-04 16:43:50.119481: train_loss -0.9195\n",
      "2024-01-04 16:43:50.126481: val_loss -0.7934\n",
      "2024-01-04 16:43:50.131481: Pseudo dice [0.9205, 0.927, 0.9366]\n",
      "2024-01-04 16:43:50.136481: Epoch time: 126.62 s\n",
      "2024-01-04 16:43:51.330683: \n",
      "2024-01-04 16:43:51.348113: Epoch 696\n",
      "2024-01-04 16:43:51.353099: Current learning rate: 0.00342\n",
      "2024-01-04 16:45:57.385797: train_loss -0.9195\n",
      "2024-01-04 16:45:57.397800: val_loss -0.8063\n",
      "2024-01-04 16:45:57.405796: Pseudo dice [0.9193, 0.928, 0.9375]\n",
      "2024-01-04 16:45:57.411796: Epoch time: 126.06 s\n",
      "2024-01-04 16:45:58.664135: \n",
      "2024-01-04 16:45:58.671123: Epoch 697\n",
      "2024-01-04 16:45:58.677155: Current learning rate: 0.00341\n",
      "2024-01-04 16:48:04.101954: train_loss -0.9239\n",
      "2024-01-04 16:48:04.109961: val_loss -0.7988\n",
      "2024-01-04 16:48:04.118960: Pseudo dice [0.9186, 0.9284, 0.9372]\n",
      "2024-01-04 16:48:04.124476: Epoch time: 125.44 s\n",
      "2024-01-04 16:48:05.548472: \n",
      "2024-01-04 16:48:05.554624: Epoch 698\n",
      "2024-01-04 16:48:05.559645: Current learning rate: 0.0034\n",
      "2024-01-04 16:50:11.019588: train_loss -0.9192\n",
      "2024-01-04 16:50:11.026590: val_loss -0.7892\n",
      "2024-01-04 16:50:11.031592: Pseudo dice [0.9237, 0.929, 0.9362]\n",
      "2024-01-04 16:50:11.035588: Epoch time: 125.47 s\n",
      "2024-01-04 16:50:12.400696: \n",
      "2024-01-04 16:50:12.406698: Epoch 699\n",
      "2024-01-04 16:50:12.410692: Current learning rate: 0.00339\n",
      "2024-01-04 16:52:18.202488: train_loss -0.9187\n",
      "2024-01-04 16:52:18.209495: val_loss -0.8025\n",
      "2024-01-04 16:52:18.216497: Pseudo dice [0.9203, 0.9287, 0.9384]\n",
      "2024-01-04 16:52:18.222015: Epoch time: 125.8 s\n",
      "2024-01-04 16:52:19.707053: \n",
      "2024-01-04 16:52:19.713877: Epoch 700\n",
      "2024-01-04 16:52:19.718937: Current learning rate: 0.00338\n",
      "2024-01-04 16:54:25.063353: train_loss -0.9227\n",
      "2024-01-04 16:54:25.070356: val_loss -0.8033\n",
      "2024-01-04 16:54:25.075356: Pseudo dice [0.9238, 0.9271, 0.9361]\n",
      "2024-01-04 16:54:25.081870: Epoch time: 125.36 s\n",
      "2024-01-04 16:54:26.265307: \n",
      "2024-01-04 16:54:26.273392: Epoch 701\n",
      "2024-01-04 16:54:26.278338: Current learning rate: 0.00337\n",
      "2024-01-04 16:56:31.988585: train_loss -0.9198\n",
      "2024-01-04 16:56:31.996583: val_loss -0.805\n",
      "2024-01-04 16:56:32.013245: Pseudo dice [0.9227, 0.9291, 0.9367]\n",
      "2024-01-04 16:56:32.020249: Epoch time: 125.72 s\n",
      "2024-01-04 16:56:33.380605: \n",
      "2024-01-04 16:56:33.386592: Epoch 702\n",
      "2024-01-04 16:56:33.390592: Current learning rate: 0.00336\n",
      "2024-01-04 16:58:38.869863: train_loss -0.922\n",
      "2024-01-04 16:58:38.878864: val_loss -0.7936\n",
      "2024-01-04 16:58:38.886864: Pseudo dice [0.9214, 0.9279, 0.9356]\n",
      "2024-01-04 16:58:38.892864: Epoch time: 125.49 s\n",
      "2024-01-04 16:58:40.104297: \n",
      "2024-01-04 16:58:40.109886: Epoch 703\n",
      "2024-01-04 16:58:40.113853: Current learning rate: 0.00335\n",
      "2024-01-04 17:00:45.680240: train_loss -0.9208\n",
      "2024-01-04 17:00:45.687239: val_loss -0.7921\n",
      "2024-01-04 17:00:45.693249: Pseudo dice [0.9202, 0.9257, 0.9354]\n",
      "2024-01-04 17:00:45.698251: Epoch time: 125.58 s\n",
      "2024-01-04 17:00:46.816198: \n",
      "2024-01-04 17:00:46.822222: Epoch 704\n",
      "2024-01-04 17:00:46.829319: Current learning rate: 0.00334\n",
      "2024-01-04 17:02:52.524081: train_loss -0.9206\n",
      "2024-01-04 17:02:52.531151: val_loss -0.8119\n",
      "2024-01-04 17:02:52.537131: Pseudo dice [0.9235, 0.9305, 0.9408]\n",
      "2024-01-04 17:02:52.542658: Epoch time: 125.71 s\n",
      "2024-01-04 17:02:53.721824: \n",
      "2024-01-04 17:02:53.728942: Epoch 705\n",
      "2024-01-04 17:02:53.733927: Current learning rate: 0.00333\n",
      "2024-01-04 17:04:59.789705: train_loss -0.9195\n",
      "2024-01-04 17:04:59.795704: val_loss -0.8025\n",
      "2024-01-04 17:04:59.802208: Pseudo dice [0.9231, 0.9279, 0.9373]\n",
      "2024-01-04 17:04:59.810928: Epoch time: 126.07 s\n",
      "2024-01-04 17:05:01.257653: \n",
      "2024-01-04 17:05:01.266275: Epoch 706\n",
      "2024-01-04 17:05:01.270702: Current learning rate: 0.00332\n",
      "2024-01-04 17:07:06.783916: train_loss -0.9181\n",
      "2024-01-04 17:07:06.790915: val_loss -0.8086\n",
      "2024-01-04 17:07:06.795915: Pseudo dice [0.9208, 0.9301, 0.9389]\n",
      "2024-01-04 17:07:06.799915: Epoch time: 125.53 s\n",
      "2024-01-04 17:07:07.939345: \n",
      "2024-01-04 17:07:07.991575: Epoch 707\n",
      "2024-01-04 17:07:08.002603: Current learning rate: 0.00331\n",
      "2024-01-04 17:09:13.716941: train_loss -0.92\n",
      "2024-01-04 17:09:13.722954: val_loss -0.7933\n",
      "2024-01-04 17:09:13.727957: Pseudo dice [0.9223, 0.9299, 0.9374]\n",
      "2024-01-04 17:09:13.733961: Epoch time: 125.78 s\n",
      "2024-01-04 17:09:14.956242: \n",
      "2024-01-04 17:09:14.963088: Epoch 708\n",
      "2024-01-04 17:09:14.969157: Current learning rate: 0.0033\n",
      "2024-01-04 17:11:20.637841: train_loss -0.921\n",
      "2024-01-04 17:11:20.644840: val_loss -0.7972\n",
      "2024-01-04 17:11:20.650840: Pseudo dice [0.9183, 0.9293, 0.9373]\n",
      "2024-01-04 17:11:20.655840: Epoch time: 125.68 s\n",
      "2024-01-04 17:11:21.864620: \n",
      "2024-01-04 17:11:21.871615: Epoch 709\n",
      "2024-01-04 17:11:21.876608: Current learning rate: 0.00329\n",
      "2024-01-04 17:13:27.199424: train_loss -0.9219\n",
      "2024-01-04 17:13:27.208417: val_loss -0.8037\n",
      "2024-01-04 17:13:27.214420: Pseudo dice [0.927, 0.9285, 0.9368]\n",
      "2024-01-04 17:13:27.218424: Epoch time: 125.34 s\n",
      "2024-01-04 17:13:28.426180: \n",
      "2024-01-04 17:13:28.432240: Epoch 710\n",
      "2024-01-04 17:13:28.437302: Current learning rate: 0.00328\n",
      "2024-01-04 17:15:34.258064: train_loss -0.919\n",
      "2024-01-04 17:15:34.265065: val_loss -0.8094\n",
      "2024-01-04 17:15:34.270065: Pseudo dice [0.9189, 0.9281, 0.9378]\n",
      "2024-01-04 17:15:34.275063: Epoch time: 125.83 s\n",
      "2024-01-04 17:15:35.442897: \n",
      "2024-01-04 17:15:35.448859: Epoch 711\n",
      "2024-01-04 17:15:35.453823: Current learning rate: 0.00327\n",
      "2024-01-04 17:17:40.938242: train_loss -0.9195\n",
      "2024-01-04 17:17:40.944242: val_loss -0.7999\n",
      "2024-01-04 17:17:40.949242: Pseudo dice [0.9242, 0.9268, 0.9367]\n",
      "2024-01-04 17:17:40.954242: Epoch time: 125.5 s\n",
      "2024-01-04 17:17:42.207658: \n",
      "2024-01-04 17:17:42.214188: Epoch 712\n",
      "2024-01-04 17:17:42.222188: Current learning rate: 0.00326\n",
      "2024-01-04 17:19:48.045783: train_loss -0.9189\n",
      "2024-01-04 17:19:48.052778: val_loss -0.7884\n",
      "2024-01-04 17:19:48.057778: Pseudo dice [0.923, 0.9283, 0.9363]\n",
      "2024-01-04 17:19:48.063291: Epoch time: 125.84 s\n",
      "2024-01-04 17:19:49.366780: \n",
      "2024-01-04 17:19:49.372755: Epoch 713\n",
      "2024-01-04 17:19:49.376796: Current learning rate: 0.00325\n",
      "2024-01-04 17:21:54.997982: train_loss -0.9223\n",
      "2024-01-04 17:21:55.005972: val_loss -0.8011\n",
      "2024-01-04 17:21:55.012977: Pseudo dice [0.9199, 0.9295, 0.9385]\n",
      "2024-01-04 17:21:55.018972: Epoch time: 125.63 s\n",
      "2024-01-04 17:21:56.072831: \n",
      "2024-01-04 17:21:56.078934: Epoch 714\n",
      "2024-01-04 17:21:56.083882: Current learning rate: 0.00324\n",
      "2024-01-04 17:24:01.737849: train_loss -0.9196\n",
      "2024-01-04 17:24:01.744879: val_loss -0.7928\n",
      "2024-01-04 17:24:01.749880: Pseudo dice [0.9198, 0.928, 0.9354]\n",
      "2024-01-04 17:24:01.755879: Epoch time: 125.67 s\n",
      "2024-01-04 17:24:03.032321: \n",
      "2024-01-04 17:24:03.039876: Epoch 715\n",
      "2024-01-04 17:24:03.046332: Current learning rate: 0.00323\n",
      "2024-01-04 17:26:08.770328: train_loss -0.9207\n",
      "2024-01-04 17:26:08.777330: val_loss -0.794\n",
      "2024-01-04 17:26:08.782330: Pseudo dice [0.9226, 0.926, 0.9346]\n",
      "2024-01-04 17:26:08.789270: Epoch time: 125.74 s\n",
      "2024-01-04 17:26:09.991877: \n",
      "2024-01-04 17:26:10.004574: Epoch 716\n",
      "2024-01-04 17:26:10.011372: Current learning rate: 0.00322\n",
      "2024-01-04 17:28:15.551785: train_loss -0.9215\n",
      "2024-01-04 17:28:15.559795: val_loss -0.8099\n",
      "2024-01-04 17:28:15.565786: Pseudo dice [0.9229, 0.9279, 0.9378]\n",
      "2024-01-04 17:28:15.570794: Epoch time: 125.56 s\n",
      "2024-01-04 17:28:16.792687: \n",
      "2024-01-04 17:28:16.797794: Epoch 717\n",
      "2024-01-04 17:28:16.803984: Current learning rate: 0.00321\n",
      "2024-01-04 17:30:22.368549: train_loss -0.9223\n",
      "2024-01-04 17:30:22.377550: val_loss -0.8034\n",
      "2024-01-04 17:30:22.385546: Pseudo dice [0.9221, 0.9288, 0.9364]\n",
      "2024-01-04 17:30:22.390632: Epoch time: 125.58 s\n",
      "2024-01-04 17:30:23.648207: \n",
      "2024-01-04 17:30:23.655183: Epoch 718\n",
      "2024-01-04 17:30:23.666134: Current learning rate: 0.0032\n",
      "2024-01-04 17:32:29.544155: train_loss -0.919\n",
      "2024-01-04 17:32:29.551155: val_loss -0.7975\n",
      "2024-01-04 17:32:29.556155: Pseudo dice [0.9217, 0.9272, 0.9353]\n",
      "2024-01-04 17:32:29.560155: Epoch time: 125.9 s\n",
      "2024-01-04 17:32:30.794064: \n",
      "2024-01-04 17:32:30.801133: Epoch 719\n",
      "2024-01-04 17:32:30.806133: Current learning rate: 0.00319\n",
      "2024-01-04 17:34:36.830233: train_loss -0.9192\n",
      "2024-01-04 17:34:36.841226: val_loss -0.8012\n",
      "2024-01-04 17:34:36.851727: Pseudo dice [0.9208, 0.9267, 0.9352]\n",
      "2024-01-04 17:34:36.857727: Epoch time: 126.04 s\n",
      "2024-01-04 17:34:38.174315: \n",
      "2024-01-04 17:34:38.184529: Epoch 720\n",
      "2024-01-04 17:34:38.191587: Current learning rate: 0.00318\n",
      "2024-01-04 17:36:43.924800: train_loss -0.9219\n",
      "2024-01-04 17:36:43.932311: val_loss -0.7952\n",
      "2024-01-04 17:36:43.941310: Pseudo dice [0.9254, 0.9276, 0.9361]\n",
      "2024-01-04 17:36:43.948309: Epoch time: 125.75 s\n",
      "2024-01-04 17:36:45.381432: \n",
      "2024-01-04 17:36:45.387506: Epoch 721\n",
      "2024-01-04 17:36:45.392542: Current learning rate: 0.00317\n",
      "2024-01-04 17:38:50.884921: train_loss -0.9208\n",
      "2024-01-04 17:38:50.894915: val_loss -0.8039\n",
      "2024-01-04 17:38:50.904925: Pseudo dice [0.9219, 0.9281, 0.9378]\n",
      "2024-01-04 17:38:50.914937: Epoch time: 125.5 s\n",
      "2024-01-04 17:38:52.246874: \n",
      "2024-01-04 17:38:52.252874: Epoch 722\n",
      "2024-01-04 17:38:52.257865: Current learning rate: 0.00316\n",
      "2024-01-04 17:40:57.732763: train_loss -0.9227\n",
      "2024-01-04 17:40:57.739764: val_loss -0.8062\n",
      "2024-01-04 17:40:57.744763: Pseudo dice [0.9198, 0.929, 0.9369]\n",
      "2024-01-04 17:40:57.750767: Epoch time: 125.49 s\n",
      "2024-01-04 17:40:58.954644: \n",
      "2024-01-04 17:40:58.960710: Epoch 723\n",
      "2024-01-04 17:40:58.964770: Current learning rate: 0.00315\n",
      "2024-01-04 17:43:04.422729: train_loss -0.9243\n",
      "2024-01-04 17:43:04.429730: val_loss -0.7962\n",
      "2024-01-04 17:43:04.436235: Pseudo dice [0.9242, 0.9284, 0.9376]\n",
      "2024-01-04 17:43:04.442240: Epoch time: 125.47 s\n",
      "2024-01-04 17:43:05.700076: \n",
      "2024-01-04 17:43:05.705079: Epoch 724\n",
      "2024-01-04 17:43:05.710083: Current learning rate: 0.00314\n",
      "2024-01-04 17:45:11.322319: train_loss -0.9221\n",
      "2024-01-04 17:45:11.329828: val_loss -0.7992\n",
      "2024-01-04 17:45:11.334828: Pseudo dice [0.9229, 0.9275, 0.9367]\n",
      "2024-01-04 17:45:11.341838: Epoch time: 125.62 s\n",
      "2024-01-04 17:45:12.530315: \n",
      "2024-01-04 17:45:12.538315: Epoch 725\n",
      "2024-01-04 17:45:12.543307: Current learning rate: 0.00313\n",
      "2024-01-04 17:47:18.321350: train_loss -0.9204\n",
      "2024-01-04 17:47:18.329350: val_loss -0.7977\n",
      "2024-01-04 17:47:18.340351: Pseudo dice [0.9213, 0.9264, 0.9361]\n",
      "2024-01-04 17:47:18.345461: Epoch time: 125.79 s\n",
      "2024-01-04 17:47:19.464476: \n",
      "2024-01-04 17:47:19.471555: Epoch 726\n",
      "2024-01-04 17:47:19.477583: Current learning rate: 0.00312\n",
      "2024-01-04 17:49:25.189884: train_loss -0.9204\n",
      "2024-01-04 17:49:25.198886: val_loss -0.8088\n",
      "2024-01-04 17:49:25.207887: Pseudo dice [0.9217, 0.9283, 0.9355]\n",
      "2024-01-04 17:49:25.214886: Epoch time: 125.73 s\n",
      "2024-01-04 17:49:26.612820: \n",
      "2024-01-04 17:49:26.623180: Epoch 727\n",
      "2024-01-04 17:49:26.632157: Current learning rate: 0.00311\n",
      "2024-01-04 17:51:32.337606: train_loss -0.9187\n",
      "2024-01-04 17:51:32.346609: val_loss -0.802\n",
      "2024-01-04 17:51:32.354607: Pseudo dice [0.9216, 0.9276, 0.9368]\n",
      "2024-01-04 17:51:32.360608: Epoch time: 125.73 s\n",
      "2024-01-04 17:51:33.847543: \n",
      "2024-01-04 17:51:33.852542: Epoch 728\n",
      "2024-01-04 17:51:33.857557: Current learning rate: 0.0031\n",
      "2024-01-04 17:53:39.435193: train_loss -0.9221\n",
      "2024-01-04 17:53:39.444184: val_loss -0.7922\n",
      "2024-01-04 17:53:39.461814: Pseudo dice [0.9199, 0.9288, 0.9374]\n",
      "2024-01-04 17:53:39.469321: Epoch time: 125.59 s\n",
      "2024-01-04 17:53:40.669003: \n",
      "2024-01-04 17:53:40.675002: Epoch 729\n",
      "2024-01-04 17:53:40.680013: Current learning rate: 0.00309\n",
      "2024-01-04 17:55:46.098838: train_loss -0.9229\n",
      "2024-01-04 17:55:46.105830: val_loss -0.8033\n",
      "2024-01-04 17:55:46.112838: Pseudo dice [0.9221, 0.927, 0.9355]\n",
      "2024-01-04 17:55:46.117929: Epoch time: 125.43 s\n",
      "2024-01-04 17:55:47.272127: \n",
      "2024-01-04 17:55:47.283124: Epoch 730\n",
      "2024-01-04 17:55:47.288112: Current learning rate: 0.00308\n",
      "2024-01-04 17:57:52.872732: train_loss -0.9193\n",
      "2024-01-04 17:57:52.879733: val_loss -0.7989\n",
      "2024-01-04 17:57:52.884731: Pseudo dice [0.9206, 0.9292, 0.9377]\n",
      "2024-01-04 17:57:52.890736: Epoch time: 125.6 s\n",
      "2024-01-04 17:57:54.139142: \n",
      "2024-01-04 17:57:54.145201: Epoch 731\n",
      "2024-01-04 17:57:54.150363: Current learning rate: 0.00307\n",
      "2024-01-04 17:59:59.946004: train_loss -0.9197\n",
      "2024-01-04 17:59:59.954030: val_loss -0.8027\n",
      "2024-01-04 17:59:59.963035: Pseudo dice [0.9216, 0.9311, 0.9396]\n",
      "2024-01-04 17:59:59.973563: Epoch time: 125.81 s\n",
      "2024-01-04 18:00:01.214303: \n",
      "2024-01-04 18:00:01.220308: Epoch 732\n",
      "2024-01-04 18:00:01.225296: Current learning rate: 0.00306\n",
      "2024-01-04 18:02:06.972954: train_loss -0.9239\n",
      "2024-01-04 18:02:06.981954: val_loss -0.8019\n",
      "2024-01-04 18:02:06.987953: Pseudo dice [0.9174, 0.9293, 0.9377]\n",
      "2024-01-04 18:02:06.993957: Epoch time: 125.76 s\n",
      "2024-01-04 18:02:08.231924: \n",
      "2024-01-04 18:02:08.238600: Epoch 733\n",
      "2024-01-04 18:02:08.243566: Current learning rate: 0.00305\n",
      "2024-01-04 18:04:14.236593: train_loss -0.9196\n",
      "2024-01-04 18:04:14.243594: val_loss -0.7961\n",
      "2024-01-04 18:04:14.250594: Pseudo dice [0.9247, 0.9233, 0.9337]\n",
      "2024-01-04 18:04:14.256623: Epoch time: 126.01 s\n",
      "2024-01-04 18:04:15.445809: \n",
      "2024-01-04 18:04:15.452996: Epoch 734\n",
      "2024-01-04 18:04:15.459486: Current learning rate: 0.00304\n",
      "2024-01-04 18:06:21.170951: train_loss -0.9196\n",
      "2024-01-04 18:06:21.179943: val_loss -0.8033\n",
      "2024-01-04 18:06:21.186996: Pseudo dice [0.9188, 0.928, 0.936]\n",
      "2024-01-04 18:06:21.192940: Epoch time: 125.73 s\n",
      "2024-01-04 18:06:22.442640: \n",
      "2024-01-04 18:06:22.449160: Epoch 735\n",
      "2024-01-04 18:06:22.453176: Current learning rate: 0.00303\n",
      "2024-01-04 18:08:27.935088: train_loss -0.9231\n",
      "2024-01-04 18:08:27.945083: val_loss -0.8019\n",
      "2024-01-04 18:08:27.953084: Pseudo dice [0.9181, 0.9268, 0.9366]\n",
      "2024-01-04 18:08:27.961084: Epoch time: 125.49 s\n",
      "2024-01-04 18:08:29.263733: \n",
      "2024-01-04 18:08:29.270987: Epoch 736\n",
      "2024-01-04 18:08:29.275989: Current learning rate: 0.00302\n",
      "2024-01-04 18:10:34.756713: train_loss -0.9235\n",
      "2024-01-04 18:10:34.767715: val_loss -0.8002\n",
      "2024-01-04 18:10:34.777410: Pseudo dice [0.9207, 0.9287, 0.9369]\n",
      "2024-01-04 18:10:34.782412: Epoch time: 125.49 s\n",
      "2024-01-04 18:10:35.950857: \n",
      "2024-01-04 18:10:35.957372: Epoch 737\n",
      "2024-01-04 18:10:35.962372: Current learning rate: 0.00301\n",
      "2024-01-04 18:12:41.705915: train_loss -0.9201\n",
      "2024-01-04 18:12:41.712919: val_loss -0.8066\n",
      "2024-01-04 18:12:41.717919: Pseudo dice [0.918, 0.9301, 0.9392]\n",
      "2024-01-04 18:12:41.725921: Epoch time: 125.76 s\n",
      "2024-01-04 18:12:42.927546: \n",
      "2024-01-04 18:12:42.936289: Epoch 738\n",
      "2024-01-04 18:12:42.942096: Current learning rate: 0.003\n",
      "2024-01-04 18:14:48.480717: train_loss -0.9219\n",
      "2024-01-04 18:14:48.487726: val_loss -0.8117\n",
      "2024-01-04 18:14:48.496265: Pseudo dice [0.9215, 0.9294, 0.9375]\n",
      "2024-01-04 18:14:48.504268: Epoch time: 125.55 s\n",
      "2024-01-04 18:14:49.681408: \n",
      "2024-01-04 18:14:49.690872: Epoch 739\n",
      "2024-01-04 18:14:49.695402: Current learning rate: 0.00299\n",
      "2024-01-04 18:16:55.361627: train_loss -0.9224\n",
      "2024-01-04 18:16:55.371628: val_loss -0.7952\n",
      "2024-01-04 18:16:55.379631: Pseudo dice [0.9217, 0.9263, 0.9344]\n",
      "2024-01-04 18:16:55.385629: Epoch time: 125.68 s\n",
      "2024-01-04 18:16:56.793423: \n",
      "2024-01-04 18:16:56.800481: Epoch 740\n",
      "2024-01-04 18:16:56.806076: Current learning rate: 0.00297\n",
      "2024-01-04 18:19:02.546472: train_loss -0.9202\n",
      "2024-01-04 18:19:02.554474: val_loss -0.7979\n",
      "2024-01-04 18:19:02.561474: Pseudo dice [0.9189, 0.9258, 0.9348]\n",
      "2024-01-04 18:19:02.567474: Epoch time: 125.75 s\n",
      "2024-01-04 18:19:03.777770: \n",
      "2024-01-04 18:19:03.785973: Epoch 741\n",
      "2024-01-04 18:19:03.796076: Current learning rate: 0.00296\n",
      "2024-01-04 18:21:09.710494: train_loss -0.9174\n",
      "2024-01-04 18:21:09.723499: val_loss -0.7934\n",
      "2024-01-04 18:21:09.731494: Pseudo dice [0.9221, 0.9263, 0.9351]\n",
      "2024-01-04 18:21:09.740494: Epoch time: 125.93 s\n",
      "2024-01-04 18:21:11.082997: \n",
      "2024-01-04 18:21:11.095994: Epoch 742\n",
      "2024-01-04 18:21:11.103037: Current learning rate: 0.00295\n",
      "2024-01-04 18:23:16.758047: train_loss -0.9219\n",
      "2024-01-04 18:23:16.768569: val_loss -0.8017\n",
      "2024-01-04 18:23:16.776556: Pseudo dice [0.9225, 0.9281, 0.9371]\n",
      "2024-01-04 18:23:16.784565: Epoch time: 125.68 s\n",
      "2024-01-04 18:23:18.382883: \n",
      "2024-01-04 18:23:18.391913: Epoch 743\n",
      "2024-01-04 18:23:18.396949: Current learning rate: 0.00294\n",
      "2024-01-04 18:25:24.236891: train_loss -0.9198\n",
      "2024-01-04 18:25:24.243882: val_loss -0.804\n",
      "2024-01-04 18:25:24.249882: Pseudo dice [0.9212, 0.9293, 0.9364]\n",
      "2024-01-04 18:25:24.254886: Epoch time: 125.85 s\n",
      "2024-01-04 18:25:25.445333: \n",
      "2024-01-04 18:25:25.457336: Epoch 744\n",
      "2024-01-04 18:25:25.462338: Current learning rate: 0.00293\n",
      "2024-01-04 18:27:31.050694: train_loss -0.9232\n",
      "2024-01-04 18:27:31.060697: val_loss -0.8132\n",
      "2024-01-04 18:27:31.070693: Pseudo dice [0.9183, 0.9291, 0.9372]\n",
      "2024-01-04 18:27:31.078694: Epoch time: 125.61 s\n",
      "2024-01-04 18:27:32.462976: \n",
      "2024-01-04 18:27:32.470964: Epoch 745\n",
      "2024-01-04 18:27:32.474978: Current learning rate: 0.00292\n",
      "2024-01-04 18:29:38.181878: train_loss -0.9222\n",
      "2024-01-04 18:29:38.189959: val_loss -0.8008\n",
      "2024-01-04 18:29:38.195881: Pseudo dice [0.9227, 0.9275, 0.9354]\n",
      "2024-01-04 18:29:38.201882: Epoch time: 125.72 s\n",
      "2024-01-04 18:29:39.298690: \n",
      "2024-01-04 18:29:39.305616: Epoch 746\n",
      "2024-01-04 18:29:39.309625: Current learning rate: 0.00291\n",
      "2024-01-04 18:31:44.990670: train_loss -0.9213\n",
      "2024-01-04 18:31:44.997658: val_loss -0.8021\n",
      "2024-01-04 18:31:45.002658: Pseudo dice [0.9222, 0.928, 0.9357]\n",
      "2024-01-04 18:31:45.007658: Epoch time: 125.69 s\n",
      "2024-01-04 18:31:46.236553: \n",
      "2024-01-04 18:31:46.243057: Epoch 747\n",
      "2024-01-04 18:31:46.250134: Current learning rate: 0.0029\n",
      "2024-01-04 18:33:51.733786: train_loss -0.9249\n",
      "2024-01-04 18:33:51.741786: val_loss -0.8004\n",
      "2024-01-04 18:33:51.748786: Pseudo dice [0.9253, 0.9293, 0.9378]\n",
      "2024-01-04 18:33:51.755785: Epoch time: 125.5 s\n",
      "2024-01-04 18:33:52.848880: \n",
      "2024-01-04 18:33:52.855880: Epoch 748\n",
      "2024-01-04 18:33:52.860942: Current learning rate: 0.00289\n",
      "2024-01-04 18:35:58.615954: train_loss -0.9225\n",
      "2024-01-04 18:35:58.623947: val_loss -0.8019\n",
      "2024-01-04 18:35:58.630950: Pseudo dice [0.9195, 0.9276, 0.9369]\n",
      "2024-01-04 18:35:58.635945: Epoch time: 125.77 s\n",
      "2024-01-04 18:35:59.854461: \n",
      "2024-01-04 18:35:59.867738: Epoch 749\n",
      "2024-01-04 18:35:59.872565: Current learning rate: 0.00288\n",
      "2024-01-04 18:38:05.460095: train_loss -0.9203\n",
      "2024-01-04 18:38:05.467092: val_loss -0.8029\n",
      "2024-01-04 18:38:05.473092: Pseudo dice [0.9217, 0.9287, 0.9371]\n",
      "2024-01-04 18:38:05.480102: Epoch time: 125.61 s\n",
      "2024-01-04 18:38:06.903887: \n",
      "2024-01-04 18:38:06.909888: Epoch 750\n",
      "2024-01-04 18:38:06.915307: Current learning rate: 0.00287\n",
      "2024-01-04 18:40:12.835787: train_loss -0.923\n",
      "2024-01-04 18:40:12.845795: val_loss -0.8003\n",
      "2024-01-04 18:40:12.853790: Pseudo dice [0.9207, 0.9291, 0.9377]\n",
      "2024-01-04 18:40:12.860787: Epoch time: 125.93 s\n",
      "2024-01-04 18:40:14.364793: \n",
      "2024-01-04 18:40:14.373778: Epoch 751\n",
      "2024-01-04 18:40:14.381779: Current learning rate: 0.00286\n",
      "2024-01-04 18:42:20.038718: train_loss -0.9218\n",
      "2024-01-04 18:42:20.046718: val_loss -0.79\n",
      "2024-01-04 18:42:20.055717: Pseudo dice [0.9232, 0.9276, 0.9356]\n",
      "2024-01-04 18:42:20.061727: Epoch time: 125.67 s\n",
      "2024-01-04 18:42:21.287440: \n",
      "2024-01-04 18:42:21.296760: Epoch 752\n",
      "2024-01-04 18:42:21.305824: Current learning rate: 0.00285\n",
      "2024-01-04 18:44:27.329772: train_loss -0.9241\n",
      "2024-01-04 18:44:27.339571: val_loss -0.7956\n",
      "2024-01-04 18:44:27.346574: Pseudo dice [0.9222, 0.9288, 0.937]\n",
      "2024-01-04 18:44:27.353573: Epoch time: 126.04 s\n",
      "2024-01-04 18:44:28.563769: \n",
      "2024-01-04 18:44:28.573915: Epoch 753\n",
      "2024-01-04 18:44:28.578986: Current learning rate: 0.00284\n",
      "2024-01-04 18:46:34.247498: train_loss -0.9214\n",
      "2024-01-04 18:46:34.253497: val_loss -0.7989\n",
      "2024-01-04 18:46:34.258497: Pseudo dice [0.9232, 0.9302, 0.9401]\n",
      "2024-01-04 18:46:34.265497: Epoch time: 125.68 s\n",
      "2024-01-04 18:46:35.442281: \n",
      "2024-01-04 18:46:35.448608: Epoch 754\n",
      "2024-01-04 18:46:35.452625: Current learning rate: 0.00283\n",
      "2024-01-04 18:48:41.278552: train_loss -0.9215\n",
      "2024-01-04 18:48:41.285552: val_loss -0.7943\n",
      "2024-01-04 18:48:41.292552: Pseudo dice [0.9208, 0.9269, 0.9356]\n",
      "2024-01-04 18:48:41.298552: Epoch time: 125.84 s\n",
      "2024-01-04 18:48:42.577892: \n",
      "2024-01-04 18:48:42.583345: Epoch 755\n",
      "2024-01-04 18:48:42.588341: Current learning rate: 0.00282\n",
      "2024-01-04 18:50:48.194153: train_loss -0.9222\n",
      "2024-01-04 18:50:48.201155: val_loss -0.7934\n",
      "2024-01-04 18:50:48.206152: Pseudo dice [0.9226, 0.9275, 0.9354]\n",
      "2024-01-04 18:50:48.211154: Epoch time: 125.62 s\n",
      "2024-01-04 18:50:49.417822: \n",
      "2024-01-04 18:50:49.428002: Epoch 756\n",
      "2024-01-04 18:50:49.440953: Current learning rate: 0.00281\n",
      "2024-01-04 18:52:54.889378: train_loss -0.9249\n",
      "2024-01-04 18:52:54.895381: val_loss -0.8044\n",
      "2024-01-04 18:52:54.903389: Pseudo dice [0.9199, 0.9277, 0.9353]\n",
      "2024-01-04 18:52:54.910380: Epoch time: 125.47 s\n",
      "2024-01-04 18:52:56.126327: \n",
      "2024-01-04 18:52:56.133471: Epoch 757\n",
      "2024-01-04 18:52:56.138552: Current learning rate: 0.0028\n",
      "2024-01-04 18:55:01.842936: train_loss -0.922\n",
      "2024-01-04 18:55:01.849939: val_loss -0.7944\n",
      "2024-01-04 18:55:01.858938: Pseudo dice [0.9215, 0.9274, 0.9369]\n",
      "2024-01-04 18:55:01.864937: Epoch time: 125.72 s\n",
      "2024-01-04 18:55:03.165863: \n",
      "2024-01-04 18:55:03.172333: Epoch 758\n",
      "2024-01-04 18:55:03.178337: Current learning rate: 0.00279\n",
      "2024-01-04 18:57:08.755858: train_loss -0.9239\n",
      "2024-01-04 18:57:08.764870: val_loss -0.8016\n",
      "2024-01-04 18:57:08.769867: Pseudo dice [0.9224, 0.9275, 0.936]\n",
      "2024-01-04 18:57:08.774380: Epoch time: 125.59 s\n",
      "2024-01-04 18:57:10.154172: \n",
      "2024-01-04 18:57:10.160117: Epoch 759\n",
      "2024-01-04 18:57:10.164125: Current learning rate: 0.00278\n",
      "2024-01-04 18:59:15.927120: train_loss -0.9216\n",
      "2024-01-04 18:59:15.935120: val_loss -0.7985\n",
      "2024-01-04 18:59:15.943124: Pseudo dice [0.925, 0.9281, 0.9365]\n",
      "2024-01-04 18:59:15.948127: Epoch time: 125.77 s\n",
      "2024-01-04 18:59:17.249318: \n",
      "2024-01-04 18:59:17.255317: Epoch 760\n",
      "2024-01-04 18:59:17.260847: Current learning rate: 0.00277\n",
      "2024-01-04 19:01:22.934491: train_loss -0.9218\n",
      "2024-01-04 19:01:22.945492: val_loss -0.7941\n",
      "2024-01-04 19:01:22.953009: Pseudo dice [0.918, 0.9292, 0.9381]\n",
      "2024-01-04 19:01:22.960010: Epoch time: 125.69 s\n",
      "2024-01-04 19:01:24.237975: \n",
      "2024-01-04 19:01:24.244211: Epoch 761\n",
      "2024-01-04 19:01:24.249287: Current learning rate: 0.00276\n",
      "2024-01-04 19:03:29.839194: train_loss -0.9225\n",
      "2024-01-04 19:03:29.848207: val_loss -0.7978\n",
      "2024-01-04 19:03:29.856203: Pseudo dice [0.922, 0.9275, 0.9369]\n",
      "2024-01-04 19:03:29.861203: Epoch time: 125.6 s\n",
      "2024-01-04 19:03:31.043044: \n",
      "2024-01-04 19:03:31.049220: Epoch 762\n",
      "2024-01-04 19:03:31.054282: Current learning rate: 0.00275\n",
      "2024-01-04 19:05:36.748972: train_loss -0.924\n",
      "2024-01-04 19:05:36.758490: val_loss -0.813\n",
      "2024-01-04 19:05:36.767489: Pseudo dice [0.9202, 0.9297, 0.9375]\n",
      "2024-01-04 19:05:36.774489: Epoch time: 125.71 s\n",
      "2024-01-04 19:05:37.941727: \n",
      "2024-01-04 19:05:37.947760: Epoch 763\n",
      "2024-01-04 19:05:37.952819: Current learning rate: 0.00274\n",
      "2024-01-04 19:07:43.766356: train_loss -0.9188\n",
      "2024-01-04 19:07:43.777357: val_loss -0.7933\n",
      "2024-01-04 19:07:43.783354: Pseudo dice [0.9222, 0.9269, 0.936]\n",
      "2024-01-04 19:07:43.788356: Epoch time: 125.83 s\n",
      "2024-01-04 19:07:44.949874: \n",
      "2024-01-04 19:07:44.960332: Epoch 764\n",
      "2024-01-04 19:07:44.966325: Current learning rate: 0.00273\n",
      "2024-01-04 19:09:50.648406: train_loss -0.9244\n",
      "2024-01-04 19:09:50.660407: val_loss -0.791\n",
      "2024-01-04 19:09:50.669407: Pseudo dice [0.9224, 0.9272, 0.9352]\n",
      "2024-01-04 19:09:50.675407: Epoch time: 125.7 s\n",
      "2024-01-04 19:09:51.913500: \n",
      "2024-01-04 19:09:51.919507: Epoch 765\n",
      "2024-01-04 19:09:51.923515: Current learning rate: 0.00272\n",
      "2024-01-04 19:11:57.640292: train_loss -0.9197\n",
      "2024-01-04 19:11:57.648295: val_loss -0.7944\n",
      "2024-01-04 19:11:57.654307: Pseudo dice [0.9192, 0.9267, 0.9356]\n",
      "2024-01-04 19:11:57.659291: Epoch time: 125.73 s\n",
      "2024-01-04 19:11:59.030997: \n",
      "2024-01-04 19:11:59.039110: Epoch 766\n",
      "2024-01-04 19:11:59.044152: Current learning rate: 0.00271\n",
      "2024-01-04 19:14:05.226680: train_loss -0.9206\n",
      "2024-01-04 19:14:05.235684: val_loss -0.7886\n",
      "2024-01-04 19:14:05.241688: Pseudo dice [0.9215, 0.9269, 0.9371]\n",
      "2024-01-04 19:14:05.248680: Epoch time: 126.2 s\n",
      "2024-01-04 19:14:06.621665: \n",
      "2024-01-04 19:14:06.629622: Epoch 767\n",
      "2024-01-04 19:14:06.634672: Current learning rate: 0.0027\n",
      "2024-01-04 19:16:12.554557: train_loss -0.9232\n",
      "2024-01-04 19:16:12.563547: val_loss -0.7943\n",
      "2024-01-04 19:16:12.569547: Pseudo dice [0.9224, 0.9278, 0.9377]\n",
      "2024-01-04 19:16:12.576547: Epoch time: 125.93 s\n",
      "2024-01-04 19:16:13.729326: \n",
      "2024-01-04 19:16:13.737385: Epoch 768\n",
      "2024-01-04 19:16:13.745441: Current learning rate: 0.00268\n",
      "2024-01-04 19:18:19.511436: train_loss -0.9229\n",
      "2024-01-04 19:18:19.522452: val_loss -0.7818\n",
      "2024-01-04 19:18:19.531448: Pseudo dice [0.9201, 0.9274, 0.9354]\n",
      "2024-01-04 19:18:19.536439: Epoch time: 125.78 s\n",
      "2024-01-04 19:18:20.751042: \n",
      "2024-01-04 19:18:20.759441: Epoch 769\n",
      "2024-01-04 19:18:20.764518: Current learning rate: 0.00267\n",
      "2024-01-04 19:20:26.424209: train_loss -0.9218\n",
      "2024-01-04 19:20:26.435209: val_loss -0.7978\n",
      "2024-01-04 19:20:26.443211: Pseudo dice [0.9224, 0.9239, 0.9334]\n",
      "2024-01-04 19:20:26.449215: Epoch time: 125.67 s\n",
      "2024-01-04 19:20:27.666653: \n",
      "2024-01-04 19:20:27.675863: Epoch 770\n",
      "2024-01-04 19:20:27.680966: Current learning rate: 0.00266\n",
      "2024-01-04 19:22:33.359848: train_loss -0.9214\n",
      "2024-01-04 19:22:33.367847: val_loss -0.7922\n",
      "2024-01-04 19:22:33.374352: Pseudo dice [0.9224, 0.9272, 0.9357]\n",
      "2024-01-04 19:22:33.381426: Epoch time: 125.69 s\n",
      "2024-01-04 19:22:34.568146: \n",
      "2024-01-04 19:22:34.582302: Epoch 771\n",
      "2024-01-04 19:22:34.590311: Current learning rate: 0.00265\n",
      "2024-01-04 19:24:40.089930: train_loss -0.926\n",
      "2024-01-04 19:24:40.097929: val_loss -0.7895\n",
      "2024-01-04 19:24:40.103927: Pseudo dice [0.9195, 0.927, 0.9369]\n",
      "2024-01-04 19:24:40.108926: Epoch time: 125.52 s\n",
      "2024-01-04 19:24:41.372914: \n",
      "2024-01-04 19:24:41.379427: Epoch 772\n",
      "2024-01-04 19:24:41.384985: Current learning rate: 0.00264\n",
      "2024-01-04 19:26:47.167658: train_loss -0.9226\n",
      "2024-01-04 19:26:47.175661: val_loss -0.798\n",
      "2024-01-04 19:26:47.181655: Pseudo dice [0.9201, 0.9288, 0.9374]\n",
      "2024-01-04 19:26:47.186657: Epoch time: 125.8 s\n",
      "2024-01-04 19:26:48.594607: \n",
      "2024-01-04 19:26:48.601626: Epoch 773\n",
      "2024-01-04 19:26:48.613292: Current learning rate: 0.00263\n",
      "2024-01-04 19:28:54.354676: train_loss -0.9225\n",
      "2024-01-04 19:28:54.361682: val_loss -0.8077\n",
      "2024-01-04 19:28:54.371686: Pseudo dice [0.92, 0.9263, 0.9368]\n",
      "2024-01-04 19:28:54.380205: Epoch time: 125.76 s\n",
      "2024-01-04 19:28:55.669010: \n",
      "2024-01-04 19:28:55.680627: Epoch 774\n",
      "2024-01-04 19:28:55.691555: Current learning rate: 0.00262\n",
      "2024-01-04 19:31:01.457153: train_loss -0.9208\n",
      "2024-01-04 19:31:01.467144: val_loss -0.7877\n",
      "2024-01-04 19:31:01.474154: Pseudo dice [0.9185, 0.9281, 0.9368]\n",
      "2024-01-04 19:31:01.482142: Epoch time: 125.79 s\n",
      "2024-01-04 19:31:02.701863: \n",
      "2024-01-04 19:31:02.707854: Epoch 775\n",
      "2024-01-04 19:31:02.711862: Current learning rate: 0.00261\n",
      "2024-01-04 19:33:08.503220: train_loss -0.9218\n",
      "2024-01-04 19:33:08.512746: val_loss -0.8039\n",
      "2024-01-04 19:33:08.521533: Pseudo dice [0.9204, 0.9298, 0.9375]\n",
      "2024-01-04 19:33:08.526538: Epoch time: 125.8 s\n",
      "2024-01-04 19:33:09.723850: \n",
      "2024-01-04 19:33:09.729849: Epoch 776\n",
      "2024-01-04 19:33:09.734910: Current learning rate: 0.0026\n",
      "2024-01-04 19:35:15.399018: train_loss -0.9249\n",
      "2024-01-04 19:35:15.407017: val_loss -0.7981\n",
      "2024-01-04 19:35:15.412016: Pseudo dice [0.9214, 0.9255, 0.9343]\n",
      "2024-01-04 19:35:15.419019: Epoch time: 125.68 s\n",
      "2024-01-04 19:35:16.705001: \n",
      "2024-01-04 19:35:16.712984: Epoch 777\n",
      "2024-01-04 19:35:16.720000: Current learning rate: 0.00259\n",
      "2024-01-04 19:37:22.356186: train_loss -0.9216\n",
      "2024-01-04 19:37:22.365175: val_loss -0.7874\n",
      "2024-01-04 19:37:22.373173: Pseudo dice [0.9221, 0.9285, 0.9382]\n",
      "2024-01-04 19:37:22.381177: Epoch time: 125.65 s\n",
      "2024-01-04 19:37:23.548653: \n",
      "2024-01-04 19:37:23.557656: Epoch 778\n",
      "2024-01-04 19:37:23.561734: Current learning rate: 0.00258\n",
      "2024-01-04 19:39:30.240055: train_loss -0.9192\n",
      "2024-01-04 19:39:30.250056: val_loss -0.789\n",
      "2024-01-04 19:39:30.259059: Pseudo dice [0.9239, 0.9281, 0.9373]\n",
      "2024-01-04 19:39:30.267061: Epoch time: 126.69 s\n",
      "2024-01-04 19:39:31.710200: \n",
      "2024-01-04 19:39:31.716203: Epoch 779\n",
      "2024-01-04 19:39:31.721201: Current learning rate: 0.00257\n",
      "2024-01-04 19:41:37.429382: train_loss -0.9216\n",
      "2024-01-04 19:41:37.442895: val_loss -0.7944\n",
      "2024-01-04 19:41:37.450893: Pseudo dice [0.9219, 0.9263, 0.9355]\n",
      "2024-01-04 19:41:37.455894: Epoch time: 125.72 s\n",
      "2024-01-04 19:41:38.848127: \n",
      "2024-01-04 19:41:38.860249: Epoch 780\n",
      "2024-01-04 19:41:38.865262: Current learning rate: 0.00256\n",
      "2024-01-04 19:43:44.691409: train_loss -0.9219\n",
      "2024-01-04 19:43:44.699406: val_loss -0.81\n",
      "2024-01-04 19:43:44.707406: Pseudo dice [0.9209, 0.928, 0.9364]\n",
      "2024-01-04 19:43:44.714411: Epoch time: 125.84 s\n",
      "2024-01-04 19:43:45.959411: \n",
      "2024-01-04 19:43:45.965415: Epoch 781\n",
      "2024-01-04 19:43:45.971359: Current learning rate: 0.00255\n",
      "2024-01-04 19:45:51.748300: train_loss -0.922\n",
      "2024-01-04 19:45:51.757298: val_loss -0.801\n",
      "2024-01-04 19:45:51.764298: Pseudo dice [0.9177, 0.9287, 0.9383]\n",
      "2024-01-04 19:45:51.771302: Epoch time: 125.79 s\n",
      "2024-01-04 19:45:53.019293: \n",
      "2024-01-04 19:45:53.025290: Epoch 782\n",
      "2024-01-04 19:45:53.030293: Current learning rate: 0.00254\n",
      "2024-01-04 19:47:58.739513: train_loss -0.9237\n",
      "2024-01-04 19:47:58.749519: val_loss -0.799\n",
      "2024-01-04 19:47:58.756519: Pseudo dice [0.9196, 0.928, 0.9371]\n",
      "2024-01-04 19:47:58.761518: Epoch time: 125.72 s\n",
      "2024-01-04 19:47:59.924879: \n",
      "2024-01-04 19:47:59.931844: Epoch 783\n",
      "2024-01-04 19:47:59.937308: Current learning rate: 0.00253\n",
      "2024-01-04 19:50:05.747457: train_loss -0.9221\n",
      "2024-01-04 19:50:05.757457: val_loss -0.8088\n",
      "2024-01-04 19:50:05.764457: Pseudo dice [0.9215, 0.9283, 0.9388]\n",
      "2024-01-04 19:50:05.771457: Epoch time: 125.82 s\n",
      "2024-01-04 19:50:07.114608: \n",
      "2024-01-04 19:50:07.121215: Epoch 784\n",
      "2024-01-04 19:50:07.125305: Current learning rate: 0.00252\n",
      "2024-01-04 19:52:13.005233: train_loss -0.9222\n",
      "2024-01-04 19:52:13.013739: val_loss -0.7956\n",
      "2024-01-04 19:52:13.019739: Pseudo dice [0.9243, 0.928, 0.9362]\n",
      "2024-01-04 19:52:13.025741: Epoch time: 125.89 s\n",
      "2024-01-04 19:52:14.290012: \n",
      "2024-01-04 19:52:14.299907: Epoch 785\n",
      "2024-01-04 19:52:14.305126: Current learning rate: 0.00251\n",
      "2024-01-04 19:54:19.725510: train_loss -0.9243\n",
      "2024-01-04 19:54:19.739511: val_loss -0.7995\n",
      "2024-01-04 19:54:19.750521: Pseudo dice [0.9234, 0.9278, 0.9365]\n",
      "2024-01-04 19:54:19.759030: Epoch time: 125.44 s\n",
      "2024-01-04 19:54:21.175284: \n",
      "2024-01-04 19:54:21.182211: Epoch 786\n",
      "2024-01-04 19:54:21.186275: Current learning rate: 0.0025\n",
      "2024-01-04 19:56:27.043623: train_loss -0.9221\n",
      "2024-01-04 19:56:27.051624: val_loss -0.8041\n",
      "2024-01-04 19:56:27.058625: Pseudo dice [0.9203, 0.9286, 0.9382]\n",
      "2024-01-04 19:56:27.063626: Epoch time: 125.87 s\n",
      "2024-01-04 19:56:28.572992: \n",
      "2024-01-04 19:56:28.581999: Epoch 787\n",
      "2024-01-04 19:56:28.586935: Current learning rate: 0.00249\n",
      "2024-01-04 19:58:34.098895: train_loss -0.9232\n",
      "2024-01-04 19:58:34.108399: val_loss -0.7972\n",
      "2024-01-04 19:58:34.115408: Pseudo dice [0.9202, 0.9282, 0.9367]\n",
      "2024-01-04 19:58:34.122404: Epoch time: 125.53 s\n",
      "2024-01-04 19:58:35.365750: \n",
      "2024-01-04 19:58:35.371750: Epoch 788\n",
      "2024-01-04 19:58:35.379752: Current learning rate: 0.00248\n",
      "2024-01-04 20:00:40.884832: train_loss -0.9234\n",
      "2024-01-04 20:00:40.895833: val_loss -0.7908\n",
      "2024-01-04 20:00:40.905832: Pseudo dice [0.9227, 0.9276, 0.9367]\n",
      "2024-01-04 20:00:40.911013: Epoch time: 125.52 s\n",
      "2024-01-04 20:00:42.093922: \n",
      "2024-01-04 20:00:42.101895: Epoch 789\n",
      "2024-01-04 20:00:42.109890: Current learning rate: 0.00247\n",
      "2024-01-04 20:02:48.088503: train_loss -0.9212\n",
      "2024-01-04 20:02:48.096566: val_loss -0.794\n",
      "2024-01-04 20:02:48.102575: Pseudo dice [0.9195, 0.9268, 0.9343]\n",
      "2024-01-04 20:02:48.108502: Epoch time: 126.0 s\n",
      "2024-01-04 20:02:49.288747: \n",
      "2024-01-04 20:02:49.295982: Epoch 790\n",
      "2024-01-04 20:02:49.304048: Current learning rate: 0.00245\n",
      "2024-01-04 20:04:55.174216: train_loss -0.9237\n",
      "2024-01-04 20:04:55.184218: val_loss -0.8001\n",
      "2024-01-04 20:04:55.190215: Pseudo dice [0.9199, 0.9277, 0.9369]\n",
      "2024-01-04 20:04:55.199216: Epoch time: 125.89 s\n",
      "2024-01-04 20:04:56.541670: \n",
      "2024-01-04 20:04:56.547150: Epoch 791\n",
      "2024-01-04 20:04:56.553170: Current learning rate: 0.00244\n",
      "2024-01-04 20:07:02.521283: train_loss -0.9212\n",
      "2024-01-04 20:07:02.531283: val_loss -0.8005\n",
      "2024-01-04 20:07:02.538285: Pseudo dice [0.9222, 0.93, 0.9375]\n",
      "2024-01-04 20:07:02.548284: Epoch time: 125.98 s\n",
      "2024-01-04 20:07:03.849018: \n",
      "2024-01-04 20:07:03.858005: Epoch 792\n",
      "2024-01-04 20:07:03.863021: Current learning rate: 0.00243\n",
      "2024-01-04 20:09:09.581377: train_loss -0.9236\n",
      "2024-01-04 20:09:09.588383: val_loss -0.8049\n",
      "2024-01-04 20:09:09.595375: Pseudo dice [0.9216, 0.9284, 0.9384]\n",
      "2024-01-04 20:09:09.601857: Epoch time: 125.73 s\n",
      "2024-01-04 20:09:10.757230: \n",
      "2024-01-04 20:09:10.765300: Epoch 793\n",
      "2024-01-04 20:09:10.770292: Current learning rate: 0.00242\n",
      "2024-01-04 20:11:16.442153: train_loss -0.9221\n",
      "2024-01-04 20:11:16.451155: val_loss -0.7979\n",
      "2024-01-04 20:11:16.457155: Pseudo dice [0.9184, 0.9282, 0.9352]\n",
      "2024-01-04 20:11:16.462173: Epoch time: 125.69 s\n",
      "2024-01-04 20:11:17.603031: \n",
      "2024-01-04 20:11:17.617888: Epoch 794\n",
      "2024-01-04 20:11:17.622889: Current learning rate: 0.00241\n",
      "2024-01-04 20:13:23.643164: train_loss -0.9211\n",
      "2024-01-04 20:13:23.650163: val_loss -0.7826\n",
      "2024-01-04 20:13:23.658177: Pseudo dice [0.9184, 0.928, 0.9355]\n",
      "2024-01-04 20:13:23.664164: Epoch time: 126.04 s\n",
      "2024-01-04 20:13:25.072854: \n",
      "2024-01-04 20:13:25.081840: Epoch 795\n",
      "2024-01-04 20:13:25.086895: Current learning rate: 0.0024\n",
      "2024-01-04 20:15:30.760982: train_loss -0.921\n",
      "2024-01-04 20:15:30.771982: val_loss -0.8003\n",
      "2024-01-04 20:15:30.779984: Pseudo dice [0.9222, 0.9288, 0.9362]\n",
      "2024-01-04 20:15:30.784985: Epoch time: 125.69 s\n",
      "2024-01-04 20:15:31.995866: \n",
      "2024-01-04 20:15:32.001014: Epoch 796\n",
      "2024-01-04 20:15:32.009155: Current learning rate: 0.00239\n",
      "2024-01-04 20:17:37.760835: train_loss -0.9222\n",
      "2024-01-04 20:17:37.769913: val_loss -0.8034\n",
      "2024-01-04 20:17:37.775910: Pseudo dice [0.9215, 0.9274, 0.9369]\n",
      "2024-01-04 20:17:37.783917: Epoch time: 125.77 s\n",
      "2024-01-04 20:17:39.132033: \n",
      "2024-01-04 20:17:39.139952: Epoch 797\n",
      "2024-01-04 20:17:39.144958: Current learning rate: 0.00238\n",
      "2024-01-04 20:19:44.898904: train_loss -0.9242\n",
      "2024-01-04 20:19:44.906896: val_loss -0.7932\n",
      "2024-01-04 20:19:44.914441: Pseudo dice [0.9204, 0.9293, 0.9366]\n",
      "2024-01-04 20:19:44.919435: Epoch time: 125.77 s\n",
      "2024-01-04 20:19:46.065017: \n",
      "2024-01-04 20:19:46.072077: Epoch 798\n",
      "2024-01-04 20:19:46.078058: Current learning rate: 0.00237\n",
      "2024-01-04 20:21:51.862273: train_loss -0.9234\n",
      "2024-01-04 20:21:51.869274: val_loss -0.8017\n",
      "2024-01-04 20:21:51.875781: Pseudo dice [0.9212, 0.9268, 0.9367]\n",
      "2024-01-04 20:21:51.882786: Epoch time: 125.8 s\n",
      "2024-01-04 20:21:53.186687: \n",
      "2024-01-04 20:21:53.192680: Epoch 799\n",
      "2024-01-04 20:21:53.197674: Current learning rate: 0.00236\n",
      "2024-01-04 20:23:58.808318: train_loss -0.9226\n",
      "2024-01-04 20:23:58.817320: val_loss -0.7885\n",
      "2024-01-04 20:23:58.822320: Pseudo dice [0.9219, 0.9265, 0.9342]\n",
      "2024-01-04 20:23:58.827326: Epoch time: 125.62 s\n",
      "2024-01-04 20:24:00.354861: \n",
      "2024-01-04 20:24:00.362236: Epoch 800\n",
      "2024-01-04 20:24:00.367312: Current learning rate: 0.00235\n",
      "2024-01-04 20:26:05.874045: train_loss -0.9257\n",
      "2024-01-04 20:26:05.883045: val_loss -0.7939\n",
      "2024-01-04 20:26:05.889045: Pseudo dice [0.9182, 0.9263, 0.9358]\n",
      "2024-01-04 20:26:05.894045: Epoch time: 125.52 s\n",
      "2024-01-04 20:26:07.195045: \n",
      "2024-01-04 20:26:07.200991: Epoch 801\n",
      "2024-01-04 20:26:07.210983: Current learning rate: 0.00234\n",
      "2024-01-04 20:28:12.928256: train_loss -0.9224\n",
      "2024-01-04 20:28:12.935970: val_loss -0.7995\n",
      "2024-01-04 20:28:12.941969: Pseudo dice [0.9219, 0.9248, 0.9356]\n",
      "2024-01-04 20:28:12.948972: Epoch time: 125.73 s\n",
      "2024-01-04 20:28:14.553960: \n",
      "2024-01-04 20:28:14.559855: Epoch 802\n",
      "2024-01-04 20:28:14.567588: Current learning rate: 0.00233\n",
      "2024-01-04 20:30:20.598184: train_loss -0.9204\n",
      "2024-01-04 20:30:20.609180: val_loss -0.7979\n",
      "2024-01-04 20:30:20.614180: Pseudo dice [0.9224, 0.9286, 0.9365]\n",
      "2024-01-04 20:30:20.619180: Epoch time: 126.05 s\n",
      "2024-01-04 20:30:21.966888: \n",
      "2024-01-04 20:30:21.973907: Epoch 803\n",
      "2024-01-04 20:30:21.981934: Current learning rate: 0.00232\n",
      "2024-01-04 20:32:27.378430: train_loss -0.9242\n",
      "2024-01-04 20:32:27.386430: val_loss -0.7966\n",
      "2024-01-04 20:32:27.394430: Pseudo dice [0.923, 0.9269, 0.9362]\n",
      "2024-01-04 20:32:27.400431: Epoch time: 125.41 s\n",
      "2024-01-04 20:32:28.699094: \n",
      "2024-01-04 20:32:28.708427: Epoch 804\n",
      "2024-01-04 20:32:28.713574: Current learning rate: 0.00231\n",
      "2024-01-04 20:34:34.362756: train_loss -0.9219\n",
      "2024-01-04 20:34:34.371674: val_loss -0.794\n",
      "2024-01-04 20:34:34.377675: Pseudo dice [0.9177, 0.9292, 0.9374]\n",
      "2024-01-04 20:34:34.382669: Epoch time: 125.66 s\n",
      "2024-01-04 20:34:35.634568: \n",
      "2024-01-04 20:34:35.641723: Epoch 805\n",
      "2024-01-04 20:34:35.649724: Current learning rate: 0.0023\n",
      "2024-01-04 20:36:41.206354: train_loss -0.9258\n",
      "2024-01-04 20:36:41.214353: val_loss -0.8024\n",
      "2024-01-04 20:36:41.221354: Pseudo dice [0.92, 0.9274, 0.9367]\n",
      "2024-01-04 20:36:41.227355: Epoch time: 125.57 s\n",
      "2024-01-04 20:36:42.541274: \n",
      "2024-01-04 20:36:42.550341: Epoch 806\n",
      "2024-01-04 20:36:42.555346: Current learning rate: 0.00229\n",
      "2024-01-04 20:38:49.964289: train_loss -0.919\n",
      "2024-01-04 20:38:49.973297: val_loss -0.803\n",
      "2024-01-04 20:38:49.982294: Pseudo dice [0.9171, 0.9289, 0.937]\n",
      "2024-01-04 20:38:49.989294: Epoch time: 127.42 s\n",
      "2024-01-04 20:38:51.308852: \n",
      "2024-01-04 20:38:51.316642: Epoch 807\n",
      "2024-01-04 20:38:51.322708: Current learning rate: 0.00228\n",
      "2024-01-04 20:40:57.525247: train_loss -0.9229\n",
      "2024-01-04 20:40:57.534256: val_loss -0.7945\n",
      "2024-01-04 20:40:57.543765: Pseudo dice [0.9187, 0.9273, 0.9373]\n",
      "2024-01-04 20:40:57.548765: Epoch time: 126.22 s\n",
      "2024-01-04 20:40:58.813052: \n",
      "2024-01-04 20:40:58.821654: Epoch 808\n",
      "2024-01-04 20:40:58.829724: Current learning rate: 0.00226\n",
      "2024-01-04 20:43:05.260159: train_loss -0.9225\n",
      "2024-01-04 20:43:05.269159: val_loss -0.8032\n",
      "2024-01-04 20:43:05.277159: Pseudo dice [0.9229, 0.9279, 0.9376]\n",
      "2024-01-04 20:43:05.288159: Epoch time: 126.45 s\n",
      "2024-01-04 20:43:06.520955: \n",
      "2024-01-04 20:43:06.530207: Epoch 809\n",
      "2024-01-04 20:43:06.534873: Current learning rate: 0.00225\n",
      "2024-01-04 20:45:12.190474: train_loss -0.9229\n",
      "2024-01-04 20:45:12.197984: val_loss -0.8003\n",
      "2024-01-04 20:45:12.206196: Pseudo dice [0.9197, 0.9303, 0.9399]\n",
      "2024-01-04 20:45:12.215205: Epoch time: 125.67 s\n",
      "2024-01-04 20:45:13.508174: \n",
      "2024-01-04 20:45:13.515105: Epoch 810\n",
      "2024-01-04 20:45:13.520802: Current learning rate: 0.00224\n",
      "2024-01-04 20:47:19.190516: train_loss -0.9234\n",
      "2024-01-04 20:47:19.197525: val_loss -0.7931\n",
      "2024-01-04 20:47:19.205521: Pseudo dice [0.9211, 0.9269, 0.936]\n",
      "2024-01-04 20:47:19.210024: Epoch time: 125.68 s\n",
      "2024-01-04 20:47:20.402094: \n",
      "2024-01-04 20:47:20.411726: Epoch 811\n",
      "2024-01-04 20:47:20.416817: Current learning rate: 0.00223\n",
      "2024-01-04 20:49:26.069230: train_loss -0.9234\n",
      "2024-01-04 20:49:26.081866: val_loss -0.7915\n",
      "2024-01-04 20:49:26.087859: Pseudo dice [0.9206, 0.928, 0.9361]\n",
      "2024-01-04 20:49:26.092946: Epoch time: 125.67 s\n",
      "2024-01-04 20:49:27.297199: \n",
      "2024-01-04 20:49:27.305125: Epoch 812\n",
      "2024-01-04 20:49:27.309192: Current learning rate: 0.00222\n",
      "2024-01-04 20:51:32.853049: train_loss -0.9253\n",
      "2024-01-04 20:51:32.862048: val_loss -0.7923\n",
      "2024-01-04 20:51:32.869048: Pseudo dice [0.9205, 0.9267, 0.9359]\n",
      "2024-01-04 20:51:32.874048: Epoch time: 125.56 s\n",
      "2024-01-04 20:51:34.010192: \n",
      "2024-01-04 20:51:34.021208: Epoch 813\n",
      "2024-01-04 20:51:34.026727: Current learning rate: 0.00221\n",
      "2024-01-04 20:53:39.991926: train_loss -0.9228\n",
      "2024-01-04 20:53:40.001928: val_loss -0.8028\n",
      "2024-01-04 20:53:40.010433: Pseudo dice [0.9228, 0.9281, 0.9367]\n",
      "2024-01-04 20:53:40.019485: Epoch time: 125.98 s\n",
      "2024-01-04 20:53:41.273787: \n",
      "2024-01-04 20:53:41.280850: Epoch 814\n",
      "2024-01-04 20:53:41.289854: Current learning rate: 0.0022\n",
      "2024-01-04 20:55:47.347789: train_loss -0.9235\n",
      "2024-01-04 20:55:47.354790: val_loss -0.7896\n",
      "2024-01-04 20:55:47.360800: Pseudo dice [0.9212, 0.9274, 0.936]\n",
      "2024-01-04 20:55:47.366299: Epoch time: 126.08 s\n",
      "2024-01-04 20:55:48.542340: \n",
      "2024-01-04 20:55:48.548340: Epoch 815\n",
      "2024-01-04 20:55:48.554406: Current learning rate: 0.00219\n",
      "2024-01-04 20:57:55.679473: train_loss -0.9218\n",
      "2024-01-04 20:57:55.689471: val_loss -0.7864\n",
      "2024-01-04 20:57:55.695475: Pseudo dice [0.9198, 0.9273, 0.9356]\n",
      "2024-01-04 20:57:55.701471: Epoch time: 127.14 s\n",
      "2024-01-04 20:57:57.093637: \n",
      "2024-01-04 20:57:57.110614: Epoch 816\n",
      "2024-01-04 20:57:57.117699: Current learning rate: 0.00218\n",
      "2024-01-04 21:00:03.734605: train_loss -0.9229\n",
      "2024-01-04 21:00:03.744606: val_loss -0.7955\n",
      "2024-01-04 21:00:03.752605: Pseudo dice [0.9195, 0.9244, 0.9346]\n",
      "2024-01-04 21:00:03.758605: Epoch time: 126.64 s\n",
      "2024-01-04 21:00:05.211583: \n",
      "2024-01-04 21:00:05.217583: Epoch 817\n",
      "2024-01-04 21:00:05.227339: Current learning rate: 0.00217\n",
      "2024-01-04 21:02:11.023405: train_loss -0.9253\n",
      "2024-01-04 21:02:11.032405: val_loss -0.7857\n",
      "2024-01-04 21:02:11.037405: Pseudo dice [0.9213, 0.928, 0.9362]\n",
      "2024-01-04 21:02:11.042406: Epoch time: 125.81 s\n",
      "2024-01-04 21:02:12.214055: \n",
      "2024-01-04 21:02:12.219635: Epoch 818\n",
      "2024-01-04 21:02:12.224710: Current learning rate: 0.00216\n",
      "2024-01-04 21:04:17.837947: train_loss -0.9246\n",
      "2024-01-04 21:04:17.844947: val_loss -0.8029\n",
      "2024-01-04 21:04:17.849946: Pseudo dice [0.9199, 0.928, 0.937]\n",
      "2024-01-04 21:04:17.856461: Epoch time: 125.62 s\n",
      "2024-01-04 21:04:19.160850: \n",
      "2024-01-04 21:04:19.169035: Epoch 819\n",
      "2024-01-04 21:04:19.174067: Current learning rate: 0.00215\n",
      "2024-01-04 21:06:24.647393: train_loss -0.9244\n",
      "2024-01-04 21:06:24.655392: val_loss -0.7969\n",
      "2024-01-04 21:06:24.662393: Pseudo dice [0.9206, 0.9283, 0.9377]\n",
      "2024-01-04 21:06:24.667392: Epoch time: 125.49 s\n",
      "2024-01-04 21:06:25.745163: \n",
      "2024-01-04 21:06:25.752089: Epoch 820\n",
      "2024-01-04 21:06:25.757096: Current learning rate: 0.00214\n",
      "2024-01-04 21:08:31.545805: train_loss -0.9244\n",
      "2024-01-04 21:08:31.555811: val_loss -0.7931\n",
      "2024-01-04 21:08:31.562814: Pseudo dice [0.921, 0.9274, 0.9358]\n",
      "2024-01-04 21:08:31.568809: Epoch time: 125.8 s\n",
      "2024-01-04 21:08:32.897185: \n",
      "2024-01-04 21:08:32.903356: Epoch 821\n",
      "2024-01-04 21:08:32.911420: Current learning rate: 0.00213\n",
      "2024-01-04 21:10:40.057728: train_loss -0.9243\n",
      "2024-01-04 21:10:40.069733: val_loss -0.7956\n",
      "2024-01-04 21:10:40.076819: Pseudo dice [0.9226, 0.9275, 0.9355]\n",
      "2024-01-04 21:10:40.084337: Epoch time: 127.16 s\n",
      "2024-01-04 21:10:41.481178: \n",
      "2024-01-04 21:10:41.486722: Epoch 822\n",
      "2024-01-04 21:10:41.493712: Current learning rate: 0.00212\n",
      "2024-01-04 21:12:47.865234: train_loss -0.9231\n",
      "2024-01-04 21:12:47.876529: val_loss -0.8042\n",
      "2024-01-04 21:12:47.888529: Pseudo dice [0.9195, 0.928, 0.9376]\n",
      "2024-01-04 21:12:47.901529: Epoch time: 126.39 s\n",
      "2024-01-04 21:12:49.333917: \n",
      "2024-01-04 21:12:49.340919: Epoch 823\n",
      "2024-01-04 21:12:49.346917: Current learning rate: 0.0021\n",
      "2024-01-04 21:14:55.886924: train_loss -0.9229\n",
      "2024-01-04 21:14:55.898913: val_loss -0.7988\n",
      "2024-01-04 21:14:55.907914: Pseudo dice [0.9201, 0.9275, 0.9361]\n",
      "2024-01-04 21:14:55.915914: Epoch time: 126.55 s\n",
      "2024-01-04 21:14:57.217895: \n",
      "2024-01-04 21:14:57.223892: Epoch 824\n",
      "2024-01-04 21:14:57.230978: Current learning rate: 0.00209\n",
      "2024-01-04 21:17:03.515519: train_loss -0.922\n",
      "2024-01-04 21:17:03.526535: val_loss -0.8046\n",
      "2024-01-04 21:17:03.534527: Pseudo dice [0.9232, 0.9283, 0.9372]\n",
      "2024-01-04 21:17:03.540040: Epoch time: 126.3 s\n",
      "2024-01-04 21:17:04.877505: \n",
      "2024-01-04 21:17:04.884493: Epoch 825\n",
      "2024-01-04 21:17:04.890597: Current learning rate: 0.00208\n",
      "2024-01-04 21:19:10.567324: train_loss -0.9228\n",
      "2024-01-04 21:19:10.575324: val_loss -0.7992\n",
      "2024-01-04 21:19:10.582328: Pseudo dice [0.9211, 0.9291, 0.9379]\n",
      "2024-01-04 21:19:10.587328: Epoch time: 125.69 s\n",
      "2024-01-04 21:19:11.864747: \n",
      "2024-01-04 21:19:11.872751: Epoch 826\n",
      "2024-01-04 21:19:11.879747: Current learning rate: 0.00207\n",
      "2024-01-04 21:21:17.410747: train_loss -0.9243\n",
      "2024-01-04 21:21:17.420748: val_loss -0.7933\n",
      "2024-01-04 21:21:17.426746: Pseudo dice [0.9246, 0.9269, 0.9355]\n",
      "2024-01-04 21:21:17.432745: Epoch time: 125.55 s\n",
      "2024-01-04 21:21:18.606129: \n",
      "2024-01-04 21:21:18.613703: Epoch 827\n",
      "2024-01-04 21:21:18.618717: Current learning rate: 0.00206\n",
      "2024-01-04 21:23:24.525939: train_loss -0.9244\n",
      "2024-01-04 21:23:24.532444: val_loss -0.7937\n",
      "2024-01-04 21:23:24.537443: Pseudo dice [0.9231, 0.926, 0.9346]\n",
      "2024-01-04 21:23:24.543443: Epoch time: 125.92 s\n",
      "2024-01-04 21:23:25.674492: \n",
      "2024-01-04 21:23:25.685578: Epoch 828\n",
      "2024-01-04 21:23:25.690552: Current learning rate: 0.00205\n",
      "2024-01-04 21:25:31.486904: train_loss -0.9218\n",
      "2024-01-04 21:25:31.496905: val_loss -0.7979\n",
      "2024-01-04 21:25:31.501907: Pseudo dice [0.9203, 0.9243, 0.9345]\n",
      "2024-01-04 21:25:31.506904: Epoch time: 125.81 s\n",
      "2024-01-04 21:25:32.624887: \n",
      "2024-01-04 21:25:32.631433: Epoch 829\n",
      "2024-01-04 21:25:32.636451: Current learning rate: 0.00204\n",
      "2024-01-04 21:27:38.385072: train_loss -0.9223\n",
      "2024-01-04 21:27:38.395079: val_loss -0.7863\n",
      "2024-01-04 21:27:38.401085: Pseudo dice [0.9224, 0.9287, 0.938]\n",
      "2024-01-04 21:27:38.408610: Epoch time: 125.76 s\n",
      "2024-01-04 21:27:39.540067: \n",
      "2024-01-04 21:27:39.547668: Epoch 830\n",
      "2024-01-04 21:27:39.553663: Current learning rate: 0.00203\n",
      "2024-01-04 21:29:45.093679: train_loss -0.9273\n",
      "2024-01-04 21:29:45.103669: val_loss -0.7986\n",
      "2024-01-04 21:29:45.109666: Pseudo dice [0.9222, 0.9285, 0.9378]\n",
      "2024-01-04 21:29:45.114667: Epoch time: 125.55 s\n",
      "2024-01-04 21:29:46.262007: \n",
      "2024-01-04 21:29:46.270085: Epoch 831\n",
      "2024-01-04 21:29:46.274069: Current learning rate: 0.00202\n",
      "2024-01-04 21:31:52.022312: train_loss -0.9241\n",
      "2024-01-04 21:31:52.031312: val_loss -0.8058\n",
      "2024-01-04 21:31:52.039316: Pseudo dice [0.9206, 0.9283, 0.9378]\n",
      "2024-01-04 21:31:52.047317: Epoch time: 125.76 s\n",
      "2024-01-04 21:31:53.243429: \n",
      "2024-01-04 21:31:53.250423: Epoch 832\n",
      "2024-01-04 21:31:53.256051: Current learning rate: 0.00201\n",
      "2024-01-04 21:33:59.039609: train_loss -0.9219\n",
      "2024-01-04 21:33:59.049608: val_loss -0.8005\n",
      "2024-01-04 21:33:59.055608: Pseudo dice [0.9181, 0.9294, 0.938]\n",
      "2024-01-04 21:33:59.061607: Epoch time: 125.8 s\n",
      "2024-01-04 21:34:00.306460: \n",
      "2024-01-04 21:34:00.319128: Epoch 833\n",
      "2024-01-04 21:34:00.325095: Current learning rate: 0.002\n",
      "2024-01-04 21:36:05.819116: train_loss -0.9255\n",
      "2024-01-04 21:36:05.827117: val_loss -0.8105\n",
      "2024-01-04 21:36:05.833117: Pseudo dice [0.9211, 0.9284, 0.9378]\n",
      "2024-01-04 21:36:05.838117: Epoch time: 125.51 s\n",
      "2024-01-04 21:36:07.071816: \n",
      "2024-01-04 21:36:07.078884: Epoch 834\n",
      "2024-01-04 21:36:07.083817: Current learning rate: 0.00199\n",
      "2024-01-04 21:38:12.716035: train_loss -0.9243\n",
      "2024-01-04 21:38:12.724038: val_loss -0.7868\n",
      "2024-01-04 21:38:12.729038: Pseudo dice [0.9185, 0.9274, 0.9356]\n",
      "2024-01-04 21:38:12.734040: Epoch time: 125.65 s\n",
      "2024-01-04 21:38:13.915551: \n",
      "2024-01-04 21:38:13.923829: Epoch 835\n",
      "2024-01-04 21:38:13.930831: Current learning rate: 0.00198\n",
      "2024-01-04 21:40:19.374398: train_loss -0.9241\n",
      "2024-01-04 21:40:19.382402: val_loss -0.7946\n",
      "2024-01-04 21:40:19.388403: Pseudo dice [0.9236, 0.9283, 0.9377]\n",
      "2024-01-04 21:40:19.393404: Epoch time: 125.46 s\n",
      "2024-01-04 21:40:20.503397: \n",
      "2024-01-04 21:40:20.509459: Epoch 836\n",
      "2024-01-04 21:40:20.515446: Current learning rate: 0.00196\n",
      "2024-01-04 21:42:25.952135: train_loss -0.9275\n",
      "2024-01-04 21:42:25.962131: val_loss -0.7981\n",
      "2024-01-04 21:42:25.970129: Pseudo dice [0.9227, 0.929, 0.9377]\n",
      "2024-01-04 21:42:25.976128: Epoch time: 125.45 s\n",
      "2024-01-04 21:42:27.157173: \n",
      "2024-01-04 21:42:27.163234: Epoch 837\n",
      "2024-01-04 21:42:27.168393: Current learning rate: 0.00195\n",
      "2024-01-04 21:44:32.751198: train_loss -0.9259\n",
      "2024-01-04 21:44:32.761207: val_loss -0.8029\n",
      "2024-01-04 21:44:32.767205: Pseudo dice [0.9195, 0.9282, 0.9382]\n",
      "2024-01-04 21:44:32.773715: Epoch time: 125.6 s\n",
      "2024-01-04 21:44:34.136302: \n",
      "2024-01-04 21:44:34.144374: Epoch 838\n",
      "2024-01-04 21:44:34.149364: Current learning rate: 0.00194\n",
      "2024-01-04 21:46:39.533238: train_loss -0.9248\n",
      "2024-01-04 21:46:39.543237: val_loss -0.7949\n",
      "2024-01-04 21:46:39.552229: Pseudo dice [0.921, 0.9293, 0.9369]\n",
      "2024-01-04 21:46:39.558228: Epoch time: 125.4 s\n",
      "2024-01-04 21:46:40.740601: \n",
      "2024-01-04 21:46:40.747518: Epoch 839\n",
      "2024-01-04 21:46:40.752584: Current learning rate: 0.00193\n",
      "2024-01-04 21:48:46.682977: train_loss -0.9269\n",
      "2024-01-04 21:48:46.694980: val_loss -0.7993\n",
      "2024-01-04 21:48:46.701982: Pseudo dice [0.9201, 0.93, 0.9374]\n",
      "2024-01-04 21:48:46.709492: Epoch time: 125.94 s\n",
      "2024-01-04 21:48:48.094855: \n",
      "2024-01-04 21:48:48.100870: Epoch 840\n",
      "2024-01-04 21:48:48.104870: Current learning rate: 0.00192\n",
      "2024-01-04 21:50:54.075373: train_loss -0.9224\n",
      "2024-01-04 21:50:54.082363: val_loss -0.7968\n",
      "2024-01-04 21:50:54.089367: Pseudo dice [0.9201, 0.9294, 0.9386]\n",
      "2024-01-04 21:50:54.096367: Epoch time: 125.98 s\n",
      "2024-01-04 21:50:55.428960: \n",
      "2024-01-04 21:50:55.437973: Epoch 841\n",
      "2024-01-04 21:50:55.446977: Current learning rate: 0.00191\n",
      "2024-01-04 21:53:00.901267: train_loss -0.9253\n",
      "2024-01-04 21:53:00.910269: val_loss -0.7843\n",
      "2024-01-04 21:53:00.917269: Pseudo dice [0.9204, 0.9282, 0.9374]\n",
      "2024-01-04 21:53:00.923268: Epoch time: 125.48 s\n",
      "2024-01-04 21:53:02.298600: \n",
      "2024-01-04 21:53:02.307601: Epoch 842\n",
      "2024-01-04 21:53:02.312601: Current learning rate: 0.0019\n",
      "2024-01-04 21:55:08.282326: train_loss -0.9255\n",
      "2024-01-04 21:55:08.289321: val_loss -0.8028\n",
      "2024-01-04 21:55:08.294320: Pseudo dice [0.9216, 0.9282, 0.937]\n",
      "2024-01-04 21:55:08.298323: Epoch time: 125.98 s\n",
      "2024-01-04 21:55:09.381642: \n",
      "2024-01-04 21:55:09.388831: Epoch 843\n",
      "2024-01-04 21:55:09.395003: Current learning rate: 0.00189\n",
      "2024-01-04 21:57:15.522597: train_loss -0.9247\n",
      "2024-01-04 21:57:15.530602: val_loss -0.7924\n",
      "2024-01-04 21:57:15.536604: Pseudo dice [0.9216, 0.9284, 0.937]\n",
      "2024-01-04 21:57:15.541603: Epoch time: 126.14 s\n",
      "2024-01-04 21:57:17.023228: \n",
      "2024-01-04 21:57:17.032230: Epoch 844\n",
      "2024-01-04 21:57:17.038232: Current learning rate: 0.00188\n",
      "2024-01-04 21:59:22.766200: train_loss -0.9218\n",
      "2024-01-04 21:59:22.773205: val_loss -0.8009\n",
      "2024-01-04 21:59:22.779216: Pseudo dice [0.9207, 0.9274, 0.9362]\n",
      "2024-01-04 21:59:22.785209: Epoch time: 125.74 s\n",
      "2024-01-04 21:59:23.938891: \n",
      "2024-01-04 21:59:23.945159: Epoch 845\n",
      "2024-01-04 21:59:23.949167: Current learning rate: 0.00187\n",
      "2024-01-04 22:01:29.361442: train_loss -0.9248\n",
      "2024-01-04 22:01:29.368443: val_loss -0.8073\n",
      "2024-01-04 22:01:29.374440: Pseudo dice [0.921, 0.931, 0.9399]\n",
      "2024-01-04 22:01:29.380440: Epoch time: 125.42 s\n",
      "2024-01-04 22:01:30.559431: \n",
      "2024-01-04 22:01:30.567994: Epoch 846\n",
      "2024-01-04 22:01:30.573065: Current learning rate: 0.00186\n",
      "2024-01-04 22:03:36.230856: train_loss -0.9243\n",
      "2024-01-04 22:03:36.237865: val_loss -0.8011\n",
      "2024-01-04 22:03:36.243848: Pseudo dice [0.9207, 0.9281, 0.936]\n",
      "2024-01-04 22:03:36.248856: Epoch time: 125.67 s\n",
      "2024-01-04 22:03:37.405059: \n",
      "2024-01-04 22:03:37.411065: Epoch 847\n",
      "2024-01-04 22:03:37.423059: Current learning rate: 0.00185\n",
      "2024-01-04 22:05:43.036334: train_loss -0.9267\n",
      "2024-01-04 22:05:43.044336: val_loss -0.8039\n",
      "2024-01-04 22:05:43.050352: Pseudo dice [0.9221, 0.929, 0.9386]\n",
      "2024-01-04 22:05:43.057869: Epoch time: 125.63 s\n",
      "2024-01-04 22:05:44.255091: \n",
      "2024-01-04 22:05:44.261762: Epoch 848\n",
      "2024-01-04 22:05:44.267761: Current learning rate: 0.00184\n",
      "2024-01-04 22:07:50.390135: train_loss -0.922\n",
      "2024-01-04 22:07:50.397136: val_loss -0.7903\n",
      "2024-01-04 22:07:50.404144: Pseudo dice [0.9214, 0.9289, 0.9373]\n",
      "2024-01-04 22:07:50.413149: Epoch time: 126.14 s\n",
      "2024-01-04 22:07:51.764796: \n",
      "2024-01-04 22:07:51.770787: Epoch 849\n",
      "2024-01-04 22:07:51.777787: Current learning rate: 0.00182\n",
      "2024-01-04 22:09:57.860276: train_loss -0.9225\n",
      "2024-01-04 22:09:57.867275: val_loss -0.7923\n",
      "2024-01-04 22:09:57.874513: Pseudo dice [0.9234, 0.9251, 0.937]\n",
      "2024-01-04 22:09:57.880533: Epoch time: 126.1 s\n",
      "2024-01-04 22:09:59.409201: \n",
      "2024-01-04 22:09:59.423355: Epoch 850\n",
      "2024-01-04 22:09:59.431408: Current learning rate: 0.00181\n",
      "2024-01-04 22:12:05.180303: train_loss -0.9236\n",
      "2024-01-04 22:12:05.190298: val_loss -0.8045\n",
      "2024-01-04 22:12:05.198299: Pseudo dice [0.9229, 0.9304, 0.9393]\n",
      "2024-01-04 22:12:05.206298: Epoch time: 125.77 s\n",
      "2024-01-04 22:12:06.539924: \n",
      "2024-01-04 22:12:06.547925: Epoch 851\n",
      "2024-01-04 22:12:06.553925: Current learning rate: 0.0018\n",
      "2024-01-04 22:14:12.551997: train_loss -0.9267\n",
      "2024-01-04 22:14:12.562994: val_loss -0.7918\n",
      "2024-01-04 22:14:12.569994: Pseudo dice [0.9184, 0.929, 0.9378]\n",
      "2024-01-04 22:14:12.576000: Epoch time: 126.01 s\n",
      "2024-01-04 22:14:13.856061: \n",
      "2024-01-04 22:14:13.862057: Epoch 852\n",
      "2024-01-04 22:14:13.867058: Current learning rate: 0.00179\n",
      "2024-01-04 22:16:19.889982: train_loss -0.9233\n",
      "2024-01-04 22:16:19.898987: val_loss -0.8039\n",
      "2024-01-04 22:16:19.905987: Pseudo dice [0.9226, 0.9286, 0.9372]\n",
      "2024-01-04 22:16:19.912214: Epoch time: 126.03 s\n",
      "2024-01-04 22:16:21.115120: \n",
      "2024-01-04 22:16:21.121465: Epoch 853\n",
      "2024-01-04 22:16:21.130479: Current learning rate: 0.00178\n",
      "2024-01-04 22:18:26.981879: train_loss -0.924\n",
      "2024-01-04 22:18:26.988880: val_loss -0.8029\n",
      "2024-01-04 22:18:26.994879: Pseudo dice [0.9216, 0.9276, 0.9377]\n",
      "2024-01-04 22:18:27.000880: Epoch time: 125.87 s\n",
      "2024-01-04 22:18:28.224747: \n",
      "2024-01-04 22:18:28.237026: Epoch 854\n",
      "2024-01-04 22:18:28.243044: Current learning rate: 0.00177\n",
      "2024-01-04 22:20:34.143815: train_loss -0.9232\n",
      "2024-01-04 22:20:34.151325: val_loss -0.7998\n",
      "2024-01-04 22:20:34.157880: Pseudo dice [0.9189, 0.9299, 0.9387]\n",
      "2024-01-04 22:20:34.166869: Epoch time: 125.92 s\n",
      "2024-01-04 22:20:35.208543: \n",
      "2024-01-04 22:20:35.214202: Epoch 855\n",
      "2024-01-04 22:20:35.222267: Current learning rate: 0.00176\n",
      "2024-01-04 22:22:40.709890: train_loss -0.9264\n",
      "2024-01-04 22:22:40.716883: val_loss -0.7999\n",
      "2024-01-04 22:22:40.722885: Pseudo dice [0.9203, 0.9279, 0.9376]\n",
      "2024-01-04 22:22:40.728391: Epoch time: 125.5 s\n",
      "2024-01-04 22:22:42.096652: \n",
      "2024-01-04 22:22:42.103528: Epoch 856\n",
      "2024-01-04 22:22:42.112685: Current learning rate: 0.00175\n",
      "2024-01-04 22:24:47.656268: train_loss -0.924\n",
      "2024-01-04 22:24:47.663269: val_loss -0.7993\n",
      "2024-01-04 22:24:47.668272: Pseudo dice [0.9207, 0.9287, 0.9379]\n",
      "2024-01-04 22:24:47.675273: Epoch time: 125.56 s\n",
      "2024-01-04 22:24:48.821504: \n",
      "2024-01-04 22:24:48.827678: Epoch 857\n",
      "2024-01-04 22:24:48.832669: Current learning rate: 0.00174\n",
      "2024-01-04 22:26:54.529974: train_loss -0.9259\n",
      "2024-01-04 22:26:54.537977: val_loss -0.793\n",
      "2024-01-04 22:26:54.543974: Pseudo dice [0.9236, 0.9263, 0.9352]\n",
      "2024-01-04 22:26:54.549973: Epoch time: 125.71 s\n",
      "2024-01-04 22:26:55.619395: \n",
      "2024-01-04 22:26:55.625208: Epoch 858\n",
      "2024-01-04 22:26:55.630223: Current learning rate: 0.00173\n",
      "2024-01-04 22:29:01.475069: train_loss -0.9271\n",
      "2024-01-04 22:29:01.487068: val_loss -0.8015\n",
      "2024-01-04 22:29:01.495069: Pseudo dice [0.9214, 0.9289, 0.9374]\n",
      "2024-01-04 22:29:01.502069: Epoch time: 125.86 s\n",
      "2024-01-04 22:29:02.795533: \n",
      "2024-01-04 22:29:02.802451: Epoch 859\n",
      "2024-01-04 22:29:02.809457: Current learning rate: 0.00172\n",
      "2024-01-04 22:31:08.717904: train_loss -0.9249\n",
      "2024-01-04 22:31:08.730893: val_loss -0.8084\n",
      "2024-01-04 22:31:08.739895: Pseudo dice [0.9226, 0.9275, 0.9365]\n",
      "2024-01-04 22:31:08.748892: Epoch time: 125.92 s\n",
      "2024-01-04 22:31:10.099325: \n",
      "2024-01-04 22:31:10.105853: Epoch 860\n",
      "2024-01-04 22:31:10.110854: Current learning rate: 0.0017\n",
      "2024-01-04 22:33:16.675877: train_loss -0.9267\n",
      "2024-01-04 22:33:16.686876: val_loss -0.8011\n",
      "2024-01-04 22:33:16.697920: Pseudo dice [0.921, 0.9277, 0.9368]\n",
      "2024-01-04 22:33:16.704866: Epoch time: 126.58 s\n",
      "2024-01-04 22:33:17.887251: \n",
      "2024-01-04 22:33:17.893251: Epoch 861\n",
      "2024-01-04 22:33:17.898220: Current learning rate: 0.00169\n",
      "2024-01-04 22:35:24.549989: train_loss -0.9273\n",
      "2024-01-04 22:35:24.561990: val_loss -0.7949\n",
      "2024-01-04 22:35:24.569990: Pseudo dice [0.9217, 0.9283, 0.937]\n",
      "2024-01-04 22:35:24.580989: Epoch time: 126.66 s\n",
      "2024-01-04 22:35:26.088216: \n",
      "2024-01-04 22:35:26.094847: Epoch 862\n",
      "2024-01-04 22:35:26.101407: Current learning rate: 0.00168\n",
      "2024-01-04 22:37:36.259553: train_loss -0.925\n",
      "2024-01-04 22:37:36.278605: val_loss -0.7969\n",
      "2024-01-04 22:37:36.293138: Pseudo dice [0.9204, 0.9279, 0.938]\n",
      "2024-01-04 22:37:36.308674: Epoch time: 130.17 s\n",
      "2024-01-04 22:37:37.804181: \n",
      "2024-01-04 22:37:37.813715: Epoch 863\n",
      "2024-01-04 22:37:37.820711: Current learning rate: 0.00167\n",
      "2024-01-04 22:39:45.083579: train_loss -0.9266\n",
      "2024-01-04 22:39:45.091579: val_loss -0.7982\n",
      "2024-01-04 22:39:45.098580: Pseudo dice [0.9194, 0.9289, 0.9374]\n",
      "2024-01-04 22:39:45.104579: Epoch time: 127.28 s\n",
      "2024-01-04 22:39:46.444694: \n",
      "2024-01-04 22:39:46.450754: Epoch 864\n",
      "2024-01-04 22:39:46.456313: Current learning rate: 0.00166\n",
      "2024-01-04 22:41:52.930731: train_loss -0.9249\n",
      "2024-01-04 22:41:52.937736: val_loss -0.7914\n",
      "2024-01-04 22:41:52.943735: Pseudo dice [0.9218, 0.9251, 0.9345]\n",
      "2024-01-04 22:41:52.949735: Epoch time: 126.49 s\n",
      "2024-01-04 22:41:54.088305: \n",
      "2024-01-04 22:41:54.093819: Epoch 865\n",
      "2024-01-04 22:41:54.098816: Current learning rate: 0.00165\n",
      "2024-01-04 22:44:00.599446: train_loss -0.9246\n",
      "2024-01-04 22:44:00.607437: val_loss -0.8068\n",
      "2024-01-04 22:44:00.612466: Pseudo dice [0.92, 0.927, 0.9363]\n",
      "2024-01-04 22:44:00.618451: Epoch time: 126.51 s\n",
      "2024-01-04 22:44:01.740890: \n",
      "2024-01-04 22:44:01.746930: Epoch 866\n",
      "2024-01-04 22:44:01.752931: Current learning rate: 0.00164\n",
      "2024-01-04 22:46:07.567000: train_loss -0.9234\n",
      "2024-01-04 22:46:07.575001: val_loss -0.7929\n",
      "2024-01-04 22:46:07.581000: Pseudo dice [0.9211, 0.9268, 0.9345]\n",
      "2024-01-04 22:46:07.586000: Epoch time: 125.83 s\n",
      "2024-01-04 22:46:08.795487: \n",
      "2024-01-04 22:46:08.802834: Epoch 867\n",
      "2024-01-04 22:46:08.810866: Current learning rate: 0.00163\n",
      "2024-01-04 22:48:15.180331: train_loss -0.9232\n",
      "2024-01-04 22:48:15.190839: val_loss -0.8049\n",
      "2024-01-04 22:48:15.202840: Pseudo dice [0.9221, 0.927, 0.9366]\n",
      "2024-01-04 22:48:15.209840: Epoch time: 126.39 s\n",
      "2024-01-04 22:48:16.513691: \n",
      "2024-01-04 22:48:16.524691: Epoch 868\n",
      "2024-01-04 22:48:16.529755: Current learning rate: 0.00162\n",
      "2024-01-04 22:50:23.081977: train_loss -0.9266\n",
      "2024-01-04 22:50:23.090976: val_loss -0.8005\n",
      "2024-01-04 22:50:23.098978: Pseudo dice [0.9197, 0.9274, 0.9367]\n",
      "2024-01-04 22:50:23.106977: Epoch time: 126.57 s\n",
      "2024-01-04 22:50:24.356333: \n",
      "2024-01-04 22:50:24.363420: Epoch 869\n",
      "2024-01-04 22:50:24.368394: Current learning rate: 0.00161\n",
      "2024-01-04 22:52:30.952930: train_loss -0.9225\n",
      "2024-01-04 22:52:30.960920: val_loss -0.8049\n",
      "2024-01-04 22:52:30.966919: Pseudo dice [0.92, 0.9277, 0.9373]\n",
      "2024-01-04 22:52:30.972608: Epoch time: 126.6 s\n",
      "2024-01-04 22:52:32.074145: \n",
      "2024-01-04 22:52:32.080142: Epoch 870\n",
      "2024-01-04 22:52:32.086145: Current learning rate: 0.00159\n",
      "2024-01-04 22:54:38.240623: train_loss -0.9246\n",
      "2024-01-04 22:54:38.249623: val_loss -0.8049\n",
      "2024-01-04 22:54:38.257720: Pseudo dice [0.919, 0.9272, 0.9363]\n",
      "2024-01-04 22:54:38.264720: Epoch time: 126.17 s\n",
      "2024-01-04 22:54:39.305187: \n",
      "2024-01-04 22:54:39.314254: Epoch 871\n",
      "2024-01-04 22:54:39.319891: Current learning rate: 0.00158\n",
      "2024-01-04 22:56:45.078045: train_loss -0.9257\n",
      "2024-01-04 22:56:45.087556: val_loss -0.8018\n",
      "2024-01-04 22:56:45.093556: Pseudo dice [0.9206, 0.9272, 0.9358]\n",
      "2024-01-04 22:56:45.099556: Epoch time: 125.77 s\n",
      "2024-01-04 22:56:46.281182: \n",
      "2024-01-04 22:56:46.290186: Epoch 872\n",
      "2024-01-04 22:56:46.295186: Current learning rate: 0.00157\n",
      "2024-01-04 22:58:52.565637: train_loss -0.9239\n",
      "2024-01-04 22:58:52.573640: val_loss -0.807\n",
      "2024-01-04 22:58:52.578645: Pseudo dice [0.9198, 0.929, 0.9376]\n",
      "2024-01-04 22:58:52.594319: Epoch time: 126.29 s\n",
      "2024-01-04 22:58:53.868510: \n",
      "2024-01-04 22:58:53.875538: Epoch 873\n",
      "2024-01-04 22:58:53.880453: Current learning rate: 0.00156\n",
      "2024-01-04 23:00:59.872608: train_loss -0.9252\n",
      "2024-01-04 23:00:59.884609: val_loss -0.7943\n",
      "2024-01-04 23:00:59.895841: Pseudo dice [0.9208, 0.9275, 0.9362]\n",
      "2024-01-04 23:00:59.905870: Epoch time: 126.01 s\n",
      "2024-01-04 23:01:01.067219: \n",
      "2024-01-04 23:01:01.076215: Epoch 874\n",
      "2024-01-04 23:01:01.083156: Current learning rate: 0.00155\n",
      "2024-01-04 23:03:07.028232: train_loss -0.9256\n",
      "2024-01-04 23:03:07.038229: val_loss -0.7862\n",
      "2024-01-04 23:03:07.046228: Pseudo dice [0.9198, 0.9278, 0.9358]\n",
      "2024-01-04 23:03:07.056231: Epoch time: 125.96 s\n",
      "2024-01-04 23:03:08.189411: \n",
      "2024-01-04 23:03:08.196416: Epoch 875\n",
      "2024-01-04 23:03:08.202402: Current learning rate: 0.00154\n",
      "2024-01-04 23:05:14.267342: train_loss -0.9259\n",
      "2024-01-04 23:05:14.275341: val_loss -0.8011\n",
      "2024-01-04 23:05:14.282341: Pseudo dice [0.9191, 0.9281, 0.9376]\n",
      "2024-01-04 23:05:14.288341: Epoch time: 126.08 s\n",
      "2024-01-04 23:05:15.416220: \n",
      "2024-01-04 23:05:15.422698: Epoch 876\n",
      "2024-01-04 23:05:15.427820: Current learning rate: 0.00153\n",
      "2024-01-04 23:07:21.214021: train_loss -0.9248\n",
      "2024-01-04 23:07:21.222023: val_loss -0.7972\n",
      "2024-01-04 23:07:21.227021: Pseudo dice [0.925, 0.9279, 0.938]\n",
      "2024-01-04 23:07:21.233028: Epoch time: 125.8 s\n",
      "2024-01-04 23:07:22.458147: \n",
      "2024-01-04 23:07:22.464287: Epoch 877\n",
      "2024-01-04 23:07:22.470303: Current learning rate: 0.00152\n",
      "2024-01-04 23:09:28.649680: train_loss -0.9256\n",
      "2024-01-04 23:09:28.658676: val_loss -0.798\n",
      "2024-01-04 23:09:28.669188: Pseudo dice [0.9218, 0.9286, 0.9386]\n",
      "2024-01-04 23:09:28.674189: Epoch time: 126.19 s\n",
      "2024-01-04 23:09:29.826849: \n",
      "2024-01-04 23:09:29.834121: Epoch 878\n",
      "2024-01-04 23:09:29.840107: Current learning rate: 0.00151\n",
      "2024-01-04 23:11:35.609766: train_loss -0.923\n",
      "2024-01-04 23:11:35.616772: val_loss -0.8102\n",
      "2024-01-04 23:11:35.622761: Pseudo dice [0.9238, 0.9283, 0.9382]\n",
      "2024-01-04 23:11:35.629763: Epoch time: 125.78 s\n",
      "2024-01-04 23:11:36.841575: \n",
      "2024-01-04 23:11:36.847632: Epoch 879\n",
      "2024-01-04 23:11:36.852568: Current learning rate: 0.00149\n",
      "2024-01-04 23:13:42.703567: train_loss -0.9278\n",
      "2024-01-04 23:13:42.712074: val_loss -0.8007\n",
      "2024-01-04 23:13:42.719077: Pseudo dice [0.9189, 0.9287, 0.9376]\n",
      "2024-01-04 23:13:42.727076: Epoch time: 125.86 s\n",
      "2024-01-04 23:13:43.938806: \n",
      "2024-01-04 23:13:43.945520: Epoch 880\n",
      "2024-01-04 23:13:43.950523: Current learning rate: 0.00148\n",
      "2024-01-04 23:15:50.053298: train_loss -0.9232\n",
      "2024-01-04 23:15:50.060299: val_loss -0.7945\n",
      "2024-01-04 23:15:50.066306: Pseudo dice [0.9229, 0.9266, 0.9363]\n",
      "2024-01-04 23:15:50.073303: Epoch time: 126.12 s\n",
      "2024-01-04 23:15:51.329530: \n",
      "2024-01-04 23:15:51.335722: Epoch 881\n",
      "2024-01-04 23:15:51.344805: Current learning rate: 0.00147\n",
      "2024-01-04 23:17:57.493814: train_loss -0.9268\n",
      "2024-01-04 23:17:57.503816: val_loss -0.8013\n",
      "2024-01-04 23:17:57.513811: Pseudo dice [0.9194, 0.9282, 0.9376]\n",
      "2024-01-04 23:17:57.522818: Epoch time: 126.17 s\n",
      "2024-01-04 23:17:58.861599: \n",
      "2024-01-04 23:17:58.867598: Epoch 882\n",
      "2024-01-04 23:17:58.874604: Current learning rate: 0.00146\n",
      "2024-01-04 23:20:04.976812: train_loss -0.9267\n",
      "2024-01-04 23:20:04.987327: val_loss -0.7829\n",
      "2024-01-04 23:20:04.994326: Pseudo dice [0.9211, 0.9275, 0.9356]\n",
      "2024-01-04 23:20:05.000325: Epoch time: 126.12 s\n",
      "2024-01-04 23:20:06.205656: \n",
      "2024-01-04 23:20:06.216714: Epoch 883\n",
      "2024-01-04 23:20:06.223787: Current learning rate: 0.00145\n",
      "2024-01-04 23:22:12.160705: train_loss -0.9268\n",
      "2024-01-04 23:22:12.168221: val_loss -0.799\n",
      "2024-01-04 23:22:12.176221: Pseudo dice [0.9195, 0.9252, 0.9346]\n",
      "2024-01-04 23:22:12.182219: Epoch time: 125.96 s\n",
      "2024-01-04 23:22:13.384907: \n",
      "2024-01-04 23:22:13.391924: Epoch 884\n",
      "2024-01-04 23:22:13.396908: Current learning rate: 0.00144\n",
      "2024-01-04 23:24:19.735446: train_loss -0.9254\n",
      "2024-01-04 23:24:19.742447: val_loss -0.7995\n",
      "2024-01-04 23:24:19.749443: Pseudo dice [0.9223, 0.9289, 0.9369]\n",
      "2024-01-04 23:24:19.754442: Epoch time: 126.35 s\n",
      "2024-01-04 23:24:20.914032: \n",
      "2024-01-04 23:24:20.927299: Epoch 885\n",
      "2024-01-04 23:24:20.932382: Current learning rate: 0.00143\n",
      "2024-01-04 23:26:27.108245: train_loss -0.9265\n",
      "2024-01-04 23:26:27.116245: val_loss -0.8035\n",
      "2024-01-04 23:26:27.122245: Pseudo dice [0.9204, 0.9276, 0.9373]\n",
      "2024-01-04 23:26:27.128242: Epoch time: 126.2 s\n",
      "2024-01-04 23:26:28.070743: \n",
      "2024-01-04 23:26:28.076804: Epoch 886\n",
      "2024-01-04 23:26:28.081823: Current learning rate: 0.00142\n",
      "2024-01-04 23:28:33.777003: train_loss -0.926\n",
      "2024-01-04 23:28:33.786005: val_loss -0.7908\n",
      "2024-01-04 23:28:33.796016: Pseudo dice [0.9231, 0.9282, 0.9374]\n",
      "2024-01-04 23:28:33.804005: Epoch time: 125.71 s\n",
      "2024-01-04 23:28:34.743767: \n",
      "2024-01-04 23:28:34.756539: Epoch 887\n",
      "2024-01-04 23:28:34.761123: Current learning rate: 0.00141\n",
      "2024-01-04 23:30:41.135598: train_loss -0.9237\n",
      "2024-01-04 23:30:41.145599: val_loss -0.8034\n",
      "2024-01-04 23:30:41.156964: Pseudo dice [0.9182, 0.9293, 0.9374]\n",
      "2024-01-04 23:30:41.173572: Epoch time: 126.39 s\n",
      "2024-01-04 23:30:42.205745: \n",
      "2024-01-04 23:30:42.212132: Epoch 888\n",
      "2024-01-04 23:30:42.217219: Current learning rate: 0.00139\n",
      "2024-01-04 23:32:47.935495: train_loss -0.9238\n",
      "2024-01-04 23:32:47.945494: val_loss -0.8029\n",
      "2024-01-04 23:32:47.954495: Pseudo dice [0.9221, 0.9293, 0.9391]\n",
      "2024-01-04 23:32:47.962496: Epoch time: 125.73 s\n",
      "2024-01-04 23:32:48.966242: \n",
      "2024-01-04 23:32:48.974648: Epoch 889\n",
      "2024-01-04 23:32:48.979708: Current learning rate: 0.00138\n",
      "2024-01-04 23:34:54.767835: train_loss -0.9258\n",
      "2024-01-04 23:34:54.777836: val_loss -0.7905\n",
      "2024-01-04 23:34:54.787837: Pseudo dice [0.9213, 0.9295, 0.9378]\n",
      "2024-01-04 23:34:54.797837: Epoch time: 125.8 s\n",
      "2024-01-04 23:34:55.799992: \n",
      "2024-01-04 23:34:55.809167: Epoch 890\n",
      "2024-01-04 23:34:55.814233: Current learning rate: 0.00137\n",
      "2024-01-04 23:37:01.278479: train_loss -0.9239\n",
      "2024-01-04 23:37:01.287476: val_loss -0.7891\n",
      "2024-01-04 23:37:01.294477: Pseudo dice [0.9202, 0.9277, 0.937]\n",
      "2024-01-04 23:37:01.299484: Epoch time: 125.48 s\n",
      "2024-01-04 23:37:02.243935: \n",
      "2024-01-04 23:37:02.254791: Epoch 891\n",
      "2024-01-04 23:37:02.264809: Current learning rate: 0.00136\n",
      "2024-01-04 23:39:07.170096: train_loss -0.9264\n",
      "2024-01-04 23:39:07.178097: val_loss -0.7787\n",
      "2024-01-04 23:39:07.187097: Pseudo dice [0.9219, 0.928, 0.9362]\n",
      "2024-01-04 23:39:07.193101: Epoch time: 124.93 s\n",
      "2024-01-04 23:39:08.126912: \n",
      "2024-01-04 23:39:08.132835: Epoch 892\n",
      "2024-01-04 23:39:08.137424: Current learning rate: 0.00135\n",
      "2024-01-04 23:41:12.922270: train_loss -0.9261\n",
      "2024-01-04 23:41:12.932833: val_loss -0.798\n",
      "2024-01-04 23:41:12.940847: Pseudo dice [0.9196, 0.926, 0.936]\n",
      "2024-01-04 23:41:12.948860: Epoch time: 124.8 s\n",
      "2024-01-04 23:41:13.899858: \n",
      "2024-01-04 23:41:13.911873: Epoch 893\n",
      "2024-01-04 23:41:13.915936: Current learning rate: 0.00134\n",
      "2024-01-04 23:43:18.323468: train_loss -0.9278\n",
      "2024-01-04 23:43:18.332469: val_loss -0.7878\n",
      "2024-01-04 23:43:18.339478: Pseudo dice [0.9223, 0.9266, 0.9362]\n",
      "2024-01-04 23:43:18.346470: Epoch time: 124.43 s\n",
      "2024-01-04 23:43:19.309890: \n",
      "2024-01-04 23:43:19.319825: Epoch 894\n",
      "2024-01-04 23:43:19.325825: Current learning rate: 0.00133\n",
      "2024-01-04 23:45:24.507964: train_loss -0.9225\n",
      "2024-01-04 23:45:24.517962: val_loss -0.7906\n",
      "2024-01-04 23:45:24.525961: Pseudo dice [0.9199, 0.9272, 0.9363]\n",
      "2024-01-04 23:45:24.533964: Epoch time: 125.2 s\n",
      "2024-01-04 23:45:25.471314: \n",
      "2024-01-04 23:45:25.481005: Epoch 895\n",
      "2024-01-04 23:45:25.486004: Current learning rate: 0.00132\n",
      "2024-01-04 23:47:30.564801: train_loss -0.9246\n",
      "2024-01-04 23:47:30.573802: val_loss -0.7942\n",
      "2024-01-04 23:47:30.581798: Pseudo dice [0.9181, 0.9275, 0.9372]\n",
      "2024-01-04 23:47:30.589431: Epoch time: 125.09 s\n",
      "2024-01-04 23:47:31.708110: \n",
      "2024-01-04 23:47:31.715078: Epoch 896\n",
      "2024-01-04 23:47:31.719653: Current learning rate: 0.0013\n",
      "2024-01-04 23:49:36.409220: train_loss -0.929\n",
      "2024-01-04 23:49:36.419221: val_loss -0.8014\n",
      "2024-01-04 23:49:36.425225: Pseudo dice [0.9173, 0.9282, 0.9376]\n",
      "2024-01-04 23:49:36.430225: Epoch time: 124.7 s\n",
      "2024-01-04 23:49:37.355923: \n",
      "2024-01-04 23:49:37.363476: Epoch 897\n",
      "2024-01-04 23:49:37.368649: Current learning rate: 0.00129\n",
      "2024-01-04 23:51:42.016137: train_loss -0.9263\n",
      "2024-01-04 23:51:42.028140: val_loss -0.7822\n",
      "2024-01-04 23:51:42.038157: Pseudo dice [0.9195, 0.9265, 0.9347]\n",
      "2024-01-04 23:51:42.047652: Epoch time: 124.66 s\n",
      "2024-01-04 23:51:43.024308: \n",
      "2024-01-04 23:51:43.035921: Epoch 898\n",
      "2024-01-04 23:51:43.040004: Current learning rate: 0.00128\n",
      "2024-01-04 23:53:47.670520: train_loss -0.9274\n",
      "2024-01-04 23:53:47.680526: val_loss -0.8052\n",
      "2024-01-04 23:53:47.687031: Pseudo dice [0.9214, 0.9258, 0.9352]\n",
      "2024-01-04 23:53:47.694041: Epoch time: 124.65 s\n",
      "2024-01-04 23:53:48.653650: \n",
      "2024-01-04 23:53:48.659946: Epoch 899\n",
      "2024-01-04 23:53:48.664959: Current learning rate: 0.00127\n",
      "2024-01-04 23:55:53.533467: train_loss -0.9251\n",
      "2024-01-04 23:55:53.543467: val_loss -0.8015\n",
      "2024-01-04 23:55:53.552469: Pseudo dice [0.9221, 0.9266, 0.9358]\n",
      "2024-01-04 23:55:53.560468: Epoch time: 124.88 s\n",
      "2024-01-04 23:55:54.828988: \n",
      "2024-01-04 23:55:54.845058: Epoch 900\n",
      "2024-01-04 23:55:54.849989: Current learning rate: 0.00126\n",
      "2024-01-04 23:57:59.911720: train_loss -0.9273\n",
      "2024-01-04 23:57:59.921720: val_loss -0.7897\n",
      "2024-01-04 23:57:59.930721: Pseudo dice [0.9177, 0.9284, 0.9379]\n",
      "2024-01-04 23:57:59.936721: Epoch time: 125.08 s\n",
      "2024-01-04 23:58:00.876812: \n",
      "2024-01-04 23:58:00.886077: Epoch 901\n",
      "2024-01-04 23:58:00.891078: Current learning rate: 0.00125\n",
      "2024-01-05 00:00:06.180466: train_loss -0.9269\n",
      "2024-01-05 00:00:06.190466: val_loss -0.8059\n",
      "2024-01-05 00:00:06.199465: Pseudo dice [0.9212, 0.9284, 0.9376]\n",
      "2024-01-05 00:00:06.208474: Epoch time: 125.3 s\n",
      "2024-01-05 00:00:07.180060: \n",
      "2024-01-05 00:00:07.188070: Epoch 902\n",
      "2024-01-05 00:00:07.192069: Current learning rate: 0.00124\n",
      "2024-01-05 00:02:12.736049: train_loss -0.924\n",
      "2024-01-05 00:02:12.745046: val_loss -0.7961\n",
      "2024-01-05 00:02:12.754050: Pseudo dice [0.9229, 0.9281, 0.937]\n",
      "2024-01-05 00:02:12.762056: Epoch time: 125.56 s\n",
      "2024-01-05 00:02:13.684205: \n",
      "2024-01-05 00:02:13.690242: Epoch 903\n",
      "2024-01-05 00:02:13.695477: Current learning rate: 0.00122\n",
      "2024-01-05 00:04:19.066548: train_loss -0.9252\n",
      "2024-01-05 00:04:19.074555: val_loss -0.7878\n",
      "2024-01-05 00:04:19.080555: Pseudo dice [0.9226, 0.9268, 0.9353]\n",
      "2024-01-05 00:04:19.086549: Epoch time: 125.38 s\n",
      "2024-01-05 00:04:20.065133: \n",
      "2024-01-05 00:04:20.072134: Epoch 904\n",
      "2024-01-05 00:04:20.076142: Current learning rate: 0.00121\n",
      "2024-01-05 00:06:25.483326: train_loss -0.9234\n",
      "2024-01-05 00:06:25.492326: val_loss -0.7959\n",
      "2024-01-05 00:06:25.500326: Pseudo dice [0.9237, 0.9262, 0.9352]\n",
      "2024-01-05 00:06:25.506326: Epoch time: 125.42 s\n",
      "2024-01-05 00:06:26.605269: \n",
      "2024-01-05 00:06:26.615444: Epoch 905\n",
      "2024-01-05 00:06:26.619519: Current learning rate: 0.0012\n",
      "2024-01-05 00:08:31.264666: train_loss -0.9295\n",
      "2024-01-05 00:08:31.272050: val_loss -0.7944\n",
      "2024-01-05 00:08:31.280050: Pseudo dice [0.9201, 0.928, 0.9369]\n",
      "2024-01-05 00:08:31.286050: Epoch time: 124.66 s\n",
      "2024-01-05 00:08:32.224319: \n",
      "2024-01-05 00:08:32.230129: Epoch 906\n",
      "2024-01-05 00:08:32.234100: Current learning rate: 0.00119\n",
      "2024-01-05 00:10:37.135141: train_loss -0.9281\n",
      "2024-01-05 00:10:37.144140: val_loss -0.7952\n",
      "2024-01-05 00:10:37.153141: Pseudo dice [0.9234, 0.9275, 0.937]\n",
      "2024-01-05 00:10:37.161140: Epoch time: 124.91 s\n",
      "2024-01-05 00:10:38.102476: \n",
      "2024-01-05 00:10:38.109399: Epoch 907\n",
      "2024-01-05 00:10:38.114403: Current learning rate: 0.00118\n",
      "2024-01-05 00:12:42.898618: train_loss -0.928\n",
      "2024-01-05 00:12:42.908622: val_loss -0.7911\n",
      "2024-01-05 00:12:42.915621: Pseudo dice [0.9242, 0.9276, 0.9355]\n",
      "2024-01-05 00:12:42.922134: Epoch time: 124.8 s\n",
      "2024-01-05 00:12:43.853162: \n",
      "2024-01-05 00:12:43.859047: Epoch 908\n",
      "2024-01-05 00:12:43.864136: Current learning rate: 0.00117\n",
      "2024-01-05 00:14:48.542518: train_loss -0.9273\n",
      "2024-01-05 00:14:48.550520: val_loss -0.7972\n",
      "2024-01-05 00:14:48.558520: Pseudo dice [0.9182, 0.9265, 0.9359]\n",
      "2024-01-05 00:14:48.566027: Epoch time: 124.69 s\n",
      "2024-01-05 00:14:49.565422: \n",
      "2024-01-05 00:14:49.571426: Epoch 909\n",
      "2024-01-05 00:14:49.575418: Current learning rate: 0.00116\n",
      "2024-01-05 00:16:54.746752: train_loss -0.9287\n",
      "2024-01-05 00:16:54.756751: val_loss -0.7884\n",
      "2024-01-05 00:16:54.763754: Pseudo dice [0.9238, 0.9268, 0.9354]\n",
      "2024-01-05 00:16:54.769751: Epoch time: 125.18 s\n",
      "2024-01-05 00:16:55.712412: \n",
      "2024-01-05 00:16:55.719402: Epoch 910\n",
      "2024-01-05 00:16:55.724402: Current learning rate: 0.00115\n",
      "2024-01-05 00:19:00.713032: train_loss -0.9282\n",
      "2024-01-05 00:19:00.721027: val_loss -0.7958\n",
      "2024-01-05 00:19:00.727027: Pseudo dice [0.9213, 0.9282, 0.9376]\n",
      "2024-01-05 00:19:00.733027: Epoch time: 125.0 s\n",
      "2024-01-05 00:19:01.683345: \n",
      "2024-01-05 00:19:01.692783: Epoch 911\n",
      "2024-01-05 00:19:01.696856: Current learning rate: 0.00113\n",
      "2024-01-05 00:21:07.021661: train_loss -0.9266\n",
      "2024-01-05 00:21:07.030658: val_loss -0.7994\n",
      "2024-01-05 00:21:07.038662: Pseudo dice [0.9184, 0.9274, 0.9369]\n",
      "2024-01-05 00:21:07.043666: Epoch time: 125.34 s\n",
      "2024-01-05 00:21:07.969775: \n",
      "2024-01-05 00:21:07.976346: Epoch 912\n",
      "2024-01-05 00:21:07.980936: Current learning rate: 0.00112\n",
      "2024-01-05 00:23:13.320961: train_loss -0.9255\n",
      "2024-01-05 00:23:13.329962: val_loss -0.7759\n",
      "2024-01-05 00:23:13.336968: Pseudo dice [0.9227, 0.9267, 0.9357]\n",
      "2024-01-05 00:23:13.343960: Epoch time: 125.35 s\n",
      "2024-01-05 00:23:14.441738: \n",
      "2024-01-05 00:23:14.454814: Epoch 913\n",
      "2024-01-05 00:23:14.460485: Current learning rate: 0.00111\n",
      "2024-01-05 00:25:19.786166: train_loss -0.927\n",
      "2024-01-05 00:25:19.795166: val_loss -0.8004\n",
      "2024-01-05 00:25:19.804166: Pseudo dice [0.9202, 0.9273, 0.9366]\n",
      "2024-01-05 00:25:19.811171: Epoch time: 125.35 s\n",
      "2024-01-05 00:25:20.776934: \n",
      "2024-01-05 00:25:20.784933: Epoch 914\n",
      "2024-01-05 00:25:20.792933: Current learning rate: 0.0011\n",
      "2024-01-05 00:27:25.751131: train_loss -0.9268\n",
      "2024-01-05 00:27:25.760148: val_loss -0.7991\n",
      "2024-01-05 00:27:25.767148: Pseudo dice [0.9207, 0.9272, 0.9369]\n",
      "2024-01-05 00:27:25.773659: Epoch time: 124.98 s\n",
      "2024-01-05 00:27:26.704660: \n",
      "2024-01-05 00:27:26.710750: Epoch 915\n",
      "2024-01-05 00:27:26.715742: Current learning rate: 0.00109\n",
      "2024-01-05 00:29:31.775035: train_loss -0.9253\n",
      "2024-01-05 00:29:31.785026: val_loss -0.7949\n",
      "2024-01-05 00:29:31.791025: Pseudo dice [0.922, 0.9283, 0.9377]\n",
      "2024-01-05 00:29:31.798025: Epoch time: 125.07 s\n",
      "2024-01-05 00:29:32.729687: \n",
      "2024-01-05 00:29:32.737687: Epoch 916\n",
      "2024-01-05 00:29:32.746687: Current learning rate: 0.00108\n",
      "2024-01-05 00:31:37.531981: train_loss -0.9252\n",
      "2024-01-05 00:31:37.542989: val_loss -0.792\n",
      "2024-01-05 00:31:37.551983: Pseudo dice [0.9205, 0.9277, 0.9373]\n",
      "2024-01-05 00:31:37.558983: Epoch time: 124.8 s\n",
      "2024-01-05 00:31:38.480213: \n",
      "2024-01-05 00:31:38.485983: Epoch 917\n",
      "2024-01-05 00:31:38.490999: Current learning rate: 0.00106\n",
      "2024-01-05 00:33:42.955638: train_loss -0.9288\n",
      "2024-01-05 00:33:42.964634: val_loss -0.7863\n",
      "2024-01-05 00:33:42.972633: Pseudo dice [0.9252, 0.9284, 0.9374]\n",
      "2024-01-05 00:33:42.980649: Epoch time: 124.48 s\n",
      "2024-01-05 00:33:43.906262: \n",
      "2024-01-05 00:33:43.920603: Epoch 918\n",
      "2024-01-05 00:33:43.928896: Current learning rate: 0.00105\n",
      "2024-01-05 00:35:48.813404: train_loss -0.9273\n",
      "2024-01-05 00:35:48.820404: val_loss -0.7983\n",
      "2024-01-05 00:35:48.830402: Pseudo dice [0.9177, 0.9288, 0.9381]\n",
      "2024-01-05 00:35:48.836401: Epoch time: 124.91 s\n",
      "2024-01-05 00:35:49.801187: \n",
      "2024-01-05 00:35:49.809597: Epoch 919\n",
      "2024-01-05 00:35:49.814518: Current learning rate: 0.00104\n",
      "2024-01-05 00:37:54.763335: train_loss -0.9287\n",
      "2024-01-05 00:37:54.773334: val_loss -0.7982\n",
      "2024-01-05 00:37:54.782333: Pseudo dice [0.9195, 0.9285, 0.9384]\n",
      "2024-01-05 00:37:54.788333: Epoch time: 124.96 s\n",
      "2024-01-05 00:37:55.722332: \n",
      "2024-01-05 00:37:55.727447: Epoch 920\n",
      "2024-01-05 00:37:55.732438: Current learning rate: 0.00103\n",
      "2024-01-05 00:40:00.719118: train_loss -0.9284\n",
      "2024-01-05 00:40:00.727119: val_loss -0.7913\n",
      "2024-01-05 00:40:00.736118: Pseudo dice [0.919, 0.9268, 0.9358]\n",
      "2024-01-05 00:40:00.742122: Epoch time: 125.0 s\n",
      "2024-01-05 00:40:01.842770: \n",
      "2024-01-05 00:40:01.848763: Epoch 921\n",
      "2024-01-05 00:40:01.856593: Current learning rate: 0.00102\n",
      "2024-01-05 00:42:06.667467: train_loss -0.9287\n",
      "2024-01-05 00:42:06.675462: val_loss -0.8015\n",
      "2024-01-05 00:42:06.684463: Pseudo dice [0.9197, 0.9284, 0.9371]\n",
      "2024-01-05 00:42:06.692470: Epoch time: 124.82 s\n",
      "2024-01-05 00:42:07.639209: \n",
      "2024-01-05 00:42:07.647091: Epoch 922\n",
      "2024-01-05 00:42:07.652162: Current learning rate: 0.00101\n",
      "2024-01-05 00:44:12.752836: train_loss -0.9252\n",
      "2024-01-05 00:44:12.764839: val_loss -0.7995\n",
      "2024-01-05 00:44:12.772837: Pseudo dice [0.9231, 0.9282, 0.9378]\n",
      "2024-01-05 00:44:12.778836: Epoch time: 125.12 s\n",
      "2024-01-05 00:44:13.738499: \n",
      "2024-01-05 00:44:13.749208: Epoch 923\n",
      "2024-01-05 00:44:13.757207: Current learning rate: 0.001\n",
      "2024-01-05 00:46:18.549258: train_loss -0.9279\n",
      "2024-01-05 00:46:18.558257: val_loss -0.7923\n",
      "2024-01-05 00:46:18.565256: Pseudo dice [0.9213, 0.9278, 0.9363]\n",
      "2024-01-05 00:46:18.573265: Epoch time: 124.81 s\n",
      "2024-01-05 00:46:19.542137: \n",
      "2024-01-05 00:46:19.551519: Epoch 924\n",
      "2024-01-05 00:46:19.555598: Current learning rate: 0.00098\n",
      "2024-01-05 00:48:24.558532: train_loss -0.9259\n",
      "2024-01-05 00:48:24.567531: val_loss -0.7986\n",
      "2024-01-05 00:48:24.576529: Pseudo dice [0.9205, 0.9257, 0.9359]\n",
      "2024-01-05 00:48:24.582529: Epoch time: 125.02 s\n",
      "2024-01-05 00:48:25.513783: \n",
      "2024-01-05 00:48:25.519357: Epoch 925\n",
      "2024-01-05 00:48:25.527901: Current learning rate: 0.00097\n",
      "2024-01-05 00:50:30.372094: train_loss -0.928\n",
      "2024-01-05 00:50:30.381636: val_loss -0.8028\n",
      "2024-01-05 00:50:30.387630: Pseudo dice [0.9183, 0.9274, 0.9374]\n",
      "2024-01-05 00:50:30.395632: Epoch time: 124.86 s\n",
      "2024-01-05 00:50:31.328968: \n",
      "2024-01-05 00:50:31.337460: Epoch 926\n",
      "2024-01-05 00:50:31.341540: Current learning rate: 0.00096\n",
      "2024-01-05 00:52:36.300314: train_loss -0.9278\n",
      "2024-01-05 00:52:36.308299: val_loss -0.7828\n",
      "2024-01-05 00:52:36.315303: Pseudo dice [0.9231, 0.9274, 0.9362]\n",
      "2024-01-05 00:52:36.322303: Epoch time: 124.97 s\n",
      "2024-01-05 00:52:37.259902: \n",
      "2024-01-05 00:52:37.266489: Epoch 927\n",
      "2024-01-05 00:52:37.271212: Current learning rate: 0.00095\n",
      "2024-01-05 00:54:42.524836: train_loss -0.928\n",
      "2024-01-05 00:54:42.533836: val_loss -0.8007\n",
      "2024-01-05 00:54:42.541351: Pseudo dice [0.9203, 0.9283, 0.9379]\n",
      "2024-01-05 00:54:42.548346: Epoch time: 125.27 s\n",
      "2024-01-05 00:54:43.483156: \n",
      "2024-01-05 00:54:43.493185: Epoch 928\n",
      "2024-01-05 00:54:43.501198: Current learning rate: 0.00094\n",
      "2024-01-05 00:56:49.116073: train_loss -0.922\n",
      "2024-01-05 00:56:49.124074: val_loss -0.7903\n",
      "2024-01-05 00:56:49.133073: Pseudo dice [0.923, 0.9275, 0.9366]\n",
      "2024-01-05 00:56:49.141073: Epoch time: 125.63 s\n",
      "2024-01-05 00:56:50.256823: \n",
      "2024-01-05 00:56:50.263306: Epoch 929\n",
      "2024-01-05 00:56:50.267374: Current learning rate: 0.00092\n",
      "2024-01-05 00:58:55.371881: train_loss -0.9298\n",
      "2024-01-05 00:58:55.381878: val_loss -0.8015\n",
      "2024-01-05 00:58:55.393876: Pseudo dice [0.9209, 0.9284, 0.9374]\n",
      "2024-01-05 00:58:55.401877: Epoch time: 125.12 s\n",
      "2024-01-05 00:58:56.355092: \n",
      "2024-01-05 00:58:56.364093: Epoch 930\n",
      "2024-01-05 00:58:56.368534: Current learning rate: 0.00091\n",
      "2024-01-05 01:01:01.255541: train_loss -0.9308\n",
      "2024-01-05 01:01:01.266539: val_loss -0.796\n",
      "2024-01-05 01:01:01.272542: Pseudo dice [0.9222, 0.9259, 0.935]\n",
      "2024-01-05 01:01:01.278542: Epoch time: 124.9 s\n",
      "2024-01-05 01:01:02.207492: \n",
      "2024-01-05 01:01:02.216441: Epoch 931\n",
      "2024-01-05 01:01:02.220441: Current learning rate: 0.0009\n",
      "2024-01-05 01:03:07.374446: train_loss -0.9281\n",
      "2024-01-05 01:03:07.383442: val_loss -0.7927\n",
      "2024-01-05 01:03:07.391442: Pseudo dice [0.9199, 0.9275, 0.9369]\n",
      "2024-01-05 01:03:07.398457: Epoch time: 125.17 s\n",
      "2024-01-05 01:03:08.339953: \n",
      "2024-01-05 01:03:08.348804: Epoch 932\n",
      "2024-01-05 01:03:08.355721: Current learning rate: 0.00089\n",
      "2024-01-05 01:05:13.553720: train_loss -0.9256\n",
      "2024-01-05 01:05:13.563731: val_loss -0.7956\n",
      "2024-01-05 01:05:13.570723: Pseudo dice [0.9217, 0.9274, 0.9373]\n",
      "2024-01-05 01:05:13.578729: Epoch time: 125.21 s\n",
      "2024-01-05 01:05:14.531845: \n",
      "2024-01-05 01:05:14.537946: Epoch 933\n",
      "2024-01-05 01:05:14.542027: Current learning rate: 0.00088\n",
      "2024-01-05 01:07:19.623348: train_loss -0.9294\n",
      "2024-01-05 01:07:19.634355: val_loss -0.7815\n",
      "2024-01-05 01:07:19.640353: Pseudo dice [0.9195, 0.9284, 0.9364]\n",
      "2024-01-05 01:07:19.647351: Epoch time: 125.09 s\n",
      "2024-01-05 01:07:20.638672: \n",
      "2024-01-05 01:07:20.647020: Epoch 934\n",
      "2024-01-05 01:07:20.653222: Current learning rate: 0.00087\n",
      "2024-01-05 01:09:25.738873: train_loss -0.9281\n",
      "2024-01-05 01:09:25.746873: val_loss -0.7996\n",
      "2024-01-05 01:09:25.753872: Pseudo dice [0.9254, 0.9277, 0.9367]\n",
      "2024-01-05 01:09:25.758873: Epoch time: 125.1 s\n",
      "2024-01-05 01:09:26.700398: \n",
      "2024-01-05 01:09:26.713459: Epoch 935\n",
      "2024-01-05 01:09:26.719414: Current learning rate: 0.00085\n",
      "2024-01-05 01:11:31.967218: train_loss -0.927\n",
      "2024-01-05 01:11:31.975219: val_loss -0.798\n",
      "2024-01-05 01:11:31.983226: Pseudo dice [0.9217, 0.9267, 0.9359]\n",
      "2024-01-05 01:11:31.989216: Epoch time: 125.27 s\n",
      "2024-01-05 01:11:32.994264: \n",
      "2024-01-05 01:11:33.002271: Epoch 936\n",
      "2024-01-05 01:11:33.008264: Current learning rate: 0.00084\n",
      "2024-01-05 01:13:38.072994: train_loss -0.9286\n",
      "2024-01-05 01:13:38.081997: val_loss -0.7988\n",
      "2024-01-05 01:13:38.088997: Pseudo dice [0.9226, 0.9263, 0.9351]\n",
      "2024-01-05 01:13:38.094994: Epoch time: 125.08 s\n",
      "2024-01-05 01:13:39.180135: \n",
      "2024-01-05 01:13:39.187207: Epoch 937\n",
      "2024-01-05 01:13:39.192173: Current learning rate: 0.00083\n",
      "2024-01-05 01:15:44.321930: train_loss -0.9282\n",
      "2024-01-05 01:15:44.329932: val_loss -0.8\n",
      "2024-01-05 01:15:44.340932: Pseudo dice [0.9195, 0.9267, 0.9367]\n",
      "2024-01-05 01:15:44.347931: Epoch time: 125.14 s\n",
      "2024-01-05 01:15:45.281412: \n",
      "2024-01-05 01:15:45.291441: Epoch 938\n",
      "2024-01-05 01:15:45.296035: Current learning rate: 0.00082\n",
      "2024-01-05 01:17:50.495629: train_loss -0.9262\n",
      "2024-01-05 01:17:50.504627: val_loss -0.8046\n",
      "2024-01-05 01:17:50.511635: Pseudo dice [0.9202, 0.9297, 0.9383]\n",
      "2024-01-05 01:17:50.518626: Epoch time: 125.22 s\n",
      "2024-01-05 01:17:51.487362: \n",
      "2024-01-05 01:17:51.495870: Epoch 939\n",
      "2024-01-05 01:17:51.502013: Current learning rate: 0.00081\n",
      "2024-01-05 01:19:56.726422: train_loss -0.9271\n",
      "2024-01-05 01:19:56.734426: val_loss -0.7983\n",
      "2024-01-05 01:19:56.743422: Pseudo dice [0.9212, 0.9261, 0.9355]\n",
      "2024-01-05 01:19:56.749430: Epoch time: 125.24 s\n",
      "2024-01-05 01:19:57.684712: \n",
      "2024-01-05 01:19:57.692698: Epoch 940\n",
      "2024-01-05 01:19:57.697713: Current learning rate: 0.00079\n",
      "2024-01-05 01:22:03.100676: train_loss -0.9263\n",
      "2024-01-05 01:22:03.109677: val_loss -0.798\n",
      "2024-01-05 01:22:03.117676: Pseudo dice [0.9235, 0.9266, 0.9361]\n",
      "2024-01-05 01:22:03.124676: Epoch time: 125.42 s\n",
      "2024-01-05 01:22:04.061410: \n",
      "2024-01-05 01:22:04.066714: Epoch 941\n",
      "2024-01-05 01:22:04.071327: Current learning rate: 0.00078\n",
      "2024-01-05 01:24:09.310273: train_loss -0.9278\n",
      "2024-01-05 01:24:09.318265: val_loss -0.7934\n",
      "2024-01-05 01:24:09.325266: Pseudo dice [0.9199, 0.9288, 0.9375]\n",
      "2024-01-05 01:24:09.332268: Epoch time: 125.25 s\n",
      "2024-01-05 01:24:10.260784: \n",
      "2024-01-05 01:24:10.270028: Epoch 942\n",
      "2024-01-05 01:24:10.274604: Current learning rate: 0.00077\n",
      "2024-01-05 01:26:15.298815: train_loss -0.9291\n",
      "2024-01-05 01:26:15.308321: val_loss -0.8016\n",
      "2024-01-05 01:26:15.316321: Pseudo dice [0.9204, 0.9282, 0.9382]\n",
      "2024-01-05 01:26:15.323322: Epoch time: 125.04 s\n",
      "2024-01-05 01:26:16.280206: \n",
      "2024-01-05 01:26:16.288265: Epoch 943\n",
      "2024-01-05 01:26:16.304799: Current learning rate: 0.00076\n",
      "2024-01-05 01:28:21.572772: train_loss -0.9277\n",
      "2024-01-05 01:28:21.580783: val_loss -0.8\n",
      "2024-01-05 01:28:21.588787: Pseudo dice [0.9221, 0.9272, 0.9369]\n",
      "2024-01-05 01:28:21.595776: Epoch time: 125.29 s\n",
      "2024-01-05 01:28:22.562930: \n",
      "2024-01-05 01:28:22.576118: Epoch 944\n",
      "2024-01-05 01:28:22.582117: Current learning rate: 0.00075\n",
      "2024-01-05 01:30:27.577298: train_loss -0.9304\n",
      "2024-01-05 01:30:27.586294: val_loss -0.7963\n",
      "2024-01-05 01:30:27.594292: Pseudo dice [0.9184, 0.9275, 0.9362]\n",
      "2024-01-05 01:30:27.601292: Epoch time: 125.02 s\n",
      "2024-01-05 01:30:28.538318: \n",
      "2024-01-05 01:30:28.545546: Epoch 945\n",
      "2024-01-05 01:30:28.550121: Current learning rate: 0.00074\n",
      "2024-01-05 01:32:33.574337: train_loss -0.9273\n",
      "2024-01-05 01:32:33.582342: val_loss -0.8004\n",
      "2024-01-05 01:32:33.589340: Pseudo dice [0.9232, 0.929, 0.938]\n",
      "2024-01-05 01:32:33.594847: Epoch time: 125.04 s\n",
      "2024-01-05 01:32:34.696075: \n",
      "2024-01-05 01:32:34.702700: Epoch 946\n",
      "2024-01-05 01:32:34.706769: Current learning rate: 0.00072\n",
      "2024-01-05 01:34:39.811178: train_loss -0.9265\n",
      "2024-01-05 01:34:39.819178: val_loss -0.8003\n",
      "2024-01-05 01:34:39.826189: Pseudo dice [0.9216, 0.927, 0.9369]\n",
      "2024-01-05 01:34:39.832189: Epoch time: 125.12 s\n",
      "2024-01-05 01:34:40.757540: \n",
      "2024-01-05 01:34:40.769071: Epoch 947\n",
      "2024-01-05 01:34:40.774142: Current learning rate: 0.00071\n",
      "2024-01-05 01:36:45.732877: train_loss -0.9261\n",
      "2024-01-05 01:36:45.742878: val_loss -0.802\n",
      "2024-01-05 01:36:45.751878: Pseudo dice [0.9204, 0.9286, 0.937]\n",
      "2024-01-05 01:36:45.760277: Epoch time: 124.98 s\n",
      "2024-01-05 01:36:46.682970: \n",
      "2024-01-05 01:36:46.692350: Epoch 948\n",
      "2024-01-05 01:36:46.697073: Current learning rate: 0.0007\n",
      "2024-01-05 01:38:51.566163: train_loss -0.9269\n",
      "2024-01-05 01:38:51.575163: val_loss -0.7927\n",
      "2024-01-05 01:38:51.581172: Pseudo dice [0.9221, 0.9274, 0.9365]\n",
      "2024-01-05 01:38:51.586172: Epoch time: 124.88 s\n",
      "2024-01-05 01:38:52.540276: \n",
      "2024-01-05 01:38:52.551642: Epoch 949\n",
      "2024-01-05 01:38:52.556636: Current learning rate: 0.00069\n",
      "2024-01-05 01:40:57.254535: train_loss -0.9286\n",
      "2024-01-05 01:40:57.264528: val_loss -0.7931\n",
      "2024-01-05 01:40:57.271539: Pseudo dice [0.9193, 0.9254, 0.9344]\n",
      "2024-01-05 01:40:57.277526: Epoch time: 124.72 s\n",
      "2024-01-05 01:40:58.481812: \n",
      "2024-01-05 01:40:58.489494: Epoch 950\n",
      "2024-01-05 01:40:58.494501: Current learning rate: 0.00067\n",
      "2024-01-05 01:43:03.301842: train_loss -0.9277\n",
      "2024-01-05 01:43:03.308848: val_loss -0.7929\n",
      "2024-01-05 01:43:03.315844: Pseudo dice [0.923, 0.9282, 0.9375]\n",
      "2024-01-05 01:43:03.323853: Epoch time: 124.82 s\n",
      "2024-01-05 01:43:04.250848: \n",
      "2024-01-05 01:43:04.262689: Epoch 951\n",
      "2024-01-05 01:43:04.270603: Current learning rate: 0.00066\n",
      "2024-01-05 01:45:08.995592: train_loss -0.9286\n",
      "2024-01-05 01:45:09.005895: val_loss -0.801\n",
      "2024-01-05 01:45:09.012897: Pseudo dice [0.9234, 0.9263, 0.9365]\n",
      "2024-01-05 01:45:09.018895: Epoch time: 124.75 s\n",
      "2024-01-05 01:45:09.941721: \n",
      "2024-01-05 01:45:09.947729: Epoch 952\n",
      "2024-01-05 01:45:09.953248: Current learning rate: 0.00065\n",
      "2024-01-05 01:47:14.960317: train_loss -0.9295\n",
      "2024-01-05 01:47:14.971317: val_loss -0.8042\n",
      "2024-01-05 01:47:14.980068: Pseudo dice [0.9216, 0.9277, 0.9373]\n",
      "2024-01-05 01:47:14.988077: Epoch time: 125.02 s\n",
      "2024-01-05 01:47:16.083228: \n",
      "2024-01-05 01:47:16.089900: Epoch 953\n",
      "2024-01-05 01:47:16.095888: Current learning rate: 0.00064\n",
      "2024-01-05 01:49:21.261594: train_loss -0.9289\n",
      "2024-01-05 01:49:21.272592: val_loss -0.7966\n",
      "2024-01-05 01:49:21.279592: Pseudo dice [0.9203, 0.9257, 0.9351]\n",
      "2024-01-05 01:49:21.284593: Epoch time: 125.18 s\n",
      "2024-01-05 01:49:22.264929: \n",
      "2024-01-05 01:49:22.277352: Epoch 954\n",
      "2024-01-05 01:49:22.282544: Current learning rate: 0.00063\n",
      "2024-01-05 01:51:27.262141: train_loss -0.9287\n",
      "2024-01-05 01:51:27.270142: val_loss -0.8002\n",
      "2024-01-05 01:51:27.278142: Pseudo dice [0.9199, 0.9277, 0.9373]\n",
      "2024-01-05 01:51:27.286142: Epoch time: 125.0 s\n",
      "2024-01-05 01:51:28.238162: \n",
      "2024-01-05 01:51:28.247418: Epoch 955\n",
      "2024-01-05 01:51:28.254421: Current learning rate: 0.00061\n",
      "2024-01-05 01:53:32.915502: train_loss -0.9279\n",
      "2024-01-05 01:53:32.924494: val_loss -0.8035\n",
      "2024-01-05 01:53:32.931493: Pseudo dice [0.9205, 0.9274, 0.9367]\n",
      "2024-01-05 01:53:32.939494: Epoch time: 124.68 s\n",
      "2024-01-05 01:53:33.883625: \n",
      "2024-01-05 01:53:33.889659: Epoch 956\n",
      "2024-01-05 01:53:33.901888: Current learning rate: 0.0006\n",
      "2024-01-05 01:55:38.461725: train_loss -0.9297\n",
      "2024-01-05 01:55:38.470725: val_loss -0.8062\n",
      "2024-01-05 01:55:38.478725: Pseudo dice [0.9233, 0.9287, 0.938]\n",
      "2024-01-05 01:55:38.485726: Epoch time: 124.58 s\n",
      "2024-01-05 01:55:39.432244: \n",
      "2024-01-05 01:55:39.438253: Epoch 957\n",
      "2024-01-05 01:55:39.443253: Current learning rate: 0.00059\n",
      "2024-01-05 01:57:44.170341: train_loss -0.9272\n",
      "2024-01-05 01:57:44.180955: val_loss -0.799\n",
      "2024-01-05 01:57:44.186954: Pseudo dice [0.9192, 0.9278, 0.9376]\n",
      "2024-01-05 01:57:44.193955: Epoch time: 124.74 s\n",
      "2024-01-05 01:57:45.143867: \n",
      "2024-01-05 01:57:45.150912: Epoch 958\n",
      "2024-01-05 01:57:45.155991: Current learning rate: 0.00058\n",
      "2024-01-05 01:59:50.008887: train_loss -0.9269\n",
      "2024-01-05 01:59:50.017886: val_loss -0.7953\n",
      "2024-01-05 01:59:50.025885: Pseudo dice [0.9248, 0.9268, 0.9356]\n",
      "2024-01-05 01:59:50.032893: Epoch time: 124.87 s\n",
      "2024-01-05 01:59:51.021034: \n",
      "2024-01-05 01:59:51.027026: Epoch 959\n",
      "2024-01-05 01:59:51.031034: Current learning rate: 0.00056\n",
      "2024-01-05 02:01:56.083504: train_loss -0.9272\n",
      "2024-01-05 02:01:56.091507: val_loss -0.7984\n",
      "2024-01-05 02:01:56.099503: Pseudo dice [0.9217, 0.9272, 0.9364]\n",
      "2024-01-05 02:01:56.106503: Epoch time: 125.06 s\n",
      "2024-01-05 02:01:57.052960: \n",
      "2024-01-05 02:01:57.061050: Epoch 960\n",
      "2024-01-05 02:01:57.066051: Current learning rate: 0.00055\n",
      "2024-01-05 02:04:02.235073: train_loss -0.9279\n",
      "2024-01-05 02:04:02.244072: val_loss -0.8047\n",
      "2024-01-05 02:04:02.251072: Pseudo dice [0.9209, 0.9283, 0.9374]\n",
      "2024-01-05 02:04:02.258082: Epoch time: 125.18 s\n",
      "2024-01-05 02:04:03.204121: \n",
      "2024-01-05 02:04:03.209851: Epoch 961\n",
      "2024-01-05 02:04:03.214851: Current learning rate: 0.00054\n",
      "2024-01-05 02:06:08.485289: train_loss -0.928\n",
      "2024-01-05 02:06:08.494287: val_loss -0.8024\n",
      "2024-01-05 02:06:08.502290: Pseudo dice [0.9192, 0.9268, 0.9367]\n",
      "2024-01-05 02:06:08.508291: Epoch time: 125.28 s\n",
      "2024-01-05 02:06:09.478302: \n",
      "2024-01-05 02:06:09.487664: Epoch 962\n",
      "2024-01-05 02:06:09.495292: Current learning rate: 0.00053\n",
      "2024-01-05 02:08:14.579571: train_loss -0.9273\n",
      "2024-01-05 02:08:14.587576: val_loss -0.803\n",
      "2024-01-05 02:08:14.594576: Pseudo dice [0.9236, 0.9277, 0.9374]\n",
      "2024-01-05 02:08:14.600089: Epoch time: 125.1 s\n",
      "2024-01-05 02:08:15.549132: \n",
      "2024-01-05 02:08:15.557300: Epoch 963\n",
      "2024-01-05 02:08:15.562368: Current learning rate: 0.00051\n",
      "2024-01-05 02:10:21.038048: train_loss -0.9272\n",
      "2024-01-05 02:10:21.046124: val_loss -0.7998\n",
      "2024-01-05 02:10:21.053125: Pseudo dice [0.9197, 0.9276, 0.9364]\n",
      "2024-01-05 02:10:21.059125: Epoch time: 125.49 s\n",
      "2024-01-05 02:10:22.042551: \n",
      "2024-01-05 02:10:22.052616: Epoch 964\n",
      "2024-01-05 02:10:22.057199: Current learning rate: 0.0005\n",
      "2024-01-05 02:12:27.437960: train_loss -0.9262\n",
      "2024-01-05 02:12:27.447959: val_loss -0.7965\n",
      "2024-01-05 02:12:27.453959: Pseudo dice [0.9228, 0.9279, 0.9369]\n",
      "2024-01-05 02:12:27.460959: Epoch time: 125.4 s\n",
      "2024-01-05 02:12:28.403129: \n",
      "2024-01-05 02:12:28.412285: Epoch 965\n",
      "2024-01-05 02:12:28.416355: Current learning rate: 0.00049\n",
      "2024-01-05 02:14:33.833521: train_loss -0.9268\n",
      "2024-01-05 02:14:33.843530: val_loss -0.7999\n",
      "2024-01-05 02:14:33.850527: Pseudo dice [0.922, 0.9274, 0.9374]\n",
      "2024-01-05 02:14:33.857522: Epoch time: 125.43 s\n",
      "2024-01-05 02:14:34.812437: \n",
      "2024-01-05 02:14:34.818134: Epoch 966\n",
      "2024-01-05 02:14:34.822705: Current learning rate: 0.00048\n",
      "2024-01-05 02:16:40.307291: train_loss -0.928\n",
      "2024-01-05 02:16:40.315291: val_loss -0.8001\n",
      "2024-01-05 02:16:40.321290: Pseudo dice [0.9218, 0.9271, 0.9363]\n",
      "2024-01-05 02:16:40.327293: Epoch time: 125.5 s\n",
      "2024-01-05 02:16:41.278399: \n",
      "2024-01-05 02:16:41.287458: Epoch 967\n",
      "2024-01-05 02:16:41.292209: Current learning rate: 0.00046\n",
      "2024-01-05 02:18:46.301615: train_loss -0.9307\n",
      "2024-01-05 02:18:46.310616: val_loss -0.7997\n",
      "2024-01-05 02:18:46.318617: Pseudo dice [0.9208, 0.9269, 0.9361]\n",
      "2024-01-05 02:18:46.324616: Epoch time: 125.03 s\n",
      "2024-01-05 02:18:47.281005: \n",
      "2024-01-05 02:18:47.290437: Epoch 968\n",
      "2024-01-05 02:18:47.295974: Current learning rate: 0.00045\n",
      "2024-01-05 02:20:52.741552: train_loss -0.9267\n",
      "2024-01-05 02:20:52.753569: val_loss -0.7962\n",
      "2024-01-05 02:20:52.762555: Pseudo dice [0.923, 0.9271, 0.9366]\n",
      "2024-01-05 02:20:52.769555: Epoch time: 125.46 s\n",
      "2024-01-05 02:20:53.914363: \n",
      "2024-01-05 02:20:53.920383: Epoch 969\n",
      "2024-01-05 02:20:53.924434: Current learning rate: 0.00044\n",
      "2024-01-05 02:22:59.088538: train_loss -0.9288\n",
      "2024-01-05 02:22:59.097539: val_loss -0.7901\n",
      "2024-01-05 02:22:59.105540: Pseudo dice [0.9222, 0.9277, 0.9365]\n",
      "2024-01-05 02:22:59.120124: Epoch time: 125.18 s\n",
      "2024-01-05 02:23:00.064508: \n",
      "2024-01-05 02:23:00.070519: Epoch 970\n",
      "2024-01-05 02:23:00.079589: Current learning rate: 0.00043\n",
      "2024-01-05 02:25:05.344425: train_loss -0.9267\n",
      "2024-01-05 02:25:05.354426: val_loss -0.7874\n",
      "2024-01-05 02:25:05.361424: Pseudo dice [0.9236, 0.9268, 0.9362]\n",
      "2024-01-05 02:25:05.367434: Epoch time: 125.28 s\n",
      "2024-01-05 02:25:06.320956: \n",
      "2024-01-05 02:25:06.331607: Epoch 971\n",
      "2024-01-05 02:25:06.335589: Current learning rate: 0.00041\n",
      "2024-01-05 02:27:11.356477: train_loss -0.9294\n",
      "2024-01-05 02:27:11.364476: val_loss -0.8\n",
      "2024-01-05 02:27:11.372483: Pseudo dice [0.9174, 0.9289, 0.9375]\n",
      "2024-01-05 02:27:11.379481: Epoch time: 125.04 s\n",
      "2024-01-05 02:27:12.349249: \n",
      "2024-01-05 02:27:12.359419: Epoch 972\n",
      "2024-01-05 02:27:12.366008: Current learning rate: 0.0004\n",
      "2024-01-05 02:29:17.688751: train_loss -0.9277\n",
      "2024-01-05 02:29:17.696751: val_loss -0.7961\n",
      "2024-01-05 02:29:17.704750: Pseudo dice [0.9251, 0.9272, 0.9371]\n",
      "2024-01-05 02:29:17.711750: Epoch time: 125.34 s\n",
      "2024-01-05 02:29:18.671310: \n",
      "2024-01-05 02:29:18.676921: Epoch 973\n",
      "2024-01-05 02:29:18.681923: Current learning rate: 0.00039\n",
      "2024-01-05 02:31:24.372643: train_loss -0.9278\n",
      "2024-01-05 02:31:24.384164: val_loss -0.7972\n",
      "2024-01-05 02:31:24.395189: Pseudo dice [0.9223, 0.9271, 0.9371]\n",
      "2024-01-05 02:31:24.405188: Epoch time: 125.7 s\n",
      "2024-01-05 02:31:25.475677: \n",
      "2024-01-05 02:31:25.480845: Epoch 974\n",
      "2024-01-05 02:31:25.486035: Current learning rate: 0.00037\n",
      "2024-01-05 02:33:30.716643: train_loss -0.9272\n",
      "2024-01-05 02:33:30.726646: val_loss -0.796\n",
      "2024-01-05 02:33:30.735189: Pseudo dice [0.9218, 0.928, 0.9378]\n",
      "2024-01-05 02:33:30.741189: Epoch time: 125.24 s\n",
      "2024-01-05 02:33:31.698811: \n",
      "2024-01-05 02:33:31.708261: Epoch 975\n",
      "2024-01-05 02:33:31.713276: Current learning rate: 0.00036\n",
      "2024-01-05 02:35:36.739022: train_loss -0.9302\n",
      "2024-01-05 02:35:36.750030: val_loss -0.7983\n",
      "2024-01-05 02:35:36.758023: Pseudo dice [0.9199, 0.9273, 0.9368]\n",
      "2024-01-05 02:35:36.766021: Epoch time: 125.04 s\n",
      "2024-01-05 02:35:37.801446: \n",
      "2024-01-05 02:35:37.809451: Epoch 976\n",
      "2024-01-05 02:35:37.814451: Current learning rate: 0.00035\n",
      "2024-01-05 02:37:42.982575: train_loss -0.9281\n",
      "2024-01-05 02:37:42.990576: val_loss -0.7969\n",
      "2024-01-05 02:37:42.997575: Pseudo dice [0.9186, 0.927, 0.9364]\n",
      "2024-01-05 02:37:43.003576: Epoch time: 125.18 s\n",
      "2024-01-05 02:37:44.136508: \n",
      "2024-01-05 02:37:44.146435: Epoch 977\n",
      "2024-01-05 02:37:44.151323: Current learning rate: 0.00034\n",
      "2024-01-05 02:39:49.342137: train_loss -0.9307\n",
      "2024-01-05 02:39:49.351146: val_loss -0.8016\n",
      "2024-01-05 02:39:49.359143: Pseudo dice [0.9203, 0.9274, 0.9366]\n",
      "2024-01-05 02:39:49.366136: Epoch time: 125.21 s\n",
      "2024-01-05 02:39:50.323039: \n",
      "2024-01-05 02:39:50.336041: Epoch 978\n",
      "2024-01-05 02:39:50.343063: Current learning rate: 0.00032\n",
      "2024-01-05 02:41:55.631853: train_loss -0.9281\n",
      "2024-01-05 02:41:55.641853: val_loss -0.8006\n",
      "2024-01-05 02:41:55.649359: Pseudo dice [0.9196, 0.9278, 0.9372]\n",
      "2024-01-05 02:41:55.656361: Epoch time: 125.31 s\n",
      "2024-01-05 02:41:56.656946: \n",
      "2024-01-05 02:41:56.661946: Epoch 979\n",
      "2024-01-05 02:41:56.666945: Current learning rate: 0.00031\n",
      "2024-01-05 02:44:01.920473: train_loss -0.9283\n",
      "2024-01-05 02:44:01.930980: val_loss -0.8007\n",
      "2024-01-05 02:44:01.938980: Pseudo dice [0.9201, 0.9272, 0.9365]\n",
      "2024-01-05 02:44:01.946980: Epoch time: 125.26 s\n",
      "2024-01-05 02:44:02.904002: \n",
      "2024-01-05 02:44:02.909934: Epoch 980\n",
      "2024-01-05 02:44:02.914467: Current learning rate: 0.0003\n",
      "2024-01-05 02:46:08.306296: train_loss -0.9265\n",
      "2024-01-05 02:46:08.314298: val_loss -0.7977\n",
      "2024-01-05 02:46:08.321296: Pseudo dice [0.9228, 0.927, 0.9363]\n",
      "2024-01-05 02:46:08.327296: Epoch time: 125.4 s\n",
      "2024-01-05 02:46:09.285902: \n",
      "2024-01-05 02:46:09.294043: Epoch 981\n",
      "2024-01-05 02:46:09.299112: Current learning rate: 0.00028\n",
      "2024-01-05 02:48:14.550353: train_loss -0.9279\n",
      "2024-01-05 02:48:14.558353: val_loss -0.7945\n",
      "2024-01-05 02:48:14.567353: Pseudo dice [0.9198, 0.9271, 0.9361]\n",
      "2024-01-05 02:48:14.574353: Epoch time: 125.27 s\n",
      "2024-01-05 02:48:15.538029: \n",
      "2024-01-05 02:48:15.544102: Epoch 982\n",
      "2024-01-05 02:48:15.548167: Current learning rate: 0.00027\n",
      "2024-01-05 02:50:20.976711: train_loss -0.9271\n",
      "2024-01-05 02:50:20.986719: val_loss -0.803\n",
      "2024-01-05 02:50:20.994712: Pseudo dice [0.9208, 0.9269, 0.9375]\n",
      "2024-01-05 02:50:21.002715: Epoch time: 125.44 s\n",
      "2024-01-05 02:50:21.962007: \n",
      "2024-01-05 02:50:21.969950: Epoch 983\n",
      "2024-01-05 02:50:21.975028: Current learning rate: 0.00026\n",
      "2024-01-05 02:52:27.135504: train_loss -0.9302\n",
      "2024-01-05 02:52:27.145504: val_loss -0.7967\n",
      "2024-01-05 02:52:27.154504: Pseudo dice [0.921, 0.9265, 0.936]\n",
      "2024-01-05 02:52:27.162504: Epoch time: 125.17 s\n",
      "2024-01-05 02:52:28.195251: \n",
      "2024-01-05 02:52:28.204226: Epoch 984\n",
      "2024-01-05 02:52:28.209294: Current learning rate: 0.00024\n",
      "2024-01-05 02:54:33.541801: train_loss -0.928\n",
      "2024-01-05 02:54:33.549809: val_loss -0.7967\n",
      "2024-01-05 02:54:33.559314: Pseudo dice [0.9224, 0.9268, 0.9361]\n",
      "2024-01-05 02:54:33.566313: Epoch time: 125.35 s\n",
      "2024-01-05 02:54:34.517188: \n",
      "2024-01-05 02:54:34.526745: Epoch 985\n",
      "2024-01-05 02:54:34.533834: Current learning rate: 0.00023\n",
      "2024-01-05 02:56:39.725077: train_loss -0.9288\n",
      "2024-01-05 02:56:39.734078: val_loss -0.8008\n",
      "2024-01-05 02:56:39.741079: Pseudo dice [0.9206, 0.9272, 0.9371]\n",
      "2024-01-05 02:56:39.747077: Epoch time: 125.21 s\n",
      "2024-01-05 02:56:40.876291: \n",
      "2024-01-05 02:56:40.886844: Epoch 986\n",
      "2024-01-05 02:56:40.891769: Current learning rate: 0.00021\n",
      "2024-01-05 02:58:46.181593: train_loss -0.9303\n",
      "2024-01-05 02:58:46.188593: val_loss -0.7997\n",
      "2024-01-05 02:58:46.194592: Pseudo dice [0.9204, 0.9273, 0.937]\n",
      "2024-01-05 02:58:46.202593: Epoch time: 125.31 s\n",
      "2024-01-05 02:58:47.165883: \n",
      "2024-01-05 02:58:47.174737: Epoch 987\n",
      "2024-01-05 02:58:47.180018: Current learning rate: 0.0002\n",
      "2024-01-05 03:00:52.357740: train_loss -0.929\n",
      "2024-01-05 03:00:52.364813: val_loss -0.8013\n",
      "2024-01-05 03:00:52.372810: Pseudo dice [0.9191, 0.9276, 0.9372]\n",
      "2024-01-05 03:00:52.378741: Epoch time: 125.19 s\n",
      "2024-01-05 03:00:53.342977: \n",
      "2024-01-05 03:00:53.351830: Epoch 988\n",
      "2024-01-05 03:00:53.356838: Current learning rate: 0.00019\n",
      "2024-01-05 03:02:58.603699: train_loss -0.9263\n",
      "2024-01-05 03:02:58.612695: val_loss -0.8015\n",
      "2024-01-05 03:02:58.620690: Pseudo dice [0.9199, 0.927, 0.9367]\n",
      "2024-01-05 03:02:58.627692: Epoch time: 125.26 s\n",
      "2024-01-05 03:02:59.619912: \n",
      "2024-01-05 03:02:59.625960: Epoch 989\n",
      "2024-01-05 03:02:59.629976: Current learning rate: 0.00017\n",
      "2024-01-05 03:05:04.737630: train_loss -0.9304\n",
      "2024-01-05 03:05:04.746631: val_loss -0.8002\n",
      "2024-01-05 03:05:04.755631: Pseudo dice [0.922, 0.9273, 0.937]\n",
      "2024-01-05 03:05:04.761631: Epoch time: 125.12 s\n",
      "2024-01-05 03:05:05.717206: \n",
      "2024-01-05 03:05:05.723270: Epoch 990\n",
      "2024-01-05 03:05:05.728269: Current learning rate: 0.00016\n",
      "2024-01-05 03:07:10.644241: train_loss -0.9309\n",
      "2024-01-05 03:07:10.652252: val_loss -0.7994\n",
      "2024-01-05 03:07:10.659243: Pseudo dice [0.9213, 0.9266, 0.9365]\n",
      "2024-01-05 03:07:10.664252: Epoch time: 124.93 s\n",
      "2024-01-05 03:07:11.617843: \n",
      "2024-01-05 03:07:11.623827: Epoch 991\n",
      "2024-01-05 03:07:11.628842: Current learning rate: 0.00014\n",
      "2024-01-05 03:09:17.000079: train_loss -0.9273\n",
      "2024-01-05 03:09:17.007076: val_loss -0.801\n",
      "2024-01-05 03:09:17.014070: Pseudo dice [0.9195, 0.9282, 0.9367]\n",
      "2024-01-05 03:09:17.020070: Epoch time: 125.38 s\n",
      "2024-01-05 03:09:17.991875: \n",
      "2024-01-05 03:09:18.000102: Epoch 992\n",
      "2024-01-05 03:09:18.006195: Current learning rate: 0.00013\n",
      "2024-01-05 03:11:23.046626: train_loss -0.928\n",
      "2024-01-05 03:11:23.056626: val_loss -0.7998\n",
      "2024-01-05 03:11:23.064626: Pseudo dice [0.92, 0.9281, 0.9376]\n",
      "2024-01-05 03:11:23.073629: Epoch time: 125.06 s\n",
      "2024-01-05 03:11:24.210093: \n",
      "2024-01-05 03:11:24.222282: Epoch 993\n",
      "2024-01-05 03:11:24.227364: Current learning rate: 0.00011\n",
      "2024-01-05 03:13:29.017457: train_loss -0.9284\n",
      "2024-01-05 03:13:29.026459: val_loss -0.7998\n",
      "2024-01-05 03:13:29.034965: Pseudo dice [0.9197, 0.9278, 0.9374]\n",
      "2024-01-05 03:13:29.043962: Epoch time: 124.81 s\n",
      "2024-01-05 03:13:30.019042: \n",
      "2024-01-05 03:13:30.030674: Epoch 994\n",
      "2024-01-05 03:13:30.036771: Current learning rate: 0.0001\n",
      "2024-01-05 03:15:34.812715: train_loss -0.9291\n",
      "2024-01-05 03:15:34.820715: val_loss -0.8045\n",
      "2024-01-05 03:15:34.827717: Pseudo dice [0.9187, 0.9282, 0.938]\n",
      "2024-01-05 03:15:34.834723: Epoch time: 124.79 s\n",
      "2024-01-05 03:15:35.790825: \n",
      "2024-01-05 03:15:35.800083: Epoch 995\n",
      "2024-01-05 03:15:35.805147: Current learning rate: 8e-05\n",
      "2024-01-05 03:17:40.751054: train_loss -0.9295\n",
      "2024-01-05 03:17:40.760062: val_loss -0.7952\n",
      "2024-01-05 03:17:40.767063: Pseudo dice [0.9213, 0.9274, 0.9366]\n",
      "2024-01-05 03:17:40.774055: Epoch time: 124.96 s\n",
      "2024-01-05 03:17:41.726852: \n",
      "2024-01-05 03:17:41.735681: Epoch 996\n",
      "2024-01-05 03:17:41.741719: Current learning rate: 7e-05\n",
      "2024-01-05 03:19:46.861805: train_loss -0.9305\n",
      "2024-01-05 03:19:46.873313: val_loss -0.8008\n",
      "2024-01-05 03:19:46.883312: Pseudo dice [0.9211, 0.9276, 0.9374]\n",
      "2024-01-05 03:19:46.891313: Epoch time: 125.14 s\n",
      "2024-01-05 03:19:47.876969: \n",
      "2024-01-05 03:19:47.887340: Epoch 997\n",
      "2024-01-05 03:19:47.891398: Current learning rate: 5e-05\n",
      "2024-01-05 03:21:53.317706: train_loss -0.9285\n",
      "2024-01-05 03:21:53.326708: val_loss -0.8003\n",
      "2024-01-05 03:21:53.335714: Pseudo dice [0.9205, 0.9275, 0.937]\n",
      "2024-01-05 03:21:53.343716: Epoch time: 125.44 s\n",
      "2024-01-05 03:21:54.292253: \n",
      "2024-01-05 03:21:54.298192: Epoch 998\n",
      "2024-01-05 03:21:54.302786: Current learning rate: 4e-05\n",
      "2024-01-05 03:23:59.243037: train_loss -0.9299\n",
      "2024-01-05 03:23:59.252041: val_loss -0.7947\n",
      "2024-01-05 03:23:59.260037: Pseudo dice [0.9225, 0.927, 0.9364]\n",
      "2024-01-05 03:23:59.267037: Epoch time: 124.95 s\n",
      "2024-01-05 03:24:00.253004: \n",
      "2024-01-05 03:24:00.259079: Epoch 999\n",
      "2024-01-05 03:24:00.264061: Current learning rate: 2e-05\n",
      "2024-01-05 03:26:05.578027: train_loss -0.9281\n",
      "2024-01-05 03:26:05.587026: val_loss -0.7986\n",
      "2024-01-05 03:26:05.593026: Pseudo dice [0.9213, 0.9277, 0.937]\n",
      "2024-01-05 03:26:05.602027: Epoch time: 125.33 s\n",
      "2024-01-05 03:26:06.986693: Training done.\n",
      "2024-01-05 03:26:07.040706: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-05 03:26:07.050237: The split file contains 5 splits.\n",
      "2024-01-05 03:26:07.056246: Desired fold for training: 2\n",
      "2024-01-05 03:26:07.060245: This split has 8 training and 2 validation cases.\n",
      "2024-01-05 03:26:07.065238: predicting case_4\n",
      "2024-01-05 03:26:09.711179: predicting case_6\n",
      "2024-01-05 03:26:19.599077: Validation complete\n",
      "2024-01-05 03:26:19.604084: Mean Validation Dice:  0.9314312112419897\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-05 03:26:20.263901: unpacking dataset...\n",
      "2024-01-05 03:26:20.523758: unpacking done...\n",
      "2024-01-05 03:26:20.528759: do_dummy_2d_data_aug: False\n",
      "2024-01-05 03:26:20.532760: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-05 03:26:20.536760: The split file contains 5 splits.\n",
      "2024-01-05 03:26:20.539760: Desired fold for training: 3\n",
      "2024-01-05 03:26:20.543747: This split has 8 training and 2 validation cases.\n",
      "2024-01-05 03:26:20.572271: Unable to plot network architecture:\n",
      "2024-01-05 03:26:20.576277: No module named 'hiddenlayer'\n",
      "2024-01-05 03:26:20.608398: \n",
      "2024-01-05 03:26:20.613203: Epoch 0\n",
      "2024-01-05 03:26:20.616836: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-05 03:28:37.592910: train_loss -0.328\n",
      "2024-01-05 03:28:37.598910: val_loss -0.5154\n",
      "2024-01-05 03:28:37.606423: Pseudo dice [0.0, 0.9026, 0.8672]\n",
      "2024-01-05 03:28:37.611422: Epoch time: 136.99 s\n",
      "2024-01-05 03:28:37.616422: Yayy! New best EMA pseudo Dice: 0.5899\n",
      "2024-01-05 03:28:38.870944: \n",
      "2024-01-05 03:28:38.878712: Epoch 1\n",
      "2024-01-05 03:28:38.883080: Current learning rate: 0.00999\n",
      "2024-01-05 03:30:43.195448: train_loss -0.6489\n",
      "2024-01-05 03:30:43.201447: val_loss -0.7012\n",
      "2024-01-05 03:30:43.207449: Pseudo dice [0.7675, 0.9245, 0.8921]\n",
      "2024-01-05 03:30:43.212456: Epoch time: 124.33 s\n",
      "2024-01-05 03:30:43.217452: Yayy! New best EMA pseudo Dice: 0.6171\n",
      "2024-01-05 03:30:44.414088: \n",
      "2024-01-05 03:30:44.423478: Epoch 2\n",
      "2024-01-05 03:30:44.442784: Current learning rate: 0.00998\n",
      "2024-01-05 03:32:48.977016: train_loss -0.7458\n",
      "2024-01-05 03:32:48.984017: val_loss -0.7371\n",
      "2024-01-05 03:32:48.989022: Pseudo dice [0.781, 0.9288, 0.9026]\n",
      "2024-01-05 03:32:48.994022: Epoch time: 124.56 s\n",
      "2024-01-05 03:32:49.000022: Yayy! New best EMA pseudo Dice: 0.6424\n",
      "2024-01-05 03:32:50.240364: \n",
      "2024-01-05 03:32:50.247212: Epoch 3\n",
      "2024-01-05 03:32:50.250745: Current learning rate: 0.00997\n",
      "2024-01-05 03:34:54.886692: train_loss -0.7702\n",
      "2024-01-05 03:34:54.894692: val_loss -0.7591\n",
      "2024-01-05 03:34:54.901197: Pseudo dice [0.8025, 0.9364, 0.9096]\n",
      "2024-01-05 03:34:54.906197: Epoch time: 124.65 s\n",
      "2024-01-05 03:34:54.912197: Yayy! New best EMA pseudo Dice: 0.6665\n",
      "2024-01-05 03:34:56.111326: \n",
      "2024-01-05 03:34:56.120142: Epoch 4\n",
      "2024-01-05 03:34:56.123171: Current learning rate: 0.00996\n",
      "2024-01-05 03:37:00.519324: train_loss -0.791\n",
      "2024-01-05 03:37:00.528331: val_loss -0.7776\n",
      "2024-01-05 03:37:00.535330: Pseudo dice [0.8261, 0.9409, 0.915]\n",
      "2024-01-05 03:37:00.540327: Epoch time: 124.41 s\n",
      "2024-01-05 03:37:00.545324: Yayy! New best EMA pseudo Dice: 0.6892\n",
      "2024-01-05 03:37:01.772491: \n",
      "2024-01-05 03:37:01.777501: Epoch 5\n",
      "2024-01-05 03:37:01.782501: Current learning rate: 0.00995\n",
      "2024-01-05 03:39:05.847425: train_loss -0.8032\n",
      "2024-01-05 03:39:05.856936: val_loss -0.7798\n",
      "2024-01-05 03:39:05.864934: Pseudo dice [0.836, 0.9395, 0.9152]\n",
      "2024-01-05 03:39:05.869933: Epoch time: 124.08 s\n",
      "2024-01-05 03:39:05.875932: Yayy! New best EMA pseudo Dice: 0.71\n",
      "2024-01-05 03:39:07.124247: \n",
      "2024-01-05 03:39:07.131908: Epoch 6\n",
      "2024-01-05 03:39:07.136993: Current learning rate: 0.00995\n",
      "2024-01-05 03:41:11.077299: train_loss -0.8083\n",
      "2024-01-05 03:41:11.085300: val_loss -0.7936\n",
      "2024-01-05 03:41:11.093309: Pseudo dice [0.8357, 0.9445, 0.922]\n",
      "2024-01-05 03:41:11.098312: Epoch time: 123.95 s\n",
      "2024-01-05 03:41:11.103834: Yayy! New best EMA pseudo Dice: 0.7291\n",
      "2024-01-05 03:41:12.291907: \n",
      "2024-01-05 03:41:12.300852: Epoch 7\n",
      "2024-01-05 03:41:12.304492: Current learning rate: 0.00994\n",
      "2024-01-05 03:43:16.407056: train_loss -0.8156\n",
      "2024-01-05 03:43:16.414055: val_loss -0.804\n",
      "2024-01-05 03:43:16.422053: Pseudo dice [0.8387, 0.9484, 0.9267]\n",
      "2024-01-05 03:43:16.429053: Epoch time: 124.12 s\n",
      "2024-01-05 03:43:16.436056: Yayy! New best EMA pseudo Dice: 0.7466\n",
      "2024-01-05 03:43:17.817929: \n",
      "2024-01-05 03:43:17.824926: Epoch 8\n",
      "2024-01-05 03:43:17.830013: Current learning rate: 0.00993\n",
      "2024-01-05 03:45:22.175153: train_loss -0.8081\n",
      "2024-01-05 03:45:22.184154: val_loss -0.7901\n",
      "2024-01-05 03:45:22.193159: Pseudo dice [0.834, 0.9432, 0.9211]\n",
      "2024-01-05 03:45:22.201673: Epoch time: 124.36 s\n",
      "2024-01-05 03:45:22.208674: Yayy! New best EMA pseudo Dice: 0.7619\n",
      "2024-01-05 03:45:23.475621: \n",
      "2024-01-05 03:45:23.479685: Epoch 9\n",
      "2024-01-05 03:45:23.483689: Current learning rate: 0.00992\n",
      "2024-01-05 03:47:27.429214: train_loss -0.8158\n",
      "2024-01-05 03:47:27.437215: val_loss -0.799\n",
      "2024-01-05 03:47:27.444222: Pseudo dice [0.8391, 0.9457, 0.9235]\n",
      "2024-01-05 03:47:27.450219: Epoch time: 123.95 s\n",
      "2024-01-05 03:47:27.455219: Yayy! New best EMA pseudo Dice: 0.776\n",
      "2024-01-05 03:47:28.645494: \n",
      "2024-01-05 03:47:28.653762: Epoch 10\n",
      "2024-01-05 03:47:28.659271: Current learning rate: 0.00991\n",
      "2024-01-05 03:49:32.992168: train_loss -0.8236\n",
      "2024-01-05 03:49:33.000170: val_loss -0.8037\n",
      "2024-01-05 03:49:33.007181: Pseudo dice [0.8432, 0.9453, 0.9232]\n",
      "2024-01-05 03:49:33.012181: Epoch time: 124.35 s\n",
      "2024-01-05 03:49:33.016182: Yayy! New best EMA pseudo Dice: 0.7888\n",
      "2024-01-05 03:49:34.232526: \n",
      "2024-01-05 03:49:34.243123: Epoch 11\n",
      "2024-01-05 03:49:34.251195: Current learning rate: 0.0099\n",
      "2024-01-05 03:51:38.740772: train_loss -0.825\n",
      "2024-01-05 03:51:38.749765: val_loss -0.7955\n",
      "2024-01-05 03:51:38.756772: Pseudo dice [0.8433, 0.9424, 0.9199]\n",
      "2024-01-05 03:51:38.763769: Epoch time: 124.51 s\n",
      "2024-01-05 03:51:38.770284: Yayy! New best EMA pseudo Dice: 0.8001\n",
      "2024-01-05 03:51:39.985893: \n",
      "2024-01-05 03:51:39.993956: Epoch 12\n",
      "2024-01-05 03:51:40.002882: Current learning rate: 0.00989\n",
      "2024-01-05 03:53:44.588362: train_loss -0.8287\n",
      "2024-01-05 03:53:44.596360: val_loss -0.8054\n",
      "2024-01-05 03:53:44.602363: Pseudo dice [0.8423, 0.9463, 0.9267]\n",
      "2024-01-05 03:53:44.607367: Epoch time: 124.6 s\n",
      "2024-01-05 03:53:44.612371: Yayy! New best EMA pseudo Dice: 0.8106\n",
      "2024-01-05 03:53:45.812556: \n",
      "2024-01-05 03:53:45.821053: Epoch 13\n",
      "2024-01-05 03:53:45.825042: Current learning rate: 0.00988\n",
      "2024-01-05 03:55:50.349858: train_loss -0.8305\n",
      "2024-01-05 03:55:50.356856: val_loss -0.8047\n",
      "2024-01-05 03:55:50.363856: Pseudo dice [0.8411, 0.9461, 0.9244]\n",
      "2024-01-05 03:55:50.370857: Epoch time: 124.54 s\n",
      "2024-01-05 03:55:50.376855: Yayy! New best EMA pseudo Dice: 0.8199\n",
      "2024-01-05 03:55:51.581234: \n",
      "2024-01-05 03:55:51.588012: Epoch 14\n",
      "2024-01-05 03:55:51.592000: Current learning rate: 0.00987\n",
      "2024-01-05 03:57:56.361111: train_loss -0.8334\n",
      "2024-01-05 03:57:56.369110: val_loss -0.8148\n",
      "2024-01-05 03:57:56.377114: Pseudo dice [0.8475, 0.9491, 0.9278]\n",
      "2024-01-05 03:57:56.384114: Epoch time: 124.78 s\n",
      "2024-01-05 03:57:56.390619: Yayy! New best EMA pseudo Dice: 0.8287\n",
      "2024-01-05 03:57:57.599689: \n",
      "2024-01-05 03:57:57.608205: Epoch 15\n",
      "2024-01-05 03:57:57.612979: Current learning rate: 0.00986\n",
      "2024-01-05 04:00:02.131763: train_loss -0.8393\n",
      "2024-01-05 04:00:02.141279: val_loss -0.8047\n",
      "2024-01-05 04:00:02.148277: Pseudo dice [0.8456, 0.9448, 0.9237]\n",
      "2024-01-05 04:00:02.154134: Epoch time: 124.53 s\n",
      "2024-01-05 04:00:02.160134: Yayy! New best EMA pseudo Dice: 0.8363\n",
      "2024-01-05 04:00:03.581929: \n",
      "2024-01-05 04:00:03.593698: Epoch 16\n",
      "2024-01-05 04:00:03.598099: Current learning rate: 0.00986\n",
      "2024-01-05 04:02:08.153251: train_loss -0.837\n",
      "2024-01-05 04:02:08.161247: val_loss -0.8089\n",
      "2024-01-05 04:02:08.168250: Pseudo dice [0.8446, 0.9477, 0.926]\n",
      "2024-01-05 04:02:08.174248: Epoch time: 124.57 s\n",
      "2024-01-05 04:02:08.182247: Yayy! New best EMA pseudo Dice: 0.8433\n",
      "2024-01-05 04:02:09.407640: \n",
      "2024-01-05 04:02:09.416496: Epoch 17\n",
      "2024-01-05 04:02:09.420650: Current learning rate: 0.00985\n",
      "2024-01-05 04:04:13.932350: train_loss -0.8411\n",
      "2024-01-05 04:04:13.939349: val_loss -0.813\n",
      "2024-01-05 04:04:13.946401: Pseudo dice [0.8528, 0.9486, 0.9267]\n",
      "2024-01-05 04:04:13.953401: Epoch time: 124.53 s\n",
      "2024-01-05 04:04:13.958399: Yayy! New best EMA pseudo Dice: 0.8499\n",
      "2024-01-05 04:04:15.176031: \n",
      "2024-01-05 04:04:15.184021: Epoch 18\n",
      "2024-01-05 04:04:15.191742: Current learning rate: 0.00984\n",
      "2024-01-05 04:06:19.653259: train_loss -0.8434\n",
      "2024-01-05 04:06:19.662255: val_loss -0.8097\n",
      "2024-01-05 04:06:19.668764: Pseudo dice [0.8507, 0.9479, 0.9251]\n",
      "2024-01-05 04:06:19.674764: Epoch time: 124.48 s\n",
      "2024-01-05 04:06:19.679764: Yayy! New best EMA pseudo Dice: 0.8557\n",
      "2024-01-05 04:06:20.904025: \n",
      "2024-01-05 04:06:20.912975: Epoch 19\n",
      "2024-01-05 04:06:20.920205: Current learning rate: 0.00983\n",
      "2024-01-05 04:08:25.177233: train_loss -0.8425\n",
      "2024-01-05 04:08:25.186235: val_loss -0.8136\n",
      "2024-01-05 04:08:25.193234: Pseudo dice [0.854, 0.9476, 0.9252]\n",
      "2024-01-05 04:08:25.201242: Epoch time: 124.27 s\n",
      "2024-01-05 04:08:25.206242: Yayy! New best EMA pseudo Dice: 0.861\n",
      "2024-01-05 04:08:26.415230: \n",
      "2024-01-05 04:08:26.424352: Epoch 20\n",
      "2024-01-05 04:08:26.432357: Current learning rate: 0.00982\n",
      "2024-01-05 04:10:31.038839: train_loss -0.8458\n",
      "2024-01-05 04:10:31.045838: val_loss -0.8122\n",
      "2024-01-05 04:10:31.052864: Pseudo dice [0.8531, 0.948, 0.9275]\n",
      "2024-01-05 04:10:31.057861: Epoch time: 124.63 s\n",
      "2024-01-05 04:10:31.062866: Yayy! New best EMA pseudo Dice: 0.8659\n",
      "2024-01-05 04:10:32.315332: \n",
      "2024-01-05 04:10:32.321714: Epoch 21\n",
      "2024-01-05 04:10:32.326787: Current learning rate: 0.00981\n",
      "2024-01-05 04:12:37.041404: train_loss -0.8457\n",
      "2024-01-05 04:12:37.050401: val_loss -0.8119\n",
      "2024-01-05 04:12:37.057401: Pseudo dice [0.8516, 0.9476, 0.9249]\n",
      "2024-01-05 04:12:37.063402: Epoch time: 124.73 s\n",
      "2024-01-05 04:12:37.070838: Yayy! New best EMA pseudo Dice: 0.8701\n",
      "2024-01-05 04:12:38.223892: \n",
      "2024-01-05 04:12:38.231851: Epoch 22\n",
      "2024-01-05 04:12:38.236876: Current learning rate: 0.0098\n",
      "2024-01-05 04:14:42.780279: train_loss -0.8494\n",
      "2024-01-05 04:14:42.788282: val_loss -0.8179\n",
      "2024-01-05 04:14:42.793282: Pseudo dice [0.8565, 0.9501, 0.928]\n",
      "2024-01-05 04:14:42.798282: Epoch time: 124.56 s\n",
      "2024-01-05 04:14:42.803282: Yayy! New best EMA pseudo Dice: 0.8742\n",
      "2024-01-05 04:14:43.964466: \n",
      "2024-01-05 04:14:43.970312: Epoch 23\n",
      "2024-01-05 04:14:43.974670: Current learning rate: 0.00979\n",
      "2024-01-05 04:16:48.773572: train_loss -0.848\n",
      "2024-01-05 04:16:48.780576: val_loss -0.8171\n",
      "2024-01-05 04:16:48.786576: Pseudo dice [0.8541, 0.9496, 0.9273]\n",
      "2024-01-05 04:16:48.794573: Epoch time: 124.81 s\n",
      "2024-01-05 04:16:48.800571: Yayy! New best EMA pseudo Dice: 0.8779\n",
      "2024-01-05 04:16:50.116013: \n",
      "2024-01-05 04:16:50.125292: Epoch 24\n",
      "2024-01-05 04:16:50.129799: Current learning rate: 0.00978\n",
      "2024-01-05 04:18:54.663120: train_loss -0.8514\n",
      "2024-01-05 04:18:54.671120: val_loss -0.8209\n",
      "2024-01-05 04:18:54.680119: Pseudo dice [0.8605, 0.9502, 0.9293]\n",
      "2024-01-05 04:18:54.686128: Epoch time: 124.55 s\n",
      "2024-01-05 04:18:54.694132: Yayy! New best EMA pseudo Dice: 0.8814\n",
      "2024-01-05 04:18:55.904542: \n",
      "2024-01-05 04:18:55.914377: Epoch 25\n",
      "2024-01-05 04:18:55.918433: Current learning rate: 0.00977\n",
      "2024-01-05 04:21:00.576059: train_loss -0.8526\n",
      "2024-01-05 04:21:00.586060: val_loss -0.8144\n",
      "2024-01-05 04:21:00.594059: Pseudo dice [0.8566, 0.9487, 0.9263]\n",
      "2024-01-05 04:21:00.602057: Epoch time: 124.67 s\n",
      "2024-01-05 04:21:00.608058: Yayy! New best EMA pseudo Dice: 0.8843\n",
      "2024-01-05 04:21:01.833341: \n",
      "2024-01-05 04:21:01.841030: Epoch 26\n",
      "2024-01-05 04:21:01.847111: Current learning rate: 0.00977\n",
      "2024-01-05 04:23:06.393780: train_loss -0.8537\n",
      "2024-01-05 04:23:06.401777: val_loss -0.8172\n",
      "2024-01-05 04:23:06.409777: Pseudo dice [0.8541, 0.9494, 0.9279]\n",
      "2024-01-05 04:23:06.416783: Epoch time: 124.56 s\n",
      "2024-01-05 04:23:06.421779: Yayy! New best EMA pseudo Dice: 0.8869\n",
      "2024-01-05 04:23:07.576982: \n",
      "2024-01-05 04:23:07.583364: Epoch 27\n",
      "2024-01-05 04:23:07.591324: Current learning rate: 0.00976\n",
      "2024-01-05 04:25:12.030502: train_loss -0.8563\n",
      "2024-01-05 04:25:12.039503: val_loss -0.8217\n",
      "2024-01-05 04:25:12.047501: Pseudo dice [0.8537, 0.9503, 0.9297]\n",
      "2024-01-05 04:25:12.053502: Epoch time: 124.45 s\n",
      "2024-01-05 04:25:12.059503: Yayy! New best EMA pseudo Dice: 0.8894\n",
      "2024-01-05 04:25:13.242615: \n",
      "2024-01-05 04:25:13.252478: Epoch 28\n",
      "2024-01-05 04:25:13.256449: Current learning rate: 0.00975\n",
      "2024-01-05 04:27:17.616210: train_loss -0.858\n",
      "2024-01-05 04:27:17.625207: val_loss -0.8172\n",
      "2024-01-05 04:27:17.633205: Pseudo dice [0.8538, 0.9488, 0.9262]\n",
      "2024-01-05 04:27:17.640206: Epoch time: 124.37 s\n",
      "2024-01-05 04:27:17.646207: Yayy! New best EMA pseudo Dice: 0.8914\n",
      "2024-01-05 04:27:18.815887: \n",
      "2024-01-05 04:27:18.820951: Epoch 29\n",
      "2024-01-05 04:27:18.825961: Current learning rate: 0.00974\n",
      "2024-01-05 04:29:23.283344: train_loss -0.8572\n",
      "2024-01-05 04:29:23.295347: val_loss -0.8142\n",
      "2024-01-05 04:29:23.304343: Pseudo dice [0.8534, 0.9492, 0.9261]\n",
      "2024-01-05 04:29:23.311345: Epoch time: 124.47 s\n",
      "2024-01-05 04:29:23.317357: Yayy! New best EMA pseudo Dice: 0.8932\n",
      "2024-01-05 04:29:24.494071: \n",
      "2024-01-05 04:29:24.503451: Epoch 30\n",
      "2024-01-05 04:29:24.507377: Current learning rate: 0.00973\n",
      "2024-01-05 04:31:29.407336: train_loss -0.8563\n",
      "2024-01-05 04:31:29.416336: val_loss -0.8287\n",
      "2024-01-05 04:31:29.423336: Pseudo dice [0.8658, 0.9528, 0.9329]\n",
      "2024-01-05 04:31:29.428351: Epoch time: 124.91 s\n",
      "2024-01-05 04:31:29.433772: Yayy! New best EMA pseudo Dice: 0.8956\n",
      "2024-01-05 04:31:30.837894: \n",
      "2024-01-05 04:31:30.843464: Epoch 31\n",
      "2024-01-05 04:31:30.850463: Current learning rate: 0.00972\n",
      "2024-01-05 04:33:35.186612: train_loss -0.8581\n",
      "2024-01-05 04:33:35.194131: val_loss -0.8254\n",
      "2024-01-05 04:33:35.200130: Pseudo dice [0.8574, 0.9519, 0.9313]\n",
      "2024-01-05 04:33:35.205129: Epoch time: 124.35 s\n",
      "2024-01-05 04:33:35.210128: Yayy! New best EMA pseudo Dice: 0.8974\n",
      "2024-01-05 04:33:36.403172: \n",
      "2024-01-05 04:33:36.408180: Epoch 32\n",
      "2024-01-05 04:33:36.412180: Current learning rate: 0.00971\n",
      "2024-01-05 04:35:41.078631: train_loss -0.8596\n",
      "2024-01-05 04:35:41.087630: val_loss -0.8213\n",
      "2024-01-05 04:35:41.095639: Pseudo dice [0.8557, 0.95, 0.9294]\n",
      "2024-01-05 04:35:41.102628: Epoch time: 124.68 s\n",
      "2024-01-05 04:35:41.109633: Yayy! New best EMA pseudo Dice: 0.8988\n",
      "2024-01-05 04:35:42.291094: \n",
      "2024-01-05 04:35:42.304389: Epoch 33\n",
      "2024-01-05 04:35:42.309479: Current learning rate: 0.0097\n",
      "2024-01-05 04:37:46.974716: train_loss -0.8585\n",
      "2024-01-05 04:37:46.983717: val_loss -0.8254\n",
      "2024-01-05 04:37:46.991221: Pseudo dice [0.8569, 0.9522, 0.9313]\n",
      "2024-01-05 04:37:46.999223: Epoch time: 124.68 s\n",
      "2024-01-05 04:37:47.005221: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2024-01-05 04:37:48.201252: \n",
      "2024-01-05 04:37:48.209045: Epoch 34\n",
      "2024-01-05 04:37:48.214125: Current learning rate: 0.00969\n",
      "2024-01-05 04:39:52.815722: train_loss -0.8591\n",
      "2024-01-05 04:39:52.824723: val_loss -0.8226\n",
      "2024-01-05 04:39:52.832724: Pseudo dice [0.8569, 0.9513, 0.9292]\n",
      "2024-01-05 04:39:52.840230: Epoch time: 124.62 s\n",
      "2024-01-05 04:39:52.847231: Yayy! New best EMA pseudo Dice: 0.9015\n",
      "2024-01-05 04:39:54.053226: \n",
      "2024-01-05 04:39:54.061764: Epoch 35\n",
      "2024-01-05 04:39:54.068715: Current learning rate: 0.00968\n",
      "2024-01-05 04:41:58.708522: train_loss -0.8612\n",
      "2024-01-05 04:41:58.717523: val_loss -0.812\n",
      "2024-01-05 04:41:58.723523: Pseudo dice [0.8532, 0.948, 0.9227]\n",
      "2024-01-05 04:41:58.729524: Epoch time: 124.66 s\n",
      "2024-01-05 04:41:58.736524: Yayy! New best EMA pseudo Dice: 0.9021\n",
      "2024-01-05 04:41:59.977053: \n",
      "2024-01-05 04:41:59.986256: Epoch 36\n",
      "2024-01-05 04:41:59.990320: Current learning rate: 0.00968\n",
      "2024-01-05 04:44:05.239127: train_loss -0.859\n",
      "2024-01-05 04:44:05.251130: val_loss -0.8202\n",
      "2024-01-05 04:44:05.260134: Pseudo dice [0.8563, 0.9509, 0.9294]\n",
      "2024-01-05 04:44:05.268133: Epoch time: 125.26 s\n",
      "2024-01-05 04:44:05.274647: Yayy! New best EMA pseudo Dice: 0.9032\n",
      "2024-01-05 04:44:06.484738: \n",
      "2024-01-05 04:44:06.496454: Epoch 37\n",
      "2024-01-05 04:44:06.504436: Current learning rate: 0.00967\n",
      "2024-01-05 04:46:11.621841: train_loss -0.8631\n",
      "2024-01-05 04:46:11.634352: val_loss -0.813\n",
      "2024-01-05 04:46:11.640353: Pseudo dice [0.8513, 0.9504, 0.928]\n",
      "2024-01-05 04:46:11.646353: Epoch time: 125.14 s\n",
      "2024-01-05 04:46:11.652357: Yayy! New best EMA pseudo Dice: 0.9038\n",
      "2024-01-05 04:46:12.917294: \n",
      "2024-01-05 04:46:12.925266: Epoch 38\n",
      "2024-01-05 04:46:12.932974: Current learning rate: 0.00966\n",
      "2024-01-05 04:48:17.687336: train_loss -0.861\n",
      "2024-01-05 04:48:17.694843: val_loss -0.8246\n",
      "2024-01-05 04:48:17.701843: Pseudo dice [0.8583, 0.9519, 0.93]\n",
      "2024-01-05 04:48:17.707851: Epoch time: 124.77 s\n",
      "2024-01-05 04:48:17.716842: Yayy! New best EMA pseudo Dice: 0.9048\n",
      "2024-01-05 04:48:19.118625: \n",
      "2024-01-05 04:48:19.128207: Epoch 39\n",
      "2024-01-05 04:48:19.132845: Current learning rate: 0.00965\n",
      "2024-01-05 04:50:23.818961: train_loss -0.8651\n",
      "2024-01-05 04:50:23.826962: val_loss -0.8271\n",
      "2024-01-05 04:50:23.831963: Pseudo dice [0.8628, 0.9523, 0.9313]\n",
      "2024-01-05 04:50:23.837963: Epoch time: 124.7 s\n",
      "2024-01-05 04:50:23.844470: Yayy! New best EMA pseudo Dice: 0.9059\n",
      "2024-01-05 04:50:25.080424: \n",
      "2024-01-05 04:50:25.090424: Epoch 40\n",
      "2024-01-05 04:50:25.097424: Current learning rate: 0.00964\n",
      "2024-01-05 04:52:29.521156: train_loss -0.8649\n",
      "2024-01-05 04:52:29.529154: val_loss -0.8237\n",
      "2024-01-05 04:52:29.534153: Pseudo dice [0.855, 0.9528, 0.9308]\n",
      "2024-01-05 04:52:29.540155: Epoch time: 124.44 s\n",
      "2024-01-05 04:52:29.546155: Yayy! New best EMA pseudo Dice: 0.9066\n",
      "2024-01-05 04:52:30.791473: \n",
      "2024-01-05 04:52:30.797117: Epoch 41\n",
      "2024-01-05 04:52:30.802051: Current learning rate: 0.00963\n",
      "2024-01-05 04:54:35.103565: train_loss -0.8676\n",
      "2024-01-05 04:54:35.110565: val_loss -0.8295\n",
      "2024-01-05 04:54:35.117572: Pseudo dice [0.8555, 0.9549, 0.9345]\n",
      "2024-01-05 04:54:35.125564: Epoch time: 124.31 s\n",
      "2024-01-05 04:54:35.131567: Yayy! New best EMA pseudo Dice: 0.9074\n",
      "2024-01-05 04:54:36.294020: \n",
      "2024-01-05 04:54:36.301450: Epoch 42\n",
      "2024-01-05 04:54:36.306512: Current learning rate: 0.00962\n",
      "2024-01-05 04:56:41.063642: train_loss -0.867\n",
      "2024-01-05 04:56:41.071632: val_loss -0.8262\n",
      "2024-01-05 04:56:41.078636: Pseudo dice [0.854, 0.9531, 0.9323]\n",
      "2024-01-05 04:56:41.085631: Epoch time: 124.77 s\n",
      "2024-01-05 04:56:41.093635: Yayy! New best EMA pseudo Dice: 0.908\n",
      "2024-01-05 04:56:42.256714: \n",
      "2024-01-05 04:56:42.264205: Epoch 43\n",
      "2024-01-05 04:56:42.269078: Current learning rate: 0.00961\n",
      "2024-01-05 04:58:47.192484: train_loss -0.8675\n",
      "2024-01-05 04:58:47.200486: val_loss -0.8298\n",
      "2024-01-05 04:58:47.207490: Pseudo dice [0.8636, 0.9539, 0.9331]\n",
      "2024-01-05 04:58:47.214497: Epoch time: 124.94 s\n",
      "2024-01-05 04:58:47.220489: Yayy! New best EMA pseudo Dice: 0.9089\n",
      "2024-01-05 04:58:48.399926: \n",
      "2024-01-05 04:58:48.405926: Epoch 44\n",
      "2024-01-05 04:58:48.410002: Current learning rate: 0.0096\n",
      "2024-01-05 05:00:53.399959: train_loss -0.8699\n",
      "2024-01-05 05:00:53.406969: val_loss -0.8271\n",
      "2024-01-05 05:00:53.412968: Pseudo dice [0.8584, 0.9537, 0.9333]\n",
      "2024-01-05 05:00:53.417966: Epoch time: 125.0 s\n",
      "2024-01-05 05:00:53.423473: Yayy! New best EMA pseudo Dice: 0.9095\n",
      "2024-01-05 05:00:54.589418: \n",
      "2024-01-05 05:00:54.601720: Epoch 45\n",
      "2024-01-05 05:00:54.611874: Current learning rate: 0.00959\n",
      "2024-01-05 05:02:59.809279: train_loss -0.8692\n",
      "2024-01-05 05:02:59.815282: val_loss -0.8208\n",
      "2024-01-05 05:02:59.823283: Pseudo dice [0.8567, 0.9509, 0.9288]\n",
      "2024-01-05 05:02:59.830279: Epoch time: 125.22 s\n",
      "2024-01-05 05:02:59.836280: Yayy! New best EMA pseudo Dice: 0.9098\n",
      "2024-01-05 05:03:01.223108: \n",
      "2024-01-05 05:03:01.228226: Epoch 46\n",
      "2024-01-05 05:03:01.232233: Current learning rate: 0.00959\n",
      "2024-01-05 05:05:05.520112: train_loss -0.8738\n",
      "2024-01-05 05:05:05.528113: val_loss -0.8289\n",
      "2024-01-05 05:05:05.536113: Pseudo dice [0.8588, 0.9537, 0.9331]\n",
      "2024-01-05 05:05:05.541621: Epoch time: 124.3 s\n",
      "2024-01-05 05:05:05.547620: Yayy! New best EMA pseudo Dice: 0.9103\n",
      "2024-01-05 05:05:06.707101: \n",
      "2024-01-05 05:05:06.712812: Epoch 47\n",
      "2024-01-05 05:05:06.716745: Current learning rate: 0.00958\n",
      "2024-01-05 05:07:11.225949: train_loss -0.8732\n",
      "2024-01-05 05:07:11.233949: val_loss -0.8272\n",
      "2024-01-05 05:07:11.240951: Pseudo dice [0.8592, 0.9534, 0.9331]\n",
      "2024-01-05 05:07:11.245951: Epoch time: 124.52 s\n",
      "2024-01-05 05:07:11.250952: Yayy! New best EMA pseudo Dice: 0.9108\n",
      "2024-01-05 05:07:12.451405: \n",
      "2024-01-05 05:07:12.459927: Epoch 48\n",
      "2024-01-05 05:07:12.464995: Current learning rate: 0.00957\n",
      "2024-01-05 05:09:16.856433: train_loss -0.8749\n",
      "2024-01-05 05:09:16.864432: val_loss -0.8303\n",
      "2024-01-05 05:09:16.871432: Pseudo dice [0.8585, 0.9544, 0.9343]\n",
      "2024-01-05 05:09:16.878619: Epoch time: 124.41 s\n",
      "2024-01-05 05:09:16.885619: Yayy! New best EMA pseudo Dice: 0.9113\n",
      "2024-01-05 05:09:18.067327: \n",
      "2024-01-05 05:09:18.075180: Epoch 49\n",
      "2024-01-05 05:09:18.083035: Current learning rate: 0.00956\n",
      "2024-01-05 05:11:22.989859: train_loss -0.8741\n",
      "2024-01-05 05:11:22.998864: val_loss -0.8275\n",
      "2024-01-05 05:11:23.005863: Pseudo dice [0.8567, 0.9527, 0.9308]\n",
      "2024-01-05 05:11:23.011863: Epoch time: 124.92 s\n",
      "2024-01-05 05:11:23.292105: Yayy! New best EMA pseudo Dice: 0.9115\n",
      "2024-01-05 05:11:24.403255: \n",
      "2024-01-05 05:11:24.408694: Epoch 50\n",
      "2024-01-05 05:11:24.413095: Current learning rate: 0.00955\n",
      "2024-01-05 05:13:28.933979: train_loss -0.877\n",
      "2024-01-05 05:13:28.943024: val_loss -0.8263\n",
      "2024-01-05 05:13:28.948030: Pseudo dice [0.8567, 0.9536, 0.9333]\n",
      "2024-01-05 05:13:28.953030: Epoch time: 124.53 s\n",
      "2024-01-05 05:13:28.957551: Yayy! New best EMA pseudo Dice: 0.9118\n",
      "2024-01-05 05:13:30.142539: \n",
      "2024-01-05 05:13:30.154046: Epoch 51\n",
      "2024-01-05 05:13:30.157588: Current learning rate: 0.00954\n",
      "2024-01-05 05:15:34.322782: train_loss -0.8785\n",
      "2024-01-05 05:15:34.329783: val_loss -0.8302\n",
      "2024-01-05 05:15:34.337782: Pseudo dice [0.8577, 0.9541, 0.9341]\n",
      "2024-01-05 05:15:34.345783: Epoch time: 124.18 s\n",
      "2024-01-05 05:15:34.352782: Yayy! New best EMA pseudo Dice: 0.9122\n",
      "2024-01-05 05:15:35.542561: \n",
      "2024-01-05 05:15:35.553533: Epoch 52\n",
      "2024-01-05 05:15:35.574263: Current learning rate: 0.00953\n",
      "2024-01-05 05:17:39.946513: train_loss -0.8799\n",
      "2024-01-05 05:17:39.954329: val_loss -0.8274\n",
      "2024-01-05 05:17:39.962336: Pseudo dice [0.859, 0.9537, 0.9334]\n",
      "2024-01-05 05:17:39.969342: Epoch time: 124.4 s\n",
      "2024-01-05 05:17:39.975335: Yayy! New best EMA pseudo Dice: 0.9125\n",
      "2024-01-05 05:17:41.171911: \n",
      "2024-01-05 05:17:41.180432: Epoch 53\n",
      "2024-01-05 05:17:41.184460: Current learning rate: 0.00952\n",
      "2024-01-05 05:19:45.938870: train_loss -0.8765\n",
      "2024-01-05 05:19:45.945877: val_loss -0.8261\n",
      "2024-01-05 05:19:45.951881: Pseudo dice [0.8614, 0.9527, 0.9333]\n",
      "2024-01-05 05:19:45.959876: Epoch time: 124.77 s\n",
      "2024-01-05 05:19:45.966877: Yayy! New best EMA pseudo Dice: 0.9128\n",
      "2024-01-05 05:19:47.312972: \n",
      "2024-01-05 05:19:47.320383: Epoch 54\n",
      "2024-01-05 05:19:47.329443: Current learning rate: 0.00951\n",
      "2024-01-05 05:21:52.154135: train_loss -0.8765\n",
      "2024-01-05 05:21:52.163135: val_loss -0.8302\n",
      "2024-01-05 05:21:52.169136: Pseudo dice [0.8605, 0.9545, 0.9353]\n",
      "2024-01-05 05:21:52.175137: Epoch time: 124.84 s\n",
      "2024-01-05 05:21:52.182140: Yayy! New best EMA pseudo Dice: 0.9132\n",
      "2024-01-05 05:21:53.360784: \n",
      "2024-01-05 05:21:53.366413: Epoch 55\n",
      "2024-01-05 05:21:53.370394: Current learning rate: 0.0095\n",
      "2024-01-05 05:23:57.760420: train_loss -0.8764\n",
      "2024-01-05 05:23:57.769426: val_loss -0.8323\n",
      "2024-01-05 05:23:57.779427: Pseudo dice [0.8662, 0.9545, 0.9338]\n",
      "2024-01-05 05:23:57.785422: Epoch time: 124.4 s\n",
      "2024-01-05 05:23:57.790945: Yayy! New best EMA pseudo Dice: 0.9137\n",
      "2024-01-05 05:23:58.965212: \n",
      "2024-01-05 05:23:58.973385: Epoch 56\n",
      "2024-01-05 05:23:58.977467: Current learning rate: 0.00949\n",
      "2024-01-05 05:26:03.674060: train_loss -0.876\n",
      "2024-01-05 05:26:03.681062: val_loss -0.829\n",
      "2024-01-05 05:26:03.688064: Pseudo dice [0.8627, 0.9539, 0.9335]\n",
      "2024-01-05 05:26:03.695058: Epoch time: 124.71 s\n",
      "2024-01-05 05:26:03.699068: Yayy! New best EMA pseudo Dice: 0.914\n",
      "2024-01-05 05:26:04.901325: \n",
      "2024-01-05 05:26:04.909246: Epoch 57\n",
      "2024-01-05 05:26:04.913405: Current learning rate: 0.00949\n",
      "2024-01-05 05:28:09.761812: train_loss -0.8776\n",
      "2024-01-05 05:28:09.769815: val_loss -0.8272\n",
      "2024-01-05 05:28:09.777812: Pseudo dice [0.8611, 0.9533, 0.9329]\n",
      "2024-01-05 05:28:09.784815: Epoch time: 124.86 s\n",
      "2024-01-05 05:28:09.790817: Yayy! New best EMA pseudo Dice: 0.9142\n",
      "2024-01-05 05:28:10.983782: \n",
      "2024-01-05 05:28:10.990748: Epoch 58\n",
      "2024-01-05 05:28:10.995274: Current learning rate: 0.00948\n",
      "2024-01-05 05:30:15.832775: train_loss -0.8795\n",
      "2024-01-05 05:30:15.839770: val_loss -0.8346\n",
      "2024-01-05 05:30:15.846780: Pseudo dice [0.854, 0.9563, 0.9364]\n",
      "2024-01-05 05:30:15.852773: Epoch time: 124.85 s\n",
      "2024-01-05 05:30:15.858770: Yayy! New best EMA pseudo Dice: 0.9143\n",
      "2024-01-05 05:30:17.058438: \n",
      "2024-01-05 05:30:17.063672: Epoch 59\n",
      "2024-01-05 05:30:17.067990: Current learning rate: 0.00947\n",
      "2024-01-05 05:32:22.008755: train_loss -0.8802\n",
      "2024-01-05 05:32:22.017756: val_loss -0.8273\n",
      "2024-01-05 05:32:22.025766: Pseudo dice [0.8594, 0.9542, 0.9332]\n",
      "2024-01-05 05:32:22.032758: Epoch time: 124.95 s\n",
      "2024-01-05 05:32:22.040761: Yayy! New best EMA pseudo Dice: 0.9144\n",
      "2024-01-05 05:32:23.249818: \n",
      "2024-01-05 05:32:23.255810: Epoch 60\n",
      "2024-01-05 05:32:23.260556: Current learning rate: 0.00946\n",
      "2024-01-05 05:34:28.107195: train_loss -0.8798\n",
      "2024-01-05 05:34:28.117192: val_loss -0.8322\n",
      "2024-01-05 05:34:28.124193: Pseudo dice [0.8638, 0.955, 0.9346]\n",
      "2024-01-05 05:34:28.131193: Epoch time: 124.86 s\n",
      "2024-01-05 05:34:28.137204: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2024-01-05 05:34:29.336892: \n",
      "2024-01-05 05:34:29.341891: Epoch 61\n",
      "2024-01-05 05:34:29.346895: Current learning rate: 0.00945\n",
      "2024-01-05 05:36:34.071314: train_loss -0.882\n",
      "2024-01-05 05:36:34.079312: val_loss -0.833\n",
      "2024-01-05 05:36:34.085312: Pseudo dice [0.8585, 0.9557, 0.9361]\n",
      "2024-01-05 05:36:34.090313: Epoch time: 124.74 s\n",
      "2024-01-05 05:36:34.097312: Yayy! New best EMA pseudo Dice: 0.915\n",
      "2024-01-05 05:36:35.455958: \n",
      "2024-01-05 05:36:35.464823: Epoch 62\n",
      "2024-01-05 05:36:35.468816: Current learning rate: 0.00944\n",
      "2024-01-05 05:38:39.958887: train_loss -0.8808\n",
      "2024-01-05 05:38:39.967884: val_loss -0.8364\n",
      "2024-01-05 05:38:39.975883: Pseudo dice [0.8626, 0.9556, 0.9356]\n",
      "2024-01-05 05:38:39.981881: Epoch time: 124.5 s\n",
      "2024-01-05 05:38:39.988885: Yayy! New best EMA pseudo Dice: 0.9153\n",
      "2024-01-05 05:38:41.209988: \n",
      "2024-01-05 05:38:41.214320: Epoch 63\n",
      "2024-01-05 05:38:41.219355: Current learning rate: 0.00943\n",
      "2024-01-05 05:40:45.795775: train_loss -0.884\n",
      "2024-01-05 05:40:45.803775: val_loss -0.8279\n",
      "2024-01-05 05:40:45.809783: Pseudo dice [0.8592, 0.9543, 0.9335]\n",
      "2024-01-05 05:40:45.815777: Epoch time: 124.59 s\n",
      "2024-01-05 05:40:45.823787: Yayy! New best EMA pseudo Dice: 0.9153\n",
      "2024-01-05 05:40:47.004955: \n",
      "2024-01-05 05:40:47.014091: Epoch 64\n",
      "2024-01-05 05:40:47.019093: Current learning rate: 0.00942\n",
      "2024-01-05 05:42:51.675237: train_loss -0.8812\n",
      "2024-01-05 05:42:51.683519: val_loss -0.8358\n",
      "2024-01-05 05:42:51.690519: Pseudo dice [0.8659, 0.9567, 0.937]\n",
      "2024-01-05 05:42:51.696519: Epoch time: 124.67 s\n",
      "2024-01-05 05:42:51.703525: Yayy! New best EMA pseudo Dice: 0.9158\n",
      "2024-01-05 05:42:52.911636: \n",
      "2024-01-05 05:42:52.918939: Epoch 65\n",
      "2024-01-05 05:42:52.923015: Current learning rate: 0.00941\n",
      "2024-01-05 05:44:57.848944: train_loss -0.881\n",
      "2024-01-05 05:44:57.856952: val_loss -0.8342\n",
      "2024-01-05 05:44:57.864950: Pseudo dice [0.8565, 0.9561, 0.9364]\n",
      "2024-01-05 05:44:57.872491: Epoch time: 124.94 s\n",
      "2024-01-05 05:44:57.879498: Yayy! New best EMA pseudo Dice: 0.9158\n",
      "2024-01-05 05:44:59.080387: \n",
      "2024-01-05 05:44:59.089698: Epoch 66\n",
      "2024-01-05 05:44:59.093780: Current learning rate: 0.0094\n",
      "2024-01-05 05:47:04.079080: train_loss -0.8811\n",
      "2024-01-05 05:47:04.086081: val_loss -0.8309\n",
      "2024-01-05 05:47:04.092083: Pseudo dice [0.8638, 0.9544, 0.9334]\n",
      "2024-01-05 05:47:04.098082: Epoch time: 125.0 s\n",
      "2024-01-05 05:47:04.105078: Yayy! New best EMA pseudo Dice: 0.916\n",
      "2024-01-05 05:47:05.302364: \n",
      "2024-01-05 05:47:05.311040: Epoch 67\n",
      "2024-01-05 05:47:05.315136: Current learning rate: 0.00939\n",
      "2024-01-05 05:49:10.196015: train_loss -0.8808\n",
      "2024-01-05 05:49:10.205028: val_loss -0.831\n",
      "2024-01-05 05:49:10.212019: Pseudo dice [0.8622, 0.9541, 0.9336]\n",
      "2024-01-05 05:49:10.217019: Epoch time: 124.89 s\n",
      "2024-01-05 05:49:10.222019: Yayy! New best EMA pseudo Dice: 0.916\n",
      "2024-01-05 05:49:11.450253: \n",
      "2024-01-05 05:49:11.458138: Epoch 68\n",
      "2024-01-05 05:49:11.463863: Current learning rate: 0.00939\n",
      "2024-01-05 05:51:16.751201: train_loss -0.8836\n",
      "2024-01-05 05:51:16.759202: val_loss -0.833\n",
      "2024-01-05 05:51:16.766210: Pseudo dice [0.8623, 0.9548, 0.9349]\n",
      "2024-01-05 05:51:16.773208: Epoch time: 125.3 s\n",
      "2024-01-05 05:51:16.779715: Yayy! New best EMA pseudo Dice: 0.9162\n",
      "2024-01-05 05:51:18.159866: \n",
      "2024-01-05 05:51:18.168006: Epoch 69\n",
      "2024-01-05 05:51:18.172018: Current learning rate: 0.00938\n",
      "2024-01-05 05:53:22.974282: train_loss -0.881\n",
      "2024-01-05 05:53:22.982793: val_loss -0.8341\n",
      "2024-01-05 05:53:22.990800: Pseudo dice [0.862, 0.956, 0.9367]\n",
      "2024-01-05 05:53:22.995805: Epoch time: 124.82 s\n",
      "2024-01-05 05:53:23.001804: Yayy! New best EMA pseudo Dice: 0.9164\n",
      "2024-01-05 05:53:24.224936: \n",
      "2024-01-05 05:53:24.233041: Epoch 70\n",
      "2024-01-05 05:53:24.238123: Current learning rate: 0.00937\n",
      "2024-01-05 05:55:29.060731: train_loss -0.8826\n",
      "2024-01-05 05:55:29.069731: val_loss -0.8328\n",
      "2024-01-05 05:55:29.076731: Pseudo dice [0.8608, 0.9555, 0.9363]\n",
      "2024-01-05 05:55:29.084735: Epoch time: 124.84 s\n",
      "2024-01-05 05:55:29.089731: Yayy! New best EMA pseudo Dice: 0.9165\n",
      "2024-01-05 05:55:30.310420: \n",
      "2024-01-05 05:55:30.315734: Epoch 71\n",
      "2024-01-05 05:55:30.320263: Current learning rate: 0.00936\n",
      "2024-01-05 05:57:35.095904: train_loss -0.8819\n",
      "2024-01-05 05:57:35.102907: val_loss -0.8357\n",
      "2024-01-05 05:57:35.111199: Pseudo dice [0.8677, 0.9555, 0.9367]\n",
      "2024-01-05 05:57:35.119712: Epoch time: 124.79 s\n",
      "2024-01-05 05:57:35.127711: Yayy! New best EMA pseudo Dice: 0.9168\n",
      "2024-01-05 05:57:36.347941: \n",
      "2024-01-05 05:57:36.359594: Epoch 72\n",
      "2024-01-05 05:57:36.363655: Current learning rate: 0.00935\n",
      "2024-01-05 05:59:41.108785: train_loss -0.8828\n",
      "2024-01-05 05:59:41.117786: val_loss -0.8298\n",
      "2024-01-05 05:59:41.125786: Pseudo dice [0.8573, 0.9544, 0.9339]\n",
      "2024-01-05 05:59:41.132786: Epoch time: 124.76 s\n",
      "2024-01-05 05:59:42.133944: \n",
      "2024-01-05 05:59:42.142035: Epoch 73\n",
      "2024-01-05 05:59:42.146554: Current learning rate: 0.00934\n",
      "2024-01-05 06:01:46.844190: train_loss -0.8838\n",
      "2024-01-05 06:01:46.854182: val_loss -0.8335\n",
      "2024-01-05 06:01:46.859189: Pseudo dice [0.8621, 0.9551, 0.9354]\n",
      "2024-01-05 06:01:46.865254: Epoch time: 124.71 s\n",
      "2024-01-05 06:01:47.832369: \n",
      "2024-01-05 06:01:47.843774: Epoch 74\n",
      "2024-01-05 06:01:47.847380: Current learning rate: 0.00933\n",
      "2024-01-05 06:03:52.798429: train_loss -0.8824\n",
      "2024-01-05 06:03:52.806438: val_loss -0.8345\n",
      "2024-01-05 06:03:52.813432: Pseudo dice [0.86, 0.9561, 0.936]\n",
      "2024-01-05 06:03:52.818434: Epoch time: 124.97 s\n",
      "2024-01-05 06:03:53.834454: \n",
      "2024-01-05 06:03:53.843992: Epoch 75\n",
      "2024-01-05 06:03:53.848063: Current learning rate: 0.00932\n",
      "2024-01-05 06:05:58.549328: train_loss -0.8829\n",
      "2024-01-05 06:05:58.556323: val_loss -0.836\n",
      "2024-01-05 06:05:58.561323: Pseudo dice [0.862, 0.9569, 0.9378]\n",
      "2024-01-05 06:05:58.567323: Epoch time: 124.72 s\n",
      "2024-01-05 06:05:58.573323: Yayy! New best EMA pseudo Dice: 0.917\n",
      "2024-01-05 06:05:59.827222: \n",
      "2024-01-05 06:05:59.835562: Epoch 76\n",
      "2024-01-05 06:05:59.839644: Current learning rate: 0.00931\n",
      "2024-01-05 06:08:04.494445: train_loss -0.8836\n",
      "2024-01-05 06:08:04.503448: val_loss -0.8311\n",
      "2024-01-05 06:08:04.510970: Pseudo dice [0.8608, 0.9552, 0.9351]\n",
      "2024-01-05 06:08:04.516966: Epoch time: 124.67 s\n",
      "2024-01-05 06:08:04.521969: Yayy! New best EMA pseudo Dice: 0.917\n",
      "2024-01-05 06:08:05.920762: \n",
      "2024-01-05 06:08:05.930919: Epoch 77\n",
      "2024-01-05 06:08:05.934978: Current learning rate: 0.0093\n",
      "2024-01-05 06:10:10.414237: train_loss -0.8839\n",
      "2024-01-05 06:10:10.425234: val_loss -0.8347\n",
      "2024-01-05 06:10:10.432234: Pseudo dice [0.8603, 0.9558, 0.9357]\n",
      "2024-01-05 06:10:10.437302: Epoch time: 124.49 s\n",
      "2024-01-05 06:10:10.444309: Yayy! New best EMA pseudo Dice: 0.9171\n",
      "2024-01-05 06:10:11.676908: \n",
      "2024-01-05 06:10:11.682007: Epoch 78\n",
      "2024-01-05 06:10:11.686007: Current learning rate: 0.0093\n",
      "2024-01-05 06:12:16.553573: train_loss -0.8811\n",
      "2024-01-05 06:12:16.566104: val_loss -0.8394\n",
      "2024-01-05 06:12:16.572095: Pseudo dice [0.8608, 0.9584, 0.9396]\n",
      "2024-01-05 06:12:16.579087: Epoch time: 124.88 s\n",
      "2024-01-05 06:12:16.586086: Yayy! New best EMA pseudo Dice: 0.9173\n",
      "2024-01-05 06:12:17.827086: \n",
      "2024-01-05 06:12:17.837923: Epoch 79\n",
      "2024-01-05 06:12:17.847568: Current learning rate: 0.00929\n",
      "2024-01-05 06:14:23.058933: train_loss -0.8831\n",
      "2024-01-05 06:14:23.070935: val_loss -0.8371\n",
      "2024-01-05 06:14:23.079935: Pseudo dice [0.8617, 0.9577, 0.939]\n",
      "2024-01-05 06:14:23.087933: Epoch time: 125.23 s\n",
      "2024-01-05 06:14:23.093932: Yayy! New best EMA pseudo Dice: 0.9175\n",
      "2024-01-05 06:14:24.347864: \n",
      "2024-01-05 06:14:24.357021: Epoch 80\n",
      "2024-01-05 06:14:24.362022: Current learning rate: 0.00928\n",
      "2024-01-05 06:16:29.010387: train_loss -0.8848\n",
      "2024-01-05 06:16:29.017387: val_loss -0.835\n",
      "2024-01-05 06:16:29.025387: Pseudo dice [0.8603, 0.9571, 0.9379]\n",
      "2024-01-05 06:16:29.031387: Epoch time: 124.66 s\n",
      "2024-01-05 06:16:29.035387: Yayy! New best EMA pseudo Dice: 0.9176\n",
      "2024-01-05 06:16:30.261048: \n",
      "2024-01-05 06:16:30.266539: Epoch 81\n",
      "2024-01-05 06:16:30.270571: Current learning rate: 0.00927\n",
      "2024-01-05 06:18:35.115321: train_loss -0.8876\n",
      "2024-01-05 06:18:35.124321: val_loss -0.8413\n",
      "2024-01-05 06:18:35.131321: Pseudo dice [0.8678, 0.9581, 0.9393]\n",
      "2024-01-05 06:18:35.138327: Epoch time: 124.86 s\n",
      "2024-01-05 06:18:35.144325: Yayy! New best EMA pseudo Dice: 0.918\n",
      "2024-01-05 06:18:36.389913: \n",
      "2024-01-05 06:18:36.398982: Epoch 82\n",
      "2024-01-05 06:18:36.403944: Current learning rate: 0.00926\n",
      "2024-01-05 06:20:41.215240: train_loss -0.8883\n",
      "2024-01-05 06:20:41.223271: val_loss -0.8392\n",
      "2024-01-05 06:20:41.229374: Pseudo dice [0.8644, 0.9571, 0.9385]\n",
      "2024-01-05 06:20:41.234362: Epoch time: 124.83 s\n",
      "2024-01-05 06:20:41.239874: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2024-01-05 06:20:42.409675: \n",
      "2024-01-05 06:20:42.415750: Epoch 83\n",
      "2024-01-05 06:20:42.419809: Current learning rate: 0.00925\n",
      "2024-01-05 06:22:47.563743: train_loss -0.8861\n",
      "2024-01-05 06:22:47.572750: val_loss -0.8335\n",
      "2024-01-05 06:22:47.580749: Pseudo dice [0.8616, 0.9558, 0.9362]\n",
      "2024-01-05 06:22:47.585254: Epoch time: 125.16 s\n",
      "2024-01-05 06:22:48.679703: \n",
      "2024-01-05 06:22:48.689350: Epoch 84\n",
      "2024-01-05 06:22:48.693284: Current learning rate: 0.00924\n",
      "2024-01-05 06:24:53.471191: train_loss -0.8872\n",
      "2024-01-05 06:24:53.479191: val_loss -0.8333\n",
      "2024-01-05 06:24:53.485191: Pseudo dice [0.8573, 0.9557, 0.9369]\n",
      "2024-01-05 06:24:53.490191: Epoch time: 124.79 s\n",
      "2024-01-05 06:24:54.447097: \n",
      "2024-01-05 06:24:54.454164: Epoch 85\n",
      "2024-01-05 06:24:54.459095: Current learning rate: 0.00923\n",
      "2024-01-05 06:26:58.941024: train_loss -0.8853\n",
      "2024-01-05 06:26:58.950024: val_loss -0.835\n",
      "2024-01-05 06:26:58.956025: Pseudo dice [0.8634, 0.9565, 0.9377]\n",
      "2024-01-05 06:26:58.962029: Epoch time: 124.49 s\n",
      "2024-01-05 06:26:59.889720: \n",
      "2024-01-05 06:26:59.894721: Epoch 86\n",
      "2024-01-05 06:26:59.899711: Current learning rate: 0.00922\n",
      "2024-01-05 06:29:04.678561: train_loss -0.886\n",
      "2024-01-05 06:29:04.685562: val_loss -0.8336\n",
      "2024-01-05 06:29:04.693562: Pseudo dice [0.8632, 0.9558, 0.9369]\n",
      "2024-01-05 06:29:04.698561: Epoch time: 124.79 s\n",
      "2024-01-05 06:29:05.635777: \n",
      "2024-01-05 06:29:05.644137: Epoch 87\n",
      "2024-01-05 06:29:05.648195: Current learning rate: 0.00921\n",
      "2024-01-05 06:31:10.514871: train_loss -0.8878\n",
      "2024-01-05 06:31:10.523871: val_loss -0.8312\n",
      "2024-01-05 06:31:10.531872: Pseudo dice [0.8605, 0.9556, 0.9369]\n",
      "2024-01-05 06:31:10.538872: Epoch time: 124.88 s\n",
      "2024-01-05 06:31:11.478225: \n",
      "2024-01-05 06:31:11.486867: Epoch 88\n",
      "2024-01-05 06:31:11.491093: Current learning rate: 0.0092\n",
      "2024-01-05 06:33:16.532182: train_loss -0.8881\n",
      "2024-01-05 06:33:16.540181: val_loss -0.8325\n",
      "2024-01-05 06:33:16.546182: Pseudo dice [0.8587, 0.9558, 0.9362]\n",
      "2024-01-05 06:33:16.551181: Epoch time: 125.05 s\n",
      "2024-01-05 06:33:17.486995: \n",
      "2024-01-05 06:33:17.495628: Epoch 89\n",
      "2024-01-05 06:33:17.499727: Current learning rate: 0.0092\n",
      "2024-01-05 06:35:22.346182: train_loss -0.8887\n",
      "2024-01-05 06:35:22.355186: val_loss -0.8351\n",
      "2024-01-05 06:35:22.362186: Pseudo dice [0.8625, 0.9563, 0.9373]\n",
      "2024-01-05 06:35:22.368187: Epoch time: 124.86 s\n",
      "2024-01-05 06:35:23.334637: \n",
      "2024-01-05 06:35:23.339634: Epoch 90\n",
      "2024-01-05 06:35:23.343713: Current learning rate: 0.00919\n",
      "2024-01-05 06:37:28.405021: train_loss -0.8886\n",
      "2024-01-05 06:37:28.412024: val_loss -0.8356\n",
      "2024-01-05 06:37:28.417023: Pseudo dice [0.8593, 0.9567, 0.9374]\n",
      "2024-01-05 06:37:28.429783: Epoch time: 125.07 s\n",
      "2024-01-05 06:37:29.358837: \n",
      "2024-01-05 06:37:29.368907: Epoch 91\n",
      "2024-01-05 06:37:29.373982: Current learning rate: 0.00918\n",
      "2024-01-05 06:39:34.463672: train_loss -0.8888\n",
      "2024-01-05 06:39:34.474194: val_loss -0.8351\n",
      "2024-01-05 06:39:34.481205: Pseudo dice [0.8585, 0.9569, 0.9371]\n",
      "2024-01-05 06:39:34.486199: Epoch time: 125.11 s\n",
      "2024-01-05 06:39:35.561003: \n",
      "2024-01-05 06:39:35.569525: Epoch 92\n",
      "2024-01-05 06:39:35.573534: Current learning rate: 0.00917\n",
      "2024-01-05 06:41:40.508791: train_loss -0.8869\n",
      "2024-01-05 06:41:40.518299: val_loss -0.8343\n",
      "2024-01-05 06:41:40.525299: Pseudo dice [0.8624, 0.9561, 0.9367]\n",
      "2024-01-05 06:41:40.530299: Epoch time: 124.95 s\n",
      "2024-01-05 06:41:41.454332: \n",
      "2024-01-05 06:41:41.466324: Epoch 93\n",
      "2024-01-05 06:41:41.470332: Current learning rate: 0.00916\n",
      "2024-01-05 06:43:46.254087: train_loss -0.8911\n",
      "2024-01-05 06:43:46.261596: val_loss -0.8372\n",
      "2024-01-05 06:43:46.268595: Pseudo dice [0.8627, 0.9566, 0.9378]\n",
      "2024-01-05 06:43:46.275603: Epoch time: 124.8 s\n",
      "2024-01-05 06:43:47.205611: \n",
      "2024-01-05 06:43:47.215515: Epoch 94\n",
      "2024-01-05 06:43:47.219571: Current learning rate: 0.00915\n",
      "2024-01-05 06:45:52.290554: train_loss -0.8895\n",
      "2024-01-05 06:45:52.299581: val_loss -0.8362\n",
      "2024-01-05 06:45:52.310102: Pseudo dice [0.8623, 0.9567, 0.9375]\n",
      "2024-01-05 06:45:52.319095: Epoch time: 125.09 s\n",
      "2024-01-05 06:45:53.310100: \n",
      "2024-01-05 06:45:53.316613: Epoch 95\n",
      "2024-01-05 06:45:53.321024: Current learning rate: 0.00914\n",
      "2024-01-05 06:47:58.275099: train_loss -0.8904\n",
      "2024-01-05 06:47:58.284101: val_loss -0.8342\n",
      "2024-01-05 06:47:58.290100: Pseudo dice [0.8576, 0.9571, 0.9374]\n",
      "2024-01-05 06:47:58.296098: Epoch time: 124.97 s\n",
      "2024-01-05 06:47:59.217445: \n",
      "2024-01-05 06:47:59.226694: Epoch 96\n",
      "2024-01-05 06:47:59.230708: Current learning rate: 0.00913\n",
      "2024-01-05 06:50:04.116463: train_loss -0.893\n",
      "2024-01-05 06:50:04.125471: val_loss -0.8381\n",
      "2024-01-05 06:50:04.135978: Pseudo dice [0.86, 0.9581, 0.94]\n",
      "2024-01-05 06:50:04.141974: Epoch time: 124.9 s\n",
      "2024-01-05 06:50:04.149976: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2024-01-05 06:50:05.366215: \n",
      "2024-01-05 06:50:05.374207: Epoch 97\n",
      "2024-01-05 06:50:05.390174: Current learning rate: 0.00912\n",
      "2024-01-05 06:52:10.528901: train_loss -0.8881\n",
      "2024-01-05 06:52:10.535901: val_loss -0.8406\n",
      "2024-01-05 06:52:10.544909: Pseudo dice [0.8636, 0.9579, 0.9385]\n",
      "2024-01-05 06:52:10.549908: Epoch time: 125.16 s\n",
      "2024-01-05 06:52:10.555901: Yayy! New best EMA pseudo Dice: 0.9184\n",
      "2024-01-05 06:52:11.755420: \n",
      "2024-01-05 06:52:11.772949: Epoch 98\n",
      "2024-01-05 06:52:11.779586: Current learning rate: 0.00911\n",
      "2024-01-05 06:54:16.736086: train_loss -0.8883\n",
      "2024-01-05 06:54:16.744589: val_loss -0.8376\n",
      "2024-01-05 06:54:16.750587: Pseudo dice [0.8654, 0.9572, 0.9379]\n",
      "2024-01-05 06:54:16.758206: Epoch time: 124.98 s\n",
      "2024-01-05 06:54:16.766200: Yayy! New best EMA pseudo Dice: 0.9186\n",
      "2024-01-05 06:54:17.940793: \n",
      "2024-01-05 06:54:17.947610: Epoch 99\n",
      "2024-01-05 06:54:17.954943: Current learning rate: 0.0091\n",
      "2024-01-05 06:56:22.553049: train_loss -0.8906\n",
      "2024-01-05 06:56:22.561049: val_loss -0.8386\n",
      "2024-01-05 06:56:22.567049: Pseudo dice [0.8587, 0.9582, 0.9398]\n",
      "2024-01-05 06:56:22.573049: Epoch time: 124.61 s\n",
      "2024-01-05 06:56:23.105765: Yayy! New best EMA pseudo Dice: 0.9186\n",
      "2024-01-05 06:56:24.243203: \n",
      "2024-01-05 06:56:24.251427: Epoch 100\n",
      "2024-01-05 06:56:24.255513: Current learning rate: 0.0091\n",
      "2024-01-05 06:58:28.981430: train_loss -0.891\n",
      "2024-01-05 06:58:28.988431: val_loss -0.8382\n",
      "2024-01-05 06:58:28.994438: Pseudo dice [0.8655, 0.9575, 0.9383]\n",
      "2024-01-05 06:58:29.001435: Epoch time: 124.74 s\n",
      "2024-01-05 06:58:29.008431: Yayy! New best EMA pseudo Dice: 0.9188\n",
      "2024-01-05 06:58:30.189268: \n",
      "2024-01-05 06:58:30.197555: Epoch 101\n",
      "2024-01-05 06:58:30.203643: Current learning rate: 0.00909\n",
      "2024-01-05 07:00:34.662055: train_loss -0.8917\n",
      "2024-01-05 07:00:34.670056: val_loss -0.8366\n",
      "2024-01-05 07:00:34.678056: Pseudo dice [0.8616, 0.9567, 0.9377]\n",
      "2024-01-05 07:00:34.685056: Epoch time: 124.47 s\n",
      "2024-01-05 07:00:35.676506: \n",
      "2024-01-05 07:00:35.688241: Epoch 102\n",
      "2024-01-05 07:00:35.692447: Current learning rate: 0.00908\n",
      "2024-01-05 07:02:40.101086: train_loss -0.8902\n",
      "2024-01-05 07:02:40.108081: val_loss -0.8339\n",
      "2024-01-05 07:02:40.114080: Pseudo dice [0.8586, 0.9567, 0.9368]\n",
      "2024-01-05 07:02:40.121085: Epoch time: 124.43 s\n",
      "2024-01-05 07:02:41.049693: \n",
      "2024-01-05 07:02:41.057313: Epoch 103\n",
      "2024-01-05 07:02:41.063859: Current learning rate: 0.00907\n",
      "2024-01-05 07:04:45.455552: train_loss -0.8935\n",
      "2024-01-05 07:04:45.462552: val_loss -0.8386\n",
      "2024-01-05 07:04:45.470562: Pseudo dice [0.8633, 0.9585, 0.9392]\n",
      "2024-01-05 07:04:45.477553: Epoch time: 124.41 s\n",
      "2024-01-05 07:04:45.483552: Yayy! New best EMA pseudo Dice: 0.9188\n",
      "2024-01-05 07:04:46.706943: \n",
      "2024-01-05 07:04:46.714579: Epoch 104\n",
      "2024-01-05 07:04:46.719571: Current learning rate: 0.00906\n",
      "2024-01-05 07:06:51.386730: train_loss -0.8897\n",
      "2024-01-05 07:06:51.397240: val_loss -0.836\n",
      "2024-01-05 07:06:51.407239: Pseudo dice [0.864, 0.957, 0.9384]\n",
      "2024-01-05 07:06:51.414235: Epoch time: 124.68 s\n",
      "2024-01-05 07:06:51.420236: Yayy! New best EMA pseudo Dice: 0.9189\n",
      "2024-01-05 07:06:52.605081: \n",
      "2024-01-05 07:06:52.613461: Epoch 105\n",
      "2024-01-05 07:06:52.617547: Current learning rate: 0.00905\n",
      "2024-01-05 07:08:57.582211: train_loss -0.8886\n",
      "2024-01-05 07:08:57.591212: val_loss -0.8308\n",
      "2024-01-05 07:08:57.599212: Pseudo dice [0.8598, 0.9543, 0.9351]\n",
      "2024-01-05 07:08:57.606213: Epoch time: 124.98 s\n",
      "2024-01-05 07:08:58.542664: \n",
      "2024-01-05 07:08:58.549662: Epoch 106\n",
      "2024-01-05 07:08:58.557871: Current learning rate: 0.00904\n",
      "2024-01-05 07:11:03.428360: train_loss -0.8866\n",
      "2024-01-05 07:11:03.434360: val_loss -0.8335\n",
      "2024-01-05 07:11:03.441360: Pseudo dice [0.8621, 0.9561, 0.9367]\n",
      "2024-01-05 07:11:03.447361: Epoch time: 124.89 s\n",
      "2024-01-05 07:11:04.558240: \n",
      "2024-01-05 07:11:04.566521: Epoch 107\n",
      "2024-01-05 07:11:04.570609: Current learning rate: 0.00903\n",
      "2024-01-05 07:13:09.296010: train_loss -0.8897\n",
      "2024-01-05 07:13:09.304997: val_loss -0.8345\n",
      "2024-01-05 07:13:09.312004: Pseudo dice [0.8597, 0.9576, 0.939]\n",
      "2024-01-05 07:13:09.316993: Epoch time: 124.74 s\n",
      "2024-01-05 07:13:10.271468: \n",
      "2024-01-05 07:13:10.280154: Epoch 108\n",
      "2024-01-05 07:13:10.284231: Current learning rate: 0.00902\n",
      "2024-01-05 07:15:15.101503: train_loss -0.8862\n",
      "2024-01-05 07:15:15.109050: val_loss -0.8381\n",
      "2024-01-05 07:15:15.116037: Pseudo dice [0.8638, 0.9576, 0.9387]\n",
      "2024-01-05 07:15:15.123037: Epoch time: 124.83 s\n",
      "2024-01-05 07:15:16.058791: \n",
      "2024-01-05 07:15:16.064105: Epoch 109\n",
      "2024-01-05 07:15:16.068187: Current learning rate: 0.00901\n",
      "2024-01-05 07:17:20.792998: train_loss -0.8885\n",
      "2024-01-05 07:17:20.802993: val_loss -0.8357\n",
      "2024-01-05 07:17:20.810993: Pseudo dice [0.862, 0.9577, 0.9386]\n",
      "2024-01-05 07:17:20.818007: Epoch time: 124.74 s\n",
      "2024-01-05 07:17:21.787699: \n",
      "2024-01-05 07:17:21.792692: Epoch 110\n",
      "2024-01-05 07:17:21.796700: Current learning rate: 0.009\n",
      "2024-01-05 07:19:26.759135: train_loss -0.8905\n",
      "2024-01-05 07:19:26.768137: val_loss -0.8412\n",
      "2024-01-05 07:19:26.776136: Pseudo dice [0.8613, 0.9587, 0.9401]\n",
      "2024-01-05 07:19:26.785137: Epoch time: 124.97 s\n",
      "2024-01-05 07:19:26.791137: Yayy! New best EMA pseudo Dice: 0.919\n",
      "2024-01-05 07:19:27.995640: \n",
      "2024-01-05 07:19:28.000775: Epoch 111\n",
      "2024-01-05 07:19:28.007849: Current learning rate: 0.009\n",
      "2024-01-05 07:21:32.920799: train_loss -0.8907\n",
      "2024-01-05 07:21:32.927797: val_loss -0.8353\n",
      "2024-01-05 07:21:32.935689: Pseudo dice [0.8616, 0.9572, 0.9382]\n",
      "2024-01-05 07:21:32.943117: Epoch time: 124.93 s\n",
      "2024-01-05 07:21:32.951117: Yayy! New best EMA pseudo Dice: 0.919\n",
      "2024-01-05 07:21:34.151196: \n",
      "2024-01-05 07:21:34.156441: Epoch 112\n",
      "2024-01-05 07:21:34.160521: Current learning rate: 0.00899\n",
      "2024-01-05 07:23:39.201392: train_loss -0.8913\n",
      "2024-01-05 07:23:39.209391: val_loss -0.8394\n",
      "2024-01-05 07:23:39.217393: Pseudo dice [0.8651, 0.9581, 0.9382]\n",
      "2024-01-05 07:23:39.225392: Epoch time: 125.05 s\n",
      "2024-01-05 07:23:39.232395: Yayy! New best EMA pseudo Dice: 0.9191\n",
      "2024-01-05 07:23:40.481651: \n",
      "2024-01-05 07:23:40.486651: Epoch 113\n",
      "2024-01-05 07:23:40.495650: Current learning rate: 0.00898\n",
      "2024-01-05 07:25:45.332286: train_loss -0.8902\n",
      "2024-01-05 07:25:45.341295: val_loss -0.8394\n",
      "2024-01-05 07:25:45.348295: Pseudo dice [0.8617, 0.9581, 0.9397]\n",
      "2024-01-05 07:25:45.355284: Epoch time: 124.85 s\n",
      "2024-01-05 07:25:45.362291: Yayy! New best EMA pseudo Dice: 0.9192\n",
      "2024-01-05 07:25:46.541790: \n",
      "2024-01-05 07:25:46.546404: Epoch 114\n",
      "2024-01-05 07:25:46.551360: Current learning rate: 0.00897\n",
      "2024-01-05 07:27:51.016643: train_loss -0.8928\n",
      "2024-01-05 07:27:51.023643: val_loss -0.8343\n",
      "2024-01-05 07:27:51.028643: Pseudo dice [0.8631, 0.9566, 0.9373]\n",
      "2024-01-05 07:27:51.033645: Epoch time: 124.48 s\n",
      "2024-01-05 07:27:52.150053: \n",
      "2024-01-05 07:27:52.157912: Epoch 115\n",
      "2024-01-05 07:27:52.161908: Current learning rate: 0.00896\n",
      "2024-01-05 07:29:56.939427: train_loss -0.8904\n",
      "2024-01-05 07:29:56.951928: val_loss -0.8356\n",
      "2024-01-05 07:29:56.963509: Pseudo dice [0.8615, 0.9572, 0.9381]\n",
      "2024-01-05 07:29:56.975512: Epoch time: 124.79 s\n",
      "2024-01-05 07:29:57.952602: \n",
      "2024-01-05 07:29:57.967852: Epoch 116\n",
      "2024-01-05 07:29:57.974853: Current learning rate: 0.00895\n",
      "2024-01-05 07:32:03.141038: train_loss -0.893\n",
      "2024-01-05 07:32:03.152040: val_loss -0.8418\n",
      "2024-01-05 07:32:03.162045: Pseudo dice [0.8675, 0.9583, 0.9401]\n",
      "2024-01-05 07:32:03.170056: Epoch time: 125.19 s\n",
      "2024-01-05 07:32:03.179043: Yayy! New best EMA pseudo Dice: 0.9194\n",
      "2024-01-05 07:32:04.373414: \n",
      "2024-01-05 07:32:04.380926: Epoch 117\n",
      "2024-01-05 07:32:04.386896: Current learning rate: 0.00894\n",
      "2024-01-05 07:34:09.279438: train_loss -0.8933\n",
      "2024-01-05 07:34:09.291433: val_loss -0.8382\n",
      "2024-01-05 07:34:09.302434: Pseudo dice [0.8671, 0.9574, 0.9378]\n",
      "2024-01-05 07:34:09.313434: Epoch time: 124.91 s\n",
      "2024-01-05 07:34:09.324434: Yayy! New best EMA pseudo Dice: 0.9196\n",
      "2024-01-05 07:34:10.524663: \n",
      "2024-01-05 07:34:10.537209: Epoch 118\n",
      "2024-01-05 07:34:10.545243: Current learning rate: 0.00893\n",
      "2024-01-05 07:36:15.417226: train_loss -0.8962\n",
      "2024-01-05 07:36:15.427227: val_loss -0.8349\n",
      "2024-01-05 07:36:15.436226: Pseudo dice [0.8569, 0.9582, 0.9396]\n",
      "2024-01-05 07:36:15.443226: Epoch time: 124.89 s\n",
      "2024-01-05 07:36:16.405410: \n",
      "2024-01-05 07:36:16.414927: Epoch 119\n",
      "2024-01-05 07:36:16.420987: Current learning rate: 0.00892\n",
      "2024-01-05 07:38:21.636713: train_loss -0.8924\n",
      "2024-01-05 07:38:21.648713: val_loss -0.838\n",
      "2024-01-05 07:38:21.657716: Pseudo dice [0.863, 0.958, 0.9396]\n",
      "2024-01-05 07:38:21.669227: Epoch time: 125.23 s\n",
      "2024-01-05 07:38:22.611788: \n",
      "2024-01-05 07:38:22.619002: Epoch 120\n",
      "2024-01-05 07:38:22.624918: Current learning rate: 0.00891\n",
      "2024-01-05 07:40:27.990124: train_loss -0.8921\n",
      "2024-01-05 07:40:28.000127: val_loss -0.8406\n",
      "2024-01-05 07:40:28.008128: Pseudo dice [0.8651, 0.9591, 0.9403]\n",
      "2024-01-05 07:40:28.017634: Epoch time: 125.38 s\n",
      "2024-01-05 07:40:28.025634: Yayy! New best EMA pseudo Dice: 0.9197\n",
      "2024-01-05 07:40:29.236385: \n",
      "2024-01-05 07:40:29.245744: Epoch 121\n",
      "2024-01-05 07:40:29.251961: Current learning rate: 0.0089\n",
      "2024-01-05 07:42:34.246124: train_loss -0.8929\n",
      "2024-01-05 07:42:34.258126: val_loss -0.8411\n",
      "2024-01-05 07:42:34.269129: Pseudo dice [0.8601, 0.9596, 0.9412]\n",
      "2024-01-05 07:42:34.276635: Epoch time: 125.01 s\n",
      "2024-01-05 07:42:34.284636: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2024-01-05 07:42:35.643674: \n",
      "2024-01-05 07:42:35.653706: Epoch 122\n",
      "2024-01-05 07:42:35.659708: Current learning rate: 0.00889\n",
      "2024-01-05 07:44:40.665155: train_loss -0.8944\n",
      "2024-01-05 07:44:40.675150: val_loss -0.8429\n",
      "2024-01-05 07:44:40.685150: Pseudo dice [0.8626, 0.9596, 0.9412]\n",
      "2024-01-05 07:44:40.695148: Epoch time: 125.02 s\n",
      "2024-01-05 07:44:40.702149: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2024-01-05 07:44:41.895537: \n",
      "2024-01-05 07:44:41.904740: Epoch 123\n",
      "2024-01-05 07:44:41.910749: Current learning rate: 0.00889\n",
      "2024-01-05 07:46:47.274256: train_loss -0.8923\n",
      "2024-01-05 07:46:47.285258: val_loss -0.8373\n",
      "2024-01-05 07:46:47.293264: Pseudo dice [0.8639, 0.957, 0.9381]\n",
      "2024-01-05 07:46:47.302262: Epoch time: 125.38 s\n",
      "2024-01-05 07:46:48.254906: \n",
      "2024-01-05 07:46:48.261628: Epoch 124\n",
      "2024-01-05 07:46:48.267563: Current learning rate: 0.00888\n",
      "2024-01-05 07:48:53.358450: train_loss -0.8936\n",
      "2024-01-05 07:48:53.369455: val_loss -0.8359\n",
      "2024-01-05 07:48:53.379962: Pseudo dice [0.8603, 0.9589, 0.9406]\n",
      "2024-01-05 07:48:53.388968: Epoch time: 125.1 s\n",
      "2024-01-05 07:48:54.346685: \n",
      "2024-01-05 07:48:54.358557: Epoch 125\n",
      "2024-01-05 07:48:54.363581: Current learning rate: 0.00887\n",
      "2024-01-05 07:50:59.506329: train_loss -0.8934\n",
      "2024-01-05 07:50:59.516327: val_loss -0.8343\n",
      "2024-01-05 07:50:59.525326: Pseudo dice [0.8589, 0.9579, 0.9383]\n",
      "2024-01-05 07:50:59.533325: Epoch time: 125.16 s\n",
      "2024-01-05 07:51:00.495935: \n",
      "2024-01-05 07:51:00.508009: Epoch 126\n",
      "2024-01-05 07:51:00.514002: Current learning rate: 0.00886\n",
      "2024-01-05 07:53:05.121997: train_loss -0.8969\n",
      "2024-01-05 07:53:05.133999: val_loss -0.8367\n",
      "2024-01-05 07:53:05.146000: Pseudo dice [0.8622, 0.958, 0.9393]\n",
      "2024-01-05 07:53:05.155999: Epoch time: 124.63 s\n",
      "2024-01-05 07:53:06.093011: \n",
      "2024-01-05 07:53:06.103451: Epoch 127\n",
      "2024-01-05 07:53:06.109695: Current learning rate: 0.00885\n",
      "2024-01-05 07:55:10.772958: train_loss -0.8945\n",
      "2024-01-05 07:55:10.783070: val_loss -0.8379\n",
      "2024-01-05 07:55:10.792077: Pseudo dice [0.8633, 0.9583, 0.9401]\n",
      "2024-01-05 07:55:10.802071: Epoch time: 124.68 s\n",
      "2024-01-05 07:55:11.755887: \n",
      "2024-01-05 07:55:11.765905: Epoch 128\n",
      "2024-01-05 07:55:11.772919: Current learning rate: 0.00884\n",
      "2024-01-05 07:57:16.309078: train_loss -0.8955\n",
      "2024-01-05 07:57:16.323077: val_loss -0.8423\n",
      "2024-01-05 07:57:16.334079: Pseudo dice [0.8669, 0.9586, 0.9401]\n",
      "2024-01-05 07:57:16.344083: Epoch time: 124.56 s\n",
      "2024-01-05 07:57:16.352604: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2024-01-05 07:57:17.539108: \n",
      "2024-01-05 07:57:17.547367: Epoch 129\n",
      "2024-01-05 07:57:17.555368: Current learning rate: 0.00883\n",
      "2024-01-05 07:59:22.091590: train_loss -0.8973\n",
      "2024-01-05 07:59:22.101577: val_loss -0.8328\n",
      "2024-01-05 07:59:22.108575: Pseudo dice [0.8655, 0.9564, 0.9361]\n",
      "2024-01-05 07:59:22.113575: Epoch time: 124.55 s\n",
      "2024-01-05 07:59:23.234123: \n",
      "2024-01-05 07:59:23.240121: Epoch 130\n",
      "2024-01-05 07:59:23.244186: Current learning rate: 0.00882\n",
      "2024-01-05 08:01:27.816596: train_loss -0.8958\n",
      "2024-01-05 08:01:27.825596: val_loss -0.8361\n",
      "2024-01-05 08:01:27.834600: Pseudo dice [0.86, 0.9574, 0.9389]\n",
      "2024-01-05 08:01:27.839603: Epoch time: 124.58 s\n",
      "2024-01-05 08:01:28.831499: \n",
      "2024-01-05 08:01:28.837510: Epoch 131\n",
      "2024-01-05 08:01:28.841560: Current learning rate: 0.00881\n",
      "2024-01-05 08:03:33.296336: train_loss -0.8968\n",
      "2024-01-05 08:03:33.305337: val_loss -0.8358\n",
      "2024-01-05 08:03:33.314337: Pseudo dice [0.8599, 0.9571, 0.938]\n",
      "2024-01-05 08:03:33.321346: Epoch time: 124.47 s\n",
      "2024-01-05 08:03:34.304227: \n",
      "2024-01-05 08:03:34.309236: Epoch 132\n",
      "2024-01-05 08:03:34.313242: Current learning rate: 0.0088\n",
      "2024-01-05 08:05:38.791069: train_loss -0.896\n",
      "2024-01-05 08:05:38.802069: val_loss -0.836\n",
      "2024-01-05 08:05:38.811069: Pseudo dice [0.8638, 0.9569, 0.9372]\n",
      "2024-01-05 08:05:38.818068: Epoch time: 124.49 s\n",
      "2024-01-05 08:05:39.786554: \n",
      "2024-01-05 08:05:39.795621: Epoch 133\n",
      "2024-01-05 08:05:39.800553: Current learning rate: 0.00879\n",
      "2024-01-05 08:07:44.543705: train_loss -0.8924\n",
      "2024-01-05 08:07:44.553705: val_loss -0.8329\n",
      "2024-01-05 08:07:44.559707: Pseudo dice [0.8631, 0.956, 0.9367]\n",
      "2024-01-05 08:07:44.566706: Epoch time: 124.76 s\n",
      "2024-01-05 08:07:45.526870: \n",
      "2024-01-05 08:07:45.532190: Epoch 134\n",
      "2024-01-05 08:07:45.536263: Current learning rate: 0.00879\n",
      "2024-01-05 08:09:50.986516: train_loss -0.8927\n",
      "2024-01-05 08:09:50.996515: val_loss -0.8356\n",
      "2024-01-05 08:09:51.004515: Pseudo dice [0.8637, 0.9573, 0.9383]\n",
      "2024-01-05 08:09:51.012514: Epoch time: 125.46 s\n",
      "2024-01-05 08:09:51.978614: \n",
      "2024-01-05 08:09:51.986561: Epoch 135\n",
      "2024-01-05 08:09:51.993578: Current learning rate: 0.00878\n",
      "2024-01-05 08:11:57.278823: train_loss -0.8958\n",
      "2024-01-05 08:11:57.287832: val_loss -0.838\n",
      "2024-01-05 08:11:57.296823: Pseudo dice [0.8597, 0.9595, 0.9409]\n",
      "2024-01-05 08:11:57.304828: Epoch time: 125.3 s\n",
      "2024-01-05 08:11:58.300368: \n",
      "2024-01-05 08:11:58.308344: Epoch 136\n",
      "2024-01-05 08:11:58.312363: Current learning rate: 0.00877\n",
      "2024-01-05 08:14:03.478258: train_loss -0.8952\n",
      "2024-01-05 08:14:03.488258: val_loss -0.8344\n",
      "2024-01-05 08:14:03.495259: Pseudo dice [0.8615, 0.9571, 0.937]\n",
      "2024-01-05 08:14:03.502259: Epoch time: 125.18 s\n",
      "2024-01-05 08:14:04.558867: \n",
      "2024-01-05 08:14:04.564865: Epoch 137\n",
      "2024-01-05 08:14:04.568865: Current learning rate: 0.00876\n",
      "2024-01-05 08:16:09.657774: train_loss -0.8956\n",
      "2024-01-05 08:16:09.673672: val_loss -0.8344\n",
      "2024-01-05 08:16:09.681865: Pseudo dice [0.8635, 0.9576, 0.939]\n",
      "2024-01-05 08:16:09.687860: Epoch time: 125.1 s\n",
      "2024-01-05 08:16:10.661298: \n",
      "2024-01-05 08:16:10.669858: Epoch 138\n",
      "2024-01-05 08:16:10.674132: Current learning rate: 0.00875\n",
      "2024-01-05 08:18:16.047800: train_loss -0.8948\n",
      "2024-01-05 08:18:16.055799: val_loss -0.8385\n",
      "2024-01-05 08:18:16.061816: Pseudo dice [0.8635, 0.9583, 0.9401]\n",
      "2024-01-05 08:18:16.069809: Epoch time: 125.39 s\n",
      "2024-01-05 08:18:17.062106: \n",
      "2024-01-05 08:18:17.070106: Epoch 139\n",
      "2024-01-05 08:18:17.074627: Current learning rate: 0.00874\n",
      "2024-01-05 08:20:22.050004: train_loss -0.8953\n",
      "2024-01-05 08:20:22.058009: val_loss -0.8327\n",
      "2024-01-05 08:20:22.065004: Pseudo dice [0.8585, 0.9573, 0.9384]\n",
      "2024-01-05 08:20:22.071003: Epoch time: 124.99 s\n",
      "2024-01-05 08:20:23.050612: \n",
      "2024-01-05 08:20:23.056278: Epoch 140\n",
      "2024-01-05 08:20:23.060355: Current learning rate: 0.00873\n",
      "2024-01-05 08:22:28.136796: train_loss -0.8969\n",
      "2024-01-05 08:22:28.146785: val_loss -0.8449\n",
      "2024-01-05 08:22:28.153784: Pseudo dice [0.8677, 0.9598, 0.9416]\n",
      "2024-01-05 08:22:28.160785: Epoch time: 125.09 s\n",
      "2024-01-05 08:22:29.128588: \n",
      "2024-01-05 08:22:29.139863: Epoch 141\n",
      "2024-01-05 08:22:29.143841: Current learning rate: 0.00872\n",
      "2024-01-05 08:24:34.207267: train_loss -0.8965\n",
      "2024-01-05 08:24:34.214268: val_loss -0.8336\n",
      "2024-01-05 08:24:34.220258: Pseudo dice [0.8597, 0.9573, 0.938]\n",
      "2024-01-05 08:24:34.226257: Epoch time: 125.08 s\n",
      "2024-01-05 08:24:35.202903: \n",
      "2024-01-05 08:24:35.208647: Epoch 142\n",
      "2024-01-05 08:24:35.212229: Current learning rate: 0.00871\n",
      "2024-01-05 08:26:40.602501: train_loss -0.895\n",
      "2024-01-05 08:26:40.609509: val_loss -0.8343\n",
      "2024-01-05 08:26:40.617500: Pseudo dice [0.8627, 0.9573, 0.9385]\n",
      "2024-01-05 08:26:40.625500: Epoch time: 125.4 s\n",
      "2024-01-05 08:26:41.606615: \n",
      "2024-01-05 08:26:41.612384: Epoch 143\n",
      "2024-01-05 08:26:41.616458: Current learning rate: 0.0087\n",
      "2024-01-05 08:28:46.775326: train_loss -0.8963\n",
      "2024-01-05 08:28:46.783318: val_loss -0.8373\n",
      "2024-01-05 08:28:46.790316: Pseudo dice [0.8643, 0.9586, 0.9396]\n",
      "2024-01-05 08:28:46.797316: Epoch time: 125.17 s\n",
      "2024-01-05 08:28:47.789114: \n",
      "2024-01-05 08:28:47.796130: Epoch 144\n",
      "2024-01-05 08:28:47.801054: Current learning rate: 0.00869\n",
      "2024-01-05 08:30:52.689815: train_loss -0.8984\n",
      "2024-01-05 08:30:52.697813: val_loss -0.8319\n",
      "2024-01-05 08:30:52.703815: Pseudo dice [0.8607, 0.9572, 0.9381]\n",
      "2024-01-05 08:30:52.709813: Epoch time: 124.9 s\n",
      "2024-01-05 08:30:53.848976: \n",
      "2024-01-05 08:30:53.856530: Epoch 145\n",
      "2024-01-05 08:30:53.860534: Current learning rate: 0.00868\n",
      "2024-01-05 08:32:59.525027: train_loss -0.8976\n",
      "2024-01-05 08:32:59.532029: val_loss -0.8343\n",
      "2024-01-05 08:32:59.540028: Pseudo dice [0.8642, 0.9567, 0.9377]\n",
      "2024-01-05 08:32:59.549043: Epoch time: 125.68 s\n",
      "2024-01-05 08:33:00.528205: \n",
      "2024-01-05 08:33:00.538584: Epoch 146\n",
      "2024-01-05 08:33:00.542654: Current learning rate: 0.00868\n",
      "2024-01-05 08:35:05.776881: train_loss -0.8947\n",
      "2024-01-05 08:35:05.786879: val_loss -0.8377\n",
      "2024-01-05 08:35:05.796894: Pseudo dice [0.8669, 0.9579, 0.9396]\n",
      "2024-01-05 08:35:05.802887: Epoch time: 125.25 s\n",
      "2024-01-05 08:35:06.770047: \n",
      "2024-01-05 08:35:06.780047: Epoch 147\n",
      "2024-01-05 08:35:06.789809: Current learning rate: 0.00867\n",
      "2024-01-05 08:37:12.005622: train_loss -0.8966\n",
      "2024-01-05 08:37:12.012622: val_loss -0.8398\n",
      "2024-01-05 08:37:12.018622: Pseudo dice [0.8619, 0.9585, 0.9401]\n",
      "2024-01-05 08:37:12.023622: Epoch time: 125.24 s\n",
      "2024-01-05 08:37:13.031143: \n",
      "2024-01-05 08:37:13.037143: Epoch 148\n",
      "2024-01-05 08:37:13.041438: Current learning rate: 0.00866\n",
      "2024-01-05 08:39:18.298003: train_loss -0.8983\n",
      "2024-01-05 08:39:18.306006: val_loss -0.8369\n",
      "2024-01-05 08:39:18.313007: Pseudo dice [0.8638, 0.9594, 0.9413]\n",
      "2024-01-05 08:39:18.318006: Epoch time: 125.27 s\n",
      "2024-01-05 08:39:18.323513: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2024-01-05 08:39:19.557331: \n",
      "2024-01-05 08:39:19.562398: Epoch 149\n",
      "2024-01-05 08:39:19.567336: Current learning rate: 0.00865\n",
      "2024-01-05 08:41:24.982668: train_loss -0.8964\n",
      "2024-01-05 08:41:24.990668: val_loss -0.8346\n",
      "2024-01-05 08:41:24.998684: Pseudo dice [0.8631, 0.9577, 0.9383]\n",
      "2024-01-05 08:41:25.005669: Epoch time: 125.43 s\n",
      "2024-01-05 08:41:26.248234: \n",
      "2024-01-05 08:41:26.260347: Epoch 150\n",
      "2024-01-05 08:41:26.264420: Current learning rate: 0.00864\n",
      "2024-01-05 08:43:31.502386: train_loss -0.8965\n",
      "2024-01-05 08:43:31.510388: val_loss -0.8416\n",
      "2024-01-05 08:43:31.516893: Pseudo dice [0.8636, 0.96, 0.9424]\n",
      "2024-01-05 08:43:31.521897: Epoch time: 125.26 s\n",
      "2024-01-05 08:43:31.527897: Yayy! New best EMA pseudo Dice: 0.9202\n",
      "2024-01-05 08:43:32.747659: \n",
      "2024-01-05 08:43:32.756312: Epoch 151\n",
      "2024-01-05 08:43:32.760330: Current learning rate: 0.00863\n",
      "2024-01-05 08:45:37.498923: train_loss -0.8988\n",
      "2024-01-05 08:45:37.507925: val_loss -0.8349\n",
      "2024-01-05 08:45:37.514920: Pseudo dice [0.8612, 0.9586, 0.9396]\n",
      "2024-01-05 08:45:37.521923: Epoch time: 124.75 s\n",
      "2024-01-05 08:45:38.495476: \n",
      "2024-01-05 08:45:38.502476: Epoch 152\n",
      "2024-01-05 08:45:38.507476: Current learning rate: 0.00862\n",
      "2024-01-05 08:47:43.072536: train_loss -0.8995\n",
      "2024-01-05 08:47:43.081535: val_loss -0.8343\n",
      "2024-01-05 08:47:43.089041: Pseudo dice [0.8637, 0.9578, 0.9386]\n",
      "2024-01-05 08:47:43.094041: Epoch time: 124.58 s\n",
      "2024-01-05 08:47:44.255609: \n",
      "2024-01-05 08:47:44.261742: Epoch 153\n",
      "2024-01-05 08:47:44.265749: Current learning rate: 0.00861\n",
      "2024-01-05 08:49:48.804060: train_loss -0.9019\n",
      "2024-01-05 08:49:48.811062: val_loss -0.8389\n",
      "2024-01-05 08:49:48.819063: Pseudo dice [0.8622, 0.9601, 0.9422]\n",
      "2024-01-05 08:49:48.827575: Epoch time: 124.55 s\n",
      "2024-01-05 08:49:48.833576: Yayy! New best EMA pseudo Dice: 0.9203\n",
      "2024-01-05 08:49:50.108089: \n",
      "2024-01-05 08:49:50.121762: Epoch 154\n",
      "2024-01-05 08:49:50.125757: Current learning rate: 0.0086\n",
      "2024-01-05 08:51:54.695445: train_loss -0.8995\n",
      "2024-01-05 08:51:54.703444: val_loss -0.8406\n",
      "2024-01-05 08:51:54.711950: Pseudo dice [0.8687, 0.9588, 0.94]\n",
      "2024-01-05 08:51:54.718962: Epoch time: 124.59 s\n",
      "2024-01-05 08:51:54.727954: Yayy! New best EMA pseudo Dice: 0.9205\n",
      "2024-01-05 08:51:55.969892: \n",
      "2024-01-05 08:51:55.974892: Epoch 155\n",
      "2024-01-05 08:51:55.979892: Current learning rate: 0.00859\n",
      "2024-01-05 08:54:00.602494: train_loss -0.8985\n",
      "2024-01-05 08:54:00.611494: val_loss -0.8371\n",
      "2024-01-05 08:54:00.617494: Pseudo dice [0.8617, 0.9582, 0.9394]\n",
      "2024-01-05 08:54:00.623493: Epoch time: 124.63 s\n",
      "2024-01-05 08:54:01.612030: \n",
      "2024-01-05 08:54:01.619030: Epoch 156\n",
      "2024-01-05 08:54:01.626545: Current learning rate: 0.00858\n",
      "2024-01-05 08:56:06.095652: train_loss -0.8977\n",
      "2024-01-05 08:56:06.103652: val_loss -0.8405\n",
      "2024-01-05 08:56:06.112176: Pseudo dice [0.8607, 0.9599, 0.9422]\n",
      "2024-01-05 08:56:06.119184: Epoch time: 124.48 s\n",
      "2024-01-05 08:56:07.106786: \n",
      "2024-01-05 08:56:07.115439: Epoch 157\n",
      "2024-01-05 08:56:07.124439: Current learning rate: 0.00858\n",
      "2024-01-05 08:58:12.200868: train_loss -0.899\n",
      "2024-01-05 08:58:12.209869: val_loss -0.839\n",
      "2024-01-05 08:58:12.218868: Pseudo dice [0.8631, 0.9601, 0.9415]\n",
      "2024-01-05 08:58:12.224868: Epoch time: 125.1 s\n",
      "2024-01-05 08:58:12.233868: Yayy! New best EMA pseudo Dice: 0.9206\n",
      "2024-01-05 08:58:13.481961: \n",
      "2024-01-05 08:58:13.489958: Epoch 158\n",
      "2024-01-05 08:58:13.501667: Current learning rate: 0.00857\n",
      "2024-01-05 09:00:18.194885: train_loss -0.8977\n",
      "2024-01-05 09:00:18.203885: val_loss -0.8343\n",
      "2024-01-05 09:00:18.210885: Pseudo dice [0.8644, 0.958, 0.9399]\n",
      "2024-01-05 09:00:18.217889: Epoch time: 124.71 s\n",
      "2024-01-05 09:00:18.223895: Yayy! New best EMA pseudo Dice: 0.9206\n",
      "2024-01-05 09:00:19.475988: \n",
      "2024-01-05 09:00:19.488329: Epoch 159\n",
      "2024-01-05 09:00:19.492379: Current learning rate: 0.00856\n",
      "2024-01-05 09:02:24.701969: train_loss -0.8876\n",
      "2024-01-05 09:02:24.708977: val_loss -0.8317\n",
      "2024-01-05 09:02:24.715976: Pseudo dice [0.8559, 0.9554, 0.9355]\n",
      "2024-01-05 09:02:24.720976: Epoch time: 125.23 s\n",
      "2024-01-05 09:02:25.885018: \n",
      "2024-01-05 09:02:25.892088: Epoch 160\n",
      "2024-01-05 09:02:25.901050: Current learning rate: 0.00855\n",
      "2024-01-05 09:04:31.356860: train_loss -0.8887\n",
      "2024-01-05 09:04:31.364863: val_loss -0.8405\n",
      "2024-01-05 09:04:31.371862: Pseudo dice [0.8645, 0.9588, 0.9405]\n",
      "2024-01-05 09:04:31.378367: Epoch time: 125.47 s\n",
      "2024-01-05 09:04:32.385494: \n",
      "2024-01-05 09:04:32.390572: Epoch 161\n",
      "2024-01-05 09:04:32.395494: Current learning rate: 0.00854\n",
      "2024-01-05 09:06:37.229390: train_loss -0.8959\n",
      "2024-01-05 09:06:37.237889: val_loss -0.839\n",
      "2024-01-05 09:06:37.244888: Pseudo dice [0.8628, 0.9591, 0.9399]\n",
      "2024-01-05 09:06:37.251892: Epoch time: 124.85 s\n",
      "2024-01-05 09:06:38.234897: \n",
      "2024-01-05 09:06:38.240324: Epoch 162\n",
      "2024-01-05 09:06:38.244402: Current learning rate: 0.00853\n",
      "2024-01-05 09:08:42.959540: train_loss -0.8988\n",
      "2024-01-05 09:08:42.968046: val_loss -0.8376\n",
      "2024-01-05 09:08:42.976047: Pseudo dice [0.8636, 0.9583, 0.9393]\n",
      "2024-01-05 09:08:42.981046: Epoch time: 124.73 s\n",
      "2024-01-05 09:08:44.014672: \n",
      "2024-01-05 09:08:44.021915: Epoch 163\n",
      "2024-01-05 09:08:44.025919: Current learning rate: 0.00852\n",
      "2024-01-05 09:10:48.895412: train_loss -0.8966\n",
      "2024-01-05 09:10:48.902411: val_loss -0.8341\n",
      "2024-01-05 09:10:48.909420: Pseudo dice [0.8609, 0.9583, 0.9386]\n",
      "2024-01-05 09:10:48.915416: Epoch time: 124.88 s\n",
      "2024-01-05 09:10:49.882347: \n",
      "2024-01-05 09:10:49.894775: Epoch 164\n",
      "2024-01-05 09:10:49.901779: Current learning rate: 0.00851\n",
      "2024-01-05 09:12:54.287671: train_loss -0.9011\n",
      "2024-01-05 09:12:54.294671: val_loss -0.8352\n",
      "2024-01-05 09:12:54.302681: Pseudo dice [0.8641, 0.9566, 0.9368]\n",
      "2024-01-05 09:12:54.309671: Epoch time: 124.41 s\n",
      "2024-01-05 09:12:55.263228: \n",
      "2024-01-05 09:12:55.268228: Epoch 165\n",
      "2024-01-05 09:12:55.272228: Current learning rate: 0.0085\n",
      "2024-01-05 09:15:00.161518: train_loss -0.8995\n",
      "2024-01-05 09:15:00.170523: val_loss -0.8358\n",
      "2024-01-05 09:15:00.177516: Pseudo dice [0.86, 0.9576, 0.9385]\n",
      "2024-01-05 09:15:00.183516: Epoch time: 124.9 s\n",
      "2024-01-05 09:15:01.139897: \n",
      "2024-01-05 09:15:01.145723: Epoch 166\n",
      "2024-01-05 09:15:01.150239: Current learning rate: 0.00849\n",
      "2024-01-05 09:17:06.672739: train_loss -0.9002\n",
      "2024-01-05 09:17:06.680738: val_loss -0.8368\n",
      "2024-01-05 09:17:06.687741: Pseudo dice [0.8634, 0.959, 0.9394]\n",
      "2024-01-05 09:17:06.694741: Epoch time: 125.53 s\n",
      "2024-01-05 09:17:07.677412: \n",
      "2024-01-05 09:17:07.683281: Epoch 167\n",
      "2024-01-05 09:17:07.687341: Current learning rate: 0.00848\n",
      "2024-01-05 09:19:13.581917: train_loss -0.9001\n",
      "2024-01-05 09:19:13.590917: val_loss -0.8309\n",
      "2024-01-05 09:19:13.598916: Pseudo dice [0.8559, 0.9566, 0.9371]\n",
      "2024-01-05 09:19:13.606915: Epoch time: 125.91 s\n",
      "2024-01-05 09:19:14.813148: \n",
      "2024-01-05 09:19:14.827171: Epoch 168\n",
      "2024-01-05 09:19:14.832084: Current learning rate: 0.00847\n",
      "2024-01-05 09:21:20.523433: train_loss -0.8971\n",
      "2024-01-05 09:21:20.532434: val_loss -0.8356\n",
      "2024-01-05 09:21:20.540434: Pseudo dice [0.8639, 0.9577, 0.9398]\n",
      "2024-01-05 09:21:20.547439: Epoch time: 125.71 s\n",
      "2024-01-05 09:21:21.578016: \n",
      "2024-01-05 09:21:21.591173: Epoch 169\n",
      "2024-01-05 09:21:21.595835: Current learning rate: 0.00847\n",
      "2024-01-05 09:23:26.928116: train_loss -0.9006\n",
      "2024-01-05 09:23:26.937117: val_loss -0.8369\n",
      "2024-01-05 09:23:26.944118: Pseudo dice [0.861, 0.9575, 0.939]\n",
      "2024-01-05 09:23:26.952118: Epoch time: 125.35 s\n",
      "2024-01-05 09:23:27.968172: \n",
      "2024-01-05 09:23:27.977322: Epoch 170\n",
      "2024-01-05 09:23:27.981314: Current learning rate: 0.00846\n",
      "2024-01-05 09:25:33.015357: train_loss -0.9004\n",
      "2024-01-05 09:25:33.024358: val_loss -0.8409\n",
      "2024-01-05 09:25:33.030358: Pseudo dice [0.8627, 0.9587, 0.9402]\n",
      "2024-01-05 09:25:33.037357: Epoch time: 125.05 s\n",
      "2024-01-05 09:25:34.019730: \n",
      "2024-01-05 09:25:34.030384: Epoch 171\n",
      "2024-01-05 09:25:34.034371: Current learning rate: 0.00845\n",
      "2024-01-05 09:27:39.392726: train_loss -0.9013\n",
      "2024-01-05 09:27:39.399727: val_loss -0.8283\n",
      "2024-01-05 09:27:39.406728: Pseudo dice [0.8619, 0.9552, 0.9373]\n",
      "2024-01-05 09:27:39.412731: Epoch time: 125.37 s\n",
      "2024-01-05 09:27:40.492845: \n",
      "2024-01-05 09:27:40.498850: Epoch 172\n",
      "2024-01-05 09:27:40.503844: Current learning rate: 0.00844\n",
      "2024-01-05 09:29:46.503872: train_loss -0.8976\n",
      "2024-01-05 09:29:46.514869: val_loss -0.8418\n",
      "2024-01-05 09:29:46.521869: Pseudo dice [0.8629, 0.9589, 0.9405]\n",
      "2024-01-05 09:29:46.527869: Epoch time: 126.01 s\n",
      "2024-01-05 09:29:47.554168: \n",
      "2024-01-05 09:29:47.559184: Epoch 173\n",
      "2024-01-05 09:29:47.564184: Current learning rate: 0.00843\n",
      "2024-01-05 09:31:53.435305: train_loss -0.8985\n",
      "2024-01-05 09:31:53.444304: val_loss -0.8341\n",
      "2024-01-05 09:31:53.451306: Pseudo dice [0.8621, 0.9583, 0.9399]\n",
      "2024-01-05 09:31:53.458306: Epoch time: 125.88 s\n",
      "2024-01-05 09:31:54.483217: \n",
      "2024-01-05 09:31:54.495224: Epoch 174\n",
      "2024-01-05 09:31:54.500254: Current learning rate: 0.00842\n",
      "2024-01-05 09:34:00.054042: train_loss -0.8992\n",
      "2024-01-05 09:34:00.063044: val_loss -0.8423\n",
      "2024-01-05 09:34:00.069050: Pseudo dice [0.8666, 0.9594, 0.9411]\n",
      "2024-01-05 09:34:00.076050: Epoch time: 125.57 s\n",
      "2024-01-05 09:34:01.262818: \n",
      "2024-01-05 09:34:01.268831: Epoch 175\n",
      "2024-01-05 09:34:01.273829: Current learning rate: 0.00841\n",
      "2024-01-05 09:36:06.608463: train_loss -0.9003\n",
      "2024-01-05 09:36:06.618544: val_loss -0.8404\n",
      "2024-01-05 09:36:06.627552: Pseudo dice [0.8669, 0.9582, 0.9406]\n",
      "2024-01-05 09:36:06.636556: Epoch time: 125.35 s\n",
      "2024-01-05 09:36:07.754472: \n",
      "2024-01-05 09:36:07.759468: Epoch 176\n",
      "2024-01-05 09:36:07.764468: Current learning rate: 0.0084\n",
      "2024-01-05 09:38:16.003880: train_loss -0.8998\n",
      "2024-01-05 09:38:16.013880: val_loss -0.842\n",
      "2024-01-05 09:38:16.020882: Pseudo dice [0.8643, 0.9605, 0.9426]\n",
      "2024-01-05 09:38:16.029881: Epoch time: 128.25 s\n",
      "2024-01-05 09:38:17.535756: \n",
      "2024-01-05 09:38:17.542757: Epoch 177\n",
      "2024-01-05 09:38:17.547261: Current learning rate: 0.00839\n",
      "2024-01-05 09:40:25.744151: train_loss -0.8996\n",
      "2024-01-05 09:40:25.753151: val_loss -0.8332\n",
      "2024-01-05 09:40:25.762155: Pseudo dice [0.8602, 0.9579, 0.9384]\n",
      "2024-01-05 09:40:25.770619: Epoch time: 128.21 s\n",
      "2024-01-05 09:40:27.155840: \n",
      "2024-01-05 09:40:27.161837: Epoch 178\n",
      "2024-01-05 09:40:27.166835: Current learning rate: 0.00838\n",
      "2024-01-05 09:42:40.399005: train_loss -0.8984\n",
      "2024-01-05 09:42:40.428685: val_loss -0.8375\n",
      "2024-01-05 09:42:40.472310: Pseudo dice [0.8646, 0.9589, 0.9406]\n",
      "2024-01-05 09:42:40.493201: Epoch time: 133.24 s\n",
      "2024-01-05 09:42:43.018626: \n",
      "2024-01-05 09:42:43.026636: Epoch 179\n",
      "2024-01-05 09:42:43.032159: Current learning rate: 0.00837\n",
      "2024-01-05 09:44:54.633141: train_loss -0.9028\n",
      "2024-01-05 09:44:54.650702: val_loss -0.8336\n",
      "2024-01-05 09:44:54.665728: Pseudo dice [0.8565, 0.9589, 0.9403]\n",
      "2024-01-05 09:44:54.675234: Epoch time: 131.62 s\n",
      "2024-01-05 09:44:56.403193: \n",
      "2024-01-05 09:44:56.411230: Epoch 180\n",
      "2024-01-05 09:44:56.416754: Current learning rate: 0.00836\n",
      "2024-01-05 09:47:04.363978: train_loss -0.8994\n",
      "2024-01-05 09:47:04.372978: val_loss -0.8369\n",
      "2024-01-05 09:47:04.381978: Pseudo dice [0.8584, 0.9588, 0.9402]\n",
      "2024-01-05 09:47:04.388978: Epoch time: 127.96 s\n",
      "2024-01-05 09:47:05.634090: \n",
      "2024-01-05 09:47:05.641011: Epoch 181\n",
      "2024-01-05 09:47:05.646079: Current learning rate: 0.00836\n",
      "2024-01-05 09:49:13.038875: train_loss -0.8998\n",
      "2024-01-05 09:49:13.048874: val_loss -0.8382\n",
      "2024-01-05 09:49:13.054875: Pseudo dice [0.8626, 0.9599, 0.9413]\n",
      "2024-01-05 09:49:13.060875: Epoch time: 127.41 s\n",
      "2024-01-05 09:49:14.323047: \n",
      "2024-01-05 09:49:14.333053: Epoch 182\n",
      "2024-01-05 09:49:14.339046: Current learning rate: 0.00835\n",
      "2024-01-05 09:51:22.460082: train_loss -0.9017\n",
      "2024-01-05 09:51:22.471081: val_loss -0.8341\n",
      "2024-01-05 09:51:22.483100: Pseudo dice [0.8596, 0.9587, 0.9399]\n",
      "2024-01-05 09:51:22.490613: Epoch time: 128.14 s\n",
      "2024-01-05 09:51:24.028647: \n",
      "2024-01-05 09:51:24.036648: Epoch 183\n",
      "2024-01-05 09:51:24.042647: Current learning rate: 0.00834\n",
      "2024-01-05 09:53:32.773891: train_loss -0.9002\n",
      "2024-01-05 09:53:32.781888: val_loss -0.8374\n",
      "2024-01-05 09:53:32.790890: Pseudo dice [0.8578, 0.9595, 0.9408]\n",
      "2024-01-05 09:53:32.799892: Epoch time: 128.75 s\n",
      "2024-01-05 09:53:33.815785: \n",
      "2024-01-05 09:53:33.822670: Epoch 184\n",
      "2024-01-05 09:53:33.830708: Current learning rate: 0.00833\n",
      "2024-01-05 09:55:41.595574: train_loss -0.8997\n",
      "2024-01-05 09:55:41.608581: val_loss -0.8372\n",
      "2024-01-05 09:55:41.617930: Pseudo dice [0.8629, 0.9589, 0.9408]\n",
      "2024-01-05 09:55:41.627829: Epoch time: 127.78 s\n",
      "2024-01-05 09:55:42.802302: \n",
      "2024-01-05 09:55:42.808324: Epoch 185\n",
      "2024-01-05 09:55:42.813313: Current learning rate: 0.00832\n",
      "2024-01-05 09:57:50.961128: train_loss -0.9032\n",
      "2024-01-05 09:57:50.972707: val_loss -0.8404\n",
      "2024-01-05 09:57:50.984708: Pseudo dice [0.8654, 0.9592, 0.9407]\n",
      "2024-01-05 09:57:50.995707: Epoch time: 128.16 s\n",
      "2024-01-05 09:57:52.318745: \n",
      "2024-01-05 09:57:52.328666: Epoch 186\n",
      "2024-01-05 09:57:52.335846: Current learning rate: 0.00831\n",
      "2024-01-05 09:59:59.983281: train_loss -0.8988\n",
      "2024-01-05 09:59:59.994873: val_loss -0.8307\n",
      "2024-01-05 10:00:00.002849: Pseudo dice [0.8591, 0.9564, 0.9365]\n",
      "2024-01-05 10:00:00.012674: Epoch time: 127.67 s\n",
      "2024-01-05 10:00:01.217743: \n",
      "2024-01-05 10:00:01.228563: Epoch 187\n",
      "2024-01-05 10:00:01.237458: Current learning rate: 0.0083\n",
      "2024-01-05 10:02:08.570297: train_loss -0.9008\n",
      "2024-01-05 10:02:08.579300: val_loss -0.8372\n",
      "2024-01-05 10:02:08.587297: Pseudo dice [0.863, 0.9597, 0.9425]\n",
      "2024-01-05 10:02:08.593297: Epoch time: 127.35 s\n",
      "2024-01-05 10:02:09.624772: \n",
      "2024-01-05 10:02:09.629773: Epoch 188\n",
      "2024-01-05 10:02:09.634264: Current learning rate: 0.00829\n",
      "2024-01-05 10:04:16.453742: train_loss -0.9015\n",
      "2024-01-05 10:04:16.463737: val_loss -0.8354\n",
      "2024-01-05 10:04:16.471739: Pseudo dice [0.8668, 0.9583, 0.9391]\n",
      "2024-01-05 10:04:16.480095: Epoch time: 126.83 s\n",
      "2024-01-05 10:04:17.518083: \n",
      "2024-01-05 10:04:17.524150: Epoch 189\n",
      "2024-01-05 10:04:17.528727: Current learning rate: 0.00828\n",
      "2024-01-05 10:06:24.515509: train_loss -0.904\n",
      "2024-01-05 10:06:24.524509: val_loss -0.8362\n",
      "2024-01-05 10:06:24.533543: Pseudo dice [0.8633, 0.9592, 0.9417]\n",
      "2024-01-05 10:06:24.540544: Epoch time: 127.0 s\n",
      "2024-01-05 10:06:25.587342: \n",
      "2024-01-05 10:06:25.593358: Epoch 190\n",
      "2024-01-05 10:06:25.597347: Current learning rate: 0.00827\n",
      "2024-01-05 10:08:32.597483: train_loss -0.9014\n",
      "2024-01-05 10:08:32.610486: val_loss -0.8372\n",
      "2024-01-05 10:08:32.619487: Pseudo dice [0.8667, 0.958, 0.9395]\n",
      "2024-01-05 10:08:32.627483: Epoch time: 127.01 s\n",
      "2024-01-05 10:08:33.932445: \n",
      "2024-01-05 10:08:33.938441: Epoch 191\n",
      "2024-01-05 10:08:33.943587: Current learning rate: 0.00826\n",
      "2024-01-05 10:10:40.727279: train_loss -0.9027\n",
      "2024-01-05 10:10:40.737792: val_loss -0.8347\n",
      "2024-01-05 10:10:40.746303: Pseudo dice [0.8644, 0.9586, 0.9395]\n",
      "2024-01-05 10:10:40.755302: Epoch time: 126.8 s\n",
      "2024-01-05 10:10:41.868583: \n",
      "2024-01-05 10:10:41.880581: Epoch 192\n",
      "2024-01-05 10:10:41.885581: Current learning rate: 0.00825\n",
      "2024-01-05 10:12:48.758150: train_loss -0.9009\n",
      "2024-01-05 10:12:48.768594: val_loss -0.8332\n",
      "2024-01-05 10:12:48.778774: Pseudo dice [0.8592, 0.9579, 0.939]\n",
      "2024-01-05 10:12:48.788294: Epoch time: 126.89 s\n",
      "2024-01-05 10:12:49.900315: \n",
      "2024-01-05 10:12:49.906296: Epoch 193\n",
      "2024-01-05 10:12:49.911307: Current learning rate: 0.00824\n",
      "2024-01-05 10:14:56.820817: train_loss -0.9041\n",
      "2024-01-05 10:14:56.831816: val_loss -0.833\n",
      "2024-01-05 10:14:56.840816: Pseudo dice [0.8643, 0.9577, 0.9382]\n",
      "2024-01-05 10:14:56.848816: Epoch time: 126.92 s\n",
      "2024-01-05 10:14:57.929664: \n",
      "2024-01-05 10:14:57.937654: Epoch 194\n",
      "2024-01-05 10:14:57.943653: Current learning rate: 0.00824\n",
      "2024-01-05 10:17:04.817019: train_loss -0.8998\n",
      "2024-01-05 10:17:04.826022: val_loss -0.841\n",
      "2024-01-05 10:17:04.834018: Pseudo dice [0.8607, 0.9605, 0.9427]\n",
      "2024-01-05 10:17:04.842049: Epoch time: 126.89 s\n",
      "2024-01-05 10:17:05.906718: \n",
      "2024-01-05 10:17:05.911718: Epoch 195\n",
      "2024-01-05 10:17:05.915721: Current learning rate: 0.00823\n",
      "2024-01-05 10:19:12.543759: train_loss -0.9009\n",
      "2024-01-05 10:19:12.552764: val_loss -0.8399\n",
      "2024-01-05 10:19:12.561756: Pseudo dice [0.8653, 0.9596, 0.9415]\n",
      "2024-01-05 10:19:12.568759: Epoch time: 126.64 s\n",
      "2024-01-05 10:19:13.604454: \n",
      "2024-01-05 10:19:13.610450: Epoch 196\n",
      "2024-01-05 10:19:13.621316: Current learning rate: 0.00822\n",
      "2024-01-05 10:21:20.147821: train_loss -0.9032\n",
      "2024-01-05 10:21:20.156820: val_loss -0.8385\n",
      "2024-01-05 10:21:20.165819: Pseudo dice [0.8645, 0.9585, 0.9402]\n",
      "2024-01-05 10:21:20.170823: Epoch time: 126.54 s\n",
      "2024-01-05 10:21:20.176818: Yayy! New best EMA pseudo Dice: 0.9206\n",
      "2024-01-05 10:21:21.518667: \n",
      "2024-01-05 10:21:21.523667: Epoch 197\n",
      "2024-01-05 10:21:21.528664: Current learning rate: 0.00821\n",
      "2024-01-05 10:23:27.355474: train_loss -0.9018\n",
      "2024-01-05 10:23:27.363969: val_loss -0.8408\n",
      "2024-01-05 10:23:27.371969: Pseudo dice [0.868, 0.9599, 0.9416]\n",
      "2024-01-05 10:23:27.377971: Epoch time: 125.84 s\n",
      "2024-01-05 10:23:27.383970: Yayy! New best EMA pseudo Dice: 0.9209\n",
      "2024-01-05 10:23:28.858294: \n",
      "2024-01-05 10:23:28.868593: Epoch 198\n",
      "2024-01-05 10:23:28.872592: Current learning rate: 0.0082\n",
      "2024-01-05 10:25:35.347153: train_loss -0.9012\n",
      "2024-01-05 10:25:35.357203: val_loss -0.8403\n",
      "2024-01-05 10:25:35.366359: Pseudo dice [0.8627, 0.9594, 0.9409]\n",
      "2024-01-05 10:25:35.376520: Epoch time: 126.49 s\n",
      "2024-01-05 10:25:35.385703: Yayy! New best EMA pseudo Dice: 0.9209\n",
      "2024-01-05 10:25:36.889102: \n",
      "2024-01-05 10:25:36.896104: Epoch 199\n",
      "2024-01-05 10:25:36.902101: Current learning rate: 0.00819\n",
      "2024-01-05 10:27:43.541194: train_loss -0.907\n",
      "2024-01-05 10:27:43.554750: val_loss -0.8407\n",
      "2024-01-05 10:27:43.565749: Pseudo dice [0.8617, 0.9596, 0.9413]\n",
      "2024-01-05 10:27:43.575747: Epoch time: 126.65 s\n",
      "2024-01-05 10:27:44.936455: \n",
      "2024-01-05 10:27:44.942445: Epoch 200\n",
      "2024-01-05 10:27:44.946446: Current learning rate: 0.00818\n",
      "2024-01-05 10:29:51.271226: train_loss -0.9037\n",
      "2024-01-05 10:29:51.282226: val_loss -0.8367\n",
      "2024-01-05 10:29:51.289229: Pseudo dice [0.8632, 0.9599, 0.9415]\n",
      "2024-01-05 10:29:51.294227: Epoch time: 126.34 s\n",
      "2024-01-05 10:29:51.300232: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2024-01-05 10:29:52.596124: \n",
      "2024-01-05 10:29:52.602116: Epoch 201\n",
      "2024-01-05 10:29:52.607140: Current learning rate: 0.00817\n",
      "2024-01-05 10:31:58.479798: train_loss -0.9063\n",
      "2024-01-05 10:31:58.489796: val_loss -0.835\n",
      "2024-01-05 10:31:58.498800: Pseudo dice [0.8646, 0.9588, 0.94]\n",
      "2024-01-05 10:31:58.506796: Epoch time: 125.88 s\n",
      "2024-01-05 10:31:58.512796: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2024-01-05 10:31:59.833114: \n",
      "2024-01-05 10:31:59.839113: Epoch 202\n",
      "2024-01-05 10:31:59.846121: Current learning rate: 0.00816\n",
      "2024-01-05 10:34:06.965882: train_loss -0.9027\n",
      "2024-01-05 10:34:06.973392: val_loss -0.8393\n",
      "2024-01-05 10:34:06.981395: Pseudo dice [0.8641, 0.96, 0.9416]\n",
      "2024-01-05 10:34:06.988398: Epoch time: 127.13 s\n",
      "2024-01-05 10:34:06.994398: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-05 10:34:08.347189: \n",
      "2024-01-05 10:34:08.353190: Epoch 203\n",
      "2024-01-05 10:34:08.358208: Current learning rate: 0.00815\n",
      "2024-01-05 10:36:15.317694: train_loss -0.9035\n",
      "2024-01-05 10:36:15.327695: val_loss -0.8344\n",
      "2024-01-05 10:36:15.336693: Pseudo dice [0.8581, 0.9588, 0.9396]\n",
      "2024-01-05 10:36:15.344688: Epoch time: 126.97 s\n",
      "2024-01-05 10:36:16.447054: \n",
      "2024-01-05 10:36:16.452662: Epoch 204\n",
      "2024-01-05 10:36:16.458674: Current learning rate: 0.00814\n",
      "2024-01-05 10:38:24.089614: train_loss -0.907\n",
      "2024-01-05 10:38:24.101615: val_loss -0.8398\n",
      "2024-01-05 10:38:24.111614: Pseudo dice [0.8656, 0.9594, 0.9407]\n",
      "2024-01-05 10:38:24.121617: Epoch time: 127.64 s\n",
      "2024-01-05 10:38:25.488577: \n",
      "2024-01-05 10:38:25.503405: Epoch 205\n",
      "2024-01-05 10:38:25.513407: Current learning rate: 0.00813\n",
      "2024-01-05 10:40:32.391953: train_loss -0.903\n",
      "2024-01-05 10:40:32.402453: val_loss -0.8412\n",
      "2024-01-05 10:40:32.410455: Pseudo dice [0.8621, 0.9599, 0.9412]\n",
      "2024-01-05 10:40:32.417454: Epoch time: 126.91 s\n",
      "2024-01-05 10:40:33.456667: \n",
      "2024-01-05 10:40:33.462685: Epoch 206\n",
      "2024-01-05 10:40:33.468923: Current learning rate: 0.00813\n",
      "2024-01-05 10:42:40.832075: train_loss -0.897\n",
      "2024-01-05 10:42:40.841074: val_loss -0.8291\n",
      "2024-01-05 10:42:40.848074: Pseudo dice [0.8586, 0.9558, 0.9366]\n",
      "2024-01-05 10:42:40.855086: Epoch time: 127.38 s\n",
      "2024-01-05 10:42:41.883318: \n",
      "2024-01-05 10:42:41.889354: Epoch 207\n",
      "2024-01-05 10:42:41.894369: Current learning rate: 0.00812\n",
      "2024-01-05 10:44:48.608826: train_loss -0.8966\n",
      "2024-01-05 10:44:48.619823: val_loss -0.8305\n",
      "2024-01-05 10:44:48.627827: Pseudo dice [0.8604, 0.9571, 0.938]\n",
      "2024-01-05 10:44:48.635827: Epoch time: 126.73 s\n",
      "2024-01-05 10:44:49.669324: \n",
      "2024-01-05 10:44:49.678324: Epoch 208\n",
      "2024-01-05 10:44:49.683315: Current learning rate: 0.00811\n",
      "2024-01-05 10:46:57.012616: train_loss -0.8947\n",
      "2024-01-05 10:46:57.022617: val_loss -0.8364\n",
      "2024-01-05 10:46:57.030616: Pseudo dice [0.8631, 0.9576, 0.9379]\n",
      "2024-01-05 10:46:57.039622: Epoch time: 127.34 s\n",
      "2024-01-05 10:46:58.124331: \n",
      "2024-01-05 10:46:58.130405: Epoch 209\n",
      "2024-01-05 10:46:58.135406: Current learning rate: 0.0081\n",
      "2024-01-05 10:49:05.584011: train_loss -0.8947\n",
      "2024-01-05 10:49:05.594072: val_loss -0.8365\n",
      "2024-01-05 10:49:05.604078: Pseudo dice [0.8674, 0.9588, 0.9402]\n",
      "2024-01-05 10:49:05.612076: Epoch time: 127.46 s\n",
      "2024-01-05 10:49:06.675542: \n",
      "2024-01-05 10:49:06.682117: Epoch 210\n",
      "2024-01-05 10:49:06.687239: Current learning rate: 0.00809\n",
      "2024-01-05 10:51:13.359982: train_loss -0.8995\n",
      "2024-01-05 10:51:13.368986: val_loss -0.8395\n",
      "2024-01-05 10:51:13.377981: Pseudo dice [0.8658, 0.9586, 0.9398]\n",
      "2024-01-05 10:51:13.386990: Epoch time: 126.69 s\n",
      "2024-01-05 10:51:14.407559: \n",
      "2024-01-05 10:51:14.414557: Epoch 211\n",
      "2024-01-05 10:51:14.420565: Current learning rate: 0.00808\n",
      "2024-01-05 10:53:21.066510: train_loss -0.9032\n",
      "2024-01-05 10:53:21.076507: val_loss -0.8415\n",
      "2024-01-05 10:53:21.083508: Pseudo dice [0.8664, 0.9593, 0.9413]\n",
      "2024-01-05 10:53:21.091507: Epoch time: 126.66 s\n",
      "2024-01-05 10:53:22.294514: \n",
      "2024-01-05 10:53:22.300522: Epoch 212\n",
      "2024-01-05 10:53:22.304517: Current learning rate: 0.00807\n",
      "2024-01-05 10:55:30.056679: train_loss -0.9028\n",
      "2024-01-05 10:55:30.069667: val_loss -0.8395\n",
      "2024-01-05 10:55:30.078666: Pseudo dice [0.8617, 0.9586, 0.9399]\n",
      "2024-01-05 10:55:30.086675: Epoch time: 127.76 s\n",
      "2024-01-05 10:55:31.112468: \n",
      "2024-01-05 10:55:31.151313: Epoch 213\n",
      "2024-01-05 10:55:31.158321: Current learning rate: 0.00806\n",
      "2024-01-05 10:57:38.755346: train_loss -0.9032\n",
      "2024-01-05 10:57:38.765351: val_loss -0.8354\n",
      "2024-01-05 10:57:38.773344: Pseudo dice [0.8616, 0.9585, 0.9397]\n",
      "2024-01-05 10:57:38.780343: Epoch time: 127.64 s\n",
      "2024-01-05 10:57:39.752988: \n",
      "2024-01-05 10:57:39.761990: Epoch 214\n",
      "2024-01-05 10:57:39.767288: Current learning rate: 0.00805\n",
      "2024-01-05 10:59:46.648373: train_loss -0.9043\n",
      "2024-01-05 10:59:46.658377: val_loss -0.8363\n",
      "2024-01-05 10:59:46.667374: Pseudo dice [0.8611, 0.9581, 0.9386]\n",
      "2024-01-05 10:59:46.676381: Epoch time: 126.9 s\n",
      "2024-01-05 10:59:47.720413: \n",
      "2024-01-05 10:59:47.726521: Epoch 215\n",
      "2024-01-05 10:59:47.730582: Current learning rate: 0.00804\n",
      "2024-01-05 11:01:54.377437: train_loss -0.9054\n",
      "2024-01-05 11:01:54.386436: val_loss -0.8409\n",
      "2024-01-05 11:01:54.397506: Pseudo dice [0.8637, 0.9594, 0.9414]\n",
      "2024-01-05 11:01:54.405516: Epoch time: 126.66 s\n",
      "2024-01-05 11:01:55.391623: \n",
      "2024-01-05 11:01:55.397622: Epoch 216\n",
      "2024-01-05 11:01:55.402618: Current learning rate: 0.00803\n",
      "2024-01-05 11:04:03.233186: train_loss -0.9038\n",
      "2024-01-05 11:04:03.243724: val_loss -0.8393\n",
      "2024-01-05 11:04:03.253244: Pseudo dice [0.8639, 0.9596, 0.9422]\n",
      "2024-01-05 11:04:03.262246: Epoch time: 127.84 s\n",
      "2024-01-05 11:04:04.364646: \n",
      "2024-01-05 11:04:04.370645: Epoch 217\n",
      "2024-01-05 11:04:04.376639: Current learning rate: 0.00802\n",
      "2024-01-05 11:06:11.216426: train_loss -0.9053\n",
      "2024-01-05 11:06:11.224432: val_loss -0.8399\n",
      "2024-01-05 11:06:11.232002: Pseudo dice [0.8629, 0.9605, 0.9433]\n",
      "2024-01-05 11:06:11.240035: Epoch time: 126.85 s\n",
      "2024-01-05 11:06:12.222670: \n",
      "2024-01-05 11:06:12.235456: Epoch 218\n",
      "2024-01-05 11:06:12.240388: Current learning rate: 0.00801\n",
      "2024-01-05 11:08:19.014388: train_loss -0.9042\n",
      "2024-01-05 11:08:19.026388: val_loss -0.8404\n",
      "2024-01-05 11:08:19.037388: Pseudo dice [0.8626, 0.9606, 0.9428]\n",
      "2024-01-05 11:08:19.046394: Epoch time: 126.79 s\n",
      "2024-01-05 11:08:20.110663: \n",
      "2024-01-05 11:08:20.115670: Epoch 219\n",
      "2024-01-05 11:08:20.123671: Current learning rate: 0.00801\n",
      "2024-01-05 11:10:26.935792: train_loss -0.9037\n",
      "2024-01-05 11:10:26.943805: val_loss -0.8385\n",
      "2024-01-05 11:10:26.950796: Pseudo dice [0.8617, 0.9595, 0.9414]\n",
      "2024-01-05 11:10:26.958029: Epoch time: 126.83 s\n",
      "2024-01-05 11:10:28.129854: \n",
      "2024-01-05 11:10:28.140880: Epoch 220\n",
      "2024-01-05 11:10:28.147870: Current learning rate: 0.008\n",
      "2024-01-05 11:12:34.560055: train_loss -0.9052\n",
      "2024-01-05 11:12:34.570057: val_loss -0.8411\n",
      "2024-01-05 11:12:34.580055: Pseudo dice [0.8644, 0.9601, 0.9419]\n",
      "2024-01-05 11:12:34.591057: Epoch time: 126.43 s\n",
      "2024-01-05 11:12:34.601058: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-05 11:12:36.015225: \n",
      "2024-01-05 11:12:36.022159: Epoch 221\n",
      "2024-01-05 11:12:36.027819: Current learning rate: 0.00799\n",
      "2024-01-05 11:14:42.626224: train_loss -0.9052\n",
      "2024-01-05 11:14:42.635257: val_loss -0.8378\n",
      "2024-01-05 11:14:42.643259: Pseudo dice [0.8602, 0.9597, 0.9411]\n",
      "2024-01-05 11:14:42.651260: Epoch time: 126.61 s\n",
      "2024-01-05 11:14:43.660040: \n",
      "2024-01-05 11:14:43.666111: Epoch 222\n",
      "2024-01-05 11:14:43.671039: Current learning rate: 0.00798\n",
      "2024-01-05 11:16:51.077018: train_loss -0.9034\n",
      "2024-01-05 11:16:51.086526: val_loss -0.8417\n",
      "2024-01-05 11:16:51.094526: Pseudo dice [0.8631, 0.9589, 0.9401]\n",
      "2024-01-05 11:16:51.101531: Epoch time: 127.42 s\n",
      "2024-01-05 11:16:52.129017: \n",
      "2024-01-05 11:16:52.135017: Epoch 223\n",
      "2024-01-05 11:16:52.140645: Current learning rate: 0.00797\n",
      "2024-01-05 11:18:59.863450: train_loss -0.9021\n",
      "2024-01-05 11:18:59.874449: val_loss -0.8411\n",
      "2024-01-05 11:18:59.884448: Pseudo dice [0.8602, 0.9597, 0.9415]\n",
      "2024-01-05 11:18:59.891454: Epoch time: 127.74 s\n",
      "2024-01-05 11:19:00.940951: \n",
      "2024-01-05 11:19:00.946942: Epoch 224\n",
      "2024-01-05 11:19:00.950953: Current learning rate: 0.00796\n",
      "2024-01-05 11:21:08.272424: train_loss -0.9053\n",
      "2024-01-05 11:21:08.282424: val_loss -0.836\n",
      "2024-01-05 11:21:08.289427: Pseudo dice [0.8612, 0.9594, 0.9412]\n",
      "2024-01-05 11:21:08.296425: Epoch time: 127.33 s\n",
      "2024-01-05 11:21:09.285173: \n",
      "2024-01-05 11:21:09.290174: Epoch 225\n",
      "2024-01-05 11:21:09.295462: Current learning rate: 0.00795\n",
      "2024-01-05 11:23:16.474447: train_loss -0.9039\n",
      "2024-01-05 11:23:16.485943: val_loss -0.834\n",
      "2024-01-05 11:23:16.495950: Pseudo dice [0.8614, 0.9593, 0.9415]\n",
      "2024-01-05 11:23:16.504956: Epoch time: 127.19 s\n",
      "2024-01-05 11:23:17.502349: \n",
      "2024-01-05 11:23:17.509295: Epoch 226\n",
      "2024-01-05 11:23:17.514817: Current learning rate: 0.00794\n",
      "2024-01-05 11:25:24.511055: train_loss -0.9035\n",
      "2024-01-05 11:25:24.518563: val_loss -0.8336\n",
      "2024-01-05 11:25:24.525565: Pseudo dice [0.8591, 0.9595, 0.9415]\n",
      "2024-01-05 11:25:24.531564: Epoch time: 127.01 s\n",
      "2024-01-05 11:25:25.546149: \n",
      "2024-01-05 11:25:25.556796: Epoch 227\n",
      "2024-01-05 11:25:25.561809: Current learning rate: 0.00793\n",
      "2024-01-05 11:27:32.781201: train_loss -0.9048\n",
      "2024-01-05 11:27:32.788205: val_loss -0.8358\n",
      "2024-01-05 11:27:32.795207: Pseudo dice [0.8649, 0.958, 0.9386]\n",
      "2024-01-05 11:27:32.803718: Epoch time: 127.24 s\n",
      "2024-01-05 11:27:33.997483: \n",
      "2024-01-05 11:27:34.003780: Epoch 228\n",
      "2024-01-05 11:27:34.007788: Current learning rate: 0.00792\n",
      "2024-01-05 11:29:42.659000: train_loss -0.9048\n",
      "2024-01-05 11:29:42.669017: val_loss -0.8345\n",
      "2024-01-05 11:29:42.679524: Pseudo dice [0.8612, 0.9593, 0.941]\n",
      "2024-01-05 11:29:42.688525: Epoch time: 128.66 s\n",
      "2024-01-05 11:29:43.722939: \n",
      "2024-01-05 11:29:43.728930: Epoch 229\n",
      "2024-01-05 11:29:43.733939: Current learning rate: 0.00791\n",
      "2024-01-05 11:31:51.087359: train_loss -0.904\n",
      "2024-01-05 11:31:51.098360: val_loss -0.8401\n",
      "2024-01-05 11:31:51.106363: Pseudo dice [0.8609, 0.9601, 0.9421]\n",
      "2024-01-05 11:31:51.114360: Epoch time: 127.37 s\n",
      "2024-01-05 11:31:52.191679: \n",
      "2024-01-05 11:31:52.198686: Epoch 230\n",
      "2024-01-05 11:31:52.203714: Current learning rate: 0.0079\n",
      "2024-01-05 11:34:00.065339: train_loss -0.9074\n",
      "2024-01-05 11:34:00.076342: val_loss -0.8378\n",
      "2024-01-05 11:34:00.084343: Pseudo dice [0.8655, 0.9593, 0.9412]\n",
      "2024-01-05 11:34:00.092340: Epoch time: 127.87 s\n",
      "2024-01-05 11:34:01.298215: \n",
      "2024-01-05 11:34:01.304781: Epoch 231\n",
      "2024-01-05 11:34:01.311695: Current learning rate: 0.00789\n",
      "2024-01-05 11:36:09.426658: train_loss -0.9057\n",
      "2024-01-05 11:36:09.438189: val_loss -0.8421\n",
      "2024-01-05 11:36:09.444186: Pseudo dice [0.8627, 0.96, 0.9426]\n",
      "2024-01-05 11:36:09.451696: Epoch time: 128.13 s\n",
      "2024-01-05 11:36:10.654428: \n",
      "2024-01-05 11:36:10.659617: Epoch 232\n",
      "2024-01-05 11:36:10.664602: Current learning rate: 0.00789\n",
      "2024-01-05 11:38:18.555419: train_loss -0.9038\n",
      "2024-01-05 11:38:18.562424: val_loss -0.8378\n",
      "2024-01-05 11:38:18.569424: Pseudo dice [0.864, 0.9581, 0.9393]\n",
      "2024-01-05 11:38:18.574931: Epoch time: 127.9 s\n",
      "2024-01-05 11:38:19.874900: \n",
      "2024-01-05 11:38:19.880538: Epoch 233\n",
      "2024-01-05 11:38:19.891516: Current learning rate: 0.00788\n",
      "2024-01-05 11:40:27.115667: train_loss -0.8994\n",
      "2024-01-05 11:40:27.126664: val_loss -0.8375\n",
      "2024-01-05 11:40:27.135672: Pseudo dice [0.8632, 0.9597, 0.9421]\n",
      "2024-01-05 11:40:27.146181: Epoch time: 127.24 s\n",
      "2024-01-05 11:40:28.440014: \n",
      "2024-01-05 11:40:28.453603: Epoch 234\n",
      "2024-01-05 11:40:28.457602: Current learning rate: 0.00787\n",
      "2024-01-05 11:42:35.492688: train_loss -0.9057\n",
      "2024-01-05 11:42:35.503168: val_loss -0.8425\n",
      "2024-01-05 11:42:35.513710: Pseudo dice [0.8676, 0.9588, 0.9407]\n",
      "2024-01-05 11:42:35.523227: Epoch time: 127.05 s\n",
      "2024-01-05 11:42:35.533748: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-05 11:42:37.234065: \n",
      "2024-01-05 11:42:37.239073: Epoch 235\n",
      "2024-01-05 11:42:37.244073: Current learning rate: 0.00786\n",
      "2024-01-05 11:44:45.251386: train_loss -0.9085\n",
      "2024-01-05 11:44:45.261375: val_loss -0.8408\n",
      "2024-01-05 11:44:45.269581: Pseudo dice [0.8633, 0.9593, 0.9408]\n",
      "2024-01-05 11:44:45.276582: Epoch time: 128.02 s\n",
      "2024-01-05 11:44:45.284700: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-05 11:44:47.114968: \n",
      "2024-01-05 11:44:47.120968: Epoch 236\n",
      "2024-01-05 11:44:47.125972: Current learning rate: 0.00785\n",
      "2024-01-05 11:46:55.328650: train_loss -0.9048\n",
      "2024-01-05 11:46:55.338649: val_loss -0.8402\n",
      "2024-01-05 11:46:55.347649: Pseudo dice [0.8618, 0.9597, 0.9417]\n",
      "2024-01-05 11:46:55.355659: Epoch time: 128.21 s\n",
      "2024-01-05 11:46:56.660127: \n",
      "2024-01-05 11:46:56.667128: Epoch 237\n",
      "2024-01-05 11:46:56.671639: Current learning rate: 0.00784\n",
      "2024-01-05 11:49:05.165531: train_loss -0.9045\n",
      "2024-01-05 11:49:05.172531: val_loss -0.8378\n",
      "2024-01-05 11:49:05.177532: Pseudo dice [0.862, 0.96, 0.9421]\n",
      "2024-01-05 11:49:05.182532: Epoch time: 128.51 s\n",
      "2024-01-05 11:49:05.188531: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2024-01-05 11:49:06.695672: \n",
      "2024-01-05 11:49:06.701672: Epoch 238\n",
      "2024-01-05 11:49:06.707681: Current learning rate: 0.00783\n",
      "2024-01-05 11:51:14.528409: train_loss -0.906\n",
      "2024-01-05 11:51:14.535412: val_loss -0.8369\n",
      "2024-01-05 11:51:14.542415: Pseudo dice [0.8644, 0.9594, 0.9415]\n",
      "2024-01-05 11:51:14.547415: Epoch time: 127.83 s\n",
      "2024-01-05 11:51:14.552918: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2024-01-05 11:51:16.049946: \n",
      "2024-01-05 11:51:16.055586: Epoch 239\n",
      "2024-01-05 11:51:16.064523: Current learning rate: 0.00782\n",
      "2024-01-05 11:53:25.027804: train_loss -0.9072\n",
      "2024-01-05 11:53:25.039804: val_loss -0.8377\n",
      "2024-01-05 11:53:25.051113: Pseudo dice [0.8634, 0.959, 0.9411]\n",
      "2024-01-05 11:53:25.059229: Epoch time: 128.98 s\n",
      "2024-01-05 11:53:26.601030: \n",
      "2024-01-05 11:53:26.607029: Epoch 240\n",
      "2024-01-05 11:53:26.613563: Current learning rate: 0.00781\n",
      "2024-01-05 11:55:35.875016: train_loss -0.9048\n",
      "2024-01-05 11:55:35.888090: val_loss -0.8384\n",
      "2024-01-05 11:55:35.894312: Pseudo dice [0.8654, 0.9594, 0.9417]\n",
      "2024-01-05 11:55:35.900313: Epoch time: 129.28 s\n",
      "2024-01-05 11:55:35.907779: Yayy! New best EMA pseudo Dice: 0.9213\n",
      "2024-01-05 11:55:37.425501: \n",
      "2024-01-05 11:55:37.433634: Epoch 241\n",
      "2024-01-05 11:55:37.440378: Current learning rate: 0.0078\n",
      "2024-01-05 11:57:45.153482: train_loss -0.9092\n",
      "2024-01-05 11:57:45.166104: val_loss -0.8383\n",
      "2024-01-05 11:57:45.180115: Pseudo dice [0.8649, 0.9583, 0.9419]\n",
      "2024-01-05 11:57:45.194244: Epoch time: 127.73 s\n",
      "2024-01-05 11:57:45.206338: Yayy! New best EMA pseudo Dice: 0.9213\n",
      "2024-01-05 11:57:47.365907: \n",
      "2024-01-05 11:57:47.372727: Epoch 242\n",
      "2024-01-05 11:57:47.378678: Current learning rate: 0.00779\n",
      "2024-01-05 11:59:54.930023: train_loss -0.908\n",
      "2024-01-05 11:59:54.940029: val_loss -0.8372\n",
      "2024-01-05 11:59:54.947480: Pseudo dice [0.8626, 0.9587, 0.9407]\n",
      "2024-01-05 11:59:54.954480: Epoch time: 127.57 s\n",
      "2024-01-05 11:59:56.398830: \n",
      "2024-01-05 11:59:56.405022: Epoch 243\n",
      "2024-01-05 11:59:56.410943: Current learning rate: 0.00778\n",
      "2024-01-05 12:02:04.396458: train_loss -0.9053\n",
      "2024-01-05 12:02:04.408459: val_loss -0.8383\n",
      "2024-01-05 12:02:04.418460: Pseudo dice [0.8612, 0.9598, 0.9421]\n",
      "2024-01-05 12:02:04.426772: Epoch time: 128.0 s\n",
      "2024-01-05 12:02:05.908340: \n",
      "2024-01-05 12:02:05.915337: Epoch 244\n",
      "2024-01-05 12:02:05.920338: Current learning rate: 0.00777\n",
      "2024-01-05 12:04:13.678833: train_loss -0.9047\n",
      "2024-01-05 12:04:13.690833: val_loss -0.8338\n",
      "2024-01-05 12:04:13.698831: Pseudo dice [0.8641, 0.958, 0.9391]\n",
      "2024-01-05 12:04:13.706832: Epoch time: 127.77 s\n",
      "2024-01-05 12:04:15.076786: \n",
      "2024-01-05 12:04:15.082789: Epoch 245\n",
      "2024-01-05 12:04:15.092845: Current learning rate: 0.00777\n",
      "2024-01-05 12:06:23.581680: train_loss -0.9066\n",
      "2024-01-05 12:06:23.589680: val_loss -0.836\n",
      "2024-01-05 12:06:23.595680: Pseudo dice [0.8599, 0.9596, 0.9423]\n",
      "2024-01-05 12:06:23.601680: Epoch time: 128.51 s\n",
      "2024-01-05 12:06:24.778698: \n",
      "2024-01-05 12:06:24.784225: Epoch 246\n",
      "2024-01-05 12:06:24.789166: Current learning rate: 0.00776\n",
      "2024-01-05 12:08:33.754633: train_loss -0.9061\n",
      "2024-01-05 12:08:33.766268: val_loss -0.8397\n",
      "2024-01-05 12:08:33.777941: Pseudo dice [0.8639, 0.9587, 0.94]\n",
      "2024-01-05 12:08:33.788396: Epoch time: 128.98 s\n",
      "2024-01-05 12:08:35.296415: \n",
      "2024-01-05 12:08:35.304422: Epoch 247\n",
      "2024-01-05 12:08:35.311421: Current learning rate: 0.00775\n",
      "2024-01-05 12:10:43.615729: train_loss -0.9059\n",
      "2024-01-05 12:10:43.626975: val_loss -0.8398\n",
      "2024-01-05 12:10:43.637565: Pseudo dice [0.8668, 0.9591, 0.9411]\n",
      "2024-01-05 12:10:43.647914: Epoch time: 128.32 s\n",
      "2024-01-05 12:10:46.100882: \n",
      "2024-01-05 12:10:46.111889: Epoch 248\n",
      "2024-01-05 12:10:46.122730: Current learning rate: 0.00774\n",
      "2024-01-05 12:12:52.914955: train_loss -0.9091\n",
      "2024-01-05 12:12:52.926459: val_loss -0.8343\n",
      "2024-01-05 12:12:52.936662: Pseudo dice [0.8618, 0.9589, 0.9404]\n",
      "2024-01-05 12:12:52.944681: Epoch time: 126.82 s\n",
      "2024-01-05 12:12:53.932979: \n",
      "2024-01-05 12:12:53.939100: Epoch 249\n",
      "2024-01-05 12:12:53.943246: Current learning rate: 0.00773\n",
      "2024-01-05 12:15:00.437138: train_loss -0.9058\n",
      "2024-01-05 12:15:00.446138: val_loss -0.8377\n",
      "2024-01-05 12:15:00.456140: Pseudo dice [0.8639, 0.9591, 0.9409]\n",
      "2024-01-05 12:15:00.467145: Epoch time: 126.51 s\n",
      "2024-01-05 12:15:01.724496: \n",
      "2024-01-05 12:15:01.732502: Epoch 250\n",
      "2024-01-05 12:15:01.737574: Current learning rate: 0.00772\n",
      "2024-01-05 12:17:08.418423: train_loss -0.9064\n",
      "2024-01-05 12:17:08.428926: val_loss -0.8373\n",
      "2024-01-05 12:17:08.436934: Pseudo dice [0.865, 0.9598, 0.9416]\n",
      "2024-01-05 12:17:08.443931: Epoch time: 126.69 s\n",
      "2024-01-05 12:17:09.613359: \n",
      "2024-01-05 12:17:09.619517: Epoch 251\n",
      "2024-01-05 12:17:09.623688: Current learning rate: 0.00771\n",
      "2024-01-05 12:19:15.777494: train_loss -0.9058\n",
      "2024-01-05 12:19:15.793070: val_loss -0.8435\n",
      "2024-01-05 12:19:15.806095: Pseudo dice [0.8653, 0.9605, 0.9427]\n",
      "2024-01-05 12:19:15.821639: Epoch time: 126.16 s\n",
      "2024-01-05 12:19:15.840661: Yayy! New best EMA pseudo Dice: 0.9214\n",
      "2024-01-05 12:19:17.351527: \n",
      "2024-01-05 12:19:17.358516: Epoch 252\n",
      "2024-01-05 12:19:17.364525: Current learning rate: 0.0077\n",
      "2024-01-05 12:21:23.514793: train_loss -0.9085\n",
      "2024-01-05 12:21:23.522695: val_loss -0.8378\n",
      "2024-01-05 12:21:23.530704: Pseudo dice [0.8616, 0.9592, 0.9414]\n",
      "2024-01-05 12:21:23.538703: Epoch time: 126.16 s\n",
      "2024-01-05 12:21:24.514132: \n",
      "2024-01-05 12:21:24.520133: Epoch 253\n",
      "2024-01-05 12:21:24.524133: Current learning rate: 0.00769\n",
      "2024-01-05 12:23:30.806150: train_loss -0.906\n",
      "2024-01-05 12:23:30.818151: val_loss -0.8461\n",
      "2024-01-05 12:23:30.826158: Pseudo dice [0.8679, 0.9606, 0.943]\n",
      "2024-01-05 12:23:30.834157: Epoch time: 126.29 s\n",
      "2024-01-05 12:23:30.840149: Yayy! New best EMA pseudo Dice: 0.9216\n",
      "2024-01-05 12:23:32.059148: \n",
      "2024-01-05 12:23:32.064159: Epoch 254\n",
      "2024-01-05 12:23:32.069153: Current learning rate: 0.00768\n",
      "2024-01-05 12:25:38.568824: train_loss -0.9056\n",
      "2024-01-05 12:25:38.576824: val_loss -0.8395\n",
      "2024-01-05 12:25:38.586824: Pseudo dice [0.8681, 0.9601, 0.9418]\n",
      "2024-01-05 12:25:38.592823: Epoch time: 126.51 s\n",
      "2024-01-05 12:25:38.599823: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2024-01-05 12:25:39.890825: \n",
      "2024-01-05 12:25:39.896814: Epoch 255\n",
      "2024-01-05 12:25:39.901813: Current learning rate: 0.00767\n",
      "2024-01-05 12:27:46.602285: train_loss -0.9087\n",
      "2024-01-05 12:27:46.613447: val_loss -0.8351\n",
      "2024-01-05 12:27:46.624462: Pseudo dice [0.8608, 0.9598, 0.9424]\n",
      "2024-01-05 12:27:46.632976: Epoch time: 126.71 s\n",
      "2024-01-05 12:27:47.657259: \n",
      "2024-01-05 12:27:47.663258: Epoch 256\n",
      "2024-01-05 12:27:47.668251: Current learning rate: 0.00766\n",
      "2024-01-05 12:29:53.114941: train_loss -0.9076\n",
      "2024-01-05 12:29:53.126678: val_loss -0.8411\n",
      "2024-01-05 12:29:53.135042: Pseudo dice [0.8615, 0.9607, 0.9432]\n",
      "2024-01-05 12:29:53.144715: Epoch time: 125.46 s\n",
      "2024-01-05 12:29:54.380226: \n",
      "2024-01-05 12:29:54.386218: Epoch 257\n",
      "2024-01-05 12:29:54.391210: Current learning rate: 0.00765\n",
      "2024-01-05 12:32:00.282908: train_loss -0.9044\n",
      "2024-01-05 12:32:00.289909: val_loss -0.8389\n",
      "2024-01-05 12:32:00.296908: Pseudo dice [0.8606, 0.9601, 0.9421]\n",
      "2024-01-05 12:32:00.301908: Epoch time: 125.9 s\n",
      "2024-01-05 12:32:01.255291: \n",
      "2024-01-05 12:32:01.263945: Epoch 258\n",
      "2024-01-05 12:32:01.268540: Current learning rate: 0.00764\n",
      "2024-01-05 12:34:06.398741: train_loss -0.9069\n",
      "2024-01-05 12:34:06.405740: val_loss -0.8359\n",
      "2024-01-05 12:34:06.413290: Pseudo dice [0.8626, 0.9586, 0.9397]\n",
      "2024-01-05 12:34:06.417356: Epoch time: 125.14 s\n",
      "2024-01-05 12:34:07.527382: \n",
      "2024-01-05 12:34:07.532449: Epoch 259\n",
      "2024-01-05 12:34:07.537092: Current learning rate: 0.00764\n",
      "2024-01-05 12:36:12.432251: train_loss -0.9073\n",
      "2024-01-05 12:36:12.441240: val_loss -0.8422\n",
      "2024-01-05 12:36:12.448266: Pseudo dice [0.8629, 0.9603, 0.9425]\n",
      "2024-01-05 12:36:12.455343: Epoch time: 124.91 s\n",
      "2024-01-05 12:36:13.391607: \n",
      "2024-01-05 12:36:13.399393: Epoch 260\n",
      "2024-01-05 12:36:13.406469: Current learning rate: 0.00763\n",
      "2024-01-05 12:38:18.233783: train_loss -0.9074\n",
      "2024-01-05 12:38:18.242771: val_loss -0.8355\n",
      "2024-01-05 12:38:18.250785: Pseudo dice [0.8593, 0.9603, 0.9427]\n",
      "2024-01-05 12:38:18.257767: Epoch time: 124.84 s\n",
      "2024-01-05 12:38:19.200610: \n",
      "2024-01-05 12:38:19.206602: Epoch 261\n",
      "2024-01-05 12:38:19.210627: Current learning rate: 0.00762\n",
      "2024-01-05 12:40:23.777000: train_loss -0.9094\n",
      "2024-01-05 12:40:23.784999: val_loss -0.8329\n",
      "2024-01-05 12:40:23.792506: Pseudo dice [0.8615, 0.96, 0.9429]\n",
      "2024-01-05 12:40:23.799507: Epoch time: 124.58 s\n",
      "2024-01-05 12:40:24.781760: \n",
      "2024-01-05 12:40:24.786491: Epoch 262\n",
      "2024-01-05 12:40:24.790476: Current learning rate: 0.00761\n",
      "2024-01-05 12:42:30.048843: train_loss -0.9073\n",
      "2024-01-05 12:42:30.056345: val_loss -0.8391\n",
      "2024-01-05 12:42:30.063349: Pseudo dice [0.8584, 0.9605, 0.9429]\n",
      "2024-01-05 12:42:30.069350: Epoch time: 125.27 s\n",
      "2024-01-05 12:42:31.018679: \n",
      "2024-01-05 12:42:31.024345: Epoch 263\n",
      "2024-01-05 12:42:31.029359: Current learning rate: 0.0076\n",
      "2024-01-05 12:44:36.515452: train_loss -0.9081\n",
      "2024-01-05 12:44:36.522451: val_loss -0.8358\n",
      "2024-01-05 12:44:36.529452: Pseudo dice [0.8592, 0.9597, 0.9412]\n",
      "2024-01-05 12:44:36.537454: Epoch time: 125.5 s\n",
      "2024-01-05 12:44:37.488739: \n",
      "2024-01-05 12:44:37.497701: Epoch 264\n",
      "2024-01-05 12:44:37.501704: Current learning rate: 0.00759\n",
      "2024-01-05 12:46:43.853770: train_loss -0.9061\n",
      "2024-01-05 12:46:43.863765: val_loss -0.8378\n",
      "2024-01-05 12:46:43.872772: Pseudo dice [0.8591, 0.9601, 0.9424]\n",
      "2024-01-05 12:46:43.881772: Epoch time: 126.37 s\n",
      "2024-01-05 12:46:44.959404: \n",
      "2024-01-05 12:46:44.965412: Epoch 265\n",
      "2024-01-05 12:46:44.969922: Current learning rate: 0.00758\n",
      "2024-01-05 12:48:51.852421: train_loss -0.9064\n",
      "2024-01-05 12:48:51.859424: val_loss -0.8355\n",
      "2024-01-05 12:48:51.864925: Pseudo dice [0.8618, 0.9592, 0.9411]\n",
      "2024-01-05 12:48:51.870931: Epoch time: 126.9 s\n",
      "2024-01-05 12:48:53.229789: \n",
      "2024-01-05 12:48:53.241131: Epoch 266\n",
      "2024-01-05 12:48:53.246092: Current learning rate: 0.00757\n",
      "2024-01-05 12:50:59.989711: train_loss -0.9076\n",
      "2024-01-05 12:50:59.998710: val_loss -0.8361\n",
      "2024-01-05 12:51:00.007711: Pseudo dice [0.8644, 0.9595, 0.9408]\n",
      "2024-01-05 12:51:00.016709: Epoch time: 126.76 s\n",
      "2024-01-05 12:51:01.341138: \n",
      "2024-01-05 12:51:01.346401: Epoch 267\n",
      "2024-01-05 12:51:01.350465: Current learning rate: 0.00756\n",
      "2024-01-05 12:53:08.365766: train_loss -0.908\n",
      "2024-01-05 12:53:08.372766: val_loss -0.84\n",
      "2024-01-05 12:53:08.380771: Pseudo dice [0.8616, 0.9592, 0.9408]\n",
      "2024-01-05 12:53:08.387973: Epoch time: 127.03 s\n",
      "2024-01-05 12:53:09.614072: \n",
      "2024-01-05 12:53:09.620319: Epoch 268\n",
      "2024-01-05 12:53:09.627977: Current learning rate: 0.00755\n",
      "2024-01-05 12:55:17.093002: train_loss -0.9063\n",
      "2024-01-05 12:55:17.102517: val_loss -0.8375\n",
      "2024-01-05 12:55:17.108517: Pseudo dice [0.8685, 0.9592, 0.941]\n",
      "2024-01-05 12:55:17.114514: Epoch time: 127.48 s\n",
      "2024-01-05 12:55:18.217013: \n",
      "2024-01-05 12:55:18.222017: Epoch 269\n",
      "2024-01-05 12:55:18.231112: Current learning rate: 0.00754\n",
      "2024-01-05 12:57:25.035746: train_loss -0.9092\n",
      "2024-01-05 12:57:25.046745: val_loss -0.8314\n",
      "2024-01-05 12:57:25.055746: Pseudo dice [0.8594, 0.9595, 0.9408]\n",
      "2024-01-05 12:57:25.064745: Epoch time: 126.82 s\n",
      "2024-01-05 12:57:26.334947: \n",
      "2024-01-05 12:57:26.340875: Epoch 270\n",
      "2024-01-05 12:57:26.344954: Current learning rate: 0.00753\n",
      "2024-01-05 12:59:33.824422: train_loss -0.9077\n",
      "2024-01-05 12:59:33.834422: val_loss -0.8376\n",
      "2024-01-05 12:59:33.844422: Pseudo dice [0.8614, 0.9596, 0.9411]\n",
      "2024-01-05 12:59:33.851423: Epoch time: 127.49 s\n",
      "2024-01-05 12:59:34.852831: \n",
      "2024-01-05 12:59:34.857919: Epoch 271\n",
      "2024-01-05 12:59:34.861512: Current learning rate: 0.00752\n",
      "2024-01-05 13:01:40.566792: train_loss -0.9062\n",
      "2024-01-05 13:01:40.575798: val_loss -0.8309\n",
      "2024-01-05 13:01:40.581796: Pseudo dice [0.8552, 0.9585, 0.9401]\n",
      "2024-01-05 13:01:40.590797: Epoch time: 125.72 s\n",
      "2024-01-05 13:01:41.546803: \n",
      "2024-01-05 13:01:41.555982: Epoch 272\n",
      "2024-01-05 13:01:41.565021: Current learning rate: 0.00751\n",
      "2024-01-05 13:03:47.403343: train_loss -0.9079\n",
      "2024-01-05 13:03:47.412342: val_loss -0.8325\n",
      "2024-01-05 13:03:47.418343: Pseudo dice [0.8591, 0.9586, 0.9391]\n",
      "2024-01-05 13:03:47.425344: Epoch time: 125.86 s\n",
      "2024-01-05 13:03:48.497807: \n",
      "2024-01-05 13:03:48.551038: Epoch 273\n",
      "2024-01-05 13:03:48.556055: Current learning rate: 0.00751\n",
      "2024-01-05 13:05:55.352281: train_loss -0.9087\n",
      "2024-01-05 13:05:55.359387: val_loss -0.8346\n",
      "2024-01-05 13:05:55.364684: Pseudo dice [0.8595, 0.9588, 0.9404]\n",
      "2024-01-05 13:05:55.372679: Epoch time: 126.86 s\n",
      "2024-01-05 13:05:56.719141: \n",
      "2024-01-05 13:05:56.732794: Epoch 274\n",
      "2024-01-05 13:05:56.741855: Current learning rate: 0.0075\n",
      "2024-01-05 13:08:03.511938: train_loss -0.9082\n",
      "2024-01-05 13:08:03.520942: val_loss -0.8436\n",
      "2024-01-05 13:08:03.526456: Pseudo dice [0.8671, 0.9607, 0.9433]\n",
      "2024-01-05 13:08:03.532970: Epoch time: 126.8 s\n",
      "2024-01-05 13:08:04.965854: \n",
      "2024-01-05 13:08:04.970856: Epoch 275\n",
      "2024-01-05 13:08:04.976367: Current learning rate: 0.00749\n",
      "2024-01-05 13:10:11.790669: train_loss -0.9081\n",
      "2024-01-05 13:10:11.798670: val_loss -0.8398\n",
      "2024-01-05 13:10:11.805671: Pseudo dice [0.8644, 0.9612, 0.9435]\n",
      "2024-01-05 13:10:11.811179: Epoch time: 126.83 s\n",
      "2024-01-05 13:10:12.979282: \n",
      "2024-01-05 13:10:13.034827: Epoch 276\n",
      "2024-01-05 13:10:13.039907: Current learning rate: 0.00748\n",
      "2024-01-05 13:12:19.001552: train_loss -0.9082\n",
      "2024-01-05 13:12:19.009554: val_loss -0.8393\n",
      "2024-01-05 13:12:19.015556: Pseudo dice [0.8621, 0.9595, 0.9418]\n",
      "2024-01-05 13:12:19.020564: Epoch time: 126.02 s\n",
      "2024-01-05 13:12:20.206718: \n",
      "2024-01-05 13:12:20.212090: Epoch 277\n",
      "2024-01-05 13:12:20.216153: Current learning rate: 0.00747\n",
      "2024-01-05 13:14:26.304287: train_loss -0.9064\n",
      "2024-01-05 13:14:26.314287: val_loss -0.8347\n",
      "2024-01-05 13:14:26.321288: Pseudo dice [0.8614, 0.9589, 0.9409]\n",
      "2024-01-05 13:14:26.326288: Epoch time: 126.1 s\n",
      "2024-01-05 13:14:27.500574: \n",
      "2024-01-05 13:14:27.506763: Epoch 278\n",
      "2024-01-05 13:14:27.515125: Current learning rate: 0.00746\n",
      "2024-01-05 13:16:33.845956: train_loss -0.909\n",
      "2024-01-05 13:16:33.853957: val_loss -0.8379\n",
      "2024-01-05 13:16:33.858962: Pseudo dice [0.8683, 0.9588, 0.9409]\n",
      "2024-01-05 13:16:33.866499: Epoch time: 126.35 s\n",
      "2024-01-05 13:16:35.044632: \n",
      "2024-01-05 13:16:35.050631: Epoch 279\n",
      "2024-01-05 13:16:35.055604: Current learning rate: 0.00745\n",
      "2024-01-05 13:18:41.484452: train_loss -0.9057\n",
      "2024-01-05 13:18:41.493431: val_loss -0.8399\n",
      "2024-01-05 13:18:41.499437: Pseudo dice [0.8647, 0.9598, 0.9418]\n",
      "2024-01-05 13:18:41.505431: Epoch time: 126.44 s\n",
      "2024-01-05 13:18:42.643424: \n",
      "2024-01-05 13:18:42.649765: Epoch 280\n",
      "2024-01-05 13:18:42.656842: Current learning rate: 0.00744\n",
      "2024-01-05 13:20:48.638256: train_loss -0.908\n",
      "2024-01-05 13:20:48.645256: val_loss -0.8346\n",
      "2024-01-05 13:20:48.654256: Pseudo dice [0.8647, 0.9589, 0.9407]\n",
      "2024-01-05 13:20:48.661259: Epoch time: 126.0 s\n",
      "2024-01-05 13:20:49.908090: \n",
      "2024-01-05 13:20:49.919193: Epoch 281\n",
      "2024-01-05 13:20:49.925146: Current learning rate: 0.00743\n",
      "2024-01-05 13:22:55.489338: train_loss -0.9082\n",
      "2024-01-05 13:22:55.498337: val_loss -0.8325\n",
      "2024-01-05 13:22:55.507338: Pseudo dice [0.8617, 0.958, 0.9395]\n",
      "2024-01-05 13:22:55.514338: Epoch time: 125.58 s\n",
      "2024-01-05 13:22:56.911064: \n",
      "2024-01-05 13:22:56.916642: Epoch 282\n",
      "2024-01-05 13:22:56.924794: Current learning rate: 0.00742\n",
      "2024-01-05 13:25:02.446088: train_loss -0.9072\n",
      "2024-01-05 13:25:02.458599: val_loss -0.8412\n",
      "2024-01-05 13:25:02.466614: Pseudo dice [0.865, 0.9603, 0.9426]\n",
      "2024-01-05 13:25:02.474598: Epoch time: 125.54 s\n",
      "2024-01-05 13:25:03.643843: \n",
      "2024-01-05 13:25:03.651802: Epoch 283\n",
      "2024-01-05 13:25:03.656892: Current learning rate: 0.00741\n",
      "2024-01-05 13:27:09.494330: train_loss -0.9095\n",
      "2024-01-05 13:27:09.503329: val_loss -0.8391\n",
      "2024-01-05 13:27:09.510329: Pseudo dice [0.8659, 0.9592, 0.942]\n",
      "2024-01-05 13:27:09.515342: Epoch time: 125.85 s\n",
      "2024-01-05 13:27:10.658213: \n",
      "2024-01-05 13:27:10.667077: Epoch 284\n",
      "2024-01-05 13:27:10.672114: Current learning rate: 0.0074\n",
      "2024-01-05 13:29:16.100884: train_loss -0.9081\n",
      "2024-01-05 13:29:16.109885: val_loss -0.8373\n",
      "2024-01-05 13:29:16.118886: Pseudo dice [0.8633, 0.9604, 0.9423]\n",
      "2024-01-05 13:29:16.123886: Epoch time: 125.44 s\n",
      "2024-01-05 13:29:17.408318: \n",
      "2024-01-05 13:29:17.414359: Epoch 285\n",
      "2024-01-05 13:29:17.418437: Current learning rate: 0.00739\n",
      "2024-01-05 13:31:22.929927: train_loss -0.9063\n",
      "2024-01-05 13:31:22.939360: val_loss -0.8412\n",
      "2024-01-05 13:31:22.948356: Pseudo dice [0.862, 0.9603, 0.943]\n",
      "2024-01-05 13:31:22.953353: Epoch time: 125.52 s\n",
      "2024-01-05 13:31:24.024485: \n",
      "2024-01-05 13:31:24.030563: Epoch 286\n",
      "2024-01-05 13:31:24.035596: Current learning rate: 0.00738\n",
      "2024-01-05 13:33:29.553740: train_loss -0.9085\n",
      "2024-01-05 13:33:29.563746: val_loss -0.8397\n",
      "2024-01-05 13:33:29.571265: Pseudo dice [0.8614, 0.9594, 0.9412]\n",
      "2024-01-05 13:33:29.578263: Epoch time: 125.53 s\n",
      "2024-01-05 13:33:30.677475: \n",
      "2024-01-05 13:33:30.695498: Epoch 287\n",
      "2024-01-05 13:33:30.699456: Current learning rate: 0.00738\n",
      "2024-01-05 13:35:36.141207: train_loss -0.9074\n",
      "2024-01-05 13:35:36.152207: val_loss -0.8282\n",
      "2024-01-05 13:35:36.158206: Pseudo dice [0.8605, 0.9566, 0.937]\n",
      "2024-01-05 13:35:36.164205: Epoch time: 125.46 s\n",
      "2024-01-05 13:35:37.479925: \n",
      "2024-01-05 13:35:37.488910: Epoch 288\n",
      "2024-01-05 13:35:37.492837: Current learning rate: 0.00737\n",
      "2024-01-05 13:37:42.782704: train_loss -0.9091\n",
      "2024-01-05 13:37:42.791708: val_loss -0.8327\n",
      "2024-01-05 13:37:42.802214: Pseudo dice [0.8597, 0.9583, 0.9397]\n",
      "2024-01-05 13:37:42.806213: Epoch time: 125.31 s\n",
      "2024-01-05 13:37:43.969207: \n",
      "2024-01-05 13:37:43.976150: Epoch 289\n",
      "2024-01-05 13:37:43.980206: Current learning rate: 0.00736\n",
      "2024-01-05 13:39:50.000955: train_loss -0.9069\n",
      "2024-01-05 13:39:50.007955: val_loss -0.84\n",
      "2024-01-05 13:39:50.016955: Pseudo dice [0.8657, 0.9595, 0.9412]\n",
      "2024-01-05 13:39:50.021955: Epoch time: 126.03 s\n",
      "2024-01-05 13:39:51.510671: \n",
      "2024-01-05 13:39:51.516935: Epoch 290\n",
      "2024-01-05 13:39:51.521009: Current learning rate: 0.00735\n",
      "2024-01-05 13:41:57.100097: train_loss -0.9068\n",
      "2024-01-05 13:41:57.108099: val_loss -0.8391\n",
      "2024-01-05 13:41:57.115201: Pseudo dice [0.8657, 0.9595, 0.9411]\n",
      "2024-01-05 13:41:57.120199: Epoch time: 125.59 s\n",
      "2024-01-05 13:41:58.293272: \n",
      "2024-01-05 13:41:58.304914: Epoch 291\n",
      "2024-01-05 13:41:58.311416: Current learning rate: 0.00734\n",
      "2024-01-05 13:44:03.992131: train_loss -0.9065\n",
      "2024-01-05 13:44:03.998141: val_loss -0.8324\n",
      "2024-01-05 13:44:04.003140: Pseudo dice [0.8617, 0.9586, 0.9404]\n",
      "2024-01-05 13:44:04.008132: Epoch time: 125.7 s\n",
      "2024-01-05 13:44:05.152106: \n",
      "2024-01-05 13:44:05.161183: Epoch 292\n",
      "2024-01-05 13:44:05.165816: Current learning rate: 0.00733\n",
      "2024-01-05 13:46:10.445476: train_loss -0.9101\n",
      "2024-01-05 13:46:10.455468: val_loss -0.8381\n",
      "2024-01-05 13:46:10.464469: Pseudo dice [0.8615, 0.9601, 0.9419]\n",
      "2024-01-05 13:46:10.473466: Epoch time: 125.29 s\n",
      "2024-01-05 13:46:11.679783: \n",
      "2024-01-05 13:46:11.688965: Epoch 293\n",
      "2024-01-05 13:46:11.692937: Current learning rate: 0.00732\n",
      "2024-01-05 13:48:17.344763: train_loss -0.9083\n",
      "2024-01-05 13:48:17.352758: val_loss -0.8446\n",
      "2024-01-05 13:48:17.359758: Pseudo dice [0.8645, 0.9598, 0.9419]\n",
      "2024-01-05 13:48:17.364757: Epoch time: 125.67 s\n",
      "2024-01-05 13:48:18.803967: \n",
      "2024-01-05 13:48:18.811922: Epoch 294\n",
      "2024-01-05 13:48:18.817856: Current learning rate: 0.00731\n",
      "2024-01-05 13:50:24.131110: train_loss -0.9053\n",
      "2024-01-05 13:50:24.139110: val_loss -0.8343\n",
      "2024-01-05 13:50:24.146110: Pseudo dice [0.8572, 0.9594, 0.9412]\n",
      "2024-01-05 13:50:24.152108: Epoch time: 125.33 s\n",
      "2024-01-05 13:50:25.410606: \n",
      "2024-01-05 13:50:25.418278: Epoch 295\n",
      "2024-01-05 13:50:25.426336: Current learning rate: 0.0073\n",
      "2024-01-05 13:52:30.925436: train_loss -0.9074\n",
      "2024-01-05 13:52:30.932437: val_loss -0.8379\n",
      "2024-01-05 13:52:30.937436: Pseudo dice [0.8608, 0.9599, 0.9427]\n",
      "2024-01-05 13:52:30.940945: Epoch time: 125.52 s\n",
      "2024-01-05 13:52:32.136512: \n",
      "2024-01-05 13:52:32.142073: Epoch 296\n",
      "2024-01-05 13:52:32.147148: Current learning rate: 0.00729\n",
      "2024-01-05 13:54:37.335300: train_loss -0.9094\n",
      "2024-01-05 13:54:37.344304: val_loss -0.8342\n",
      "2024-01-05 13:54:37.351303: Pseudo dice [0.8612, 0.9595, 0.9417]\n",
      "2024-01-05 13:54:37.359304: Epoch time: 125.2 s\n",
      "2024-01-05 13:54:38.693018: \n",
      "2024-01-05 13:54:38.698709: Epoch 297\n",
      "2024-01-05 13:54:38.709763: Current learning rate: 0.00728\n",
      "2024-01-05 13:56:44.081972: train_loss -0.9099\n",
      "2024-01-05 13:56:44.088894: val_loss -0.8334\n",
      "2024-01-05 13:56:44.093966: Pseudo dice [0.8631, 0.9585, 0.9398]\n",
      "2024-01-05 13:56:44.098975: Epoch time: 125.39 s\n",
      "2024-01-05 13:56:45.237909: \n",
      "2024-01-05 13:56:45.244230: Epoch 298\n",
      "2024-01-05 13:56:45.250297: Current learning rate: 0.00727\n",
      "2024-01-05 13:58:50.893273: train_loss -0.9069\n",
      "2024-01-05 13:58:50.900272: val_loss -0.8382\n",
      "2024-01-05 13:58:50.907272: Pseudo dice [0.8644, 0.9589, 0.9399]\n",
      "2024-01-05 13:58:50.913274: Epoch time: 125.66 s\n",
      "2024-01-05 13:58:52.129112: \n",
      "2024-01-05 13:58:52.136176: Epoch 299\n",
      "2024-01-05 13:58:52.144113: Current learning rate: 0.00726\n",
      "2024-01-05 14:00:57.687063: train_loss -0.9084\n",
      "2024-01-05 14:00:57.693599: val_loss -0.8366\n",
      "2024-01-05 14:00:57.698607: Pseudo dice [0.8609, 0.9584, 0.9394]\n",
      "2024-01-05 14:00:57.702595: Epoch time: 125.56 s\n",
      "2024-01-05 14:00:59.270550: \n",
      "2024-01-05 14:00:59.278711: Epoch 300\n",
      "2024-01-05 14:00:59.289721: Current learning rate: 0.00725\n",
      "2024-01-05 14:03:04.728274: train_loss -0.9096\n",
      "2024-01-05 14:03:04.734274: val_loss -0.8374\n",
      "2024-01-05 14:03:04.742274: Pseudo dice [0.8625, 0.9587, 0.9401]\n",
      "2024-01-05 14:03:04.747274: Epoch time: 125.46 s\n",
      "2024-01-05 14:03:06.021772: \n",
      "2024-01-05 14:03:06.035545: Epoch 301\n",
      "2024-01-05 14:03:06.042607: Current learning rate: 0.00724\n",
      "2024-01-05 14:05:11.048038: train_loss -0.9116\n",
      "2024-01-05 14:05:11.057281: val_loss -0.8359\n",
      "2024-01-05 14:05:11.066227: Pseudo dice [0.8625, 0.9593, 0.9412]\n",
      "2024-01-05 14:05:11.075238: Epoch time: 125.03 s\n",
      "2024-01-05 14:05:12.258046: \n",
      "2024-01-05 14:05:12.266023: Epoch 302\n",
      "2024-01-05 14:05:12.270738: Current learning rate: 0.00724\n",
      "2024-01-05 14:07:17.768101: train_loss -0.9083\n",
      "2024-01-05 14:07:17.779102: val_loss -0.838\n",
      "2024-01-05 14:07:17.786102: Pseudo dice [0.8589, 0.9595, 0.9413]\n",
      "2024-01-05 14:07:17.793101: Epoch time: 125.51 s\n",
      "2024-01-05 14:07:19.063451: \n",
      "2024-01-05 14:07:19.071052: Epoch 303\n",
      "2024-01-05 14:07:19.075042: Current learning rate: 0.00723\n",
      "2024-01-05 14:09:24.690557: train_loss -0.9094\n",
      "2024-01-05 14:09:24.696558: val_loss -0.8282\n",
      "2024-01-05 14:09:24.701558: Pseudo dice [0.856, 0.9584, 0.9399]\n",
      "2024-01-05 14:09:24.707566: Epoch time: 125.63 s\n",
      "2024-01-05 14:09:25.816075: \n",
      "2024-01-05 14:09:25.821531: Epoch 304\n",
      "2024-01-05 14:09:25.826535: Current learning rate: 0.00722\n",
      "2024-01-05 14:11:32.031810: train_loss -0.9047\n",
      "2024-01-05 14:11:32.039806: val_loss -0.836\n",
      "2024-01-05 14:11:32.045818: Pseudo dice [0.8595, 0.9596, 0.9415]\n",
      "2024-01-05 14:11:32.051817: Epoch time: 126.22 s\n",
      "2024-01-05 14:11:33.346942: \n",
      "2024-01-05 14:11:33.352947: Epoch 305\n",
      "2024-01-05 14:11:33.360948: Current learning rate: 0.00721\n",
      "2024-01-05 14:13:38.847244: train_loss -0.9076\n",
      "2024-01-05 14:13:38.855246: val_loss -0.8374\n",
      "2024-01-05 14:13:38.861756: Pseudo dice [0.8604, 0.9594, 0.9415]\n",
      "2024-01-05 14:13:38.867757: Epoch time: 125.5 s\n",
      "2024-01-05 14:13:40.089294: \n",
      "2024-01-05 14:13:40.098504: Epoch 306\n",
      "2024-01-05 14:13:40.102566: Current learning rate: 0.0072\n",
      "2024-01-05 14:15:45.496853: train_loss -0.9097\n",
      "2024-01-05 14:15:45.503844: val_loss -0.8352\n",
      "2024-01-05 14:15:45.508844: Pseudo dice [0.8648, 0.9587, 0.9399]\n",
      "2024-01-05 14:15:45.513844: Epoch time: 125.41 s\n",
      "2024-01-05 14:15:46.694708: \n",
      "2024-01-05 14:15:46.699972: Epoch 307\n",
      "2024-01-05 14:15:46.708043: Current learning rate: 0.00719\n",
      "2024-01-05 14:17:52.501900: train_loss -0.9086\n",
      "2024-01-05 14:17:52.511901: val_loss -0.8395\n",
      "2024-01-05 14:17:52.518903: Pseudo dice [0.8626, 0.9605, 0.9431]\n",
      "2024-01-05 14:17:52.523920: Epoch time: 125.81 s\n",
      "2024-01-05 14:17:53.674730: \n",
      "2024-01-05 14:17:53.682091: Epoch 308\n",
      "2024-01-05 14:17:53.687595: Current learning rate: 0.00718\n",
      "2024-01-05 14:19:59.275616: train_loss -0.9084\n",
      "2024-01-05 14:19:59.284612: val_loss -0.8355\n",
      "2024-01-05 14:19:59.293143: Pseudo dice [0.8599, 0.96, 0.9421]\n",
      "2024-01-05 14:19:59.298143: Epoch time: 125.6 s\n",
      "2024-01-05 14:20:00.720042: \n",
      "2024-01-05 14:20:00.725038: Epoch 309\n",
      "2024-01-05 14:20:00.729037: Current learning rate: 0.00717\n",
      "2024-01-05 14:22:05.991657: train_loss -0.9086\n",
      "2024-01-05 14:22:06.000166: val_loss -0.8358\n",
      "2024-01-05 14:22:06.007342: Pseudo dice [0.8637, 0.9588, 0.9405]\n",
      "2024-01-05 14:22:06.012343: Epoch time: 125.27 s\n",
      "2024-01-05 14:22:07.281044: \n",
      "2024-01-05 14:22:07.288046: Epoch 310\n",
      "2024-01-05 14:22:07.292102: Current learning rate: 0.00716\n",
      "2024-01-05 14:24:12.858144: train_loss -0.9083\n",
      "2024-01-05 14:24:12.864148: val_loss -0.8372\n",
      "2024-01-05 14:24:12.870145: Pseudo dice [0.8649, 0.9589, 0.9403]\n",
      "2024-01-05 14:24:12.875145: Epoch time: 125.58 s\n",
      "2024-01-05 14:24:13.959902: \n",
      "2024-01-05 14:24:13.967865: Epoch 311\n",
      "2024-01-05 14:24:13.973895: Current learning rate: 0.00715\n",
      "2024-01-05 14:26:19.574816: train_loss -0.9079\n",
      "2024-01-05 14:26:19.582817: val_loss -0.8441\n",
      "2024-01-05 14:26:19.590823: Pseudo dice [0.8633, 0.9603, 0.943]\n",
      "2024-01-05 14:26:19.598822: Epoch time: 125.62 s\n",
      "2024-01-05 14:26:20.967176: \n",
      "2024-01-05 14:26:20.972717: Epoch 312\n",
      "2024-01-05 14:26:20.978649: Current learning rate: 0.00714\n",
      "2024-01-05 14:28:26.377957: train_loss -0.912\n",
      "2024-01-05 14:28:26.386960: val_loss -0.839\n",
      "2024-01-05 14:28:26.394966: Pseudo dice [0.8626, 0.9592, 0.9404]\n",
      "2024-01-05 14:28:26.402957: Epoch time: 125.41 s\n",
      "2024-01-05 14:28:27.574244: \n",
      "2024-01-05 14:28:27.580354: Epoch 313\n",
      "2024-01-05 14:28:27.585444: Current learning rate: 0.00713\n",
      "2024-01-05 14:30:33.085709: train_loss -0.9074\n",
      "2024-01-05 14:30:33.095718: val_loss -0.8409\n",
      "2024-01-05 14:30:33.105711: Pseudo dice [0.8597, 0.9595, 0.9421]\n",
      "2024-01-05 14:30:33.114710: Epoch time: 125.51 s\n",
      "2024-01-05 14:30:34.357369: \n",
      "2024-01-05 14:30:34.366198: Epoch 314\n",
      "2024-01-05 14:30:34.373137: Current learning rate: 0.00712\n",
      "2024-01-05 14:32:39.918378: train_loss -0.9076\n",
      "2024-01-05 14:32:39.926886: val_loss -0.8262\n",
      "2024-01-05 14:32:39.933886: Pseudo dice [0.8599, 0.9576, 0.9386]\n",
      "2024-01-05 14:32:39.939886: Epoch time: 125.56 s\n",
      "2024-01-05 14:32:41.176492: \n",
      "2024-01-05 14:32:41.182491: Epoch 315\n",
      "2024-01-05 14:32:41.186423: Current learning rate: 0.00711\n",
      "2024-01-05 14:34:46.365917: train_loss -0.911\n",
      "2024-01-05 14:34:46.372927: val_loss -0.8364\n",
      "2024-01-05 14:34:46.379425: Pseudo dice [0.8637, 0.9595, 0.9416]\n",
      "2024-01-05 14:34:46.385426: Epoch time: 125.19 s\n",
      "2024-01-05 14:34:47.558275: \n",
      "2024-01-05 14:34:47.564615: Epoch 316\n",
      "2024-01-05 14:34:47.568587: Current learning rate: 0.0071\n",
      "2024-01-05 14:36:52.629850: train_loss -0.9109\n",
      "2024-01-05 14:36:52.637853: val_loss -0.8395\n",
      "2024-01-05 14:36:52.642855: Pseudo dice [0.8631, 0.9595, 0.9417]\n",
      "2024-01-05 14:36:52.649360: Epoch time: 125.07 s\n",
      "2024-01-05 14:36:53.854748: \n",
      "2024-01-05 14:36:53.861609: Epoch 317\n",
      "2024-01-05 14:36:53.865689: Current learning rate: 0.0071\n",
      "2024-01-05 14:38:59.124741: train_loss -0.9111\n",
      "2024-01-05 14:38:59.131245: val_loss -0.8301\n",
      "2024-01-05 14:38:59.137243: Pseudo dice [0.8632, 0.9594, 0.9412]\n",
      "2024-01-05 14:38:59.142250: Epoch time: 125.27 s\n",
      "2024-01-05 14:39:00.266908: \n",
      "2024-01-05 14:39:00.272772: Epoch 318\n",
      "2024-01-05 14:39:00.278832: Current learning rate: 0.00709\n",
      "2024-01-05 14:41:05.790254: train_loss -0.9095\n",
      "2024-01-05 14:41:05.799254: val_loss -0.8342\n",
      "2024-01-05 14:41:05.808259: Pseudo dice [0.8603, 0.9591, 0.9402]\n",
      "2024-01-05 14:41:05.814255: Epoch time: 125.52 s\n",
      "2024-01-05 14:41:07.066343: \n",
      "2024-01-05 14:41:07.074031: Epoch 319\n",
      "2024-01-05 14:41:07.079027: Current learning rate: 0.00708\n",
      "2024-01-05 14:43:13.056602: train_loss -0.9078\n",
      "2024-01-05 14:43:13.063603: val_loss -0.8333\n",
      "2024-01-05 14:43:13.068603: Pseudo dice [0.8595, 0.959, 0.9411]\n",
      "2024-01-05 14:43:13.073604: Epoch time: 125.99 s\n",
      "2024-01-05 14:43:14.376321: \n",
      "2024-01-05 14:43:14.385295: Epoch 320\n",
      "2024-01-05 14:43:14.393224: Current learning rate: 0.00707\n",
      "2024-01-05 14:45:19.853875: train_loss -0.9083\n",
      "2024-01-05 14:45:19.860888: val_loss -0.8355\n",
      "2024-01-05 14:45:19.866879: Pseudo dice [0.863, 0.9597, 0.9418]\n",
      "2024-01-05 14:45:19.871397: Epoch time: 125.48 s\n",
      "2024-01-05 14:45:21.044160: \n",
      "2024-01-05 14:45:21.049811: Epoch 321\n",
      "2024-01-05 14:45:21.061009: Current learning rate: 0.00706\n",
      "2024-01-05 14:47:26.602454: train_loss -0.9094\n",
      "2024-01-05 14:47:26.609456: val_loss -0.8405\n",
      "2024-01-05 14:47:26.618452: Pseudo dice [0.8655, 0.9606, 0.9423]\n",
      "2024-01-05 14:47:26.623457: Epoch time: 125.56 s\n",
      "2024-01-05 14:47:27.931162: \n",
      "2024-01-05 14:47:27.936167: Epoch 322\n",
      "2024-01-05 14:47:27.941255: Current learning rate: 0.00705\n",
      "2024-01-05 14:49:33.231846: train_loss -0.9094\n",
      "2024-01-05 14:49:33.239847: val_loss -0.8399\n",
      "2024-01-05 14:49:33.246427: Pseudo dice [0.8653, 0.9606, 0.9425]\n",
      "2024-01-05 14:49:33.250441: Epoch time: 125.3 s\n",
      "2024-01-05 14:49:34.481772: \n",
      "2024-01-05 14:49:34.487775: Epoch 323\n",
      "2024-01-05 14:49:34.493706: Current learning rate: 0.00704\n",
      "2024-01-05 14:51:39.293921: train_loss -0.912\n",
      "2024-01-05 14:51:39.302922: val_loss -0.8368\n",
      "2024-01-05 14:51:39.309920: Pseudo dice [0.8615, 0.9599, 0.9423]\n",
      "2024-01-05 14:51:39.313924: Epoch time: 124.81 s\n",
      "2024-01-05 14:51:40.486240: \n",
      "2024-01-05 14:51:40.499785: Epoch 324\n",
      "2024-01-05 14:51:40.508088: Current learning rate: 0.00703\n",
      "2024-01-05 14:53:45.595515: train_loss -0.9107\n",
      "2024-01-05 14:53:45.604507: val_loss -0.835\n",
      "2024-01-05 14:53:45.610019: Pseudo dice [0.8612, 0.9599, 0.942]\n",
      "2024-01-05 14:53:45.615022: Epoch time: 125.11 s\n",
      "2024-01-05 14:53:46.768059: \n",
      "2024-01-05 14:53:46.782343: Epoch 325\n",
      "2024-01-05 14:53:46.788338: Current learning rate: 0.00702\n",
      "2024-01-05 14:55:52.048841: train_loss -0.9122\n",
      "2024-01-05 14:55:52.057358: val_loss -0.8368\n",
      "2024-01-05 14:55:52.065870: Pseudo dice [0.8653, 0.9594, 0.9406]\n",
      "2024-01-05 14:55:52.071375: Epoch time: 125.28 s\n",
      "2024-01-05 14:55:53.309825: \n",
      "2024-01-05 14:55:53.315907: Epoch 326\n",
      "2024-01-05 14:55:53.320923: Current learning rate: 0.00701\n",
      "2024-01-05 14:57:58.848094: train_loss -0.9114\n",
      "2024-01-05 14:57:58.855095: val_loss -0.8305\n",
      "2024-01-05 14:57:58.861094: Pseudo dice [0.8593, 0.9586, 0.94]\n",
      "2024-01-05 14:57:58.866098: Epoch time: 125.54 s\n",
      "2024-01-05 14:58:00.028680: \n",
      "2024-01-05 14:58:00.033677: Epoch 327\n",
      "2024-01-05 14:58:00.037672: Current learning rate: 0.007\n",
      "2024-01-05 15:00:06.298961: train_loss -0.8874\n",
      "2024-01-05 15:00:06.305960: val_loss -0.8255\n",
      "2024-01-05 15:00:06.311960: Pseudo dice [0.8526, 0.9563, 0.9372]\n",
      "2024-01-05 15:00:06.315960: Epoch time: 126.27 s\n",
      "2024-01-05 15:00:07.703658: \n",
      "2024-01-05 15:00:07.712717: Epoch 328\n",
      "2024-01-05 15:00:07.718724: Current learning rate: 0.00699\n",
      "2024-01-05 15:02:13.221261: train_loss -0.8968\n",
      "2024-01-05 15:02:13.230261: val_loss -0.8326\n",
      "2024-01-05 15:02:13.237261: Pseudo dice [0.8537, 0.9595, 0.9412]\n",
      "2024-01-05 15:02:13.242261: Epoch time: 125.52 s\n",
      "2024-01-05 15:02:14.490939: \n",
      "2024-01-05 15:02:14.496824: Epoch 329\n",
      "2024-01-05 15:02:14.503906: Current learning rate: 0.00698\n",
      "2024-01-05 15:04:19.760262: train_loss -0.9067\n",
      "2024-01-05 15:04:19.769263: val_loss -0.8391\n",
      "2024-01-05 15:04:19.775263: Pseudo dice [0.8608, 0.958, 0.9387]\n",
      "2024-01-05 15:04:19.781262: Epoch time: 125.27 s\n",
      "2024-01-05 15:04:21.069992: \n",
      "2024-01-05 15:04:21.074557: Epoch 330\n",
      "2024-01-05 15:04:21.083560: Current learning rate: 0.00697\n",
      "2024-01-05 15:06:26.453712: train_loss -0.9038\n",
      "2024-01-05 15:06:26.462712: val_loss -0.8435\n",
      "2024-01-05 15:06:26.467711: Pseudo dice [0.8613, 0.9594, 0.9411]\n",
      "2024-01-05 15:06:26.474711: Epoch time: 125.38 s\n",
      "2024-01-05 15:06:27.776240: \n",
      "2024-01-05 15:06:27.786178: Epoch 331\n",
      "2024-01-05 15:06:27.791240: Current learning rate: 0.00696\n",
      "2024-01-05 15:08:33.042778: train_loss -0.9073\n",
      "2024-01-05 15:08:33.050858: val_loss -0.838\n",
      "2024-01-05 15:08:33.056865: Pseudo dice [0.8677, 0.9588, 0.9407]\n",
      "2024-01-05 15:08:33.061783: Epoch time: 125.27 s\n",
      "2024-01-05 15:08:34.255221: \n",
      "2024-01-05 15:08:34.267128: Epoch 332\n",
      "2024-01-05 15:08:34.276122: Current learning rate: 0.00696\n",
      "2024-01-05 15:10:39.433418: train_loss -0.9079\n",
      "2024-01-05 15:10:39.441417: val_loss -0.8398\n",
      "2024-01-05 15:10:39.448419: Pseudo dice [0.8612, 0.9603, 0.9424]\n",
      "2024-01-05 15:10:39.453419: Epoch time: 125.18 s\n",
      "2024-01-05 15:10:40.622677: \n",
      "2024-01-05 15:10:40.628202: Epoch 333\n",
      "2024-01-05 15:10:40.632208: Current learning rate: 0.00695\n",
      "2024-01-05 15:12:46.069539: train_loss -0.9061\n",
      "2024-01-05 15:12:46.078537: val_loss -0.8345\n",
      "2024-01-05 15:12:46.084537: Pseudo dice [0.8569, 0.96, 0.942]\n",
      "2024-01-05 15:12:46.092537: Epoch time: 125.45 s\n",
      "2024-01-05 15:12:47.271350: \n",
      "2024-01-05 15:12:47.278010: Epoch 334\n",
      "2024-01-05 15:12:47.282075: Current learning rate: 0.00694\n",
      "2024-01-05 15:14:52.963882: train_loss -0.908\n",
      "2024-01-05 15:14:52.970883: val_loss -0.8356\n",
      "2024-01-05 15:14:52.975882: Pseudo dice [0.8627, 0.9591, 0.9409]\n",
      "2024-01-05 15:14:52.980882: Epoch time: 125.69 s\n",
      "2024-01-05 15:14:54.337649: \n",
      "2024-01-05 15:14:54.343165: Epoch 335\n",
      "2024-01-05 15:14:54.351180: Current learning rate: 0.00693\n",
      "2024-01-05 15:17:00.513856: train_loss -0.9112\n",
      "2024-01-05 15:17:00.521856: val_loss -0.8313\n",
      "2024-01-05 15:17:00.527859: Pseudo dice [0.8594, 0.9586, 0.9402]\n",
      "2024-01-05 15:17:00.532855: Epoch time: 126.18 s\n",
      "2024-01-05 15:17:01.650953: \n",
      "2024-01-05 15:17:01.657161: Epoch 336\n",
      "2024-01-05 15:17:01.663251: Current learning rate: 0.00692\n",
      "2024-01-05 15:19:09.166192: train_loss -0.9106\n",
      "2024-01-05 15:19:09.177849: val_loss -0.8402\n",
      "2024-01-05 15:19:09.187327: Pseudo dice [0.8636, 0.9602, 0.9428]\n",
      "2024-01-05 15:19:09.196884: Epoch time: 127.52 s\n",
      "2024-01-05 15:19:10.935479: \n",
      "2024-01-05 15:19:10.943458: Epoch 337\n",
      "2024-01-05 15:19:10.949450: Current learning rate: 0.00691\n",
      "2024-01-05 15:21:17.710289: train_loss -0.9083\n",
      "2024-01-05 15:21:17.717288: val_loss -0.8318\n",
      "2024-01-05 15:21:17.723288: Pseudo dice [0.8621, 0.9564, 0.9378]\n",
      "2024-01-05 15:21:17.728294: Epoch time: 126.78 s\n",
      "2024-01-05 15:21:19.042261: \n",
      "2024-01-05 15:21:19.050937: Epoch 338\n",
      "2024-01-05 15:21:19.059927: Current learning rate: 0.0069\n",
      "2024-01-05 15:23:25.630752: train_loss -0.9116\n",
      "2024-01-05 15:23:25.639761: val_loss -0.8346\n",
      "2024-01-05 15:23:25.649763: Pseudo dice [0.8616, 0.9597, 0.9411]\n",
      "2024-01-05 15:23:25.658818: Epoch time: 126.59 s\n",
      "2024-01-05 15:23:27.014453: \n",
      "2024-01-05 15:23:27.019452: Epoch 339\n",
      "2024-01-05 15:23:27.024453: Current learning rate: 0.00689\n",
      "2024-01-05 15:25:33.182103: train_loss -0.9087\n",
      "2024-01-05 15:25:33.189106: val_loss -0.8392\n",
      "2024-01-05 15:25:33.194105: Pseudo dice [0.8622, 0.9595, 0.9415]\n",
      "2024-01-05 15:25:33.199104: Epoch time: 126.17 s\n",
      "2024-01-05 15:25:34.398396: \n",
      "2024-01-05 15:25:34.405432: Epoch 340\n",
      "2024-01-05 15:25:34.409463: Current learning rate: 0.00688\n",
      "2024-01-05 15:27:40.131823: train_loss -0.9087\n",
      "2024-01-05 15:27:40.141366: val_loss -0.8402\n",
      "2024-01-05 15:27:40.146362: Pseudo dice [0.8617, 0.9605, 0.9418]\n",
      "2024-01-05 15:27:40.153365: Epoch time: 125.73 s\n",
      "2024-01-05 15:27:41.432349: \n",
      "2024-01-05 15:27:41.437391: Epoch 341\n",
      "2024-01-05 15:27:41.444124: Current learning rate: 0.00687\n",
      "2024-01-05 15:29:49.265967: train_loss -0.9084\n",
      "2024-01-05 15:29:49.275968: val_loss -0.8391\n",
      "2024-01-05 15:29:49.284967: Pseudo dice [0.8651, 0.9594, 0.9411]\n",
      "2024-01-05 15:29:49.292966: Epoch time: 127.84 s\n",
      "2024-01-05 15:29:50.717209: \n",
      "2024-01-05 15:29:50.726223: Epoch 342\n",
      "2024-01-05 15:29:50.730227: Current learning rate: 0.00686\n",
      "2024-01-05 15:31:57.384954: train_loss -0.9118\n",
      "2024-01-05 15:31:57.392458: val_loss -0.8347\n",
      "2024-01-05 15:31:57.402464: Pseudo dice [0.8616, 0.9594, 0.9429]\n",
      "2024-01-05 15:31:57.407465: Epoch time: 126.67 s\n",
      "2024-01-05 15:31:58.868157: \n",
      "2024-01-05 15:31:58.874117: Epoch 343\n",
      "2024-01-05 15:31:58.878109: Current learning rate: 0.00685\n",
      "2024-01-05 15:34:05.029472: train_loss -0.9128\n",
      "2024-01-05 15:34:05.037980: val_loss -0.836\n",
      "2024-01-05 15:34:05.042981: Pseudo dice [0.8623, 0.9598, 0.9418]\n",
      "2024-01-05 15:34:05.050982: Epoch time: 126.16 s\n",
      "2024-01-05 15:34:06.240462: \n",
      "2024-01-05 15:34:06.246892: Epoch 344\n",
      "2024-01-05 15:34:06.251954: Current learning rate: 0.00684\n",
      "2024-01-05 15:36:12.695743: train_loss -0.9114\n",
      "2024-01-05 15:36:12.701743: val_loss -0.8396\n",
      "2024-01-05 15:36:12.708745: Pseudo dice [0.8635, 0.9589, 0.9407]\n",
      "2024-01-05 15:36:12.714749: Epoch time: 126.46 s\n",
      "2024-01-05 15:36:13.907740: \n",
      "2024-01-05 15:36:13.916818: Epoch 345\n",
      "2024-01-05 15:36:13.921822: Current learning rate: 0.00683\n",
      "2024-01-05 15:38:19.719501: train_loss -0.9099\n",
      "2024-01-05 15:38:19.727499: val_loss -0.8327\n",
      "2024-01-05 15:38:19.735008: Pseudo dice [0.8657, 0.9594, 0.9412]\n",
      "2024-01-05 15:38:19.742007: Epoch time: 125.81 s\n",
      "2024-01-05 15:38:21.031555: \n",
      "2024-01-05 15:38:21.039245: Epoch 346\n",
      "2024-01-05 15:38:21.043302: Current learning rate: 0.00682\n",
      "2024-01-05 15:40:26.966717: train_loss -0.908\n",
      "2024-01-05 15:40:26.972716: val_loss -0.8314\n",
      "2024-01-05 15:40:26.977716: Pseudo dice [0.8632, 0.9582, 0.9395]\n",
      "2024-01-05 15:40:26.981719: Epoch time: 125.94 s\n",
      "2024-01-05 15:40:28.249880: \n",
      "2024-01-05 15:40:28.256943: Epoch 347\n",
      "2024-01-05 15:40:28.263948: Current learning rate: 0.00681\n",
      "2024-01-05 15:42:34.183855: train_loss -0.911\n",
      "2024-01-05 15:42:34.192854: val_loss -0.837\n",
      "2024-01-05 15:42:34.199854: Pseudo dice [0.8647, 0.9593, 0.9409]\n",
      "2024-01-05 15:42:34.207854: Epoch time: 125.93 s\n",
      "2024-01-05 15:42:35.358626: \n",
      "2024-01-05 15:42:35.365453: Epoch 348\n",
      "2024-01-05 15:42:35.373599: Current learning rate: 0.0068\n",
      "2024-01-05 15:44:41.695914: train_loss -0.912\n",
      "2024-01-05 15:44:41.708914: val_loss -0.8328\n",
      "2024-01-05 15:44:41.718915: Pseudo dice [0.8667, 0.9585, 0.9398]\n",
      "2024-01-05 15:44:41.730044: Epoch time: 126.34 s\n",
      "2024-01-05 15:44:43.090875: \n",
      "2024-01-05 15:44:43.099878: Epoch 349\n",
      "2024-01-05 15:44:43.104875: Current learning rate: 0.0068\n",
      "2024-01-05 15:46:49.953323: train_loss -0.9117\n",
      "2024-01-05 15:46:49.966317: val_loss -0.8341\n",
      "2024-01-05 15:46:49.975317: Pseudo dice [0.8661, 0.9601, 0.9423]\n",
      "2024-01-05 15:46:49.985318: Epoch time: 126.86 s\n",
      "2024-01-05 15:46:51.504662: \n",
      "2024-01-05 15:46:51.512943: Epoch 350\n",
      "2024-01-05 15:46:51.517953: Current learning rate: 0.00679\n",
      "2024-01-05 15:48:58.056809: train_loss -0.9134\n",
      "2024-01-05 15:48:58.065814: val_loss -0.831\n",
      "2024-01-05 15:48:58.072814: Pseudo dice [0.8591, 0.9583, 0.9397]\n",
      "2024-01-05 15:48:58.081324: Epoch time: 126.55 s\n",
      "2024-01-05 15:48:59.085189: \n",
      "2024-01-05 15:48:59.091093: Epoch 351\n",
      "2024-01-05 15:48:59.095176: Current learning rate: 0.00678\n",
      "2024-01-05 15:51:05.623673: train_loss -0.9146\n",
      "2024-01-05 15:51:05.635761: val_loss -0.8383\n",
      "2024-01-05 15:51:05.645605: Pseudo dice [0.862, 0.9597, 0.9412]\n",
      "2024-01-05 15:51:05.654666: Epoch time: 126.54 s\n",
      "2024-01-05 15:51:06.796287: \n",
      "2024-01-05 15:51:06.803185: Epoch 352\n",
      "2024-01-05 15:51:06.808211: Current learning rate: 0.00677\n",
      "2024-01-05 15:53:12.801310: train_loss -0.9128\n",
      "2024-01-05 15:53:12.810311: val_loss -0.8424\n",
      "2024-01-05 15:53:12.817310: Pseudo dice [0.8682, 0.9602, 0.9429]\n",
      "2024-01-05 15:53:12.823311: Epoch time: 126.01 s\n",
      "2024-01-05 15:53:13.862870: \n",
      "2024-01-05 15:53:13.873245: Epoch 353\n",
      "2024-01-05 15:53:13.878245: Current learning rate: 0.00676\n",
      "2024-01-05 15:55:20.124563: train_loss -0.9086\n",
      "2024-01-05 15:55:20.133749: val_loss -0.8403\n",
      "2024-01-05 15:55:20.140741: Pseudo dice [0.8611, 0.9613, 0.9442]\n",
      "2024-01-05 15:55:20.146742: Epoch time: 126.26 s\n",
      "2024-01-05 15:55:21.142120: \n",
      "2024-01-05 15:55:21.147540: Epoch 354\n",
      "2024-01-05 15:55:21.151628: Current learning rate: 0.00675\n",
      "2024-01-05 15:57:27.482449: train_loss -0.9105\n",
      "2024-01-05 15:57:27.489450: val_loss -0.8372\n",
      "2024-01-05 15:57:27.495450: Pseudo dice [0.862, 0.9597, 0.9411]\n",
      "2024-01-05 15:57:27.500449: Epoch time: 126.34 s\n",
      "2024-01-05 15:57:28.796195: \n",
      "2024-01-05 15:57:28.803201: Epoch 355\n",
      "2024-01-05 15:57:28.807865: Current learning rate: 0.00674\n",
      "2024-01-05 15:59:34.826828: train_loss -0.9114\n",
      "2024-01-05 15:59:34.833819: val_loss -0.8352\n",
      "2024-01-05 15:59:34.838822: Pseudo dice [0.8604, 0.9606, 0.9433]\n",
      "2024-01-05 15:59:34.843824: Epoch time: 126.03 s\n",
      "2024-01-05 15:59:36.056905: \n",
      "2024-01-05 15:59:36.063072: Epoch 356\n",
      "2024-01-05 15:59:36.070101: Current learning rate: 0.00673\n",
      "2024-01-05 16:01:42.136080: train_loss -0.9096\n",
      "2024-01-05 16:01:42.146081: val_loss -0.8405\n",
      "2024-01-05 16:01:42.154090: Pseudo dice [0.8635, 0.96, 0.9425]\n",
      "2024-01-05 16:01:42.163090: Epoch time: 126.08 s\n",
      "2024-01-05 16:01:43.871560: \n",
      "2024-01-05 16:01:43.877617: Epoch 357\n",
      "2024-01-05 16:01:43.882565: Current learning rate: 0.00672\n",
      "2024-01-05 16:03:49.755505: train_loss -0.9118\n",
      "2024-01-05 16:03:49.762503: val_loss -0.833\n",
      "2024-01-05 16:03:49.769503: Pseudo dice [0.8617, 0.9599, 0.9424]\n",
      "2024-01-05 16:03:49.776504: Epoch time: 125.89 s\n",
      "2024-01-05 16:03:51.123534: \n",
      "2024-01-05 16:03:51.130254: Epoch 358\n",
      "2024-01-05 16:03:51.135254: Current learning rate: 0.00671\n",
      "2024-01-05 16:05:57.174319: train_loss -0.9125\n",
      "2024-01-05 16:05:57.180329: val_loss -0.8318\n",
      "2024-01-05 16:05:57.186326: Pseudo dice [0.8604, 0.9593, 0.9411]\n",
      "2024-01-05 16:05:57.193912: Epoch time: 126.05 s\n",
      "2024-01-05 16:05:58.323629: \n",
      "2024-01-05 16:05:58.329758: Epoch 359\n",
      "2024-01-05 16:05:58.334760: Current learning rate: 0.0067\n",
      "2024-01-05 16:08:04.434186: train_loss -0.9089\n",
      "2024-01-05 16:08:04.445185: val_loss -0.8369\n",
      "2024-01-05 16:08:04.452185: Pseudo dice [0.8604, 0.9592, 0.9412]\n",
      "2024-01-05 16:08:04.458185: Epoch time: 126.11 s\n",
      "2024-01-05 16:08:05.837388: \n",
      "2024-01-05 16:08:05.845388: Epoch 360\n",
      "2024-01-05 16:08:05.850982: Current learning rate: 0.00669\n",
      "2024-01-05 16:10:11.480331: train_loss -0.9123\n",
      "2024-01-05 16:10:11.488352: val_loss -0.8354\n",
      "2024-01-05 16:10:11.493349: Pseudo dice [0.8613, 0.9594, 0.9414]\n",
      "2024-01-05 16:10:11.499871: Epoch time: 125.64 s\n",
      "2024-01-05 16:10:12.747843: \n",
      "2024-01-05 16:10:12.757032: Epoch 361\n",
      "2024-01-05 16:10:12.762007: Current learning rate: 0.00668\n",
      "2024-01-05 16:12:18.688558: train_loss -0.9116\n",
      "2024-01-05 16:12:18.695567: val_loss -0.836\n",
      "2024-01-05 16:12:18.702566: Pseudo dice [0.8657, 0.96, 0.9424]\n",
      "2024-01-05 16:12:18.707564: Epoch time: 125.94 s\n",
      "2024-01-05 16:12:19.832607: \n",
      "2024-01-05 16:12:19.840659: Epoch 362\n",
      "2024-01-05 16:12:19.845666: Current learning rate: 0.00667\n",
      "2024-01-05 16:14:25.640674: train_loss -0.9095\n",
      "2024-01-05 16:14:25.647675: val_loss -0.8447\n",
      "2024-01-05 16:14:25.653674: Pseudo dice [0.8599, 0.9611, 0.9439]\n",
      "2024-01-05 16:14:25.659678: Epoch time: 125.81 s\n",
      "2024-01-05 16:14:26.732268: \n",
      "2024-01-05 16:14:26.740854: Epoch 363\n",
      "2024-01-05 16:14:26.744932: Current learning rate: 0.00666\n",
      "2024-01-05 16:16:32.338837: train_loss -0.913\n",
      "2024-01-05 16:16:32.345837: val_loss -0.8376\n",
      "2024-01-05 16:16:32.350837: Pseudo dice [0.8625, 0.9584, 0.9402]\n",
      "2024-01-05 16:16:32.354836: Epoch time: 125.61 s\n",
      "2024-01-05 16:16:33.785179: \n",
      "2024-01-05 16:16:33.791183: Epoch 364\n",
      "2024-01-05 16:16:33.796180: Current learning rate: 0.00665\n",
      "2024-01-05 16:18:39.760562: train_loss -0.9097\n",
      "2024-01-05 16:18:39.767563: val_loss -0.8393\n",
      "2024-01-05 16:18:39.773564: Pseudo dice [0.8572, 0.9593, 0.9416]\n",
      "2024-01-05 16:18:39.779572: Epoch time: 125.98 s\n",
      "2024-01-05 16:18:40.990496: \n",
      "2024-01-05 16:18:40.996957: Epoch 365\n",
      "2024-01-05 16:18:41.006488: Current learning rate: 0.00665\n",
      "2024-01-05 16:20:47.114242: train_loss -0.9111\n",
      "2024-01-05 16:20:47.122241: val_loss -0.834\n",
      "2024-01-05 16:20:47.129241: Pseudo dice [0.8629, 0.958, 0.9403]\n",
      "2024-01-05 16:20:47.135242: Epoch time: 126.12 s\n",
      "2024-01-05 16:20:48.323079: \n",
      "2024-01-05 16:20:48.330044: Epoch 366\n",
      "2024-01-05 16:20:48.335110: Current learning rate: 0.00664\n",
      "2024-01-05 16:22:54.231973: train_loss -0.9105\n",
      "2024-01-05 16:22:54.239974: val_loss -0.835\n",
      "2024-01-05 16:22:54.245975: Pseudo dice [0.8656, 0.9585, 0.939]\n",
      "2024-01-05 16:22:54.250973: Epoch time: 125.91 s\n",
      "2024-01-05 16:22:55.373405: \n",
      "2024-01-05 16:22:55.381425: Epoch 367\n",
      "2024-01-05 16:22:55.387519: Current learning rate: 0.00663\n",
      "2024-01-05 16:25:01.069791: train_loss -0.9132\n",
      "2024-01-05 16:25:01.076791: val_loss -0.8425\n",
      "2024-01-05 16:25:01.081791: Pseudo dice [0.8649, 0.9605, 0.9437]\n",
      "2024-01-05 16:25:01.086791: Epoch time: 125.7 s\n",
      "2024-01-05 16:25:02.177217: \n",
      "2024-01-05 16:25:02.183088: Epoch 368\n",
      "2024-01-05 16:25:02.191092: Current learning rate: 0.00662\n",
      "2024-01-05 16:27:07.995474: train_loss -0.9128\n",
      "2024-01-05 16:27:08.004474: val_loss -0.8384\n",
      "2024-01-05 16:27:08.012474: Pseudo dice [0.8623, 0.9597, 0.9417]\n",
      "2024-01-05 16:27:08.020478: Epoch time: 125.82 s\n",
      "2024-01-05 16:27:09.276545: \n",
      "2024-01-05 16:27:09.287595: Epoch 369\n",
      "2024-01-05 16:27:09.292605: Current learning rate: 0.00661\n",
      "2024-01-05 16:29:14.964942: train_loss -0.9117\n",
      "2024-01-05 16:29:14.972943: val_loss -0.8269\n",
      "2024-01-05 16:29:14.978943: Pseudo dice [0.8585, 0.9575, 0.9387]\n",
      "2024-01-05 16:29:14.983943: Epoch time: 125.69 s\n",
      "2024-01-05 16:29:16.201101: \n",
      "2024-01-05 16:29:16.207092: Epoch 370\n",
      "2024-01-05 16:29:16.211100: Current learning rate: 0.0066\n",
      "2024-01-05 16:31:21.848747: train_loss -0.9127\n",
      "2024-01-05 16:31:21.854748: val_loss -0.8368\n",
      "2024-01-05 16:31:21.861749: Pseudo dice [0.8629, 0.9588, 0.9407]\n",
      "2024-01-05 16:31:21.866749: Epoch time: 125.65 s\n",
      "2024-01-05 16:31:22.996579: \n",
      "2024-01-05 16:31:23.002687: Epoch 371\n",
      "2024-01-05 16:31:23.006683: Current learning rate: 0.00659\n",
      "2024-01-05 16:33:28.761966: train_loss -0.912\n",
      "2024-01-05 16:33:28.771971: val_loss -0.8386\n",
      "2024-01-05 16:33:28.778977: Pseudo dice [0.8652, 0.9588, 0.9406]\n",
      "2024-01-05 16:33:28.784973: Epoch time: 125.77 s\n",
      "2024-01-05 16:33:30.200238: \n",
      "2024-01-05 16:33:30.214677: Epoch 372\n",
      "2024-01-05 16:33:30.218723: Current learning rate: 0.00658\n",
      "2024-01-05 16:35:35.919083: train_loss -0.9125\n",
      "2024-01-05 16:35:35.926089: val_loss -0.8398\n",
      "2024-01-05 16:35:35.932157: Pseudo dice [0.8646, 0.9605, 0.9428]\n",
      "2024-01-05 16:35:35.941089: Epoch time: 125.72 s\n",
      "2024-01-05 16:35:37.081283: \n",
      "2024-01-05 16:35:37.087397: Epoch 373\n",
      "2024-01-05 16:35:37.093919: Current learning rate: 0.00657\n",
      "2024-01-05 16:37:42.666795: train_loss -0.9088\n",
      "2024-01-05 16:37:42.674304: val_loss -0.8389\n",
      "2024-01-05 16:37:42.680305: Pseudo dice [0.8654, 0.9599, 0.9429]\n",
      "2024-01-05 16:37:42.685305: Epoch time: 125.59 s\n",
      "2024-01-05 16:37:43.914832: \n",
      "2024-01-05 16:37:43.921910: Epoch 374\n",
      "2024-01-05 16:37:43.930912: Current learning rate: 0.00656\n",
      "2024-01-05 16:39:49.996639: train_loss -0.909\n",
      "2024-01-05 16:39:50.002647: val_loss -0.84\n",
      "2024-01-05 16:39:50.007639: Pseudo dice [0.8662, 0.9592, 0.9415]\n",
      "2024-01-05 16:39:50.011640: Epoch time: 126.08 s\n",
      "2024-01-05 16:39:51.199823: \n",
      "2024-01-05 16:39:51.205807: Epoch 375\n",
      "2024-01-05 16:39:51.211792: Current learning rate: 0.00655\n",
      "2024-01-05 16:41:57.142910: train_loss -0.9134\n",
      "2024-01-05 16:41:57.151911: val_loss -0.8368\n",
      "2024-01-05 16:41:57.159668: Pseudo dice [0.8648, 0.96, 0.9421]\n",
      "2024-01-05 16:41:57.172441: Epoch time: 125.94 s\n",
      "2024-01-05 16:41:58.501507: \n",
      "2024-01-05 16:41:58.507588: Epoch 376\n",
      "2024-01-05 16:41:58.511577: Current learning rate: 0.00654\n",
      "2024-01-05 16:44:04.410793: train_loss -0.9112\n",
      "2024-01-05 16:44:04.417793: val_loss -0.839\n",
      "2024-01-05 16:44:04.421793: Pseudo dice [0.8653, 0.9599, 0.9418]\n",
      "2024-01-05 16:44:04.426795: Epoch time: 125.91 s\n",
      "2024-01-05 16:44:05.674654: \n",
      "2024-01-05 16:44:05.683748: Epoch 377\n",
      "2024-01-05 16:44:05.687887: Current learning rate: 0.00653\n",
      "2024-01-05 16:46:11.294197: train_loss -0.9113\n",
      "2024-01-05 16:46:11.302197: val_loss -0.834\n",
      "2024-01-05 16:46:11.309198: Pseudo dice [0.8612, 0.9585, 0.9406]\n",
      "2024-01-05 16:46:11.314198: Epoch time: 125.62 s\n",
      "2024-01-05 16:46:12.420746: \n",
      "2024-01-05 16:46:12.430682: Epoch 378\n",
      "2024-01-05 16:46:12.435684: Current learning rate: 0.00652\n",
      "2024-01-05 16:48:17.749861: train_loss -0.9131\n",
      "2024-01-05 16:48:17.756853: val_loss -0.8404\n",
      "2024-01-05 16:48:17.762851: Pseudo dice [0.8619, 0.9601, 0.9426]\n",
      "2024-01-05 16:48:17.768855: Epoch time: 125.33 s\n",
      "2024-01-05 16:48:19.050967: \n",
      "2024-01-05 16:48:19.057967: Epoch 379\n",
      "2024-01-05 16:48:19.062894: Current learning rate: 0.00651\n",
      "2024-01-05 16:50:24.805509: train_loss -0.9116\n",
      "2024-01-05 16:50:24.819512: val_loss -0.8417\n",
      "2024-01-05 16:50:24.828507: Pseudo dice [0.8659, 0.9594, 0.9416]\n",
      "2024-01-05 16:50:24.834508: Epoch time: 125.76 s\n",
      "2024-01-05 16:50:26.037931: \n",
      "2024-01-05 16:50:26.043988: Epoch 380\n",
      "2024-01-05 16:50:26.048986: Current learning rate: 0.0065\n",
      "2024-01-05 16:52:31.564549: train_loss -0.9135\n",
      "2024-01-05 16:52:31.571549: val_loss -0.8385\n",
      "2024-01-05 16:52:31.576548: Pseudo dice [0.8592, 0.9594, 0.942]\n",
      "2024-01-05 16:52:31.580548: Epoch time: 125.53 s\n",
      "2024-01-05 16:52:32.897837: \n",
      "2024-01-05 16:52:32.903678: Epoch 381\n",
      "2024-01-05 16:52:32.907670: Current learning rate: 0.00649\n",
      "2024-01-05 16:54:38.907664: train_loss -0.9112\n",
      "2024-01-05 16:54:38.915666: val_loss -0.8429\n",
      "2024-01-05 16:54:38.921663: Pseudo dice [0.8615, 0.9607, 0.9426]\n",
      "2024-01-05 16:54:38.926663: Epoch time: 126.01 s\n",
      "2024-01-05 16:54:40.078202: \n",
      "2024-01-05 16:54:40.088958: Epoch 382\n",
      "2024-01-05 16:54:40.094984: Current learning rate: 0.00648\n",
      "2024-01-05 16:56:45.573981: train_loss -0.9138\n",
      "2024-01-05 16:56:45.580981: val_loss -0.8371\n",
      "2024-01-05 16:56:45.586980: Pseudo dice [0.8628, 0.9592, 0.9416]\n",
      "2024-01-05 16:56:45.592986: Epoch time: 125.5 s\n",
      "2024-01-05 16:56:46.815552: \n",
      "2024-01-05 16:56:46.820626: Epoch 383\n",
      "2024-01-05 16:56:46.825660: Current learning rate: 0.00648\n",
      "2024-01-05 16:58:52.742064: train_loss -0.9127\n",
      "2024-01-05 16:58:52.749063: val_loss -0.8354\n",
      "2024-01-05 16:58:52.755066: Pseudo dice [0.8638, 0.9582, 0.9399]\n",
      "2024-01-05 16:58:52.760073: Epoch time: 125.93 s\n",
      "2024-01-05 16:58:53.968149: \n",
      "2024-01-05 16:58:53.978233: Epoch 384\n",
      "2024-01-05 16:58:53.983221: Current learning rate: 0.00647\n",
      "2024-01-05 17:00:59.643216: train_loss -0.9137\n",
      "2024-01-05 17:00:59.652219: val_loss -0.8351\n",
      "2024-01-05 17:00:59.658211: Pseudo dice [0.8559, 0.9601, 0.9419]\n",
      "2024-01-05 17:00:59.664215: Epoch time: 125.68 s\n",
      "2024-01-05 17:01:00.890091: \n",
      "2024-01-05 17:01:00.908785: Epoch 385\n",
      "2024-01-05 17:01:00.913797: Current learning rate: 0.00646\n",
      "2024-01-05 17:03:06.760535: train_loss -0.9135\n",
      "2024-01-05 17:03:06.768042: val_loss -0.8354\n",
      "2024-01-05 17:03:06.773042: Pseudo dice [0.8606, 0.9587, 0.9411]\n",
      "2024-01-05 17:03:06.778042: Epoch time: 125.87 s\n",
      "2024-01-05 17:03:08.144368: \n",
      "2024-01-05 17:03:08.149381: Epoch 386\n",
      "2024-01-05 17:03:08.155330: Current learning rate: 0.00645\n",
      "2024-01-05 17:05:13.817371: train_loss -0.9116\n",
      "2024-01-05 17:05:13.826370: val_loss -0.8386\n",
      "2024-01-05 17:05:13.834371: Pseudo dice [0.8622, 0.9604, 0.9425]\n",
      "2024-01-05 17:05:13.840880: Epoch time: 125.67 s\n",
      "2024-01-05 17:05:15.140573: \n",
      "2024-01-05 17:05:15.147574: Epoch 387\n",
      "2024-01-05 17:05:15.151573: Current learning rate: 0.00644\n",
      "2024-01-05 17:07:20.793115: train_loss -0.9141\n",
      "2024-01-05 17:07:20.800115: val_loss -0.8398\n",
      "2024-01-05 17:07:20.807113: Pseudo dice [0.8643, 0.9601, 0.9424]\n",
      "2024-01-05 17:07:20.812113: Epoch time: 125.65 s\n",
      "2024-01-05 17:07:21.992590: \n",
      "2024-01-05 17:07:21.998589: Epoch 388\n",
      "2024-01-05 17:07:22.002589: Current learning rate: 0.00643\n",
      "2024-01-05 17:09:27.816573: train_loss -0.9105\n",
      "2024-01-05 17:09:27.825572: val_loss -0.8378\n",
      "2024-01-05 17:09:27.833583: Pseudo dice [0.8636, 0.9578, 0.9388]\n",
      "2024-01-05 17:09:27.842581: Epoch time: 125.82 s\n",
      "2024-01-05 17:09:29.177542: \n",
      "2024-01-05 17:09:29.186542: Epoch 389\n",
      "2024-01-05 17:09:29.190865: Current learning rate: 0.00642\n",
      "2024-01-05 17:11:34.846368: train_loss -0.9108\n",
      "2024-01-05 17:11:34.853368: val_loss -0.8409\n",
      "2024-01-05 17:11:34.858369: Pseudo dice [0.863, 0.9613, 0.9438]\n",
      "2024-01-05 17:11:34.862369: Epoch time: 125.67 s\n",
      "2024-01-05 17:11:35.968530: \n",
      "2024-01-05 17:11:35.975540: Epoch 390\n",
      "2024-01-05 17:11:35.982613: Current learning rate: 0.00641\n",
      "2024-01-05 17:13:41.935405: train_loss -0.9111\n",
      "2024-01-05 17:13:41.943405: val_loss -0.8405\n",
      "2024-01-05 17:13:41.951404: Pseudo dice [0.8667, 0.9597, 0.9427]\n",
      "2024-01-05 17:13:41.958405: Epoch time: 125.97 s\n",
      "2024-01-05 17:13:43.286469: \n",
      "2024-01-05 17:13:43.292626: Epoch 391\n",
      "2024-01-05 17:13:43.302477: Current learning rate: 0.0064\n",
      "2024-01-05 17:15:48.962646: train_loss -0.9121\n",
      "2024-01-05 17:15:48.972162: val_loss -0.8353\n",
      "2024-01-05 17:15:48.980230: Pseudo dice [0.8632, 0.9587, 0.9403]\n",
      "2024-01-05 17:15:48.985246: Epoch time: 125.68 s\n",
      "2024-01-05 17:15:50.148834: \n",
      "2024-01-05 17:15:50.153849: Epoch 392\n",
      "2024-01-05 17:15:50.158840: Current learning rate: 0.00639\n",
      "2024-01-05 17:17:55.706799: train_loss -0.9152\n",
      "2024-01-05 17:17:55.714791: val_loss -0.8393\n",
      "2024-01-05 17:17:55.720309: Pseudo dice [0.8677, 0.9595, 0.9421]\n",
      "2024-01-05 17:17:55.726304: Epoch time: 125.56 s\n",
      "2024-01-05 17:17:56.863802: \n",
      "2024-01-05 17:17:56.870801: Epoch 393\n",
      "2024-01-05 17:17:56.875863: Current learning rate: 0.00638\n",
      "2024-01-05 17:20:02.551126: train_loss -0.9159\n",
      "2024-01-05 17:20:02.558121: val_loss -0.8406\n",
      "2024-01-05 17:20:02.563123: Pseudo dice [0.8611, 0.9599, 0.9426]\n",
      "2024-01-05 17:20:02.570124: Epoch time: 125.69 s\n",
      "2024-01-05 17:20:04.030580: \n",
      "2024-01-05 17:20:04.038322: Epoch 394\n",
      "2024-01-05 17:20:04.043355: Current learning rate: 0.00637\n",
      "2024-01-05 17:22:10.069035: train_loss -0.9144\n",
      "2024-01-05 17:22:10.076035: val_loss -0.8419\n",
      "2024-01-05 17:22:10.083036: Pseudo dice [0.8681, 0.9607, 0.9438]\n",
      "2024-01-05 17:22:10.088038: Epoch time: 126.04 s\n",
      "2024-01-05 17:22:10.094038: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2024-01-05 17:22:11.549526: \n",
      "2024-01-05 17:22:11.556018: Epoch 395\n",
      "2024-01-05 17:22:11.561018: Current learning rate: 0.00636\n",
      "2024-01-05 17:24:17.641170: train_loss -0.9103\n",
      "2024-01-05 17:24:17.648168: val_loss -0.8377\n",
      "2024-01-05 17:24:17.654168: Pseudo dice [0.8595, 0.9598, 0.9417]\n",
      "2024-01-05 17:24:17.659169: Epoch time: 126.09 s\n",
      "2024-01-05 17:24:18.803380: \n",
      "2024-01-05 17:24:18.811398: Epoch 396\n",
      "2024-01-05 17:24:18.815355: Current learning rate: 0.00635\n",
      "2024-01-05 17:26:25.164201: train_loss -0.9115\n",
      "2024-01-05 17:26:25.170200: val_loss -0.8362\n",
      "2024-01-05 17:26:25.178284: Pseudo dice [0.8608, 0.9596, 0.9423]\n",
      "2024-01-05 17:26:25.185205: Epoch time: 126.36 s\n",
      "2024-01-05 17:26:26.388668: \n",
      "2024-01-05 17:26:26.394854: Epoch 397\n",
      "2024-01-05 17:26:26.403430: Current learning rate: 0.00634\n",
      "2024-01-05 17:28:31.863125: train_loss -0.9142\n",
      "2024-01-05 17:28:31.870125: val_loss -0.8398\n",
      "2024-01-05 17:28:31.915178: Pseudo dice [0.8613, 0.9599, 0.942]\n",
      "2024-01-05 17:28:31.926202: Epoch time: 125.48 s\n",
      "2024-01-05 17:28:33.034605: \n",
      "2024-01-05 17:28:33.040596: Epoch 398\n",
      "2024-01-05 17:28:33.045596: Current learning rate: 0.00633\n",
      "2024-01-05 17:30:38.702618: train_loss -0.9129\n",
      "2024-01-05 17:30:38.709618: val_loss -0.8368\n",
      "2024-01-05 17:30:38.716617: Pseudo dice [0.8642, 0.9599, 0.9414]\n",
      "2024-01-05 17:30:38.720631: Epoch time: 125.67 s\n",
      "2024-01-05 17:30:39.952349: \n",
      "2024-01-05 17:30:39.961353: Epoch 399\n",
      "2024-01-05 17:30:39.972424: Current learning rate: 0.00632\n",
      "2024-01-05 17:32:45.463942: train_loss -0.9145\n",
      "2024-01-05 17:32:45.473943: val_loss -0.8358\n",
      "2024-01-05 17:32:45.480943: Pseudo dice [0.8589, 0.9607, 0.9432]\n",
      "2024-01-05 17:32:45.489951: Epoch time: 125.51 s\n",
      "2024-01-05 17:32:47.108747: \n",
      "2024-01-05 17:32:47.119850: Epoch 400\n",
      "2024-01-05 17:32:47.126834: Current learning rate: 0.00631\n",
      "2024-01-05 17:34:53.014246: train_loss -0.9123\n",
      "2024-01-05 17:34:53.023246: val_loss -0.8412\n",
      "2024-01-05 17:34:53.029246: Pseudo dice [0.8673, 0.9589, 0.9407]\n",
      "2024-01-05 17:34:53.034247: Epoch time: 125.91 s\n",
      "2024-01-05 17:34:54.234197: \n",
      "2024-01-05 17:34:54.247194: Epoch 401\n",
      "2024-01-05 17:34:54.252203: Current learning rate: 0.0063\n",
      "2024-01-05 17:37:00.023257: train_loss -0.9025\n",
      "2024-01-05 17:37:00.029262: val_loss -0.8361\n",
      "2024-01-05 17:37:00.034337: Pseudo dice [0.864, 0.9586, 0.9401]\n",
      "2024-01-05 17:37:00.041342: Epoch time: 125.79 s\n",
      "2024-01-05 17:37:01.196447: \n",
      "2024-01-05 17:37:01.203100: Epoch 402\n",
      "2024-01-05 17:37:01.212211: Current learning rate: 0.0063\n",
      "2024-01-05 17:39:06.947427: train_loss -0.9042\n",
      "2024-01-05 17:39:06.956426: val_loss -0.8459\n",
      "2024-01-05 17:39:06.961920: Pseudo dice [0.8642, 0.9604, 0.9434]\n",
      "2024-01-05 17:39:06.967918: Epoch time: 125.75 s\n",
      "2024-01-05 17:39:08.075469: \n",
      "2024-01-05 17:39:08.086636: Epoch 403\n",
      "2024-01-05 17:39:08.095184: Current learning rate: 0.00629\n",
      "2024-01-05 17:41:13.904786: train_loss -0.9071\n",
      "2024-01-05 17:41:13.911784: val_loss -0.8202\n",
      "2024-01-05 17:41:13.917793: Pseudo dice [0.8532, 0.9564, 0.9361]\n",
      "2024-01-05 17:41:13.922786: Epoch time: 125.83 s\n",
      "2024-01-05 17:41:15.140074: \n",
      "2024-01-05 17:41:15.149131: Epoch 404\n",
      "2024-01-05 17:41:15.154063: Current learning rate: 0.00628\n",
      "2024-01-05 17:43:20.770528: train_loss -0.8965\n",
      "2024-01-05 17:43:20.780077: val_loss -0.8302\n",
      "2024-01-05 17:43:20.787002: Pseudo dice [0.8654, 0.9561, 0.9368]\n",
      "2024-01-05 17:43:20.829039: Epoch time: 125.63 s\n",
      "2024-01-05 17:43:21.930781: \n",
      "2024-01-05 17:43:21.943340: Epoch 405\n",
      "2024-01-05 17:43:21.947397: Current learning rate: 0.00627\n",
      "2024-01-05 17:45:27.812084: train_loss -0.8941\n",
      "2024-01-05 17:45:27.822085: val_loss -0.8391\n",
      "2024-01-05 17:45:27.829088: Pseudo dice [0.8632, 0.9602, 0.9424]\n",
      "2024-01-05 17:45:27.834083: Epoch time: 125.88 s\n",
      "2024-01-05 17:45:29.018811: \n",
      "2024-01-05 17:45:29.024797: Epoch 406\n",
      "2024-01-05 17:45:29.029378: Current learning rate: 0.00626\n",
      "2024-01-05 17:47:34.976723: train_loss -0.8981\n",
      "2024-01-05 17:47:34.985725: val_loss -0.8196\n",
      "2024-01-05 17:47:34.994738: Pseudo dice [0.85, 0.954, 0.9322]\n",
      "2024-01-05 17:47:34.999724: Epoch time: 125.96 s\n",
      "2024-01-05 17:47:36.294058: \n",
      "2024-01-05 17:47:36.300158: Epoch 407\n",
      "2024-01-05 17:47:36.305234: Current learning rate: 0.00625\n",
      "2024-01-05 17:49:42.087849: train_loss -0.898\n",
      "2024-01-05 17:49:42.096847: val_loss -0.8389\n",
      "2024-01-05 17:49:42.102354: Pseudo dice [0.8594, 0.9584, 0.9406]\n",
      "2024-01-05 17:49:42.107354: Epoch time: 125.79 s\n",
      "2024-01-05 17:49:43.462017: \n",
      "2024-01-05 17:49:43.468004: Epoch 408\n",
      "2024-01-05 17:49:43.472941: Current learning rate: 0.00624\n",
      "2024-01-05 17:51:48.917559: train_loss -0.908\n",
      "2024-01-05 17:51:48.925560: val_loss -0.8394\n",
      "2024-01-05 17:51:48.931560: Pseudo dice [0.8648, 0.9594, 0.9414]\n",
      "2024-01-05 17:51:48.937560: Epoch time: 125.46 s\n",
      "2024-01-05 17:51:50.193849: \n",
      "2024-01-05 17:51:50.199705: Epoch 409\n",
      "2024-01-05 17:51:50.207831: Current learning rate: 0.00623\n",
      "2024-01-05 17:53:55.807966: train_loss -0.9068\n",
      "2024-01-05 17:53:55.813968: val_loss -0.8346\n",
      "2024-01-05 17:53:55.818976: Pseudo dice [0.8625, 0.959, 0.9413]\n",
      "2024-01-05 17:53:55.823967: Epoch time: 125.62 s\n",
      "2024-01-05 17:53:57.087360: \n",
      "2024-01-05 17:53:57.097313: Epoch 410\n",
      "2024-01-05 17:53:57.103308: Current learning rate: 0.00622\n",
      "2024-01-05 17:56:02.692102: train_loss -0.9101\n",
      "2024-01-05 17:56:02.702101: val_loss -0.8373\n",
      "2024-01-05 17:56:02.708104: Pseudo dice [0.8589, 0.9599, 0.9419]\n",
      "2024-01-05 17:56:02.713768: Epoch time: 125.61 s\n",
      "2024-01-05 17:56:03.892849: \n",
      "2024-01-05 17:56:03.899877: Epoch 411\n",
      "2024-01-05 17:56:03.903861: Current learning rate: 0.00621\n",
      "2024-01-05 17:58:09.829853: train_loss -0.9051\n",
      "2024-01-05 17:58:09.835853: val_loss -0.8299\n",
      "2024-01-05 17:58:09.840852: Pseudo dice [0.8607, 0.9572, 0.9386]\n",
      "2024-01-05 17:58:09.845853: Epoch time: 125.94 s\n",
      "2024-01-05 17:58:11.021020: \n",
      "2024-01-05 17:58:11.026946: Epoch 412\n",
      "2024-01-05 17:58:11.031941: Current learning rate: 0.0062\n",
      "2024-01-05 18:00:16.718731: train_loss -0.9095\n",
      "2024-01-05 18:00:16.726732: val_loss -0.8378\n",
      "2024-01-05 18:00:16.732736: Pseudo dice [0.8647, 0.9588, 0.941]\n",
      "2024-01-05 18:00:16.738736: Epoch time: 125.7 s\n",
      "2024-01-05 18:00:17.953224: \n",
      "2024-01-05 18:00:17.959672: Epoch 413\n",
      "2024-01-05 18:00:17.963690: Current learning rate: 0.00619\n",
      "2024-01-05 18:02:24.181614: train_loss -0.9077\n",
      "2024-01-05 18:02:24.188614: val_loss -0.8394\n",
      "2024-01-05 18:02:24.194614: Pseudo dice [0.8634, 0.96, 0.9429]\n",
      "2024-01-05 18:02:24.199614: Epoch time: 126.23 s\n",
      "2024-01-05 18:02:25.285647: \n",
      "2024-01-05 18:02:25.291916: Epoch 414\n",
      "2024-01-05 18:02:25.296995: Current learning rate: 0.00618\n",
      "2024-01-05 18:04:30.977755: train_loss -0.9106\n",
      "2024-01-05 18:04:30.984269: val_loss -0.8383\n",
      "2024-01-05 18:04:30.990269: Pseudo dice [0.8658, 0.9589, 0.9402]\n",
      "2024-01-05 18:04:30.996268: Epoch time: 125.69 s\n",
      "2024-01-05 18:04:32.383834: \n",
      "2024-01-05 18:04:32.389849: Epoch 415\n",
      "2024-01-05 18:04:32.393924: Current learning rate: 0.00617\n",
      "2024-01-05 18:06:38.108459: train_loss -0.9148\n",
      "2024-01-05 18:06:38.119459: val_loss -0.8436\n",
      "2024-01-05 18:06:38.127460: Pseudo dice [0.8614, 0.9588, 0.9406]\n",
      "2024-01-05 18:06:38.136471: Epoch time: 125.73 s\n",
      "2024-01-05 18:06:39.474453: \n",
      "2024-01-05 18:06:39.483453: Epoch 416\n",
      "2024-01-05 18:06:39.487911: Current learning rate: 0.00616\n",
      "2024-01-05 18:08:44.807499: train_loss -0.9157\n",
      "2024-01-05 18:08:44.815499: val_loss -0.833\n",
      "2024-01-05 18:08:44.822502: Pseudo dice [0.8628, 0.9588, 0.9402]\n",
      "2024-01-05 18:08:44.826502: Epoch time: 125.33 s\n",
      "2024-01-05 18:08:45.877348: \n",
      "2024-01-05 18:08:45.885086: Epoch 417\n",
      "2024-01-05 18:08:45.895110: Current learning rate: 0.00615\n",
      "2024-01-05 18:10:51.446479: train_loss -0.9145\n",
      "2024-01-05 18:10:51.455479: val_loss -0.8449\n",
      "2024-01-05 18:10:51.460490: Pseudo dice [0.8645, 0.9607, 0.9434]\n",
      "2024-01-05 18:10:51.465482: Epoch time: 125.57 s\n",
      "2024-01-05 18:10:52.573590: \n",
      "2024-01-05 18:10:52.582598: Epoch 418\n",
      "2024-01-05 18:10:52.587587: Current learning rate: 0.00614\n",
      "2024-01-05 18:12:58.142402: train_loss -0.9137\n",
      "2024-01-05 18:12:58.149402: val_loss -0.84\n",
      "2024-01-05 18:12:58.157401: Pseudo dice [0.863, 0.9599, 0.942]\n",
      "2024-01-05 18:12:58.164400: Epoch time: 125.57 s\n",
      "2024-01-05 18:12:59.419726: \n",
      "2024-01-05 18:12:59.427721: Epoch 419\n",
      "2024-01-05 18:12:59.434741: Current learning rate: 0.00613\n",
      "2024-01-05 18:15:05.307000: train_loss -0.9134\n",
      "2024-01-05 18:15:05.313999: val_loss -0.8427\n",
      "2024-01-05 18:15:05.319999: Pseudo dice [0.8601, 0.9608, 0.9438]\n",
      "2024-01-05 18:15:05.325000: Epoch time: 125.89 s\n",
      "2024-01-05 18:15:06.419123: \n",
      "2024-01-05 18:15:06.425129: Epoch 420\n",
      "2024-01-05 18:15:06.433132: Current learning rate: 0.00612\n",
      "2024-01-05 18:17:12.144331: train_loss -0.9101\n",
      "2024-01-05 18:17:12.150330: val_loss -0.8436\n",
      "2024-01-05 18:17:12.154838: Pseudo dice [0.8606, 0.9603, 0.9436]\n",
      "2024-01-05 18:17:12.158838: Epoch time: 125.73 s\n",
      "2024-01-05 18:17:13.146562: \n",
      "2024-01-05 18:17:13.154433: Epoch 421\n",
      "2024-01-05 18:17:13.163504: Current learning rate: 0.00612\n",
      "2024-01-05 18:19:18.660704: train_loss -0.9129\n",
      "2024-01-05 18:19:18.669703: val_loss -0.8412\n",
      "2024-01-05 18:19:18.676702: Pseudo dice [0.8653, 0.9614, 0.9443]\n",
      "2024-01-05 18:19:18.681702: Epoch time: 125.52 s\n",
      "2024-01-05 18:19:19.822553: \n",
      "2024-01-05 18:19:19.831234: Epoch 422\n",
      "2024-01-05 18:19:19.836236: Current learning rate: 0.00611\n",
      "2024-01-05 18:21:25.789294: train_loss -0.9108\n",
      "2024-01-05 18:21:25.797292: val_loss -0.8405\n",
      "2024-01-05 18:21:25.804292: Pseudo dice [0.8599, 0.96, 0.9424]\n",
      "2024-01-05 18:21:25.809294: Epoch time: 125.97 s\n",
      "2024-01-05 18:21:27.157440: \n",
      "2024-01-05 18:21:27.165451: Epoch 423\n",
      "2024-01-05 18:21:27.169448: Current learning rate: 0.0061\n",
      "2024-01-05 18:23:33.373313: train_loss -0.9127\n",
      "2024-01-05 18:23:33.382313: val_loss -0.8422\n",
      "2024-01-05 18:23:33.389314: Pseudo dice [0.8587, 0.9602, 0.9428]\n",
      "2024-01-05 18:23:33.396314: Epoch time: 126.22 s\n",
      "2024-01-05 18:23:34.644350: \n",
      "2024-01-05 18:23:34.650413: Epoch 424\n",
      "2024-01-05 18:23:34.655205: Current learning rate: 0.00609\n",
      "2024-01-05 18:25:40.329031: train_loss -0.9128\n",
      "2024-01-05 18:25:40.336043: val_loss -0.8364\n",
      "2024-01-05 18:25:40.343053: Pseudo dice [0.8658, 0.9599, 0.9421]\n",
      "2024-01-05 18:25:40.350040: Epoch time: 125.69 s\n",
      "2024-01-05 18:25:41.415829: \n",
      "2024-01-05 18:25:41.427769: Epoch 425\n",
      "2024-01-05 18:25:41.432769: Current learning rate: 0.00608\n",
      "2024-01-05 18:27:47.110759: train_loss -0.9144\n",
      "2024-01-05 18:27:47.119757: val_loss -0.8407\n",
      "2024-01-05 18:27:47.125759: Pseudo dice [0.867, 0.9599, 0.9418]\n",
      "2024-01-05 18:27:47.131758: Epoch time: 125.7 s\n",
      "2024-01-05 18:27:48.327991: \n",
      "2024-01-05 18:27:48.333985: Epoch 426\n",
      "2024-01-05 18:27:48.338651: Current learning rate: 0.00607\n",
      "2024-01-05 18:29:54.137911: train_loss -0.9138\n",
      "2024-01-05 18:29:54.148916: val_loss -0.8359\n",
      "2024-01-05 18:29:54.157429: Pseudo dice [0.8648, 0.9597, 0.9419]\n",
      "2024-01-05 18:29:54.175377: Epoch time: 125.81 s\n",
      "2024-01-05 18:29:55.400377: \n",
      "2024-01-05 18:29:55.405377: Epoch 427\n",
      "2024-01-05 18:29:55.410377: Current learning rate: 0.00606\n",
      "2024-01-05 18:32:00.934340: train_loss -0.915\n",
      "2024-01-05 18:32:00.940342: val_loss -0.8444\n",
      "2024-01-05 18:32:00.947348: Pseudo dice [0.8618, 0.9601, 0.9431]\n",
      "2024-01-05 18:32:00.955350: Epoch time: 125.53 s\n",
      "2024-01-05 18:32:02.138345: \n",
      "2024-01-05 18:32:02.145367: Epoch 428\n",
      "2024-01-05 18:32:02.150345: Current learning rate: 0.00605\n",
      "2024-01-05 18:34:07.947398: train_loss -0.9104\n",
      "2024-01-05 18:34:07.955398: val_loss -0.8264\n",
      "2024-01-05 18:34:07.962406: Pseudo dice [0.8591, 0.9582, 0.9393]\n",
      "2024-01-05 18:34:07.967397: Epoch time: 125.81 s\n",
      "2024-01-05 18:34:09.050190: \n",
      "2024-01-05 18:34:09.058662: Epoch 429\n",
      "2024-01-05 18:34:09.062646: Current learning rate: 0.00604\n",
      "2024-01-05 18:36:14.892552: train_loss -0.9129\n",
      "2024-01-05 18:36:14.899558: val_loss -0.8416\n",
      "2024-01-05 18:36:14.903552: Pseudo dice [0.8624, 0.9605, 0.9431]\n",
      "2024-01-05 18:36:14.908544: Epoch time: 125.84 s\n",
      "2024-01-05 18:36:15.978274: \n",
      "2024-01-05 18:36:15.984801: Epoch 430\n",
      "2024-01-05 18:36:15.992956: Current learning rate: 0.00603\n",
      "2024-01-05 18:38:21.625053: train_loss -0.8914\n",
      "2024-01-05 18:38:21.633051: val_loss -0.8374\n",
      "2024-01-05 18:38:21.638559: Pseudo dice [0.8611, 0.9588, 0.941]\n",
      "2024-01-05 18:38:21.644568: Epoch time: 125.65 s\n",
      "2024-01-05 18:38:22.909573: \n",
      "2024-01-05 18:38:22.915811: Epoch 431\n",
      "2024-01-05 18:38:22.919805: Current learning rate: 0.00602\n",
      "2024-01-05 18:40:28.571034: train_loss -0.9012\n",
      "2024-01-05 18:40:28.577031: val_loss -0.8433\n",
      "2024-01-05 18:40:28.583035: Pseudo dice [0.8634, 0.9593, 0.9415]\n",
      "2024-01-05 18:40:28.589027: Epoch time: 125.66 s\n",
      "2024-01-05 18:40:29.697406: \n",
      "2024-01-05 18:40:29.703450: Epoch 432\n",
      "2024-01-05 18:40:29.712521: Current learning rate: 0.00601\n",
      "2024-01-05 18:42:35.139586: train_loss -0.9082\n",
      "2024-01-05 18:42:35.148578: val_loss -0.8385\n",
      "2024-01-05 18:42:35.156576: Pseudo dice [0.8644, 0.9608, 0.9429]\n",
      "2024-01-05 18:42:35.163088: Epoch time: 125.44 s\n",
      "2024-01-05 18:42:36.351796: \n",
      "2024-01-05 18:42:36.358805: Epoch 433\n",
      "2024-01-05 18:42:36.363724: Current learning rate: 0.006\n",
      "2024-01-05 18:44:42.337545: train_loss -0.9053\n",
      "2024-01-05 18:44:42.344544: val_loss -0.8355\n",
      "2024-01-05 18:44:42.349548: Pseudo dice [0.865, 0.9594, 0.942]\n",
      "2024-01-05 18:44:42.354544: Epoch time: 125.99 s\n",
      "2024-01-05 18:44:43.496429: \n",
      "2024-01-05 18:44:43.507841: Epoch 434\n",
      "2024-01-05 18:44:43.511876: Current learning rate: 0.00599\n",
      "2024-01-05 18:46:49.219571: train_loss -0.9092\n",
      "2024-01-05 18:46:49.229571: val_loss -0.8444\n",
      "2024-01-05 18:46:49.236572: Pseudo dice [0.866, 0.9607, 0.9432]\n",
      "2024-01-05 18:46:49.244571: Epoch time: 125.73 s\n",
      "2024-01-05 18:46:50.428984: \n",
      "2024-01-05 18:46:50.435986: Epoch 435\n",
      "2024-01-05 18:46:50.440771: Current learning rate: 0.00598\n",
      "2024-01-05 18:48:56.091035: train_loss -0.9123\n",
      "2024-01-05 18:48:56.099035: val_loss -0.8433\n",
      "2024-01-05 18:48:56.107034: Pseudo dice [0.8652, 0.96, 0.9424]\n",
      "2024-01-05 18:48:56.116035: Epoch time: 125.66 s\n",
      "2024-01-05 18:48:57.238158: \n",
      "2024-01-05 18:48:57.247206: Epoch 436\n",
      "2024-01-05 18:48:57.251266: Current learning rate: 0.00597\n",
      "2024-01-05 18:51:02.832963: train_loss -0.908\n",
      "2024-01-05 18:51:02.838963: val_loss -0.8322\n",
      "2024-01-05 18:51:02.843967: Pseudo dice [0.8622, 0.9585, 0.9405]\n",
      "2024-01-05 18:51:02.847965: Epoch time: 125.6 s\n",
      "2024-01-05 18:51:03.992490: \n",
      "2024-01-05 18:51:03.997499: Epoch 437\n",
      "2024-01-05 18:51:04.001498: Current learning rate: 0.00596\n",
      "2024-01-05 18:53:10.032188: train_loss -0.912\n",
      "2024-01-05 18:53:10.042191: val_loss -0.8356\n",
      "2024-01-05 18:53:10.047185: Pseudo dice [0.8635, 0.9599, 0.9422]\n",
      "2024-01-05 18:53:10.053192: Epoch time: 126.04 s\n",
      "2024-01-05 18:53:11.426582: \n",
      "2024-01-05 18:53:11.431473: Epoch 438\n",
      "2024-01-05 18:53:11.439538: Current learning rate: 0.00595\n",
      "2024-01-05 18:55:17.242334: train_loss -0.9117\n",
      "2024-01-05 18:55:17.248334: val_loss -0.8397\n",
      "2024-01-05 18:55:17.256343: Pseudo dice [0.8635, 0.9607, 0.9433]\n",
      "2024-01-05 18:55:17.262346: Epoch time: 125.82 s\n",
      "2024-01-05 18:55:18.464549: \n",
      "2024-01-05 18:55:18.472622: Epoch 439\n",
      "2024-01-05 18:55:18.477625: Current learning rate: 0.00594\n",
      "2024-01-05 18:57:24.204286: train_loss -0.9102\n",
      "2024-01-05 18:57:24.211286: val_loss -0.8369\n",
      "2024-01-05 18:57:24.216285: Pseudo dice [0.868, 0.9602, 0.9428]\n",
      "2024-01-05 18:57:24.221289: Epoch time: 125.74 s\n",
      "2024-01-05 18:57:24.225288: Yayy! New best EMA pseudo Dice: 0.9219\n",
      "2024-01-05 18:57:25.689287: \n",
      "2024-01-05 18:57:25.696370: Epoch 440\n",
      "2024-01-05 18:57:25.701366: Current learning rate: 0.00593\n",
      "2024-01-05 18:59:31.507246: train_loss -0.9114\n",
      "2024-01-05 18:59:31.514243: val_loss -0.8326\n",
      "2024-01-05 18:59:31.520242: Pseudo dice [0.8642, 0.9594, 0.9408]\n",
      "2024-01-05 18:59:31.530248: Epoch time: 125.82 s\n",
      "2024-01-05 18:59:32.844100: \n",
      "2024-01-05 18:59:32.850101: Epoch 441\n",
      "2024-01-05 18:59:32.856132: Current learning rate: 0.00592\n",
      "2024-01-05 19:01:38.758194: train_loss -0.911\n",
      "2024-01-05 19:01:38.769193: val_loss -0.8388\n",
      "2024-01-05 19:01:38.782789: Pseudo dice [0.863, 0.959, 0.9411]\n",
      "2024-01-05 19:01:38.788781: Epoch time: 125.92 s\n",
      "2024-01-05 19:01:39.838096: \n",
      "2024-01-05 19:01:39.846753: Epoch 442\n",
      "2024-01-05 19:01:39.853687: Current learning rate: 0.00592\n",
      "2024-01-05 19:03:45.734613: train_loss -0.913\n",
      "2024-01-05 19:03:45.740623: val_loss -0.8378\n",
      "2024-01-05 19:03:45.748612: Pseudo dice [0.8634, 0.9588, 0.9413]\n",
      "2024-01-05 19:03:45.755613: Epoch time: 125.9 s\n",
      "2024-01-05 19:03:46.928702: \n",
      "2024-01-05 19:03:46.934734: Epoch 443\n",
      "2024-01-05 19:03:46.938703: Current learning rate: 0.00591\n",
      "2024-01-05 19:05:52.632289: train_loss -0.9112\n",
      "2024-01-05 19:05:52.639279: val_loss -0.8398\n",
      "2024-01-05 19:05:52.643786: Pseudo dice [0.866, 0.9606, 0.9433]\n",
      "2024-01-05 19:05:52.648786: Epoch time: 125.7 s\n",
      "2024-01-05 19:05:53.819524: \n",
      "2024-01-05 19:05:53.824525: Epoch 444\n",
      "2024-01-05 19:05:53.828539: Current learning rate: 0.0059\n",
      "2024-01-05 19:07:59.490377: train_loss -0.9137\n",
      "2024-01-05 19:07:59.496917: val_loss -0.833\n",
      "2024-01-05 19:07:59.502909: Pseudo dice [0.8615, 0.9587, 0.9399]\n",
      "2024-01-05 19:07:59.509912: Epoch time: 125.67 s\n",
      "2024-01-05 19:08:00.644432: \n",
      "2024-01-05 19:08:00.650429: Epoch 445\n",
      "2024-01-05 19:08:00.657430: Current learning rate: 0.00589\n",
      "2024-01-05 19:10:05.973647: train_loss -0.9147\n",
      "2024-01-05 19:10:05.980647: val_loss -0.8377\n",
      "2024-01-05 19:10:05.986647: Pseudo dice [0.8663, 0.9591, 0.9413]\n",
      "2024-01-05 19:10:05.991654: Epoch time: 125.33 s\n",
      "2024-01-05 19:10:07.342080: \n",
      "2024-01-05 19:10:07.349181: Epoch 446\n",
      "2024-01-05 19:10:07.353320: Current learning rate: 0.00588\n",
      "2024-01-05 19:12:12.964687: train_loss -0.9152\n",
      "2024-01-05 19:12:12.972687: val_loss -0.8354\n",
      "2024-01-05 19:12:12.977686: Pseudo dice [0.8624, 0.9607, 0.9433]\n",
      "2024-01-05 19:12:12.982687: Epoch time: 125.62 s\n",
      "2024-01-05 19:12:14.072921: \n",
      "2024-01-05 19:12:14.077920: Epoch 447\n",
      "2024-01-05 19:12:14.081951: Current learning rate: 0.00587\n",
      "2024-01-05 19:14:19.788204: train_loss -0.9135\n",
      "2024-01-05 19:14:19.798195: val_loss -0.8325\n",
      "2024-01-05 19:14:19.805192: Pseudo dice [0.8637, 0.9591, 0.9417]\n",
      "2024-01-05 19:14:19.813194: Epoch time: 125.72 s\n",
      "2024-01-05 19:14:21.046705: \n",
      "2024-01-05 19:14:21.054688: Epoch 448\n",
      "2024-01-05 19:14:21.062636: Current learning rate: 0.00586\n",
      "2024-01-05 19:16:26.634406: train_loss -0.9161\n",
      "2024-01-05 19:16:26.642407: val_loss -0.8447\n",
      "2024-01-05 19:16:26.648424: Pseudo dice [0.866, 0.9601, 0.9426]\n",
      "2024-01-05 19:16:26.653425: Epoch time: 125.59 s\n",
      "2024-01-05 19:16:27.940279: \n",
      "2024-01-05 19:16:27.945791: Epoch 449\n",
      "2024-01-05 19:16:27.952803: Current learning rate: 0.00585\n",
      "2024-01-05 19:18:33.532810: train_loss -0.9125\n",
      "2024-01-05 19:18:33.541816: val_loss -0.8277\n",
      "2024-01-05 19:18:33.548818: Pseudo dice [0.8635, 0.958, 0.9388]\n",
      "2024-01-05 19:18:33.557322: Epoch time: 125.59 s\n",
      "2024-01-05 19:18:35.207269: \n",
      "2024-01-05 19:18:35.216852: Epoch 450\n",
      "2024-01-05 19:18:35.224823: Current learning rate: 0.00584\n",
      "2024-01-05 19:20:40.796897: train_loss -0.9115\n",
      "2024-01-05 19:20:40.802897: val_loss -0.8328\n",
      "2024-01-05 19:20:40.813897: Pseudo dice [0.8574, 0.9569, 0.9388]\n",
      "2024-01-05 19:20:40.819897: Epoch time: 125.59 s\n",
      "2024-01-05 19:20:42.020842: \n",
      "2024-01-05 19:20:42.028505: Epoch 451\n",
      "2024-01-05 19:20:42.033573: Current learning rate: 0.00583\n",
      "2024-01-05 19:22:47.882672: train_loss -0.8945\n",
      "2024-01-05 19:22:47.890186: val_loss -0.8322\n",
      "2024-01-05 19:22:47.895184: Pseudo dice [0.8602, 0.9574, 0.9379]\n",
      "2024-01-05 19:22:47.900193: Epoch time: 125.86 s\n",
      "2024-01-05 19:22:48.944967: \n",
      "2024-01-05 19:22:48.952962: Epoch 452\n",
      "2024-01-05 19:22:48.960890: Current learning rate: 0.00582\n",
      "2024-01-05 19:24:54.512429: train_loss -0.9088\n",
      "2024-01-05 19:24:54.519428: val_loss -0.8383\n",
      "2024-01-05 19:24:54.526435: Pseudo dice [0.8606, 0.9579, 0.9394]\n",
      "2024-01-05 19:24:54.531429: Epoch time: 125.57 s\n",
      "2024-01-05 19:24:55.557405: \n",
      "2024-01-05 19:24:55.562467: Epoch 453\n",
      "2024-01-05 19:24:55.567466: Current learning rate: 0.00581\n",
      "2024-01-05 19:27:01.507975: train_loss -0.9118\n",
      "2024-01-05 19:27:01.514985: val_loss -0.8361\n",
      "2024-01-05 19:27:01.520968: Pseudo dice [0.8577, 0.9601, 0.9413]\n",
      "2024-01-05 19:27:01.525965: Epoch time: 125.95 s\n",
      "2024-01-05 19:27:02.739018: \n",
      "2024-01-05 19:27:02.744532: Epoch 454\n",
      "2024-01-05 19:27:02.748548: Current learning rate: 0.0058\n",
      "2024-01-05 19:29:08.423852: train_loss -0.9132\n",
      "2024-01-05 19:29:08.431852: val_loss -0.8303\n",
      "2024-01-05 19:29:08.438854: Pseudo dice [0.8578, 0.958, 0.9384]\n",
      "2024-01-05 19:29:08.444863: Epoch time: 125.69 s\n",
      "2024-01-05 19:29:09.538322: \n",
      "2024-01-05 19:29:09.542879: Epoch 455\n",
      "2024-01-05 19:29:09.551848: Current learning rate: 0.00579\n",
      "2024-01-05 19:31:15.295880: train_loss -0.9121\n",
      "2024-01-05 19:31:15.304879: val_loss -0.8362\n",
      "2024-01-05 19:31:15.310879: Pseudo dice [0.8662, 0.9584, 0.9401]\n",
      "2024-01-05 19:31:15.318878: Epoch time: 125.76 s\n",
      "2024-01-05 19:31:16.516676: \n",
      "2024-01-05 19:31:16.525326: Epoch 456\n",
      "2024-01-05 19:31:16.530380: Current learning rate: 0.00578\n",
      "2024-01-05 19:33:22.217585: train_loss -0.9138\n",
      "2024-01-05 19:33:22.225586: val_loss -0.8286\n",
      "2024-01-05 19:33:22.231586: Pseudo dice [0.8594, 0.9587, 0.9397]\n",
      "2024-01-05 19:33:22.236585: Epoch time: 125.7 s\n",
      "2024-01-05 19:33:23.302912: \n",
      "2024-01-05 19:33:23.308651: Epoch 457\n",
      "2024-01-05 19:33:23.316584: Current learning rate: 0.00577\n",
      "2024-01-05 19:35:28.614980: train_loss -0.9133\n",
      "2024-01-05 19:35:28.624026: val_loss -0.8352\n",
      "2024-01-05 19:35:28.631528: Pseudo dice [0.8654, 0.9595, 0.941]\n",
      "2024-01-05 19:35:28.638529: Epoch time: 125.31 s\n",
      "2024-01-05 19:35:29.796122: \n",
      "2024-01-05 19:35:29.804123: Epoch 458\n",
      "2024-01-05 19:35:29.809114: Current learning rate: 0.00576\n",
      "2024-01-05 19:37:35.663383: train_loss -0.9124\n",
      "2024-01-05 19:37:35.673396: val_loss -0.8205\n",
      "2024-01-05 19:37:35.679891: Pseudo dice [0.8561, 0.9565, 0.9374]\n",
      "2024-01-05 19:37:35.683969: Epoch time: 125.87 s\n",
      "2024-01-05 19:37:36.811249: \n",
      "2024-01-05 19:37:36.816247: Epoch 459\n",
      "2024-01-05 19:37:36.821238: Current learning rate: 0.00575\n",
      "2024-01-05 19:39:42.501884: train_loss -0.9147\n",
      "2024-01-05 19:39:42.509633: val_loss -0.823\n",
      "2024-01-05 19:39:42.520311: Pseudo dice [0.8505, 0.9574, 0.9377]\n",
      "2024-01-05 19:39:42.525317: Epoch time: 125.69 s\n",
      "2024-01-05 19:39:43.778299: \n",
      "2024-01-05 19:39:43.784309: Epoch 460\n",
      "2024-01-05 19:39:43.787956: Current learning rate: 0.00574\n",
      "2024-01-05 19:41:49.401480: train_loss -0.9152\n",
      "2024-01-05 19:41:49.409479: val_loss -0.8307\n",
      "2024-01-05 19:41:49.416480: Pseudo dice [0.8555, 0.9577, 0.9387]\n",
      "2024-01-05 19:41:49.422482: Epoch time: 125.62 s\n",
      "2024-01-05 19:41:50.588397: \n",
      "2024-01-05 19:41:50.595370: Epoch 461\n",
      "2024-01-05 19:41:50.599417: Current learning rate: 0.00573\n",
      "2024-01-05 19:43:56.299452: train_loss -0.9139\n",
      "2024-01-05 19:43:56.307959: val_loss -0.8357\n",
      "2024-01-05 19:43:56.315961: Pseudo dice [0.8575, 0.9591, 0.94]\n",
      "2024-01-05 19:43:56.322972: Epoch time: 125.71 s\n",
      "2024-01-05 19:43:57.594394: \n",
      "2024-01-05 19:43:57.599339: Epoch 462\n",
      "2024-01-05 19:43:57.603338: Current learning rate: 0.00572\n",
      "2024-01-05 19:46:03.351535: train_loss -0.9117\n",
      "2024-01-05 19:46:03.357538: val_loss -0.8275\n",
      "2024-01-05 19:46:03.362535: Pseudo dice [0.8539, 0.9577, 0.9387]\n",
      "2024-01-05 19:46:03.367535: Epoch time: 125.76 s\n",
      "2024-01-05 19:46:04.500890: \n",
      "2024-01-05 19:46:04.508892: Epoch 463\n",
      "2024-01-05 19:46:04.513835: Current learning rate: 0.00571\n",
      "2024-01-05 19:48:10.105012: train_loss -0.9147\n",
      "2024-01-05 19:48:10.115012: val_loss -0.8359\n",
      "2024-01-05 19:48:10.123019: Pseudo dice [0.86, 0.9596, 0.9417]\n",
      "2024-01-05 19:48:10.129020: Epoch time: 125.61 s\n",
      "2024-01-05 19:48:11.197634: \n",
      "2024-01-05 19:48:11.202629: Epoch 464\n",
      "2024-01-05 19:48:11.206634: Current learning rate: 0.0057\n",
      "2024-01-05 19:50:17.208967: train_loss -0.9166\n",
      "2024-01-05 19:50:17.215976: val_loss -0.8322\n",
      "2024-01-05 19:50:17.221968: Pseudo dice [0.8579, 0.9597, 0.942]\n",
      "2024-01-05 19:50:17.227968: Epoch time: 126.01 s\n",
      "2024-01-05 19:50:18.438601: \n",
      "2024-01-05 19:50:18.446607: Epoch 465\n",
      "2024-01-05 19:50:18.451618: Current learning rate: 0.0057\n",
      "2024-01-05 19:52:24.187008: train_loss -0.9145\n",
      "2024-01-05 19:52:24.193516: val_loss -0.8321\n",
      "2024-01-05 19:52:24.198515: Pseudo dice [0.8626, 0.9595, 0.9416]\n",
      "2024-01-05 19:52:24.205516: Epoch time: 125.75 s\n",
      "2024-01-05 19:52:25.297158: \n",
      "2024-01-05 19:52:25.303008: Epoch 466\n",
      "2024-01-05 19:52:25.307999: Current learning rate: 0.00569\n",
      "2024-01-05 19:54:30.916943: train_loss -0.9153\n",
      "2024-01-05 19:54:30.922943: val_loss -0.8372\n",
      "2024-01-05 19:54:30.927943: Pseudo dice [0.8626, 0.9587, 0.94]\n",
      "2024-01-05 19:54:30.932943: Epoch time: 125.62 s\n",
      "2024-01-05 19:54:32.157675: \n",
      "2024-01-05 19:54:32.166680: Epoch 467\n",
      "2024-01-05 19:54:32.170770: Current learning rate: 0.00568\n",
      "2024-01-05 19:56:37.989384: train_loss -0.915\n",
      "2024-01-05 19:56:37.998324: val_loss -0.84\n",
      "2024-01-05 19:56:38.005404: Pseudo dice [0.8643, 0.9598, 0.9415]\n",
      "2024-01-05 19:56:38.010393: Epoch time: 125.83 s\n",
      "2024-01-05 19:56:39.089649: \n",
      "2024-01-05 19:56:39.094893: Epoch 468\n",
      "2024-01-05 19:56:39.101471: Current learning rate: 0.00567\n",
      "2024-01-05 19:58:44.971454: train_loss -0.9126\n",
      "2024-01-05 19:58:44.978963: val_loss -0.838\n",
      "2024-01-05 19:58:44.983972: Pseudo dice [0.8617, 0.9595, 0.9416]\n",
      "2024-01-05 19:58:44.987969: Epoch time: 125.88 s\n",
      "2024-01-05 19:58:46.120076: \n",
      "2024-01-05 19:58:46.126065: Epoch 469\n",
      "2024-01-05 19:58:46.136994: Current learning rate: 0.00566\n",
      "2024-01-05 20:00:52.071632: train_loss -0.9136\n",
      "2024-01-05 20:00:52.079628: val_loss -0.8354\n",
      "2024-01-05 20:00:52.085629: Pseudo dice [0.8588, 0.9563, 0.9369]\n",
      "2024-01-05 20:00:52.090627: Epoch time: 125.95 s\n",
      "2024-01-05 20:00:53.462358: \n",
      "2024-01-05 20:00:53.468572: Epoch 470\n",
      "2024-01-05 20:00:53.472646: Current learning rate: 0.00565\n",
      "2024-01-05 20:02:59.196297: train_loss -0.9149\n",
      "2024-01-05 20:02:59.206807: val_loss -0.8327\n",
      "2024-01-05 20:02:59.213810: Pseudo dice [0.8602, 0.9588, 0.9405]\n",
      "2024-01-05 20:02:59.220809: Epoch time: 125.73 s\n",
      "2024-01-05 20:03:00.502994: \n",
      "2024-01-05 20:03:00.508003: Epoch 471\n",
      "2024-01-05 20:03:00.512004: Current learning rate: 0.00564\n",
      "2024-01-05 20:05:06.374259: train_loss -0.9139\n",
      "2024-01-05 20:05:06.382253: val_loss -0.8365\n",
      "2024-01-05 20:05:06.387254: Pseudo dice [0.8565, 0.9584, 0.9397]\n",
      "2024-01-05 20:05:06.391253: Epoch time: 125.87 s\n",
      "2024-01-05 20:05:07.652401: \n",
      "2024-01-05 20:05:07.658407: Epoch 472\n",
      "2024-01-05 20:05:07.663407: Current learning rate: 0.00563\n",
      "2024-01-05 20:07:13.402219: train_loss -0.9139\n",
      "2024-01-05 20:07:13.409212: val_loss -0.8371\n",
      "2024-01-05 20:07:13.416736: Pseudo dice [0.8588, 0.959, 0.9408]\n",
      "2024-01-05 20:07:13.425748: Epoch time: 125.75 s\n",
      "2024-01-05 20:07:14.568579: \n",
      "2024-01-05 20:07:14.574600: Epoch 473\n",
      "2024-01-05 20:07:14.578582: Current learning rate: 0.00562\n",
      "2024-01-05 20:09:20.418304: train_loss -0.9148\n",
      "2024-01-05 20:09:20.425331: val_loss -0.8362\n",
      "2024-01-05 20:09:20.429308: Pseudo dice [0.8641, 0.9572, 0.9389]\n",
      "2024-01-05 20:09:20.434306: Epoch time: 125.85 s\n",
      "2024-01-05 20:09:21.645073: \n",
      "2024-01-05 20:09:21.651043: Epoch 474\n",
      "2024-01-05 20:09:21.655075: Current learning rate: 0.00561\n",
      "2024-01-05 20:11:27.349619: train_loss -0.9159\n",
      "2024-01-05 20:11:27.356124: val_loss -0.8377\n",
      "2024-01-05 20:11:27.360133: Pseudo dice [0.859, 0.9586, 0.9408]\n",
      "2024-01-05 20:11:27.365124: Epoch time: 125.71 s\n",
      "2024-01-05 20:11:28.561253: \n",
      "2024-01-05 20:11:28.569308: Epoch 475\n",
      "2024-01-05 20:11:28.573305: Current learning rate: 0.0056\n",
      "2024-01-05 20:13:34.941370: train_loss -0.9124\n",
      "2024-01-05 20:13:34.949371: val_loss -0.8378\n",
      "2024-01-05 20:13:34.955449: Pseudo dice [0.8595, 0.9589, 0.9407]\n",
      "2024-01-05 20:13:34.959445: Epoch time: 126.38 s\n",
      "2024-01-05 20:13:36.115395: \n",
      "2024-01-05 20:13:36.124330: Epoch 476\n",
      "2024-01-05 20:13:36.132061: Current learning rate: 0.00559\n",
      "2024-01-05 20:15:41.951729: train_loss -0.9142\n",
      "2024-01-05 20:15:41.957740: val_loss -0.8258\n",
      "2024-01-05 20:15:41.962735: Pseudo dice [0.8494, 0.9574, 0.938]\n",
      "2024-01-05 20:15:41.968729: Epoch time: 125.84 s\n",
      "2024-01-05 20:15:43.029906: \n",
      "2024-01-05 20:15:43.038989: Epoch 477\n",
      "2024-01-05 20:15:43.043984: Current learning rate: 0.00558\n",
      "2024-01-05 20:17:48.715415: train_loss -0.9161\n",
      "2024-01-05 20:17:48.722406: val_loss -0.8366\n",
      "2024-01-05 20:17:48.728407: Pseudo dice [0.8565, 0.9593, 0.9418]\n",
      "2024-01-05 20:17:48.733405: Epoch time: 125.69 s\n",
      "2024-01-05 20:17:49.944768: \n",
      "2024-01-05 20:17:49.955161: Epoch 478\n",
      "2024-01-05 20:17:49.961094: Current learning rate: 0.00557\n",
      "2024-01-05 20:19:55.684390: train_loss -0.9178\n",
      "2024-01-05 20:19:55.691389: val_loss -0.8324\n",
      "2024-01-05 20:19:55.698390: Pseudo dice [0.8589, 0.958, 0.9392]\n",
      "2024-01-05 20:19:55.704389: Epoch time: 125.74 s\n",
      "2024-01-05 20:19:56.924452: \n",
      "2024-01-05 20:19:56.930602: Epoch 479\n",
      "2024-01-05 20:19:56.935608: Current learning rate: 0.00556\n",
      "2024-01-05 20:22:02.581270: train_loss -0.9164\n",
      "2024-01-05 20:22:02.587271: val_loss -0.829\n",
      "2024-01-05 20:22:02.594272: Pseudo dice [0.8583, 0.9569, 0.9378]\n",
      "2024-01-05 20:22:02.600271: Epoch time: 125.66 s\n",
      "2024-01-05 20:22:03.682249: \n",
      "2024-01-05 20:22:03.688187: Epoch 480\n",
      "2024-01-05 20:22:03.693252: Current learning rate: 0.00555\n",
      "2024-01-05 20:24:09.766536: train_loss -0.9143\n",
      "2024-01-05 20:24:09.777056: val_loss -0.832\n",
      "2024-01-05 20:24:09.783059: Pseudo dice [0.8568, 0.958, 0.9393]\n",
      "2024-01-05 20:24:09.792063: Epoch time: 126.09 s\n",
      "2024-01-05 20:24:11.049928: \n",
      "2024-01-05 20:24:11.057928: Epoch 481\n",
      "2024-01-05 20:24:11.062169: Current learning rate: 0.00554\n",
      "2024-01-05 20:26:17.075557: train_loss -0.9145\n",
      "2024-01-05 20:26:17.081558: val_loss -0.8279\n",
      "2024-01-05 20:26:17.090067: Pseudo dice [0.853, 0.9576, 0.9384]\n",
      "2024-01-05 20:26:17.095075: Epoch time: 126.03 s\n",
      "2024-01-05 20:26:18.203379: \n",
      "2024-01-05 20:26:18.218323: Epoch 482\n",
      "2024-01-05 20:26:18.225408: Current learning rate: 0.00553\n",
      "2024-01-05 20:28:24.246725: train_loss -0.9156\n",
      "2024-01-05 20:28:24.255725: val_loss -0.8352\n",
      "2024-01-05 20:28:24.261727: Pseudo dice [0.8632, 0.9589, 0.9407]\n",
      "2024-01-05 20:28:24.266726: Epoch time: 126.05 s\n",
      "2024-01-05 20:28:25.594920: \n",
      "2024-01-05 20:28:25.604865: Epoch 483\n",
      "2024-01-05 20:28:25.609785: Current learning rate: 0.00552\n",
      "2024-01-05 20:30:31.238521: train_loss -0.9149\n",
      "2024-01-05 20:30:31.244521: val_loss -0.8353\n",
      "2024-01-05 20:30:31.250520: Pseudo dice [0.8606, 0.9594, 0.9418]\n",
      "2024-01-05 20:30:31.257531: Epoch time: 125.64 s\n",
      "2024-01-05 20:30:32.432703: \n",
      "2024-01-05 20:30:32.438633: Epoch 484\n",
      "2024-01-05 20:30:32.443673: Current learning rate: 0.00551\n",
      "2024-01-05 20:32:38.372101: train_loss -0.9142\n",
      "2024-01-05 20:32:38.379102: val_loss -0.8318\n",
      "2024-01-05 20:32:38.384102: Pseudo dice [0.8597, 0.9581, 0.9396]\n",
      "2024-01-05 20:32:38.393094: Epoch time: 125.94 s\n",
      "2024-01-05 20:32:39.544184: \n",
      "2024-01-05 20:32:39.549785: Epoch 485\n",
      "2024-01-05 20:32:39.557526: Current learning rate: 0.0055\n",
      "2024-01-05 20:34:45.520372: train_loss -0.9141\n",
      "2024-01-05 20:34:45.527373: val_loss -0.8269\n",
      "2024-01-05 20:34:45.532377: Pseudo dice [0.8536, 0.9583, 0.9395]\n",
      "2024-01-05 20:34:45.537376: Epoch time: 125.98 s\n",
      "2024-01-05 20:34:46.714650: \n",
      "2024-01-05 20:34:46.722860: Epoch 486\n",
      "2024-01-05 20:34:46.727867: Current learning rate: 0.00549\n",
      "2024-01-05 20:36:52.668965: train_loss -0.9129\n",
      "2024-01-05 20:36:52.674966: val_loss -0.8338\n",
      "2024-01-05 20:36:52.682966: Pseudo dice [0.8611, 0.9595, 0.9425]\n",
      "2024-01-05 20:36:52.688970: Epoch time: 125.96 s\n",
      "2024-01-05 20:36:53.768765: \n",
      "2024-01-05 20:36:53.777845: Epoch 487\n",
      "2024-01-05 20:36:53.781837: Current learning rate: 0.00548\n",
      "2024-01-05 20:38:59.633237: train_loss -0.915\n",
      "2024-01-05 20:38:59.640240: val_loss -0.8378\n",
      "2024-01-05 20:38:59.645238: Pseudo dice [0.8565, 0.9575, 0.9387]\n",
      "2024-01-05 20:38:59.649237: Epoch time: 125.86 s\n",
      "2024-01-05 20:39:00.700483: \n",
      "2024-01-05 20:39:00.706698: Epoch 488\n",
      "2024-01-05 20:39:00.710725: Current learning rate: 0.00547\n",
      "2024-01-05 20:41:06.868912: train_loss -0.9106\n",
      "2024-01-05 20:41:06.876491: val_loss -0.8373\n",
      "2024-01-05 20:41:06.885492: Pseudo dice [0.8563, 0.9598, 0.9422]\n",
      "2024-01-05 20:41:06.894491: Epoch time: 126.17 s\n",
      "2024-01-05 20:41:08.022804: \n",
      "2024-01-05 20:41:08.028309: Epoch 489\n",
      "2024-01-05 20:41:08.035388: Current learning rate: 0.00546\n",
      "2024-01-05 20:43:13.870881: train_loss -0.913\n",
      "2024-01-05 20:43:13.879727: val_loss -0.8357\n",
      "2024-01-05 20:43:13.886727: Pseudo dice [0.8597, 0.9605, 0.9435]\n",
      "2024-01-05 20:43:13.891726: Epoch time: 125.85 s\n",
      "2024-01-05 20:43:15.032849: \n",
      "2024-01-05 20:43:15.038069: Epoch 490\n",
      "2024-01-05 20:43:15.042126: Current learning rate: 0.00546\n",
      "2024-01-05 20:45:21.185954: train_loss -0.9129\n",
      "2024-01-05 20:45:21.193964: val_loss -0.832\n",
      "2024-01-05 20:45:21.210746: Pseudo dice [0.8546, 0.9575, 0.9385]\n",
      "2024-01-05 20:45:21.215746: Epoch time: 126.15 s\n",
      "2024-01-05 20:45:22.526772: \n",
      "2024-01-05 20:45:22.532920: Epoch 491\n",
      "2024-01-05 20:45:22.538867: Current learning rate: 0.00545\n",
      "2024-01-05 20:47:28.133679: train_loss -0.9181\n",
      "2024-01-05 20:47:28.140682: val_loss -0.8402\n",
      "2024-01-05 20:47:28.148680: Pseudo dice [0.8598, 0.9598, 0.9428]\n",
      "2024-01-05 20:47:28.153690: Epoch time: 125.61 s\n",
      "2024-01-05 20:47:29.276878: \n",
      "2024-01-05 20:47:29.282187: Epoch 492\n",
      "2024-01-05 20:47:29.289189: Current learning rate: 0.00544\n",
      "2024-01-05 20:49:35.317769: train_loss -0.9126\n",
      "2024-01-05 20:49:35.327768: val_loss -0.8327\n",
      "2024-01-05 20:49:35.337768: Pseudo dice [0.862, 0.959, 0.9405]\n",
      "2024-01-05 20:49:35.344767: Epoch time: 126.04 s\n",
      "2024-01-05 20:49:36.509768: \n",
      "2024-01-05 20:49:36.516697: Epoch 493\n",
      "2024-01-05 20:49:36.522750: Current learning rate: 0.00543\n",
      "2024-01-05 20:51:42.228258: train_loss -0.9155\n",
      "2024-01-05 20:51:42.235257: val_loss -0.8365\n",
      "2024-01-05 20:51:42.240257: Pseudo dice [0.8649, 0.9589, 0.94]\n",
      "2024-01-05 20:51:42.245257: Epoch time: 125.72 s\n",
      "2024-01-05 20:51:43.537015: \n",
      "2024-01-05 20:51:43.545020: Epoch 494\n",
      "2024-01-05 20:51:43.549270: Current learning rate: 0.00542\n",
      "2024-01-05 20:53:49.470193: train_loss -0.9139\n",
      "2024-01-05 20:53:49.478193: val_loss -0.8374\n",
      "2024-01-05 20:53:49.483193: Pseudo dice [0.8633, 0.9601, 0.942]\n",
      "2024-01-05 20:53:49.490195: Epoch time: 125.93 s\n",
      "2024-01-05 20:53:50.665412: \n",
      "2024-01-05 20:53:50.671068: Epoch 495\n",
      "2024-01-05 20:53:50.679074: Current learning rate: 0.00541\n",
      "2024-01-05 20:55:57.092111: train_loss -0.9139\n",
      "2024-01-05 20:55:57.102112: val_loss -0.8392\n",
      "2024-01-05 20:55:57.111119: Pseudo dice [0.864, 0.9593, 0.9407]\n",
      "2024-01-05 20:55:57.119123: Epoch time: 126.43 s\n",
      "2024-01-05 20:55:58.492914: \n",
      "2024-01-05 20:55:58.499914: Epoch 496\n",
      "2024-01-05 20:55:58.509941: Current learning rate: 0.0054\n",
      "2024-01-05 20:58:04.252502: train_loss -0.9163\n",
      "2024-01-05 20:58:04.261507: val_loss -0.8368\n",
      "2024-01-05 20:58:04.266507: Pseudo dice [0.8603, 0.9583, 0.9405]\n",
      "2024-01-05 20:58:04.272015: Epoch time: 125.76 s\n",
      "2024-01-05 20:58:05.427389: \n",
      "2024-01-05 20:58:05.435329: Epoch 497\n",
      "2024-01-05 20:58:05.441397: Current learning rate: 0.00539\n",
      "2024-01-05 21:00:11.179887: train_loss -0.9154\n",
      "2024-01-05 21:00:11.186888: val_loss -0.8307\n",
      "2024-01-05 21:00:11.193886: Pseudo dice [0.8581, 0.9592, 0.9407]\n",
      "2024-01-05 21:00:11.197886: Epoch time: 125.75 s\n",
      "2024-01-05 21:00:12.340059: \n",
      "2024-01-05 21:00:12.346092: Epoch 498\n",
      "2024-01-05 21:00:12.351694: Current learning rate: 0.00538\n",
      "2024-01-05 21:02:18.591480: train_loss -0.9158\n",
      "2024-01-05 21:02:18.597478: val_loss -0.8225\n",
      "2024-01-05 21:02:18.604480: Pseudo dice [0.8522, 0.9574, 0.9378]\n",
      "2024-01-05 21:02:18.609480: Epoch time: 126.25 s\n",
      "2024-01-05 21:02:19.776090: \n",
      "2024-01-05 21:02:19.782082: Epoch 499\n",
      "2024-01-05 21:02:19.787089: Current learning rate: 0.00537\n",
      "2024-01-05 21:04:25.680710: train_loss -0.9143\n",
      "2024-01-05 21:04:25.691711: val_loss -0.8371\n",
      "2024-01-05 21:04:25.697719: Pseudo dice [0.8629, 0.96, 0.9422]\n",
      "2024-01-05 21:04:25.702712: Epoch time: 125.91 s\n",
      "2024-01-05 21:04:27.259295: \n",
      "2024-01-05 21:04:27.266157: Epoch 500\n",
      "2024-01-05 21:04:27.270154: Current learning rate: 0.00536\n",
      "2024-01-05 21:06:32.848061: train_loss -0.9154\n",
      "2024-01-05 21:06:32.857059: val_loss -0.8402\n",
      "2024-01-05 21:06:32.864067: Pseudo dice [0.8617, 0.9594, 0.9423]\n",
      "2024-01-05 21:06:32.871061: Epoch time: 125.59 s\n",
      "2024-01-05 21:06:34.288447: \n",
      "2024-01-05 21:06:34.296513: Epoch 501\n",
      "2024-01-05 21:06:34.301452: Current learning rate: 0.00535\n",
      "2024-01-05 21:08:40.208636: train_loss -0.9167\n",
      "2024-01-05 21:08:40.217637: val_loss -0.8411\n",
      "2024-01-05 21:08:40.222643: Pseudo dice [0.8626, 0.9594, 0.9424]\n",
      "2024-01-05 21:08:40.227639: Epoch time: 125.92 s\n",
      "2024-01-05 21:08:41.479355: \n",
      "2024-01-05 21:08:41.486284: Epoch 502\n",
      "2024-01-05 21:08:41.490932: Current learning rate: 0.00534\n",
      "2024-01-05 21:10:47.069036: train_loss -0.918\n",
      "2024-01-05 21:10:47.075034: val_loss -0.842\n",
      "2024-01-05 21:10:47.080036: Pseudo dice [0.8603, 0.9601, 0.942]\n",
      "2024-01-05 21:10:47.084034: Epoch time: 125.59 s\n",
      "2024-01-05 21:10:48.310120: \n",
      "2024-01-05 21:10:48.316121: Epoch 503\n",
      "2024-01-05 21:10:48.321120: Current learning rate: 0.00533\n",
      "2024-01-05 21:12:54.011827: train_loss -0.9155\n",
      "2024-01-05 21:12:54.018827: val_loss -0.8307\n",
      "2024-01-05 21:12:54.023836: Pseudo dice [0.8548, 0.9587, 0.9411]\n",
      "2024-01-05 21:12:54.028837: Epoch time: 125.7 s\n",
      "2024-01-05 21:12:55.118595: \n",
      "2024-01-05 21:12:55.128579: Epoch 504\n",
      "2024-01-05 21:12:55.138530: Current learning rate: 0.00532\n",
      "2024-01-05 21:15:00.848869: train_loss -0.9178\n",
      "2024-01-05 21:15:00.858869: val_loss -0.8406\n",
      "2024-01-05 21:15:00.866869: Pseudo dice [0.8623, 0.9602, 0.9426]\n",
      "2024-01-05 21:15:00.871869: Epoch time: 125.73 s\n",
      "2024-01-05 21:15:02.045789: \n",
      "2024-01-05 21:15:02.052792: Epoch 505\n",
      "2024-01-05 21:15:02.062874: Current learning rate: 0.00531\n",
      "2024-01-05 21:17:08.110568: train_loss -0.9144\n",
      "2024-01-05 21:17:08.121568: val_loss -0.8322\n",
      "2024-01-05 21:17:08.126568: Pseudo dice [0.8625, 0.9581, 0.9393]\n",
      "2024-01-05 21:17:08.131572: Epoch time: 126.07 s\n",
      "2024-01-05 21:17:09.277566: \n",
      "2024-01-05 21:17:09.285652: Epoch 506\n",
      "2024-01-05 21:17:09.290709: Current learning rate: 0.0053\n",
      "2024-01-05 21:19:14.927556: train_loss -0.9155\n",
      "2024-01-05 21:19:14.935557: val_loss -0.8417\n",
      "2024-01-05 21:19:14.940559: Pseudo dice [0.8623, 0.9596, 0.9425]\n",
      "2024-01-05 21:19:14.945559: Epoch time: 125.65 s\n",
      "2024-01-05 21:19:16.102343: \n",
      "2024-01-05 21:19:16.108334: Epoch 507\n",
      "2024-01-05 21:19:16.113129: Current learning rate: 0.00529\n",
      "2024-01-05 21:21:21.901904: train_loss -0.9166\n",
      "2024-01-05 21:21:21.907903: val_loss -0.8461\n",
      "2024-01-05 21:21:21.913904: Pseudo dice [0.8657, 0.9597, 0.942]\n",
      "2024-01-05 21:21:21.918906: Epoch time: 125.8 s\n",
      "2024-01-05 21:21:23.106382: \n",
      "2024-01-05 21:21:23.114277: Epoch 508\n",
      "2024-01-05 21:21:23.120245: Current learning rate: 0.00528\n",
      "2024-01-05 21:23:28.921423: train_loss -0.9155\n",
      "2024-01-05 21:23:28.930414: val_loss -0.8377\n",
      "2024-01-05 21:23:28.935418: Pseudo dice [0.8627, 0.9594, 0.9424]\n",
      "2024-01-05 21:23:28.940431: Epoch time: 125.82 s\n",
      "2024-01-05 21:23:30.153002: \n",
      "2024-01-05 21:23:30.162663: Epoch 509\n",
      "2024-01-05 21:23:30.167829: Current learning rate: 0.00527\n",
      "2024-01-05 21:25:36.324759: train_loss -0.9143\n",
      "2024-01-05 21:25:36.332765: val_loss -0.8372\n",
      "2024-01-05 21:25:36.337766: Pseudo dice [0.8627, 0.9596, 0.9418]\n",
      "2024-01-05 21:25:36.343269: Epoch time: 126.17 s\n",
      "2024-01-05 21:25:37.513960: \n",
      "2024-01-05 21:25:37.520000: Epoch 510\n",
      "2024-01-05 21:25:37.526999: Current learning rate: 0.00526\n",
      "2024-01-05 21:27:43.628993: train_loss -0.9144\n",
      "2024-01-05 21:27:43.635988: val_loss -0.8275\n",
      "2024-01-05 21:27:43.641496: Pseudo dice [0.8523, 0.9586, 0.9398]\n",
      "2024-01-05 21:27:43.646496: Epoch time: 126.12 s\n",
      "2024-01-05 21:27:44.762234: \n",
      "2024-01-05 21:27:44.770233: Epoch 511\n",
      "2024-01-05 21:27:44.775233: Current learning rate: 0.00525\n",
      "2024-01-05 21:29:50.587398: train_loss -0.9138\n",
      "2024-01-05 21:29:50.595397: val_loss -0.8287\n",
      "2024-01-05 21:29:50.604476: Pseudo dice [0.8599, 0.959, 0.9415]\n",
      "2024-01-05 21:29:50.612404: Epoch time: 125.83 s\n",
      "2024-01-05 21:29:51.686644: \n",
      "2024-01-05 21:29:51.694391: Epoch 512\n",
      "2024-01-05 21:29:51.700387: Current learning rate: 0.00524\n",
      "2024-01-05 21:31:57.458105: train_loss -0.9152\n",
      "2024-01-05 21:31:57.468101: val_loss -0.8432\n",
      "2024-01-05 21:31:57.475103: Pseudo dice [0.8622, 0.9597, 0.942]\n",
      "2024-01-05 21:31:57.481112: Epoch time: 125.77 s\n",
      "2024-01-05 21:31:58.646449: \n",
      "2024-01-05 21:31:58.652454: Epoch 513\n",
      "2024-01-05 21:31:58.656527: Current learning rate: 0.00523\n",
      "2024-01-05 21:34:04.257568: train_loss -0.9149\n",
      "2024-01-05 21:34:04.265569: val_loss -0.838\n",
      "2024-01-05 21:34:04.270578: Pseudo dice [0.8629, 0.9592, 0.9407]\n",
      "2024-01-05 21:34:04.275578: Epoch time: 125.61 s\n",
      "2024-01-05 21:34:05.411912: \n",
      "2024-01-05 21:34:05.419135: Epoch 514\n",
      "2024-01-05 21:34:05.423135: Current learning rate: 0.00522\n",
      "2024-01-05 21:36:11.202793: train_loss -0.9152\n",
      "2024-01-05 21:36:11.213307: val_loss -0.8416\n",
      "2024-01-05 21:36:11.220307: Pseudo dice [0.8655, 0.9592, 0.9413]\n",
      "2024-01-05 21:36:11.230318: Epoch time: 125.79 s\n",
      "2024-01-05 21:36:12.372194: \n",
      "2024-01-05 21:36:12.377576: Epoch 515\n",
      "2024-01-05 21:36:12.384650: Current learning rate: 0.00521\n",
      "2024-01-05 21:38:18.241488: train_loss -0.916\n",
      "2024-01-05 21:38:18.248489: val_loss -0.8412\n",
      "2024-01-05 21:38:18.253488: Pseudo dice [0.8608, 0.9596, 0.9424]\n",
      "2024-01-05 21:38:18.258489: Epoch time: 125.87 s\n",
      "2024-01-05 21:38:19.404567: \n",
      "2024-01-05 21:38:19.410629: Epoch 516\n",
      "2024-01-05 21:38:19.415477: Current learning rate: 0.0052\n",
      "2024-01-05 21:40:25.412014: train_loss -0.9155\n",
      "2024-01-05 21:40:25.421013: val_loss -0.8389\n",
      "2024-01-05 21:40:25.428082: Pseudo dice [0.8631, 0.9592, 0.9412]\n",
      "2024-01-05 21:40:25.434081: Epoch time: 126.01 s\n",
      "2024-01-05 21:40:26.753784: \n",
      "2024-01-05 21:40:26.759793: Epoch 517\n",
      "2024-01-05 21:40:26.764863: Current learning rate: 0.00519\n",
      "2024-01-05 21:42:32.562527: train_loss -0.9151\n",
      "2024-01-05 21:42:32.571532: val_loss -0.8396\n",
      "2024-01-05 21:42:32.579523: Pseudo dice [0.8635, 0.9592, 0.941]\n",
      "2024-01-05 21:42:32.584527: Epoch time: 125.81 s\n",
      "2024-01-05 21:42:33.877190: \n",
      "2024-01-05 21:42:33.882383: Epoch 518\n",
      "2024-01-05 21:42:33.887459: Current learning rate: 0.00518\n",
      "2024-01-05 21:44:39.940771: train_loss -0.9174\n",
      "2024-01-05 21:44:39.948763: val_loss -0.8428\n",
      "2024-01-05 21:44:39.954838: Pseudo dice [0.8642, 0.9607, 0.9439]\n",
      "2024-01-05 21:44:39.959849: Epoch time: 126.06 s\n",
      "2024-01-05 21:44:41.091243: \n",
      "2024-01-05 21:44:41.097314: Epoch 519\n",
      "2024-01-05 21:44:41.102306: Current learning rate: 0.00518\n",
      "2024-01-05 21:46:46.867674: train_loss -0.9162\n",
      "2024-01-05 21:46:46.874677: val_loss -0.8386\n",
      "2024-01-05 21:46:46.880678: Pseudo dice [0.8622, 0.9591, 0.941]\n",
      "2024-01-05 21:46:46.884697: Epoch time: 125.78 s\n",
      "2024-01-05 21:46:48.105362: \n",
      "2024-01-05 21:46:48.111072: Epoch 520\n",
      "2024-01-05 21:46:48.119152: Current learning rate: 0.00517\n",
      "2024-01-05 21:48:54.181397: train_loss -0.915\n",
      "2024-01-05 21:48:54.189406: val_loss -0.8277\n",
      "2024-01-05 21:48:54.195400: Pseudo dice [0.8635, 0.9581, 0.9396]\n",
      "2024-01-05 21:48:54.204401: Epoch time: 126.08 s\n",
      "2024-01-05 21:48:55.289097: \n",
      "2024-01-05 21:48:55.300037: Epoch 521\n",
      "2024-01-05 21:48:55.305040: Current learning rate: 0.00516\n",
      "2024-01-05 21:51:01.156922: train_loss -0.9143\n",
      "2024-01-05 21:51:01.164941: val_loss -0.8395\n",
      "2024-01-05 21:51:01.169941: Pseudo dice [0.8642, 0.9597, 0.9424]\n",
      "2024-01-05 21:51:01.175452: Epoch time: 125.87 s\n",
      "2024-01-05 21:51:02.274372: \n",
      "2024-01-05 21:51:02.280371: Epoch 522\n",
      "2024-01-05 21:51:02.285372: Current learning rate: 0.00515\n",
      "2024-01-05 21:53:07.960810: train_loss -0.9178\n",
      "2024-01-05 21:53:07.969809: val_loss -0.8363\n",
      "2024-01-05 21:53:07.975808: Pseudo dice [0.8662, 0.9599, 0.9418]\n",
      "2024-01-05 21:53:07.979806: Epoch time: 125.69 s\n",
      "2024-01-05 21:53:09.034621: \n",
      "2024-01-05 21:53:09.040655: Epoch 523\n",
      "2024-01-05 21:53:09.045706: Current learning rate: 0.00514\n",
      "2024-01-05 21:55:14.896673: train_loss -0.9153\n",
      "2024-01-05 21:55:14.905681: val_loss -0.8332\n",
      "2024-01-05 21:55:14.911682: Pseudo dice [0.8627, 0.9596, 0.9413]\n",
      "2024-01-05 21:55:14.917673: Epoch time: 125.86 s\n",
      "2024-01-05 21:55:15.972135: \n",
      "2024-01-05 21:55:15.977140: Epoch 524\n",
      "2024-01-05 21:55:15.982132: Current learning rate: 0.00513\n",
      "2024-01-05 21:57:21.939919: train_loss -0.9177\n",
      "2024-01-05 21:57:21.948920: val_loss -0.8398\n",
      "2024-01-05 21:57:21.955912: Pseudo dice [0.8665, 0.9588, 0.9409]\n",
      "2024-01-05 21:57:21.961913: Epoch time: 125.97 s\n",
      "2024-01-05 21:57:23.318166: \n",
      "2024-01-05 21:57:23.325112: Epoch 525\n",
      "2024-01-05 21:57:23.329115: Current learning rate: 0.00512\n",
      "2024-01-05 21:59:29.001693: train_loss -0.9179\n",
      "2024-01-05 21:59:29.011218: val_loss -0.8342\n",
      "2024-01-05 21:59:29.016220: Pseudo dice [0.8637, 0.959, 0.941]\n",
      "2024-01-05 21:59:29.021219: Epoch time: 125.68 s\n",
      "2024-01-05 21:59:30.190332: \n",
      "2024-01-05 21:59:30.196472: Epoch 526\n",
      "2024-01-05 21:59:30.204461: Current learning rate: 0.00511\n",
      "2024-01-05 22:01:35.941257: train_loss -0.9177\n",
      "2024-01-05 22:01:35.951256: val_loss -0.8326\n",
      "2024-01-05 22:01:35.960254: Pseudo dice [0.8624, 0.9591, 0.9409]\n",
      "2024-01-05 22:01:35.965248: Epoch time: 125.75 s\n",
      "2024-01-05 22:01:37.150471: \n",
      "2024-01-05 22:01:37.161971: Epoch 527\n",
      "2024-01-05 22:01:37.166951: Current learning rate: 0.0051\n",
      "2024-01-05 22:03:42.850409: train_loss -0.919\n",
      "2024-01-05 22:03:42.859412: val_loss -0.8346\n",
      "2024-01-05 22:03:42.864414: Pseudo dice [0.8639, 0.9586, 0.941]\n",
      "2024-01-05 22:03:42.869413: Epoch time: 125.7 s\n",
      "2024-01-05 22:03:44.019544: \n",
      "2024-01-05 22:03:44.027544: Epoch 528\n",
      "2024-01-05 22:03:44.031543: Current learning rate: 0.00509\n",
      "2024-01-05 22:05:49.915016: train_loss -0.9163\n",
      "2024-01-05 22:05:49.922009: val_loss -0.8382\n",
      "2024-01-05 22:05:49.928012: Pseudo dice [0.8548, 0.9604, 0.9432]\n",
      "2024-01-05 22:05:49.935008: Epoch time: 125.9 s\n",
      "2024-01-05 22:05:51.182690: \n",
      "2024-01-05 22:05:51.190617: Epoch 529\n",
      "2024-01-05 22:05:51.200619: Current learning rate: 0.00508\n",
      "2024-01-05 22:07:56.704499: train_loss -0.9166\n",
      "2024-01-05 22:07:56.711502: val_loss -0.8398\n",
      "2024-01-05 22:07:56.717502: Pseudo dice [0.8653, 0.9598, 0.9425]\n",
      "2024-01-05 22:07:56.722502: Epoch time: 125.52 s\n",
      "2024-01-05 22:07:57.912682: \n",
      "2024-01-05 22:07:57.918566: Epoch 530\n",
      "2024-01-05 22:07:57.925648: Current learning rate: 0.00507\n",
      "2024-01-05 22:10:03.834235: train_loss -0.9149\n",
      "2024-01-05 22:10:03.843235: val_loss -0.8386\n",
      "2024-01-05 22:10:03.849237: Pseudo dice [0.8619, 0.9598, 0.9418]\n",
      "2024-01-05 22:10:03.856236: Epoch time: 125.92 s\n",
      "2024-01-05 22:10:04.979948: \n",
      "2024-01-05 22:10:04.987596: Epoch 531\n",
      "2024-01-05 22:10:04.994605: Current learning rate: 0.00506\n",
      "2024-01-05 22:12:10.409283: train_loss -0.9181\n",
      "2024-01-05 22:12:10.415284: val_loss -0.8362\n",
      "2024-01-05 22:12:10.420284: Pseudo dice [0.8658, 0.9599, 0.9418]\n",
      "2024-01-05 22:12:10.424284: Epoch time: 125.43 s\n",
      "2024-01-05 22:12:11.680900: \n",
      "2024-01-05 22:12:11.691644: Epoch 532\n",
      "2024-01-05 22:12:11.696727: Current learning rate: 0.00505\n",
      "2024-01-05 22:14:17.313401: train_loss -0.9196\n",
      "2024-01-05 22:14:17.320400: val_loss -0.8331\n",
      "2024-01-05 22:14:17.328400: Pseudo dice [0.8583, 0.9601, 0.9424]\n",
      "2024-01-05 22:14:17.333400: Epoch time: 125.63 s\n",
      "2024-01-05 22:14:18.562485: \n",
      "2024-01-05 22:14:18.570843: Epoch 533\n",
      "2024-01-05 22:14:18.577878: Current learning rate: 0.00504\n",
      "2024-01-05 22:16:24.236302: train_loss -0.9175\n",
      "2024-01-05 22:16:24.242299: val_loss -0.8374\n",
      "2024-01-05 22:16:24.246299: Pseudo dice [0.8625, 0.959, 0.9409]\n",
      "2024-01-05 22:16:24.251815: Epoch time: 125.67 s\n",
      "2024-01-05 22:16:25.447454: \n",
      "2024-01-05 22:16:25.456199: Epoch 534\n",
      "2024-01-05 22:16:25.461132: Current learning rate: 0.00503\n",
      "2024-01-05 22:18:31.208391: train_loss -0.9179\n",
      "2024-01-05 22:18:31.215396: val_loss -0.8359\n",
      "2024-01-05 22:18:31.221396: Pseudo dice [0.8634, 0.9593, 0.9419]\n",
      "2024-01-05 22:18:31.229394: Epoch time: 125.76 s\n",
      "2024-01-05 22:18:32.516078: \n",
      "2024-01-05 22:18:32.522307: Epoch 535\n",
      "2024-01-05 22:18:32.526296: Current learning rate: 0.00502\n",
      "2024-01-05 22:20:38.594187: train_loss -0.9143\n",
      "2024-01-05 22:20:38.606288: val_loss -0.84\n",
      "2024-01-05 22:20:38.615280: Pseudo dice [0.8644, 0.9603, 0.9426]\n",
      "2024-01-05 22:20:38.623280: Epoch time: 126.08 s\n",
      "2024-01-05 22:20:40.184834: \n",
      "2024-01-05 22:20:40.190873: Epoch 536\n",
      "2024-01-05 22:20:40.198798: Current learning rate: 0.00501\n",
      "2024-01-05 22:22:46.222319: train_loss -0.9176\n",
      "2024-01-05 22:22:46.229333: val_loss -0.83\n",
      "2024-01-05 22:22:46.234328: Pseudo dice [0.8611, 0.9595, 0.9416]\n",
      "2024-01-05 22:22:46.246993: Epoch time: 126.04 s\n",
      "2024-01-05 22:22:47.370465: \n",
      "2024-01-05 22:22:47.376338: Epoch 537\n",
      "2024-01-05 22:22:47.380646: Current learning rate: 0.005\n",
      "2024-01-05 22:24:53.162684: train_loss -0.9151\n",
      "2024-01-05 22:24:53.170691: val_loss -0.8373\n",
      "2024-01-05 22:24:53.179690: Pseudo dice [0.862, 0.96, 0.9425]\n",
      "2024-01-05 22:24:53.184690: Epoch time: 125.79 s\n",
      "2024-01-05 22:24:54.269570: \n",
      "2024-01-05 22:24:54.276571: Epoch 538\n",
      "2024-01-05 22:24:54.280575: Current learning rate: 0.00499\n",
      "2024-01-05 22:27:00.391898: train_loss -0.9178\n",
      "2024-01-05 22:27:00.399889: val_loss -0.8341\n",
      "2024-01-05 22:27:00.405902: Pseudo dice [0.8653, 0.9597, 0.942]\n",
      "2024-01-05 22:27:00.410905: Epoch time: 126.12 s\n",
      "2024-01-05 22:27:01.873117: \n",
      "2024-01-05 22:27:01.880133: Epoch 539\n",
      "2024-01-05 22:27:01.886126: Current learning rate: 0.00498\n",
      "2024-01-05 22:29:07.533750: train_loss -0.918\n",
      "2024-01-05 22:29:07.543759: val_loss -0.8309\n",
      "2024-01-05 22:29:07.550759: Pseudo dice [0.8635, 0.9596, 0.9421]\n",
      "2024-01-05 22:29:07.558752: Epoch time: 125.66 s\n",
      "2024-01-05 22:29:08.809988: \n",
      "2024-01-05 22:29:08.816990: Epoch 540\n",
      "2024-01-05 22:29:08.822990: Current learning rate: 0.00497\n",
      "2024-01-05 22:31:14.437067: train_loss -0.9197\n",
      "2024-01-05 22:31:14.444066: val_loss -0.8367\n",
      "2024-01-05 22:31:14.451112: Pseudo dice [0.8609, 0.9598, 0.9426]\n",
      "2024-01-05 22:31:14.455112: Epoch time: 125.63 s\n",
      "2024-01-05 22:31:15.522290: \n",
      "2024-01-05 22:31:15.529302: Epoch 541\n",
      "2024-01-05 22:31:15.534296: Current learning rate: 0.00496\n",
      "2024-01-05 22:33:21.376047: train_loss -0.9144\n",
      "2024-01-05 22:33:21.383054: val_loss -0.8361\n",
      "2024-01-05 22:33:21.389060: Pseudo dice [0.8617, 0.9599, 0.9419]\n",
      "2024-01-05 22:33:21.395059: Epoch time: 125.85 s\n",
      "2024-01-05 22:33:22.466327: \n",
      "2024-01-05 22:33:22.472183: Epoch 542\n",
      "2024-01-05 22:33:22.476243: Current learning rate: 0.00495\n",
      "2024-01-05 22:35:28.235191: train_loss -0.9173\n",
      "2024-01-05 22:35:28.243195: val_loss -0.836\n",
      "2024-01-05 22:35:28.249198: Pseudo dice [0.8659, 0.9604, 0.9427]\n",
      "2024-01-05 22:35:28.263144: Epoch time: 125.77 s\n",
      "2024-01-05 22:35:29.444032: \n",
      "2024-01-05 22:35:29.452031: Epoch 543\n",
      "2024-01-05 22:35:29.459119: Current learning rate: 0.00494\n",
      "2024-01-05 22:37:35.304238: train_loss -0.9175\n",
      "2024-01-05 22:37:35.310247: val_loss -0.8303\n",
      "2024-01-05 22:37:35.315246: Pseudo dice [0.8612, 0.9592, 0.9409]\n",
      "2024-01-05 22:37:35.320243: Epoch time: 125.86 s\n",
      "2024-01-05 22:37:36.418981: \n",
      "2024-01-05 22:37:36.427431: Epoch 544\n",
      "2024-01-05 22:37:36.431516: Current learning rate: 0.00493\n",
      "2024-01-05 22:39:42.403478: train_loss -0.9168\n",
      "2024-01-05 22:39:42.410479: val_loss -0.8332\n",
      "2024-01-05 22:39:42.421498: Pseudo dice [0.8619, 0.959, 0.9406]\n",
      "2024-01-05 22:39:42.431029: Epoch time: 125.99 s\n",
      "2024-01-05 22:39:43.581500: \n",
      "2024-01-05 22:39:43.589981: Epoch 545\n",
      "2024-01-05 22:39:43.594047: Current learning rate: 0.00492\n",
      "2024-01-05 22:41:49.502115: train_loss -0.9164\n",
      "2024-01-05 22:41:49.511107: val_loss -0.8427\n",
      "2024-01-05 22:41:49.516614: Pseudo dice [0.8651, 0.9601, 0.9432]\n",
      "2024-01-05 22:41:49.521614: Epoch time: 125.92 s\n",
      "2024-01-05 22:41:50.639964: \n",
      "2024-01-05 22:41:50.644963: Epoch 546\n",
      "2024-01-05 22:41:50.649972: Current learning rate: 0.00491\n",
      "2024-01-05 22:43:56.658692: train_loss -0.9142\n",
      "2024-01-05 22:43:56.665683: val_loss -0.8407\n",
      "2024-01-05 22:43:56.670766: Pseudo dice [0.863, 0.9596, 0.9421]\n",
      "2024-01-05 22:43:56.679771: Epoch time: 126.02 s\n",
      "2024-01-05 22:43:57.801852: \n",
      "2024-01-05 22:43:57.807511: Epoch 547\n",
      "2024-01-05 22:43:57.817586: Current learning rate: 0.0049\n",
      "2024-01-05 22:46:03.647673: train_loss -0.9175\n",
      "2024-01-05 22:46:03.654672: val_loss -0.8418\n",
      "2024-01-05 22:46:03.661672: Pseudo dice [0.8645, 0.9597, 0.9424]\n",
      "2024-01-05 22:46:03.667672: Epoch time: 125.85 s\n",
      "2024-01-05 22:46:04.991797: \n",
      "2024-01-05 22:46:04.996801: Epoch 548\n",
      "2024-01-05 22:46:05.002763: Current learning rate: 0.00489\n",
      "2024-01-05 22:48:11.021941: train_loss -0.9152\n",
      "2024-01-05 22:48:11.027943: val_loss -0.8382\n",
      "2024-01-05 22:48:11.034943: Pseudo dice [0.8632, 0.9584, 0.9402]\n",
      "2024-01-05 22:48:11.043942: Epoch time: 126.03 s\n",
      "2024-01-05 22:48:12.323905: \n",
      "2024-01-05 22:48:12.334908: Epoch 549\n",
      "2024-01-05 22:48:12.342918: Current learning rate: 0.00488\n",
      "2024-01-05 22:50:18.310474: train_loss -0.9156\n",
      "2024-01-05 22:50:18.317482: val_loss -0.8347\n",
      "2024-01-05 22:50:18.322482: Pseudo dice [0.8634, 0.9595, 0.9419]\n",
      "2024-01-05 22:50:18.326478: Epoch time: 125.99 s\n",
      "2024-01-05 22:50:19.893556: \n",
      "2024-01-05 22:50:19.902628: Epoch 550\n",
      "2024-01-05 22:50:19.907578: Current learning rate: 0.00487\n",
      "2024-01-05 22:52:26.000894: train_loss -0.9166\n",
      "2024-01-05 22:52:26.010895: val_loss -0.8406\n",
      "2024-01-05 22:52:26.016966: Pseudo dice [0.8615, 0.9591, 0.9414]\n",
      "2024-01-05 22:52:26.023893: Epoch time: 126.11 s\n",
      "2024-01-05 22:52:27.084082: \n",
      "2024-01-05 22:52:27.095659: Epoch 551\n",
      "2024-01-05 22:52:27.105733: Current learning rate: 0.00486\n",
      "2024-01-05 22:54:32.746202: train_loss -0.9179\n",
      "2024-01-05 22:54:32.754203: val_loss -0.8407\n",
      "2024-01-05 22:54:32.759201: Pseudo dice [0.8638, 0.961, 0.9437]\n",
      "2024-01-05 22:54:32.764201: Epoch time: 125.67 s\n",
      "2024-01-05 22:54:33.826102: \n",
      "2024-01-05 22:54:33.835141: Epoch 552\n",
      "2024-01-05 22:54:33.839624: Current learning rate: 0.00485\n",
      "2024-01-05 22:56:39.583092: train_loss -0.918\n",
      "2024-01-05 22:56:39.592094: val_loss -0.8397\n",
      "2024-01-05 22:56:39.601093: Pseudo dice [0.8594, 0.9603, 0.9429]\n",
      "2024-01-05 22:56:39.606095: Epoch time: 125.76 s\n",
      "2024-01-05 22:56:40.726518: \n",
      "2024-01-05 22:56:40.732515: Epoch 553\n",
      "2024-01-05 22:56:40.739606: Current learning rate: 0.00484\n",
      "2024-01-05 22:58:46.666773: train_loss -0.9166\n",
      "2024-01-05 22:58:46.673774: val_loss -0.8427\n",
      "2024-01-05 22:58:46.681773: Pseudo dice [0.8612, 0.9592, 0.9413]\n",
      "2024-01-05 22:58:46.686773: Epoch time: 125.94 s\n",
      "2024-01-05 22:58:47.913174: \n",
      "2024-01-05 22:58:47.923128: Epoch 554\n",
      "2024-01-05 22:58:47.928126: Current learning rate: 0.00484\n",
      "2024-01-05 23:00:54.006862: train_loss -0.9162\n",
      "2024-01-05 23:00:54.013862: val_loss -0.837\n",
      "2024-01-05 23:00:54.019882: Pseudo dice [0.8633, 0.9578, 0.9406]\n",
      "2024-01-05 23:00:54.024881: Epoch time: 126.09 s\n",
      "2024-01-05 23:00:55.422762: \n",
      "2024-01-05 23:00:55.428767: Epoch 555\n",
      "2024-01-05 23:00:55.436316: Current learning rate: 0.00483\n",
      "2024-01-05 23:03:01.180894: train_loss -0.9191\n",
      "2024-01-05 23:03:01.189897: val_loss -0.8377\n",
      "2024-01-05 23:03:01.195894: Pseudo dice [0.8621, 0.9583, 0.9396]\n",
      "2024-01-05 23:03:01.201896: Epoch time: 125.76 s\n",
      "2024-01-05 23:03:02.427282: \n",
      "2024-01-05 23:03:02.432858: Epoch 556\n",
      "2024-01-05 23:03:02.440804: Current learning rate: 0.00482\n",
      "2024-01-05 23:05:08.157473: train_loss -0.9164\n",
      "2024-01-05 23:05:08.168466: val_loss -0.832\n",
      "2024-01-05 23:05:08.174466: Pseudo dice [0.8597, 0.9601, 0.9423]\n",
      "2024-01-05 23:05:08.179466: Epoch time: 125.73 s\n",
      "2024-01-05 23:05:09.441000: \n",
      "2024-01-05 23:05:09.447037: Epoch 557\n",
      "2024-01-05 23:05:09.451014: Current learning rate: 0.00481\n",
      "2024-01-05 23:07:15.003328: train_loss -0.9184\n",
      "2024-01-05 23:07:15.010331: val_loss -0.8382\n",
      "2024-01-05 23:07:15.017330: Pseudo dice [0.8594, 0.9603, 0.943]\n",
      "2024-01-05 23:07:15.022329: Epoch time: 125.56 s\n",
      "2024-01-05 23:07:16.166586: \n",
      "2024-01-05 23:07:16.172170: Epoch 558\n",
      "2024-01-05 23:07:16.176836: Current learning rate: 0.0048\n",
      "2024-01-05 23:09:21.903466: train_loss -0.9177\n",
      "2024-01-05 23:09:21.912388: val_loss -0.8372\n",
      "2024-01-05 23:09:21.921388: Pseudo dice [0.8605, 0.9596, 0.9421]\n",
      "2024-01-05 23:09:21.928389: Epoch time: 125.74 s\n",
      "2024-01-05 23:09:23.040568: \n",
      "2024-01-05 23:09:23.052620: Epoch 559\n",
      "2024-01-05 23:09:23.063191: Current learning rate: 0.00479\n",
      "2024-01-05 23:11:29.060667: train_loss -0.9146\n",
      "2024-01-05 23:11:29.069658: val_loss -0.836\n",
      "2024-01-05 23:11:29.074668: Pseudo dice [0.8631, 0.9596, 0.9423]\n",
      "2024-01-05 23:11:29.080658: Epoch time: 126.02 s\n",
      "2024-01-05 23:11:30.247624: \n",
      "2024-01-05 23:11:30.253285: Epoch 560\n",
      "2024-01-05 23:11:30.257352: Current learning rate: 0.00478\n",
      "2024-01-05 23:13:36.342800: train_loss -0.9168\n",
      "2024-01-05 23:13:36.350800: val_loss -0.8356\n",
      "2024-01-05 23:13:36.359800: Pseudo dice [0.8627, 0.9593, 0.9413]\n",
      "2024-01-05 23:13:36.369804: Epoch time: 126.1 s\n",
      "2024-01-05 23:13:37.537423: \n",
      "2024-01-05 23:13:37.549073: Epoch 561\n",
      "2024-01-05 23:13:37.553765: Current learning rate: 0.00477\n",
      "2024-01-05 23:15:43.511970: train_loss -0.9149\n",
      "2024-01-05 23:15:43.517970: val_loss -0.831\n",
      "2024-01-05 23:15:43.522969: Pseudo dice [0.8577, 0.9598, 0.9416]\n",
      "2024-01-05 23:15:43.528970: Epoch time: 125.98 s\n",
      "2024-01-05 23:15:44.803927: \n",
      "2024-01-05 23:15:44.812006: Epoch 562\n",
      "2024-01-05 23:15:44.817957: Current learning rate: 0.00476\n",
      "2024-01-05 23:17:50.598010: train_loss -0.9194\n",
      "2024-01-05 23:17:50.605010: val_loss -0.8338\n",
      "2024-01-05 23:17:50.613021: Pseudo dice [0.8619, 0.959, 0.9405]\n",
      "2024-01-05 23:17:50.619016: Epoch time: 125.8 s\n",
      "2024-01-05 23:17:51.888237: \n",
      "2024-01-05 23:17:51.896237: Epoch 563\n",
      "2024-01-05 23:17:51.900299: Current learning rate: 0.00475\n",
      "2024-01-05 23:19:57.492501: train_loss -0.9204\n",
      "2024-01-05 23:19:57.501503: val_loss -0.8286\n",
      "2024-01-05 23:19:57.507499: Pseudo dice [0.8584, 0.9589, 0.9414]\n",
      "2024-01-05 23:19:57.514498: Epoch time: 125.61 s\n",
      "2024-01-05 23:19:58.711989: \n",
      "2024-01-05 23:19:58.718816: Epoch 564\n",
      "2024-01-05 23:19:58.727035: Current learning rate: 0.00474\n",
      "2024-01-05 23:22:04.406295: train_loss -0.9187\n",
      "2024-01-05 23:22:04.414569: val_loss -0.827\n",
      "2024-01-05 23:22:04.422575: Pseudo dice [0.8602, 0.9585, 0.94]\n",
      "2024-01-05 23:22:04.429574: Epoch time: 125.7 s\n",
      "2024-01-05 23:22:05.720562: \n",
      "2024-01-05 23:22:05.727712: Epoch 565\n",
      "2024-01-05 23:22:05.737733: Current learning rate: 0.00473\n",
      "2024-01-05 23:24:11.836653: train_loss -0.9188\n",
      "2024-01-05 23:24:11.842651: val_loss -0.835\n",
      "2024-01-05 23:24:11.848654: Pseudo dice [0.8621, 0.9585, 0.9405]\n",
      "2024-01-05 23:24:11.855657: Epoch time: 126.12 s\n",
      "2024-01-05 23:24:12.958097: \n",
      "2024-01-05 23:24:12.966097: Epoch 566\n",
      "2024-01-05 23:24:12.970097: Current learning rate: 0.00472\n",
      "2024-01-05 23:26:18.751824: train_loss -0.9182\n",
      "2024-01-05 23:26:18.761340: val_loss -0.8305\n",
      "2024-01-05 23:26:18.766339: Pseudo dice [0.8623, 0.9597, 0.9414]\n",
      "2024-01-05 23:26:18.773351: Epoch time: 125.79 s\n",
      "2024-01-05 23:26:20.024670: \n",
      "2024-01-05 23:26:20.030924: Epoch 567\n",
      "2024-01-05 23:26:20.038004: Current learning rate: 0.00471\n",
      "2024-01-05 23:28:25.890917: train_loss -0.9171\n",
      "2024-01-05 23:28:25.902472: val_loss -0.8318\n",
      "2024-01-05 23:28:25.913477: Pseudo dice [0.8601, 0.9586, 0.9406]\n",
      "2024-01-05 23:28:25.924474: Epoch time: 125.87 s\n",
      "2024-01-05 23:28:27.084166: \n",
      "2024-01-05 23:28:27.095441: Epoch 568\n",
      "2024-01-05 23:28:27.107448: Current learning rate: 0.0047\n",
      "2024-01-05 23:30:32.864538: train_loss -0.9194\n",
      "2024-01-05 23:30:32.876542: val_loss -0.8375\n",
      "2024-01-05 23:30:32.882545: Pseudo dice [0.8616, 0.9597, 0.9412]\n",
      "2024-01-05 23:30:32.888540: Epoch time: 125.78 s\n",
      "2024-01-05 23:30:34.066964: \n",
      "2024-01-05 23:30:34.072965: Epoch 569\n",
      "2024-01-05 23:30:34.077964: Current learning rate: 0.00469\n",
      "2024-01-05 23:32:40.346788: train_loss -0.9156\n",
      "2024-01-05 23:32:40.354787: val_loss -0.8376\n",
      "2024-01-05 23:32:40.363786: Pseudo dice [0.8656, 0.9604, 0.943]\n",
      "2024-01-05 23:32:40.368786: Epoch time: 126.28 s\n",
      "2024-01-05 23:32:41.634485: \n",
      "2024-01-05 23:32:41.640501: Epoch 570\n",
      "2024-01-05 23:32:41.645492: Current learning rate: 0.00468\n",
      "2024-01-05 23:34:51.201763: train_loss -0.9184\n",
      "2024-01-05 23:34:51.240571: val_loss -0.8365\n",
      "2024-01-05 23:34:51.258211: Pseudo dice [0.8624, 0.9595, 0.9417]\n",
      "2024-01-05 23:34:51.273180: Epoch time: 129.57 s\n",
      "2024-01-05 23:34:53.549400: \n",
      "2024-01-05 23:34:53.557400: Epoch 571\n",
      "2024-01-05 23:34:53.563929: Current learning rate: 0.00467\n",
      "2024-01-05 23:37:04.146432: train_loss -0.9151\n",
      "2024-01-05 23:37:04.159484: val_loss -0.8406\n",
      "2024-01-05 23:37:04.171478: Pseudo dice [0.8625, 0.9603, 0.943]\n",
      "2024-01-05 23:37:04.181995: Epoch time: 130.6 s\n",
      "2024-01-05 23:37:05.908674: \n",
      "2024-01-05 23:37:05.915686: Epoch 572\n",
      "2024-01-05 23:37:05.922204: Current learning rate: 0.00466\n",
      "2024-01-05 23:39:15.633543: train_loss -0.9182\n",
      "2024-01-05 23:39:15.645566: val_loss -0.829\n",
      "2024-01-05 23:39:15.659740: Pseudo dice [0.8635, 0.9588, 0.9404]\n",
      "2024-01-05 23:39:15.672261: Epoch time: 129.73 s\n",
      "2024-01-05 23:39:17.164158: \n",
      "2024-01-05 23:39:17.171690: Epoch 573\n",
      "2024-01-05 23:39:17.178194: Current learning rate: 0.00465\n",
      "2024-01-05 23:41:24.688990: train_loss -0.9164\n",
      "2024-01-05 23:41:24.695991: val_loss -0.8323\n",
      "2024-01-05 23:41:24.705989: Pseudo dice [0.8595, 0.9586, 0.9413]\n",
      "2024-01-05 23:41:24.711995: Epoch time: 127.53 s\n",
      "2024-01-05 23:41:25.967698: \n",
      "2024-01-05 23:41:25.973702: Epoch 574\n",
      "2024-01-05 23:41:25.981691: Current learning rate: 0.00464\n",
      "2024-01-05 23:43:33.268221: train_loss -0.9177\n",
      "2024-01-05 23:43:33.278743: val_loss -0.8354\n",
      "2024-01-05 23:43:33.287978: Pseudo dice [0.8622, 0.9591, 0.941]\n",
      "2024-01-05 23:43:33.294978: Epoch time: 127.3 s\n",
      "2024-01-05 23:43:34.571859: \n",
      "2024-01-05 23:43:34.579854: Epoch 575\n",
      "2024-01-05 23:43:34.586858: Current learning rate: 0.00463\n",
      "2024-01-05 23:45:42.127761: train_loss -0.9123\n",
      "2024-01-05 23:45:42.137760: val_loss -0.8285\n",
      "2024-01-05 23:45:42.142761: Pseudo dice [0.8528, 0.9539, 0.9337]\n",
      "2024-01-05 23:45:42.148775: Epoch time: 127.56 s\n",
      "2024-01-05 23:45:43.399515: \n",
      "2024-01-05 23:45:43.406593: Epoch 576\n",
      "2024-01-05 23:45:43.413581: Current learning rate: 0.00462\n",
      "2024-01-05 23:47:49.470858: train_loss -0.8951\n",
      "2024-01-05 23:47:49.478856: val_loss -0.8416\n",
      "2024-01-05 23:47:49.487881: Pseudo dice [0.8679, 0.9588, 0.9401]\n",
      "2024-01-05 23:47:49.494884: Epoch time: 126.07 s\n",
      "2024-01-05 23:47:50.653948: \n",
      "2024-01-05 23:47:50.660233: Epoch 577\n",
      "2024-01-05 23:47:50.665174: Current learning rate: 0.00461\n",
      "2024-01-05 23:49:56.380673: train_loss -0.8993\n",
      "2024-01-05 23:49:56.388674: val_loss -0.828\n",
      "2024-01-05 23:49:56.394683: Pseudo dice [0.8609, 0.9562, 0.9375]\n",
      "2024-01-05 23:49:56.401684: Epoch time: 125.73 s\n",
      "2024-01-05 23:49:57.656409: \n",
      "2024-01-05 23:49:57.662425: Epoch 578\n",
      "2024-01-05 23:49:57.666424: Current learning rate: 0.0046\n",
      "2024-01-05 23:52:03.572611: train_loss -0.9041\n",
      "2024-01-05 23:52:03.579609: val_loss -0.8326\n",
      "2024-01-05 23:52:03.585609: Pseudo dice [0.8569, 0.9589, 0.9401]\n",
      "2024-01-05 23:52:03.600324: Epoch time: 125.92 s\n",
      "2024-01-05 23:52:04.972645: \n",
      "2024-01-05 23:52:04.978309: Epoch 579\n",
      "2024-01-05 23:52:04.984247: Current learning rate: 0.00459\n",
      "2024-01-05 23:54:12.281649: train_loss -0.9133\n",
      "2024-01-05 23:54:12.292864: val_loss -0.8309\n",
      "2024-01-05 23:54:12.304352: Pseudo dice [0.8629, 0.9587, 0.9405]\n",
      "2024-01-05 23:54:12.314590: Epoch time: 127.31 s\n",
      "2024-01-05 23:54:13.732824: \n",
      "2024-01-05 23:54:13.740568: Epoch 580\n",
      "2024-01-05 23:54:13.748644: Current learning rate: 0.00458\n",
      "2024-01-05 23:56:19.554633: train_loss -0.9156\n",
      "2024-01-05 23:56:19.566643: val_loss -0.8308\n",
      "2024-01-05 23:56:19.580153: Pseudo dice [0.8622, 0.9589, 0.9414]\n",
      "2024-01-05 23:56:19.591153: Epoch time: 125.82 s\n",
      "2024-01-05 23:56:20.657331: \n",
      "2024-01-05 23:56:20.663341: Epoch 581\n",
      "2024-01-05 23:56:20.671424: Current learning rate: 0.00457\n",
      "2024-01-05 23:58:26.087116: train_loss -0.9171\n",
      "2024-01-05 23:58:26.095109: val_loss -0.8348\n",
      "2024-01-05 23:58:26.103105: Pseudo dice [0.86, 0.9597, 0.9416]\n",
      "2024-01-05 23:58:26.109104: Epoch time: 125.43 s\n",
      "2024-01-05 23:58:27.074721: \n",
      "2024-01-05 23:58:27.082795: Epoch 582\n",
      "2024-01-05 23:58:27.090490: Current learning rate: 0.00456\n",
      "2024-01-06 00:00:32.491745: train_loss -0.9177\n",
      "2024-01-06 00:00:32.502745: val_loss -0.837\n",
      "2024-01-06 00:00:32.508748: Pseudo dice [0.8646, 0.9591, 0.941]\n",
      "2024-01-06 00:00:32.515744: Epoch time: 125.42 s\n",
      "2024-01-06 00:00:33.489714: \n",
      "2024-01-06 00:00:33.495722: Epoch 583\n",
      "2024-01-06 00:00:33.500798: Current learning rate: 0.00455\n",
      "2024-01-06 00:02:38.447332: train_loss -0.9176\n",
      "2024-01-06 00:02:38.458328: val_loss -0.8325\n",
      "2024-01-06 00:02:38.465328: Pseudo dice [0.8548, 0.9583, 0.9394]\n",
      "2024-01-06 00:02:38.471328: Epoch time: 124.96 s\n",
      "2024-01-06 00:02:39.427105: \n",
      "2024-01-06 00:02:39.434773: Epoch 584\n",
      "2024-01-06 00:02:39.439514: Current learning rate: 0.00454\n",
      "2024-01-06 00:04:44.941535: train_loss -0.9142\n",
      "2024-01-06 00:04:44.950538: val_loss -0.8342\n",
      "2024-01-06 00:04:44.956533: Pseudo dice [0.8659, 0.9584, 0.9397]\n",
      "2024-01-06 00:04:44.962532: Epoch time: 125.52 s\n",
      "2024-01-06 00:04:45.923176: \n",
      "2024-01-06 00:04:45.930759: Epoch 585\n",
      "2024-01-06 00:04:45.936130: Current learning rate: 0.00453\n",
      "2024-01-06 00:06:51.194543: train_loss -0.9131\n",
      "2024-01-06 00:06:51.202543: val_loss -0.8274\n",
      "2024-01-06 00:06:51.208543: Pseudo dice [0.8603, 0.9592, 0.941]\n",
      "2024-01-06 00:06:51.213544: Epoch time: 125.27 s\n",
      "2024-01-06 00:06:52.352261: \n",
      "2024-01-06 00:06:52.358058: Epoch 586\n",
      "2024-01-06 00:06:52.363146: Current learning rate: 0.00452\n",
      "2024-01-06 00:08:57.159585: train_loss -0.918\n",
      "2024-01-06 00:08:57.168582: val_loss -0.8275\n",
      "2024-01-06 00:08:57.175582: Pseudo dice [0.8597, 0.9596, 0.9421]\n",
      "2024-01-06 00:08:57.182582: Epoch time: 124.81 s\n",
      "2024-01-06 00:08:58.134259: \n",
      "2024-01-06 00:08:58.139478: Epoch 587\n",
      "2024-01-06 00:08:58.143555: Current learning rate: 0.00451\n",
      "2024-01-06 00:11:03.253471: train_loss -0.916\n",
      "2024-01-06 00:11:03.264472: val_loss -0.8408\n",
      "2024-01-06 00:11:03.271473: Pseudo dice [0.8667, 0.9602, 0.9421]\n",
      "2024-01-06 00:11:03.276471: Epoch time: 125.12 s\n",
      "2024-01-06 00:11:04.254278: \n",
      "2024-01-06 00:11:04.260410: Epoch 588\n",
      "2024-01-06 00:11:04.264477: Current learning rate: 0.0045\n",
      "2024-01-06 00:13:09.267536: train_loss -0.9181\n",
      "2024-01-06 00:13:09.278536: val_loss -0.8342\n",
      "2024-01-06 00:13:09.285534: Pseudo dice [0.8642, 0.9603, 0.9429]\n",
      "2024-01-06 00:13:09.293535: Epoch time: 125.01 s\n",
      "2024-01-06 00:13:10.259229: \n",
      "2024-01-06 00:13:10.267817: Epoch 589\n",
      "2024-01-06 00:13:10.272820: Current learning rate: 0.00449\n",
      "2024-01-06 00:15:15.149315: train_loss -0.9168\n",
      "2024-01-06 00:15:15.158315: val_loss -0.8296\n",
      "2024-01-06 00:15:15.167315: Pseudo dice [0.8638, 0.9593, 0.941]\n",
      "2024-01-06 00:15:15.173315: Epoch time: 124.89 s\n",
      "2024-01-06 00:15:16.142292: \n",
      "2024-01-06 00:15:16.151291: Epoch 590\n",
      "2024-01-06 00:15:16.157297: Current learning rate: 0.00448\n",
      "2024-01-06 00:17:21.357214: train_loss -0.9158\n",
      "2024-01-06 00:17:21.367208: val_loss -0.8373\n",
      "2024-01-06 00:17:21.375207: Pseudo dice [0.8627, 0.9592, 0.9411]\n",
      "2024-01-06 00:17:21.382206: Epoch time: 125.22 s\n",
      "2024-01-06 00:17:22.338892: \n",
      "2024-01-06 00:17:22.349452: Epoch 591\n",
      "2024-01-06 00:17:22.354366: Current learning rate: 0.00447\n",
      "2024-01-06 00:19:27.635963: train_loss -0.9126\n",
      "2024-01-06 00:19:27.647967: val_loss -0.8374\n",
      "2024-01-06 00:19:27.654966: Pseudo dice [0.8627, 0.9594, 0.9413]\n",
      "2024-01-06 00:19:27.662471: Epoch time: 125.3 s\n",
      "2024-01-06 00:19:28.619392: \n",
      "2024-01-06 00:19:28.630926: Epoch 592\n",
      "2024-01-06 00:19:28.635920: Current learning rate: 0.00446\n",
      "2024-01-06 00:21:33.771066: train_loss -0.9149\n",
      "2024-01-06 00:21:33.779066: val_loss -0.8348\n",
      "2024-01-06 00:21:33.787069: Pseudo dice [0.8632, 0.9593, 0.9411]\n",
      "2024-01-06 00:21:33.794066: Epoch time: 125.15 s\n",
      "2024-01-06 00:21:34.753917: \n",
      "2024-01-06 00:21:34.762556: Epoch 593\n",
      "2024-01-06 00:21:34.769637: Current learning rate: 0.00445\n",
      "2024-01-06 00:23:39.925602: train_loss -0.9152\n",
      "2024-01-06 00:23:39.932604: val_loss -0.8292\n",
      "2024-01-06 00:23:39.940603: Pseudo dice [0.8612, 0.96, 0.9418]\n",
      "2024-01-06 00:23:39.946607: Epoch time: 125.17 s\n",
      "2024-01-06 00:23:41.077446: \n",
      "2024-01-06 00:23:41.089885: Epoch 594\n",
      "2024-01-06 00:23:41.094888: Current learning rate: 0.00444\n",
      "2024-01-06 00:25:46.155412: train_loss -0.9184\n",
      "2024-01-06 00:25:46.163414: val_loss -0.8292\n",
      "2024-01-06 00:25:46.172409: Pseudo dice [0.864, 0.9581, 0.9393]\n",
      "2024-01-06 00:25:46.178406: Epoch time: 125.08 s\n",
      "2024-01-06 00:25:47.149222: \n",
      "2024-01-06 00:25:47.158200: Epoch 595\n",
      "2024-01-06 00:25:47.166134: Current learning rate: 0.00443\n",
      "2024-01-06 00:27:52.194194: train_loss -0.915\n",
      "2024-01-06 00:27:52.202197: val_loss -0.8367\n",
      "2024-01-06 00:27:52.210205: Pseudo dice [0.8623, 0.9594, 0.9413]\n",
      "2024-01-06 00:27:52.215727: Epoch time: 125.05 s\n",
      "2024-01-06 00:27:53.177532: \n",
      "2024-01-06 00:27:53.183040: Epoch 596\n",
      "2024-01-06 00:27:53.187556: Current learning rate: 0.00442\n",
      "2024-01-06 00:29:58.127265: train_loss -0.9164\n",
      "2024-01-06 00:29:58.134266: val_loss -0.8317\n",
      "2024-01-06 00:29:58.140274: Pseudo dice [0.8608, 0.9588, 0.9407]\n",
      "2024-01-06 00:29:58.148274: Epoch time: 124.95 s\n",
      "2024-01-06 00:29:59.104086: \n",
      "2024-01-06 00:29:59.110202: Epoch 597\n",
      "2024-01-06 00:29:59.114740: Current learning rate: 0.00441\n",
      "2024-01-06 00:32:04.219209: train_loss -0.9174\n",
      "2024-01-06 00:32:04.228211: val_loss -0.8389\n",
      "2024-01-06 00:32:04.237207: Pseudo dice [0.869, 0.959, 0.9414]\n",
      "2024-01-06 00:32:04.245208: Epoch time: 125.12 s\n",
      "2024-01-06 00:32:05.211529: \n",
      "2024-01-06 00:32:05.217611: Epoch 598\n",
      "2024-01-06 00:32:05.222600: Current learning rate: 0.0044\n",
      "2024-01-06 00:34:10.396163: train_loss -0.9176\n",
      "2024-01-06 00:34:10.405161: val_loss -0.8389\n",
      "2024-01-06 00:34:10.411165: Pseudo dice [0.8636, 0.9595, 0.9408]\n",
      "2024-01-06 00:34:10.416165: Epoch time: 125.19 s\n",
      "2024-01-06 00:34:11.371073: \n",
      "2024-01-06 00:34:11.380128: Epoch 599\n",
      "2024-01-06 00:34:11.384129: Current learning rate: 0.00439\n",
      "2024-01-06 00:36:16.258540: train_loss -0.9129\n",
      "2024-01-06 00:36:16.267544: val_loss -0.8318\n",
      "2024-01-06 00:36:16.276053: Pseudo dice [0.8608, 0.9583, 0.9389]\n",
      "2024-01-06 00:36:16.283056: Epoch time: 124.89 s\n",
      "2024-01-06 00:36:17.530460: \n",
      "2024-01-06 00:36:17.537120: Epoch 600\n",
      "2024-01-06 00:36:17.544051: Current learning rate: 0.00438\n",
      "2024-01-06 00:38:22.492669: train_loss -0.9176\n",
      "2024-01-06 00:38:22.501668: val_loss -0.8364\n",
      "2024-01-06 00:38:22.509668: Pseudo dice [0.8621, 0.96, 0.942]\n",
      "2024-01-06 00:38:22.516677: Epoch time: 124.96 s\n",
      "2024-01-06 00:38:23.475255: \n",
      "2024-01-06 00:38:23.480256: Epoch 601\n",
      "2024-01-06 00:38:23.485249: Current learning rate: 0.00437\n",
      "2024-01-06 00:40:28.030090: train_loss -0.919\n",
      "2024-01-06 00:40:28.039085: val_loss -0.8327\n",
      "2024-01-06 00:40:28.046085: Pseudo dice [0.8571, 0.96, 0.9418]\n",
      "2024-01-06 00:40:28.051087: Epoch time: 124.56 s\n",
      "2024-01-06 00:40:29.191412: \n",
      "2024-01-06 00:40:29.196751: Epoch 602\n",
      "2024-01-06 00:40:29.201759: Current learning rate: 0.00436\n",
      "2024-01-06 00:42:34.125797: train_loss -0.9208\n",
      "2024-01-06 00:42:34.134797: val_loss -0.8356\n",
      "2024-01-06 00:42:34.140807: Pseudo dice [0.8634, 0.9589, 0.9406]\n",
      "2024-01-06 00:42:34.147807: Epoch time: 124.94 s\n",
      "2024-01-06 00:42:35.117500: \n",
      "2024-01-06 00:42:35.123579: Epoch 603\n",
      "2024-01-06 00:42:35.127585: Current learning rate: 0.00435\n",
      "2024-01-06 00:44:40.065458: train_loss -0.9169\n",
      "2024-01-06 00:44:40.073458: val_loss -0.8276\n",
      "2024-01-06 00:44:40.081458: Pseudo dice [0.8588, 0.96, 0.9417]\n",
      "2024-01-06 00:44:40.087458: Epoch time: 124.95 s\n",
      "2024-01-06 00:44:41.049252: \n",
      "2024-01-06 00:44:41.054249: Epoch 604\n",
      "2024-01-06 00:44:41.058166: Current learning rate: 0.00434\n",
      "2024-01-06 00:46:46.300887: train_loss -0.9154\n",
      "2024-01-06 00:46:46.308885: val_loss -0.8353\n",
      "2024-01-06 00:46:46.316889: Pseudo dice [0.8596, 0.9602, 0.9422]\n",
      "2024-01-06 00:46:46.321900: Epoch time: 125.25 s\n",
      "2024-01-06 00:46:47.277601: \n",
      "2024-01-06 00:46:47.284142: Epoch 605\n",
      "2024-01-06 00:46:47.291656: Current learning rate: 0.00433\n",
      "2024-01-06 00:48:52.324554: train_loss -0.9183\n",
      "2024-01-06 00:48:52.333551: val_loss -0.8361\n",
      "2024-01-06 00:48:52.339550: Pseudo dice [0.8598, 0.9599, 0.9424]\n",
      "2024-01-06 00:48:52.344550: Epoch time: 125.05 s\n",
      "2024-01-06 00:48:53.316938: \n",
      "2024-01-06 00:48:53.325211: Epoch 606\n",
      "2024-01-06 00:48:53.329275: Current learning rate: 0.00432\n",
      "2024-01-06 00:50:58.537136: train_loss -0.916\n",
      "2024-01-06 00:50:58.546129: val_loss -0.8298\n",
      "2024-01-06 00:50:58.553134: Pseudo dice [0.8612, 0.9582, 0.9401]\n",
      "2024-01-06 00:50:58.558129: Epoch time: 125.22 s\n",
      "2024-01-06 00:50:59.513985: \n",
      "2024-01-06 00:50:59.520191: Epoch 607\n",
      "2024-01-06 00:50:59.524745: Current learning rate: 0.00431\n",
      "2024-01-06 00:53:04.505065: train_loss -0.9183\n",
      "2024-01-06 00:53:04.517009: val_loss -0.8276\n",
      "2024-01-06 00:53:04.524991: Pseudo dice [0.8621, 0.9591, 0.9407]\n",
      "2024-01-06 00:53:04.531990: Epoch time: 124.99 s\n",
      "2024-01-06 00:53:05.486116: \n",
      "2024-01-06 00:53:05.494458: Epoch 608\n",
      "2024-01-06 00:53:05.498974: Current learning rate: 0.0043\n",
      "2024-01-06 00:55:10.612650: train_loss -0.9174\n",
      "2024-01-06 00:55:10.621651: val_loss -0.8308\n",
      "2024-01-06 00:55:10.628163: Pseudo dice [0.859, 0.9596, 0.9416]\n",
      "2024-01-06 00:55:10.634164: Epoch time: 125.13 s\n",
      "2024-01-06 00:55:11.606834: \n",
      "2024-01-06 00:55:11.611861: Epoch 609\n",
      "2024-01-06 00:55:11.616853: Current learning rate: 0.00429\n",
      "2024-01-06 00:57:16.599926: train_loss -0.9187\n",
      "2024-01-06 00:57:16.609926: val_loss -0.8327\n",
      "2024-01-06 00:57:16.615926: Pseudo dice [0.8624, 0.9607, 0.9428]\n",
      "2024-01-06 00:57:16.622927: Epoch time: 124.99 s\n",
      "2024-01-06 00:57:17.755671: \n",
      "2024-01-06 00:57:17.761117: Epoch 610\n",
      "2024-01-06 00:57:17.764740: Current learning rate: 0.00429\n",
      "2024-01-06 00:59:23.020645: train_loss -0.9167\n",
      "2024-01-06 00:59:23.028151: val_loss -0.8293\n",
      "2024-01-06 00:59:23.035152: Pseudo dice [0.8623, 0.9588, 0.9397]\n",
      "2024-01-06 00:59:23.043151: Epoch time: 125.27 s\n",
      "2024-01-06 00:59:24.000687: \n",
      "2024-01-06 00:59:24.009838: Epoch 611\n",
      "2024-01-06 00:59:24.014452: Current learning rate: 0.00428\n",
      "2024-01-06 01:01:28.950128: train_loss -0.9191\n",
      "2024-01-06 01:01:28.958131: val_loss -0.832\n",
      "2024-01-06 01:01:28.965132: Pseudo dice [0.8599, 0.9591, 0.9411]\n",
      "2024-01-06 01:01:28.972637: Epoch time: 124.95 s\n",
      "2024-01-06 01:01:29.941072: \n",
      "2024-01-06 01:01:29.951520: Epoch 612\n",
      "2024-01-06 01:01:29.955629: Current learning rate: 0.00427\n",
      "2024-01-06 01:03:35.095705: train_loss -0.9191\n",
      "2024-01-06 01:03:35.103702: val_loss -0.8386\n",
      "2024-01-06 01:03:35.109708: Pseudo dice [0.8664, 0.9592, 0.9414]\n",
      "2024-01-06 01:03:35.115709: Epoch time: 125.16 s\n",
      "2024-01-06 01:03:36.130673: \n",
      "2024-01-06 01:03:36.136668: Epoch 613\n",
      "2024-01-06 01:03:36.140674: Current learning rate: 0.00426\n",
      "2024-01-06 01:05:41.159122: train_loss -0.9182\n",
      "2024-01-06 01:05:41.167627: val_loss -0.8375\n",
      "2024-01-06 01:05:41.174627: Pseudo dice [0.8651, 0.9601, 0.9421]\n",
      "2024-01-06 01:05:41.182627: Epoch time: 125.03 s\n",
      "2024-01-06 01:05:42.145635: \n",
      "2024-01-06 01:05:42.150771: Epoch 614\n",
      "2024-01-06 01:05:42.155771: Current learning rate: 0.00425\n",
      "2024-01-06 01:07:47.278631: train_loss -0.9185\n",
      "2024-01-06 01:07:47.285636: val_loss -0.835\n",
      "2024-01-06 01:07:47.295639: Pseudo dice [0.863, 0.9599, 0.9415]\n",
      "2024-01-06 01:07:47.302148: Epoch time: 125.13 s\n",
      "2024-01-06 01:07:48.315061: \n",
      "2024-01-06 01:07:48.326056: Epoch 615\n",
      "2024-01-06 01:07:48.331064: Current learning rate: 0.00424\n",
      "2024-01-06 01:09:53.555018: train_loss -0.9158\n",
      "2024-01-06 01:09:53.562017: val_loss -0.8338\n",
      "2024-01-06 01:09:53.569526: Pseudo dice [0.8584, 0.96, 0.9421]\n",
      "2024-01-06 01:09:53.575526: Epoch time: 125.24 s\n",
      "2024-01-06 01:09:54.550460: \n",
      "2024-01-06 01:09:54.561974: Epoch 616\n",
      "2024-01-06 01:09:54.566537: Current learning rate: 0.00423\n",
      "2024-01-06 01:11:59.535855: train_loss -0.9194\n",
      "2024-01-06 01:11:59.546845: val_loss -0.8322\n",
      "2024-01-06 01:11:59.555847: Pseudo dice [0.8608, 0.9592, 0.9413]\n",
      "2024-01-06 01:11:59.562846: Epoch time: 124.99 s\n",
      "2024-01-06 01:12:00.534938: \n",
      "2024-01-06 01:12:00.540015: Epoch 617\n",
      "2024-01-06 01:12:00.545016: Current learning rate: 0.00422\n",
      "2024-01-06 01:14:05.521053: train_loss -0.9191\n",
      "2024-01-06 01:14:05.530053: val_loss -0.8351\n",
      "2024-01-06 01:14:05.539063: Pseudo dice [0.8651, 0.96, 0.9428]\n",
      "2024-01-06 01:14:05.545063: Epoch time: 124.99 s\n",
      "2024-01-06 01:14:06.679391: \n",
      "2024-01-06 01:14:06.684391: Epoch 618\n",
      "2024-01-06 01:14:06.688391: Current learning rate: 0.00421\n",
      "2024-01-06 01:16:11.583557: train_loss -0.9186\n",
      "2024-01-06 01:16:11.591557: val_loss -0.8421\n",
      "2024-01-06 01:16:11.597557: Pseudo dice [0.8617, 0.96, 0.9428]\n",
      "2024-01-06 01:16:11.602557: Epoch time: 124.91 s\n",
      "2024-01-06 01:16:12.558471: \n",
      "2024-01-06 01:16:12.564249: Epoch 619\n",
      "2024-01-06 01:16:12.568337: Current learning rate: 0.0042\n",
      "2024-01-06 01:18:17.699006: train_loss -0.9127\n",
      "2024-01-06 01:18:17.707009: val_loss -0.8403\n",
      "2024-01-06 01:18:17.714009: Pseudo dice [0.8623, 0.9586, 0.9404]\n",
      "2024-01-06 01:18:17.719515: Epoch time: 125.14 s\n",
      "2024-01-06 01:18:18.694147: \n",
      "2024-01-06 01:18:18.700144: Epoch 620\n",
      "2024-01-06 01:18:18.704158: Current learning rate: 0.00419\n",
      "2024-01-06 01:20:23.508961: train_loss -0.9185\n",
      "2024-01-06 01:20:23.516963: val_loss -0.8322\n",
      "2024-01-06 01:20:23.524960: Pseudo dice [0.8623, 0.959, 0.9414]\n",
      "2024-01-06 01:20:23.530959: Epoch time: 124.82 s\n",
      "2024-01-06 01:20:24.521384: \n",
      "2024-01-06 01:20:24.530609: Epoch 621\n",
      "2024-01-06 01:20:24.539749: Current learning rate: 0.00418\n",
      "2024-01-06 01:22:29.359940: train_loss -0.9192\n",
      "2024-01-06 01:22:29.369941: val_loss -0.8391\n",
      "2024-01-06 01:22:29.378940: Pseudo dice [0.8648, 0.9597, 0.9423]\n",
      "2024-01-06 01:22:29.384939: Epoch time: 124.84 s\n",
      "2024-01-06 01:22:30.346735: \n",
      "2024-01-06 01:22:30.352270: Epoch 622\n",
      "2024-01-06 01:22:30.357456: Current learning rate: 0.00417\n",
      "2024-01-06 01:24:35.545421: train_loss -0.9152\n",
      "2024-01-06 01:24:35.552419: val_loss -0.8395\n",
      "2024-01-06 01:24:35.557492: Pseudo dice [0.8594, 0.9592, 0.9413]\n",
      "2024-01-06 01:24:35.563433: Epoch time: 125.2 s\n",
      "2024-01-06 01:24:36.538014: \n",
      "2024-01-06 01:24:36.543005: Epoch 623\n",
      "2024-01-06 01:24:36.548010: Current learning rate: 0.00416\n",
      "2024-01-06 01:26:41.466477: train_loss -0.9202\n",
      "2024-01-06 01:26:41.473477: val_loss -0.8429\n",
      "2024-01-06 01:26:41.481477: Pseudo dice [0.8625, 0.9611, 0.9438]\n",
      "2024-01-06 01:26:41.486477: Epoch time: 124.93 s\n",
      "2024-01-06 01:26:42.447518: \n",
      "2024-01-06 01:26:42.456225: Epoch 624\n",
      "2024-01-06 01:26:42.460742: Current learning rate: 0.00415\n",
      "2024-01-06 01:28:47.649517: train_loss -0.9172\n",
      "2024-01-06 01:28:47.659026: val_loss -0.8344\n",
      "2024-01-06 01:28:47.667025: Pseudo dice [0.8589, 0.9596, 0.9422]\n",
      "2024-01-06 01:28:47.673029: Epoch time: 125.2 s\n",
      "2024-01-06 01:28:48.637626: \n",
      "2024-01-06 01:28:48.645386: Epoch 625\n",
      "2024-01-06 01:28:48.650472: Current learning rate: 0.00414\n",
      "2024-01-06 01:30:53.519813: train_loss -0.9214\n",
      "2024-01-06 01:30:53.529824: val_loss -0.8392\n",
      "2024-01-06 01:30:53.535812: Pseudo dice [0.8612, 0.96, 0.9424]\n",
      "2024-01-06 01:30:53.543812: Epoch time: 124.88 s\n",
      "2024-01-06 01:30:54.666453: \n",
      "2024-01-06 01:30:54.676300: Epoch 626\n",
      "2024-01-06 01:30:54.681302: Current learning rate: 0.00413\n",
      "2024-01-06 01:32:59.683747: train_loss -0.92\n",
      "2024-01-06 01:32:59.692745: val_loss -0.83\n",
      "2024-01-06 01:32:59.699745: Pseudo dice [0.8611, 0.9584, 0.9398]\n",
      "2024-01-06 01:32:59.705750: Epoch time: 125.02 s\n",
      "2024-01-06 01:33:00.676664: \n",
      "2024-01-06 01:33:00.682667: Epoch 627\n",
      "2024-01-06 01:33:00.687332: Current learning rate: 0.00412\n",
      "2024-01-06 01:35:05.724540: train_loss -0.9199\n",
      "2024-01-06 01:35:05.733533: val_loss -0.8262\n",
      "2024-01-06 01:35:05.742049: Pseudo dice [0.8578, 0.9594, 0.9414]\n",
      "2024-01-06 01:35:05.748046: Epoch time: 125.05 s\n",
      "2024-01-06 01:35:06.707203: \n",
      "2024-01-06 01:35:06.713213: Epoch 628\n",
      "2024-01-06 01:35:06.717765: Current learning rate: 0.00411\n",
      "2024-01-06 01:37:12.057247: train_loss -0.9181\n",
      "2024-01-06 01:37:12.065249: val_loss -0.8384\n",
      "2024-01-06 01:37:12.072249: Pseudo dice [0.8651, 0.9593, 0.9414]\n",
      "2024-01-06 01:37:12.079260: Epoch time: 125.35 s\n",
      "2024-01-06 01:37:13.045180: \n",
      "2024-01-06 01:37:13.053498: Epoch 629\n",
      "2024-01-06 01:37:13.061096: Current learning rate: 0.0041\n",
      "2024-01-06 01:39:18.231898: train_loss -0.919\n",
      "2024-01-06 01:39:18.238900: val_loss -0.8459\n",
      "2024-01-06 01:39:18.246909: Pseudo dice [0.8636, 0.9602, 0.9427]\n",
      "2024-01-06 01:39:18.253423: Epoch time: 125.19 s\n",
      "2024-01-06 01:39:19.224622: \n",
      "2024-01-06 01:39:19.234400: Epoch 630\n",
      "2024-01-06 01:39:19.238481: Current learning rate: 0.00409\n",
      "2024-01-06 01:41:24.484745: train_loss -0.9198\n",
      "2024-01-06 01:41:24.494747: val_loss -0.8336\n",
      "2024-01-06 01:41:24.503248: Pseudo dice [0.855, 0.9582, 0.94]\n",
      "2024-01-06 01:41:24.511252: Epoch time: 125.26 s\n",
      "2024-01-06 01:41:25.493969: \n",
      "2024-01-06 01:41:25.503574: Epoch 631\n",
      "2024-01-06 01:41:25.507589: Current learning rate: 0.00408\n",
      "2024-01-06 01:43:30.718019: train_loss -0.9163\n",
      "2024-01-06 01:43:30.728559: val_loss -0.8236\n",
      "2024-01-06 01:43:30.738623: Pseudo dice [0.8596, 0.9568, 0.937]\n",
      "2024-01-06 01:43:30.746555: Epoch time: 125.23 s\n",
      "2024-01-06 01:43:31.715175: \n",
      "2024-01-06 01:43:31.724175: Epoch 632\n",
      "2024-01-06 01:43:31.728170: Current learning rate: 0.00407\n",
      "2024-01-06 01:45:36.739235: train_loss -0.9191\n",
      "2024-01-06 01:45:36.749745: val_loss -0.8357\n",
      "2024-01-06 01:45:36.755749: Pseudo dice [0.8648, 0.9594, 0.9412]\n",
      "2024-01-06 01:45:36.761748: Epoch time: 125.03 s\n",
      "2024-01-06 01:45:37.888939: \n",
      "2024-01-06 01:45:37.893939: Epoch 633\n",
      "2024-01-06 01:45:37.899072: Current learning rate: 0.00406\n",
      "2024-01-06 01:47:43.134404: train_loss -0.9178\n",
      "2024-01-06 01:47:43.143418: val_loss -0.8354\n",
      "2024-01-06 01:47:43.150406: Pseudo dice [0.8623, 0.9591, 0.9408]\n",
      "2024-01-06 01:47:43.157412: Epoch time: 125.25 s\n",
      "2024-01-06 01:47:44.126899: \n",
      "2024-01-06 01:47:44.134883: Epoch 634\n",
      "2024-01-06 01:47:44.139887: Current learning rate: 0.00405\n",
      "2024-01-06 01:49:49.217272: train_loss -0.9183\n",
      "2024-01-06 01:49:49.226269: val_loss -0.8384\n",
      "2024-01-06 01:49:49.235266: Pseudo dice [0.8622, 0.9592, 0.9415]\n",
      "2024-01-06 01:49:49.240266: Epoch time: 125.09 s\n",
      "2024-01-06 01:49:50.208005: \n",
      "2024-01-06 01:49:50.218903: Epoch 635\n",
      "2024-01-06 01:49:50.223825: Current learning rate: 0.00404\n",
      "2024-01-06 01:51:55.434487: train_loss -0.9152\n",
      "2024-01-06 01:51:55.441487: val_loss -0.8285\n",
      "2024-01-06 01:51:55.449993: Pseudo dice [0.8571, 0.9589, 0.941]\n",
      "2024-01-06 01:51:55.455992: Epoch time: 125.23 s\n",
      "2024-01-06 01:51:56.424466: \n",
      "2024-01-06 01:51:56.433766: Epoch 636\n",
      "2024-01-06 01:51:56.437786: Current learning rate: 0.00403\n",
      "2024-01-06 01:54:01.704446: train_loss -0.9187\n",
      "2024-01-06 01:54:01.714446: val_loss -0.8319\n",
      "2024-01-06 01:54:01.721448: Pseudo dice [0.8609, 0.9581, 0.9394]\n",
      "2024-01-06 01:54:01.728446: Epoch time: 125.28 s\n",
      "2024-01-06 01:54:02.697860: \n",
      "2024-01-06 01:54:02.705716: Epoch 637\n",
      "2024-01-06 01:54:02.711260: Current learning rate: 0.00402\n",
      "2024-01-06 01:56:07.978266: train_loss -0.9188\n",
      "2024-01-06 01:56:07.986271: val_loss -0.8326\n",
      "2024-01-06 01:56:07.993279: Pseudo dice [0.8597, 0.9592, 0.9413]\n",
      "2024-01-06 01:56:07.999275: Epoch time: 125.28 s\n",
      "2024-01-06 01:56:08.960999: \n",
      "2024-01-06 01:56:08.966996: Epoch 638\n",
      "2024-01-06 01:56:08.971075: Current learning rate: 0.00401\n",
      "2024-01-06 01:58:14.460342: train_loss -0.9191\n",
      "2024-01-06 01:58:14.467343: val_loss -0.8326\n",
      "2024-01-06 01:58:14.475860: Pseudo dice [0.8593, 0.9599, 0.9419]\n",
      "2024-01-06 01:58:14.483856: Epoch time: 125.5 s\n",
      "2024-01-06 01:58:15.440161: \n",
      "2024-01-06 01:58:15.450244: Epoch 639\n",
      "2024-01-06 01:58:15.454240: Current learning rate: 0.004\n",
      "2024-01-06 02:00:20.251586: train_loss -0.9214\n",
      "2024-01-06 02:00:20.259587: val_loss -0.8322\n",
      "2024-01-06 02:00:20.267588: Pseudo dice [0.8588, 0.9587, 0.9406]\n",
      "2024-01-06 02:00:20.273585: Epoch time: 124.81 s\n",
      "2024-01-06 02:00:21.235299: \n",
      "2024-01-06 02:00:21.243915: Epoch 640\n",
      "2024-01-06 02:00:21.248978: Current learning rate: 0.00399\n",
      "2024-01-06 02:02:26.076143: train_loss -0.9224\n",
      "2024-01-06 02:02:26.086146: val_loss -0.8336\n",
      "2024-01-06 02:02:26.094146: Pseudo dice [0.8601, 0.9592, 0.9407]\n",
      "2024-01-06 02:02:26.100654: Epoch time: 124.84 s\n",
      "2024-01-06 02:02:27.261603: \n",
      "2024-01-06 02:02:27.270770: Epoch 641\n",
      "2024-01-06 02:02:27.274816: Current learning rate: 0.00398\n",
      "2024-01-06 02:04:32.286473: train_loss -0.9212\n",
      "2024-01-06 02:04:32.293473: val_loss -0.8313\n",
      "2024-01-06 02:04:32.300478: Pseudo dice [0.8587, 0.9593, 0.9416]\n",
      "2024-01-06 02:04:32.306474: Epoch time: 125.03 s\n",
      "2024-01-06 02:04:33.273602: \n",
      "2024-01-06 02:04:33.284774: Epoch 642\n",
      "2024-01-06 02:04:33.314052: Current learning rate: 0.00397\n",
      "2024-01-06 02:06:38.302855: train_loss -0.9214\n",
      "2024-01-06 02:06:38.309855: val_loss -0.8307\n",
      "2024-01-06 02:06:38.317370: Pseudo dice [0.864, 0.9586, 0.9402]\n",
      "2024-01-06 02:06:38.322383: Epoch time: 125.03 s\n",
      "2024-01-06 02:06:39.276375: \n",
      "2024-01-06 02:06:39.284472: Epoch 643\n",
      "2024-01-06 02:06:39.290064: Current learning rate: 0.00396\n",
      "2024-01-06 02:08:44.886335: train_loss -0.9178\n",
      "2024-01-06 02:08:44.894336: val_loss -0.8384\n",
      "2024-01-06 02:08:44.902337: Pseudo dice [0.8635, 0.9593, 0.9419]\n",
      "2024-01-06 02:08:44.910336: Epoch time: 125.61 s\n",
      "2024-01-06 02:08:45.900623: \n",
      "2024-01-06 02:08:45.909348: Epoch 644\n",
      "2024-01-06 02:08:45.914360: Current learning rate: 0.00395\n",
      "2024-01-06 02:10:50.850430: train_loss -0.9165\n",
      "2024-01-06 02:10:50.857953: val_loss -0.8297\n",
      "2024-01-06 02:10:50.864953: Pseudo dice [0.8652, 0.9596, 0.9419]\n",
      "2024-01-06 02:10:50.870954: Epoch time: 124.95 s\n",
      "2024-01-06 02:10:51.834528: \n",
      "2024-01-06 02:10:51.839542: Epoch 645\n",
      "2024-01-06 02:10:51.846542: Current learning rate: 0.00394\n",
      "2024-01-06 02:12:57.113023: train_loss -0.9174\n",
      "2024-01-06 02:12:57.123533: val_loss -0.8398\n",
      "2024-01-06 02:12:57.131541: Pseudo dice [0.8611, 0.9601, 0.9425]\n",
      "2024-01-06 02:12:57.137540: Epoch time: 125.28 s\n",
      "2024-01-06 02:12:58.103887: \n",
      "2024-01-06 02:12:58.110968: Epoch 646\n",
      "2024-01-06 02:12:58.121058: Current learning rate: 0.00393\n",
      "2024-01-06 02:15:03.212110: train_loss -0.9204\n",
      "2024-01-06 02:15:03.220109: val_loss -0.8316\n",
      "2024-01-06 02:15:03.228112: Pseudo dice [0.8582, 0.9589, 0.9404]\n",
      "2024-01-06 02:15:03.234112: Epoch time: 125.11 s\n",
      "2024-01-06 02:15:04.214455: \n",
      "2024-01-06 02:15:04.220455: Epoch 647\n",
      "2024-01-06 02:15:04.225073: Current learning rate: 0.00392\n",
      "2024-01-06 02:17:09.435314: train_loss -0.9195\n",
      "2024-01-06 02:17:09.443314: val_loss -0.835\n",
      "2024-01-06 02:17:09.451314: Pseudo dice [0.8655, 0.96, 0.9425]\n",
      "2024-01-06 02:17:09.456314: Epoch time: 125.22 s\n",
      "2024-01-06 02:17:10.423060: \n",
      "2024-01-06 02:17:10.428072: Epoch 648\n",
      "2024-01-06 02:17:10.432072: Current learning rate: 0.00391\n",
      "2024-01-06 02:19:15.363403: train_loss -0.9224\n",
      "2024-01-06 02:19:15.371580: val_loss -0.8367\n",
      "2024-01-06 02:19:15.378580: Pseudo dice [0.866, 0.9606, 0.9432]\n",
      "2024-01-06 02:19:15.385581: Epoch time: 124.94 s\n",
      "2024-01-06 02:19:16.540950: \n",
      "2024-01-06 02:19:16.549088: Epoch 649\n",
      "2024-01-06 02:19:16.553232: Current learning rate: 0.0039\n",
      "2024-01-06 02:21:21.726840: train_loss -0.9208\n",
      "2024-01-06 02:21:21.736843: val_loss -0.8293\n",
      "2024-01-06 02:21:21.742837: Pseudo dice [0.8619, 0.9576, 0.9389]\n",
      "2024-01-06 02:21:21.748841: Epoch time: 125.19 s\n",
      "2024-01-06 02:21:22.972838: \n",
      "2024-01-06 02:21:22.977899: Epoch 650\n",
      "2024-01-06 02:21:22.982841: Current learning rate: 0.00389\n",
      "2024-01-06 02:23:27.753496: train_loss -0.919\n",
      "2024-01-06 02:23:27.762496: val_loss -0.8346\n",
      "2024-01-06 02:23:27.768499: Pseudo dice [0.8594, 0.9595, 0.9414]\n",
      "2024-01-06 02:23:27.774501: Epoch time: 124.78 s\n",
      "2024-01-06 02:23:28.733172: \n",
      "2024-01-06 02:23:28.741291: Epoch 651\n",
      "2024-01-06 02:23:28.747635: Current learning rate: 0.00388\n",
      "2024-01-06 02:25:33.684381: train_loss -0.9203\n",
      "2024-01-06 02:25:33.692382: val_loss -0.8341\n",
      "2024-01-06 02:25:33.699382: Pseudo dice [0.859, 0.9587, 0.9405]\n",
      "2024-01-06 02:25:33.704381: Epoch time: 124.95 s\n",
      "2024-01-06 02:25:34.667450: \n",
      "2024-01-06 02:25:34.676507: Epoch 652\n",
      "2024-01-06 02:25:34.680578: Current learning rate: 0.00387\n",
      "2024-01-06 02:27:39.453866: train_loss -0.9202\n",
      "2024-01-06 02:27:39.460864: val_loss -0.8315\n",
      "2024-01-06 02:27:39.467866: Pseudo dice [0.8583, 0.959, 0.9409]\n",
      "2024-01-06 02:27:39.473868: Epoch time: 124.79 s\n",
      "2024-01-06 02:27:40.431917: \n",
      "2024-01-06 02:27:40.436947: Epoch 653\n",
      "2024-01-06 02:27:40.442176: Current learning rate: 0.00386\n",
      "2024-01-06 02:29:45.595745: train_loss -0.9176\n",
      "2024-01-06 02:29:45.602746: val_loss -0.8406\n",
      "2024-01-06 02:29:45.611746: Pseudo dice [0.862, 0.9595, 0.9419]\n",
      "2024-01-06 02:29:45.620748: Epoch time: 125.17 s\n",
      "2024-01-06 02:29:46.609452: \n",
      "2024-01-06 02:29:46.615450: Epoch 654\n",
      "2024-01-06 02:29:46.619515: Current learning rate: 0.00385\n",
      "2024-01-06 02:31:51.877157: train_loss -0.919\n",
      "2024-01-06 02:31:51.886163: val_loss -0.8326\n",
      "2024-01-06 02:31:51.894159: Pseudo dice [0.8636, 0.9581, 0.9394]\n",
      "2024-01-06 02:31:51.903167: Epoch time: 125.27 s\n",
      "2024-01-06 02:31:52.889865: \n",
      "2024-01-06 02:31:52.897939: Epoch 655\n",
      "2024-01-06 02:31:52.902936: Current learning rate: 0.00384\n",
      "2024-01-06 02:33:57.725240: train_loss -0.9191\n",
      "2024-01-06 02:33:57.732238: val_loss -0.8321\n",
      "2024-01-06 02:33:57.740232: Pseudo dice [0.8603, 0.9595, 0.9412]\n",
      "2024-01-06 02:33:57.745235: Epoch time: 124.84 s\n",
      "2024-01-06 02:33:58.872368: \n",
      "2024-01-06 02:33:58.881383: Epoch 656\n",
      "2024-01-06 02:33:58.886460: Current learning rate: 0.00383\n",
      "2024-01-06 02:36:03.978495: train_loss -0.9181\n",
      "2024-01-06 02:36:03.987495: val_loss -0.8313\n",
      "2024-01-06 02:36:03.994497: Pseudo dice [0.8607, 0.959, 0.9409]\n",
      "2024-01-06 02:36:04.002496: Epoch time: 125.11 s\n",
      "2024-01-06 02:36:05.008307: \n",
      "2024-01-06 02:36:05.020294: Epoch 657\n",
      "2024-01-06 02:36:05.028360: Current learning rate: 0.00382\n",
      "2024-01-06 02:38:09.941600: train_loss -0.9201\n",
      "2024-01-06 02:38:09.949600: val_loss -0.8359\n",
      "2024-01-06 02:38:09.957601: Pseudo dice [0.8596, 0.9592, 0.9416]\n",
      "2024-01-06 02:38:09.964600: Epoch time: 124.93 s\n",
      "2024-01-06 02:38:10.918782: \n",
      "2024-01-06 02:38:10.928239: Epoch 658\n",
      "2024-01-06 02:38:10.934293: Current learning rate: 0.00381\n",
      "2024-01-06 02:40:15.845098: train_loss -0.9204\n",
      "2024-01-06 02:40:15.853098: val_loss -0.8333\n",
      "2024-01-06 02:40:15.861101: Pseudo dice [0.8588, 0.9591, 0.9408]\n",
      "2024-01-06 02:40:15.867110: Epoch time: 124.93 s\n",
      "2024-01-06 02:40:16.838457: \n",
      "2024-01-06 02:40:16.846457: Epoch 659\n",
      "2024-01-06 02:40:16.851144: Current learning rate: 0.0038\n",
      "2024-01-06 02:42:22.211640: train_loss -0.9172\n",
      "2024-01-06 02:42:22.223163: val_loss -0.8345\n",
      "2024-01-06 02:42:22.229161: Pseudo dice [0.8541, 0.9592, 0.9412]\n",
      "2024-01-06 02:42:22.235157: Epoch time: 125.37 s\n",
      "2024-01-06 02:42:23.226973: \n",
      "2024-01-06 02:42:23.232541: Epoch 660\n",
      "2024-01-06 02:42:23.236612: Current learning rate: 0.00379\n",
      "2024-01-06 02:44:28.244118: train_loss -0.9193\n",
      "2024-01-06 02:44:28.252122: val_loss -0.8302\n",
      "2024-01-06 02:44:28.260125: Pseudo dice [0.8593, 0.9595, 0.9418]\n",
      "2024-01-06 02:44:28.266280: Epoch time: 125.02 s\n",
      "2024-01-06 02:44:29.236649: \n",
      "2024-01-06 02:44:29.245385: Epoch 661\n",
      "2024-01-06 02:44:29.250397: Current learning rate: 0.00378\n",
      "2024-01-06 02:46:34.173460: train_loss -0.9191\n",
      "2024-01-06 02:46:34.182461: val_loss -0.8359\n",
      "2024-01-06 02:46:34.188461: Pseudo dice [0.8566, 0.9594, 0.9419]\n",
      "2024-01-06 02:46:34.194461: Epoch time: 124.94 s\n",
      "2024-01-06 02:46:35.166899: \n",
      "2024-01-06 02:46:35.175010: Epoch 662\n",
      "2024-01-06 02:46:35.179087: Current learning rate: 0.00377\n",
      "2024-01-06 02:48:40.513703: train_loss -0.9198\n",
      "2024-01-06 02:48:40.525703: val_loss -0.8359\n",
      "2024-01-06 02:48:40.533711: Pseudo dice [0.8625, 0.9594, 0.9412]\n",
      "2024-01-06 02:48:40.542710: Epoch time: 125.35 s\n",
      "2024-01-06 02:48:41.529110: \n",
      "2024-01-06 02:48:41.538160: Epoch 663\n",
      "2024-01-06 02:48:41.542090: Current learning rate: 0.00376\n",
      "2024-01-06 02:50:46.414164: train_loss -0.9202\n",
      "2024-01-06 02:50:46.423165: val_loss -0.8398\n",
      "2024-01-06 02:50:46.431164: Pseudo dice [0.8595, 0.9598, 0.9424]\n",
      "2024-01-06 02:50:46.438167: Epoch time: 124.89 s\n",
      "2024-01-06 02:50:47.565783: \n",
      "2024-01-06 02:50:47.575124: Epoch 664\n",
      "2024-01-06 02:50:47.579201: Current learning rate: 0.00375\n",
      "2024-01-06 02:52:52.750751: train_loss -0.9193\n",
      "2024-01-06 02:52:52.759744: val_loss -0.8393\n",
      "2024-01-06 02:52:52.766747: Pseudo dice [0.862, 0.9595, 0.9419]\n",
      "2024-01-06 02:52:52.774749: Epoch time: 125.19 s\n",
      "2024-01-06 02:52:53.739154: \n",
      "2024-01-06 02:52:53.744154: Epoch 665\n",
      "2024-01-06 02:52:53.748154: Current learning rate: 0.00374\n",
      "2024-01-06 02:54:58.634940: train_loss -0.9214\n",
      "2024-01-06 02:54:58.642943: val_loss -0.8378\n",
      "2024-01-06 02:54:58.649943: Pseudo dice [0.8638, 0.9594, 0.9413]\n",
      "2024-01-06 02:54:58.655450: Epoch time: 124.9 s\n",
      "2024-01-06 02:54:59.625530: \n",
      "2024-01-06 02:54:59.631532: Epoch 666\n",
      "2024-01-06 02:54:59.635530: Current learning rate: 0.00373\n",
      "2024-01-06 02:57:05.178346: train_loss -0.9193\n",
      "2024-01-06 02:57:05.186347: val_loss -0.8366\n",
      "2024-01-06 02:57:05.192350: Pseudo dice [0.8612, 0.9585, 0.94]\n",
      "2024-01-06 02:57:05.198351: Epoch time: 125.55 s\n",
      "2024-01-06 02:57:06.158307: \n",
      "2024-01-06 02:57:06.167700: Epoch 667\n",
      "2024-01-06 02:57:06.174037: Current learning rate: 0.00372\n",
      "2024-01-06 02:59:11.015356: train_loss -0.9206\n",
      "2024-01-06 02:59:11.024361: val_loss -0.8372\n",
      "2024-01-06 02:59:11.030363: Pseudo dice [0.859, 0.9606, 0.9423]\n",
      "2024-01-06 02:59:11.035359: Epoch time: 124.86 s\n",
      "2024-01-06 02:59:12.017817: \n",
      "2024-01-06 02:59:12.029941: Epoch 668\n",
      "2024-01-06 02:59:12.034959: Current learning rate: 0.00371\n",
      "2024-01-06 03:01:17.123053: train_loss -0.9202\n",
      "2024-01-06 03:01:17.131052: val_loss -0.8393\n",
      "2024-01-06 03:01:17.139061: Pseudo dice [0.8593, 0.9593, 0.9414]\n",
      "2024-01-06 03:01:17.147062: Epoch time: 125.11 s\n",
      "2024-01-06 03:01:18.127480: \n",
      "2024-01-06 03:01:18.132745: Epoch 669\n",
      "2024-01-06 03:01:18.136876: Current learning rate: 0.0037\n",
      "2024-01-06 03:03:23.241991: train_loss -0.9168\n",
      "2024-01-06 03:03:23.253994: val_loss -0.8397\n",
      "2024-01-06 03:03:23.261986: Pseudo dice [0.8636, 0.9581, 0.9403]\n",
      "2024-01-06 03:03:23.268981: Epoch time: 125.12 s\n",
      "2024-01-06 03:03:24.254465: \n",
      "2024-01-06 03:03:24.263042: Epoch 670\n",
      "2024-01-06 03:03:24.270027: Current learning rate: 0.00369\n",
      "2024-01-06 03:05:29.289667: train_loss -0.9193\n",
      "2024-01-06 03:05:29.300672: val_loss -0.8338\n",
      "2024-01-06 03:05:29.307663: Pseudo dice [0.8616, 0.9587, 0.9399]\n",
      "2024-01-06 03:05:29.312665: Epoch time: 125.04 s\n",
      "2024-01-06 03:05:30.277998: \n",
      "2024-01-06 03:05:30.289541: Epoch 671\n",
      "2024-01-06 03:05:30.296707: Current learning rate: 0.00368\n",
      "2024-01-06 03:07:35.383585: train_loss -0.9177\n",
      "2024-01-06 03:07:35.392592: val_loss -0.8344\n",
      "2024-01-06 03:07:35.400586: Pseudo dice [0.8612, 0.9601, 0.9423]\n",
      "2024-01-06 03:07:35.406586: Epoch time: 125.11 s\n",
      "2024-01-06 03:07:36.556331: \n",
      "2024-01-06 03:07:36.561408: Epoch 672\n",
      "2024-01-06 03:07:36.566403: Current learning rate: 0.00367\n",
      "2024-01-06 03:09:41.468462: train_loss -0.9186\n",
      "2024-01-06 03:09:41.475464: val_loss -0.8385\n",
      "2024-01-06 03:09:41.483462: Pseudo dice [0.8624, 0.9598, 0.9423]\n",
      "2024-01-06 03:09:41.490462: Epoch time: 124.91 s\n",
      "2024-01-06 03:09:42.460711: \n",
      "2024-01-06 03:09:42.470030: Epoch 673\n",
      "2024-01-06 03:09:42.479498: Current learning rate: 0.00366\n",
      "2024-01-06 03:11:47.367248: train_loss -0.921\n",
      "2024-01-06 03:11:47.375749: val_loss -0.8389\n",
      "2024-01-06 03:11:47.381749: Pseudo dice [0.8617, 0.96, 0.9424]\n",
      "2024-01-06 03:11:47.386749: Epoch time: 124.91 s\n",
      "2024-01-06 03:11:48.365944: \n",
      "2024-01-06 03:11:48.405824: Epoch 674\n",
      "2024-01-06 03:11:48.410824: Current learning rate: 0.00365\n",
      "2024-01-06 03:13:53.444408: train_loss -0.9203\n",
      "2024-01-06 03:13:53.456415: val_loss -0.8359\n",
      "2024-01-06 03:13:53.464923: Pseudo dice [0.8615, 0.9596, 0.9415]\n",
      "2024-01-06 03:13:53.470924: Epoch time: 125.08 s\n",
      "2024-01-06 03:13:54.453862: \n",
      "2024-01-06 03:13:54.462539: Epoch 675\n",
      "2024-01-06 03:13:54.468549: Current learning rate: 0.00364\n",
      "2024-01-06 03:15:59.407597: train_loss -0.9166\n",
      "2024-01-06 03:15:59.415595: val_loss -0.8202\n",
      "2024-01-06 03:15:59.424153: Pseudo dice [0.8607, 0.9554, 0.9353]\n",
      "2024-01-06 03:15:59.429153: Epoch time: 124.95 s\n",
      "2024-01-06 03:16:00.419773: \n",
      "2024-01-06 03:16:00.425850: Epoch 676\n",
      "2024-01-06 03:16:00.429844: Current learning rate: 0.00363\n",
      "2024-01-06 03:18:05.492445: train_loss -0.9111\n",
      "2024-01-06 03:18:05.499444: val_loss -0.8329\n",
      "2024-01-06 03:18:05.505448: Pseudo dice [0.8588, 0.9588, 0.9386]\n",
      "2024-01-06 03:18:05.510450: Epoch time: 125.07 s\n",
      "2024-01-06 03:18:06.492111: \n",
      "2024-01-06 03:18:06.501323: Epoch 677\n",
      "2024-01-06 03:18:06.506392: Current learning rate: 0.00362\n",
      "2024-01-06 03:20:11.512004: train_loss -0.9103\n",
      "2024-01-06 03:20:11.520003: val_loss -0.8389\n",
      "2024-01-06 03:20:11.529002: Pseudo dice [0.8624, 0.9599, 0.9412]\n",
      "2024-01-06 03:20:11.538002: Epoch time: 125.02 s\n",
      "2024-01-06 03:20:12.518218: \n",
      "2024-01-06 03:20:12.524599: Epoch 678\n",
      "2024-01-06 03:20:12.528683: Current learning rate: 0.00361\n",
      "2024-01-06 03:22:17.365450: train_loss -0.9148\n",
      "2024-01-06 03:22:17.372452: val_loss -0.8327\n",
      "2024-01-06 03:22:17.380451: Pseudo dice [0.8551, 0.9565, 0.9372]\n",
      "2024-01-06 03:22:17.388451: Epoch time: 124.85 s\n",
      "2024-01-06 03:22:18.542185: \n",
      "2024-01-06 03:22:18.549712: Epoch 679\n",
      "2024-01-06 03:22:18.559239: Current learning rate: 0.0036\n",
      "2024-01-06 03:24:23.290755: train_loss -0.9149\n",
      "2024-01-06 03:24:23.298755: val_loss -0.8303\n",
      "2024-01-06 03:24:23.304755: Pseudo dice [0.8641, 0.9599, 0.9425]\n",
      "2024-01-06 03:24:23.309755: Epoch time: 124.75 s\n",
      "2024-01-06 03:24:24.305755: \n",
      "2024-01-06 03:24:24.312781: Epoch 680\n",
      "2024-01-06 03:24:24.316763: Current learning rate: 0.00359\n",
      "2024-01-06 03:26:28.992847: train_loss -0.9201\n",
      "2024-01-06 03:26:29.001357: val_loss -0.8362\n",
      "2024-01-06 03:26:29.009361: Pseudo dice [0.8646, 0.959, 0.9409]\n",
      "2024-01-06 03:26:29.014370: Epoch time: 124.69 s\n",
      "2024-01-06 03:26:29.985450: \n",
      "2024-01-06 03:26:29.995193: Epoch 681\n",
      "2024-01-06 03:26:29.999259: Current learning rate: 0.00358\n",
      "2024-01-06 03:28:34.756292: train_loss -0.918\n",
      "2024-01-06 03:28:34.763309: val_loss -0.8432\n",
      "2024-01-06 03:28:34.769305: Pseudo dice [0.8654, 0.9596, 0.9419]\n",
      "2024-01-06 03:28:34.775300: Epoch time: 124.77 s\n",
      "2024-01-06 03:28:35.770443: \n",
      "2024-01-06 03:28:35.775854: Epoch 682\n",
      "2024-01-06 03:28:35.779868: Current learning rate: 0.00357\n",
      "2024-01-06 03:30:40.539142: train_loss -0.9206\n",
      "2024-01-06 03:30:40.548166: val_loss -0.8326\n",
      "2024-01-06 03:30:40.554164: Pseudo dice [0.8634, 0.9589, 0.9413]\n",
      "2024-01-06 03:30:40.559670: Epoch time: 124.77 s\n",
      "2024-01-06 03:30:41.535437: \n",
      "2024-01-06 03:30:41.543298: Epoch 683\n",
      "2024-01-06 03:30:41.547436: Current learning rate: 0.00356\n",
      "2024-01-06 03:32:46.168118: train_loss -0.9198\n",
      "2024-01-06 03:32:46.177118: val_loss -0.8376\n",
      "2024-01-06 03:32:46.185120: Pseudo dice [0.8629, 0.9594, 0.941]\n",
      "2024-01-06 03:32:46.191118: Epoch time: 124.63 s\n",
      "2024-01-06 03:32:47.171160: \n",
      "2024-01-06 03:32:47.181417: Epoch 684\n",
      "2024-01-06 03:32:47.186417: Current learning rate: 0.00355\n",
      "2024-01-06 03:34:51.577965: train_loss -0.9218\n",
      "2024-01-06 03:34:51.584969: val_loss -0.8343\n",
      "2024-01-06 03:34:51.590969: Pseudo dice [0.8629, 0.9586, 0.94]\n",
      "2024-01-06 03:34:51.598483: Epoch time: 124.41 s\n",
      "2024-01-06 03:34:52.581680: \n",
      "2024-01-06 03:34:52.589685: Epoch 685\n",
      "2024-01-06 03:34:52.595607: Current learning rate: 0.00354\n",
      "2024-01-06 03:36:56.963300: train_loss -0.9207\n",
      "2024-01-06 03:36:56.973293: val_loss -0.8303\n",
      "2024-01-06 03:36:56.980809: Pseudo dice [0.8595, 0.9593, 0.9414]\n",
      "2024-01-06 03:36:56.987809: Epoch time: 124.38 s\n",
      "2024-01-06 03:36:57.961558: \n",
      "2024-01-06 03:36:57.969443: Epoch 686\n",
      "2024-01-06 03:36:57.974496: Current learning rate: 0.00353\n",
      "2024-01-06 03:39:02.543337: train_loss -0.9187\n",
      "2024-01-06 03:39:02.550341: val_loss -0.8216\n",
      "2024-01-06 03:39:02.556341: Pseudo dice [0.856, 0.9563, 0.9373]\n",
      "2024-01-06 03:39:02.561345: Epoch time: 124.58 s\n",
      "2024-01-06 03:39:03.703477: \n",
      "2024-01-06 03:39:03.710872: Epoch 687\n",
      "2024-01-06 03:39:03.718418: Current learning rate: 0.00352\n",
      "2024-01-06 03:41:08.674671: train_loss -0.9175\n",
      "2024-01-06 03:41:08.686671: val_loss -0.8284\n",
      "2024-01-06 03:41:08.696671: Pseudo dice [0.8561, 0.9566, 0.937]\n",
      "2024-01-06 03:41:08.706682: Epoch time: 124.97 s\n",
      "2024-01-06 03:41:09.867638: \n",
      "2024-01-06 03:41:09.873634: Epoch 688\n",
      "2024-01-06 03:41:09.878634: Current learning rate: 0.00351\n",
      "2024-01-06 03:43:14.352376: train_loss -0.92\n",
      "2024-01-06 03:43:14.360366: val_loss -0.821\n",
      "2024-01-06 03:43:14.367366: Pseudo dice [0.8495, 0.9559, 0.936]\n",
      "2024-01-06 03:43:14.374368: Epoch time: 124.49 s\n",
      "2024-01-06 03:43:15.338728: \n",
      "2024-01-06 03:43:15.343740: Epoch 689\n",
      "2024-01-06 03:43:15.348738: Current learning rate: 0.0035\n",
      "2024-01-06 03:45:19.890597: train_loss -0.9206\n",
      "2024-01-06 03:45:19.898597: val_loss -0.8228\n",
      "2024-01-06 03:45:19.904597: Pseudo dice [0.8511, 0.9561, 0.9359]\n",
      "2024-01-06 03:45:19.910597: Epoch time: 124.55 s\n",
      "2024-01-06 03:45:20.901534: \n",
      "2024-01-06 03:45:20.909844: Epoch 690\n",
      "2024-01-06 03:45:20.914783: Current learning rate: 0.00349\n",
      "2024-01-06 03:47:25.988463: train_loss -0.9197\n",
      "2024-01-06 03:47:25.997462: val_loss -0.8136\n",
      "2024-01-06 03:47:26.004462: Pseudo dice [0.852, 0.9557, 0.9354]\n",
      "2024-01-06 03:47:26.010462: Epoch time: 125.09 s\n",
      "2024-01-06 03:47:26.988489: \n",
      "2024-01-06 03:47:26.994514: Epoch 691\n",
      "2024-01-06 03:47:26.998862: Current learning rate: 0.00348\n",
      "2024-01-06 03:49:31.550327: train_loss -0.9178\n",
      "2024-01-06 03:49:31.559329: val_loss -0.8273\n",
      "2024-01-06 03:49:31.567328: Pseudo dice [0.8524, 0.9557, 0.9361]\n",
      "2024-01-06 03:49:31.574328: Epoch time: 124.56 s\n",
      "2024-01-06 03:49:32.557924: \n",
      "2024-01-06 03:49:32.566174: Epoch 692\n",
      "2024-01-06 03:49:32.574179: Current learning rate: 0.00346\n",
      "2024-01-06 03:51:37.098698: train_loss -0.9212\n",
      "2024-01-06 03:51:37.107691: val_loss -0.8307\n",
      "2024-01-06 03:51:37.115702: Pseudo dice [0.8555, 0.9573, 0.9386]\n",
      "2024-01-06 03:51:37.121706: Epoch time: 124.54 s\n",
      "2024-01-06 03:51:38.102007: \n",
      "2024-01-06 03:51:38.110004: Epoch 693\n",
      "2024-01-06 03:51:38.114479: Current learning rate: 0.00345\n",
      "2024-01-06 03:53:42.679197: train_loss -0.9214\n",
      "2024-01-06 03:53:42.686194: val_loss -0.8291\n",
      "2024-01-06 03:53:42.692706: Pseudo dice [0.8552, 0.9576, 0.9391]\n",
      "2024-01-06 03:53:42.700707: Epoch time: 124.58 s\n",
      "2024-01-06 03:53:43.687992: \n",
      "2024-01-06 03:53:43.693513: Epoch 694\n",
      "2024-01-06 03:53:43.698021: Current learning rate: 0.00344\n",
      "2024-01-06 03:55:48.004010: train_loss -0.9201\n",
      "2024-01-06 03:55:48.013018: val_loss -0.8341\n",
      "2024-01-06 03:55:48.022019: Pseudo dice [0.859, 0.9583, 0.9398]\n",
      "2024-01-06 03:55:48.029007: Epoch time: 124.32 s\n",
      "2024-01-06 03:55:49.173110: \n",
      "2024-01-06 03:55:49.183211: Epoch 695\n",
      "2024-01-06 03:55:49.187273: Current learning rate: 0.00343\n",
      "2024-01-06 03:57:54.011278: train_loss -0.9186\n",
      "2024-01-06 03:57:54.017782: val_loss -0.8208\n",
      "2024-01-06 03:57:54.025784: Pseudo dice [0.8563, 0.9579, 0.9394]\n",
      "2024-01-06 03:57:54.032785: Epoch time: 124.84 s\n",
      "2024-01-06 03:57:55.003108: \n",
      "2024-01-06 03:57:55.011115: Epoch 696\n",
      "2024-01-06 03:57:55.015631: Current learning rate: 0.00342\n",
      "2024-01-06 03:59:59.598714: train_loss -0.9206\n",
      "2024-01-06 03:59:59.605713: val_loss -0.8314\n",
      "2024-01-06 03:59:59.611714: Pseudo dice [0.8586, 0.9581, 0.939]\n",
      "2024-01-06 03:59:59.616714: Epoch time: 124.6 s\n",
      "2024-01-06 04:00:00.624419: \n",
      "2024-01-06 04:00:00.632219: Epoch 697\n",
      "2024-01-06 04:00:00.644246: Current learning rate: 0.00341\n",
      "2024-01-06 04:02:05.432245: train_loss -0.9207\n",
      "2024-01-06 04:02:05.441252: val_loss -0.8307\n",
      "2024-01-06 04:02:05.448250: Pseudo dice [0.8633, 0.9588, 0.9406]\n",
      "2024-01-06 04:02:05.454249: Epoch time: 124.81 s\n",
      "2024-01-06 04:02:06.427451: \n",
      "2024-01-06 04:02:06.431935: Epoch 698\n",
      "2024-01-06 04:02:06.435336: Current learning rate: 0.0034\n",
      "2024-01-06 04:04:11.113599: train_loss -0.9203\n",
      "2024-01-06 04:04:11.122600: val_loss -0.8275\n",
      "2024-01-06 04:04:11.129601: Pseudo dice [0.857, 0.958, 0.9394]\n",
      "2024-01-06 04:04:11.134600: Epoch time: 124.69 s\n",
      "2024-01-06 04:04:12.100029: \n",
      "2024-01-06 04:04:12.109913: Epoch 699\n",
      "2024-01-06 04:04:12.113827: Current learning rate: 0.00339\n",
      "2024-01-06 04:06:17.012043: train_loss -0.9213\n",
      "2024-01-06 04:06:17.023040: val_loss -0.8236\n",
      "2024-01-06 04:06:17.035044: Pseudo dice [0.8592, 0.9579, 0.9394]\n",
      "2024-01-06 04:06:17.045038: Epoch time: 124.91 s\n",
      "2024-01-06 04:06:18.334624: \n",
      "2024-01-06 04:06:18.341430: Epoch 700\n",
      "2024-01-06 04:06:18.347440: Current learning rate: 0.00338\n",
      "2024-01-06 04:08:22.649249: train_loss -0.9237\n",
      "2024-01-06 04:08:22.659249: val_loss -0.8281\n",
      "2024-01-06 04:08:22.666250: Pseudo dice [0.8577, 0.9577, 0.9391]\n",
      "2024-01-06 04:08:22.671250: Epoch time: 124.32 s\n",
      "2024-01-06 04:08:23.684362: \n",
      "2024-01-06 04:08:23.693964: Epoch 701\n",
      "2024-01-06 04:08:23.698489: Current learning rate: 0.00337\n",
      "2024-01-06 04:10:28.122138: train_loss -0.9211\n",
      "2024-01-06 04:10:28.129140: val_loss -0.8341\n",
      "2024-01-06 04:10:28.138140: Pseudo dice [0.8623, 0.9588, 0.9404]\n",
      "2024-01-06 04:10:28.145138: Epoch time: 124.44 s\n",
      "2024-01-06 04:10:29.272757: \n",
      "2024-01-06 04:10:29.279829: Epoch 702\n",
      "2024-01-06 04:10:29.284834: Current learning rate: 0.00336\n",
      "2024-01-06 04:12:34.127172: train_loss -0.9184\n",
      "2024-01-06 04:12:34.136182: val_loss -0.8205\n",
      "2024-01-06 04:12:34.144177: Pseudo dice [0.8554, 0.958, 0.9397]\n",
      "2024-01-06 04:12:34.152376: Epoch time: 124.86 s\n",
      "2024-01-06 04:12:35.121166: \n",
      "2024-01-06 04:12:35.128658: Epoch 703\n",
      "2024-01-06 04:12:35.134641: Current learning rate: 0.00335\n",
      "2024-01-06 04:14:40.113626: train_loss -0.9191\n",
      "2024-01-06 04:14:40.122207: val_loss -0.8354\n",
      "2024-01-06 04:14:40.130207: Pseudo dice [0.8572, 0.958, 0.9396]\n",
      "2024-01-06 04:14:40.138207: Epoch time: 124.99 s\n",
      "2024-01-06 04:14:41.108481: \n",
      "2024-01-06 04:14:41.117097: Epoch 704\n",
      "2024-01-06 04:14:41.126976: Current learning rate: 0.00334\n",
      "2024-01-06 04:16:45.888699: train_loss -0.9194\n",
      "2024-01-06 04:16:45.898699: val_loss -0.8356\n",
      "2024-01-06 04:16:45.906738: Pseudo dice [0.8613, 0.9598, 0.9425]\n",
      "2024-01-06 04:16:45.912741: Epoch time: 124.78 s\n",
      "2024-01-06 04:16:46.888674: \n",
      "2024-01-06 04:16:46.893675: Epoch 705\n",
      "2024-01-06 04:16:46.898286: Current learning rate: 0.00333\n",
      "2024-01-06 04:18:51.813973: train_loss -0.9221\n",
      "2024-01-06 04:18:51.821975: val_loss -0.8362\n",
      "2024-01-06 04:18:51.826981: Pseudo dice [0.8613, 0.9585, 0.9407]\n",
      "2024-01-06 04:18:51.832977: Epoch time: 124.93 s\n",
      "2024-01-06 04:18:52.815846: \n",
      "2024-01-06 04:18:52.822921: Epoch 706\n",
      "2024-01-06 04:18:52.826924: Current learning rate: 0.00332\n",
      "2024-01-06 04:20:57.692228: train_loss -0.9181\n",
      "2024-01-06 04:20:57.701228: val_loss -0.8307\n",
      "2024-01-06 04:20:57.709226: Pseudo dice [0.8618, 0.9588, 0.9406]\n",
      "2024-01-06 04:20:57.715226: Epoch time: 124.88 s\n",
      "2024-01-06 04:20:58.691151: \n",
      "2024-01-06 04:20:58.697171: Epoch 707\n",
      "2024-01-06 04:20:58.701961: Current learning rate: 0.00331\n",
      "2024-01-06 04:23:03.331065: train_loss -0.9207\n",
      "2024-01-06 04:23:03.340065: val_loss -0.838\n",
      "2024-01-06 04:23:03.346066: Pseudo dice [0.8609, 0.9598, 0.9421]\n",
      "2024-01-06 04:23:03.351066: Epoch time: 124.64 s\n",
      "2024-01-06 04:23:04.331934: \n",
      "2024-01-06 04:23:04.340786: Epoch 708\n",
      "2024-01-06 04:23:04.345855: Current learning rate: 0.0033\n",
      "2024-01-06 04:25:09.104062: train_loss -0.9188\n",
      "2024-01-06 04:25:09.115062: val_loss -0.8391\n",
      "2024-01-06 04:25:09.123062: Pseudo dice [0.8594, 0.9602, 0.9424]\n",
      "2024-01-06 04:25:09.129061: Epoch time: 124.77 s\n",
      "2024-01-06 04:25:10.301376: \n",
      "2024-01-06 04:25:10.307131: Epoch 709\n",
      "2024-01-06 04:25:10.311224: Current learning rate: 0.00329\n",
      "2024-01-06 04:27:15.206022: train_loss -0.9196\n",
      "2024-01-06 04:27:15.215022: val_loss -0.839\n",
      "2024-01-06 04:27:15.222024: Pseudo dice [0.8635, 0.9591, 0.942]\n",
      "2024-01-06 04:27:15.230034: Epoch time: 124.91 s\n",
      "2024-01-06 04:27:16.222362: \n",
      "2024-01-06 04:27:16.227384: Epoch 710\n",
      "2024-01-06 04:27:16.232388: Current learning rate: 0.00328\n",
      "2024-01-06 04:29:20.661880: train_loss -0.9214\n",
      "2024-01-06 04:29:20.670880: val_loss -0.8396\n",
      "2024-01-06 04:29:20.679890: Pseudo dice [0.8618, 0.9595, 0.9417]\n",
      "2024-01-06 04:29:20.686892: Epoch time: 124.44 s\n",
      "2024-01-06 04:29:21.701014: \n",
      "2024-01-06 04:29:21.706002: Epoch 711\n",
      "2024-01-06 04:29:21.714014: Current learning rate: 0.00327\n",
      "2024-01-06 04:31:26.356304: train_loss -0.9209\n",
      "2024-01-06 04:31:26.364306: val_loss -0.8358\n",
      "2024-01-06 04:31:26.374815: Pseudo dice [0.8624, 0.9593, 0.9415]\n",
      "2024-01-06 04:31:26.382815: Epoch time: 124.66 s\n",
      "2024-01-06 04:31:27.369694: \n",
      "2024-01-06 04:31:27.375471: Epoch 712\n",
      "2024-01-06 04:31:27.379543: Current learning rate: 0.00326\n",
      "2024-01-06 04:33:32.039057: train_loss -0.92\n",
      "2024-01-06 04:33:32.047058: val_loss -0.8285\n",
      "2024-01-06 04:33:32.053062: Pseudo dice [0.861, 0.9594, 0.9413]\n",
      "2024-01-06 04:33:32.060060: Epoch time: 124.67 s\n",
      "2024-01-06 04:33:33.030608: \n",
      "2024-01-06 04:33:33.035724: Epoch 713\n",
      "2024-01-06 04:33:33.040652: Current learning rate: 0.00325\n",
      "2024-01-06 04:35:37.701541: train_loss -0.9188\n",
      "2024-01-06 04:35:37.710551: val_loss -0.8353\n",
      "2024-01-06 04:35:37.716540: Pseudo dice [0.8623, 0.96, 0.9419]\n",
      "2024-01-06 04:35:37.721546: Epoch time: 124.67 s\n",
      "2024-01-06 04:35:38.688734: \n",
      "2024-01-06 04:35:38.694725: Epoch 714\n",
      "2024-01-06 04:35:38.699068: Current learning rate: 0.00324\n",
      "2024-01-06 04:37:43.017040: train_loss -0.8991\n",
      "2024-01-06 04:37:43.025550: val_loss -0.8205\n",
      "2024-01-06 04:37:43.033549: Pseudo dice [0.8557, 0.9547, 0.9355]\n",
      "2024-01-06 04:37:43.039549: Epoch time: 124.33 s\n",
      "2024-01-06 04:37:44.018721: \n",
      "2024-01-06 04:37:44.028248: Epoch 715\n",
      "2024-01-06 04:37:44.036865: Current learning rate: 0.00323\n",
      "2024-01-06 04:39:48.146120: train_loss -0.8908\n",
      "2024-01-06 04:39:48.157122: val_loss -0.8322\n",
      "2024-01-06 04:39:48.163122: Pseudo dice [0.8605, 0.957, 0.9375]\n",
      "2024-01-06 04:39:48.170633: Epoch time: 124.13 s\n",
      "2024-01-06 04:39:49.162822: \n",
      "2024-01-06 04:39:49.167821: Epoch 716\n",
      "2024-01-06 04:39:49.172340: Current learning rate: 0.00322\n",
      "2024-01-06 04:41:53.307952: train_loss -0.9021\n",
      "2024-01-06 04:41:53.317460: val_loss -0.8273\n",
      "2024-01-06 04:41:53.330178: Pseudo dice [0.8592, 0.958, 0.9391]\n",
      "2024-01-06 04:41:53.337181: Epoch time: 124.15 s\n",
      "2024-01-06 04:41:54.503417: \n",
      "2024-01-06 04:41:54.511930: Epoch 717\n",
      "2024-01-06 04:41:54.517499: Current learning rate: 0.00321\n",
      "2024-01-06 04:43:58.571527: train_loss -0.9077\n",
      "2024-01-06 04:43:58.580523: val_loss -0.8387\n",
      "2024-01-06 04:43:58.588524: Pseudo dice [0.8585, 0.9606, 0.9436]\n",
      "2024-01-06 04:43:58.595523: Epoch time: 124.07 s\n",
      "2024-01-06 04:43:59.576490: \n",
      "2024-01-06 04:43:59.587246: Epoch 718\n",
      "2024-01-06 04:43:59.591169: Current learning rate: 0.0032\n",
      "2024-01-06 04:46:03.666851: train_loss -0.9129\n",
      "2024-01-06 04:46:03.675844: val_loss -0.8352\n",
      "2024-01-06 04:46:03.682844: Pseudo dice [0.8618, 0.9589, 0.9403]\n",
      "2024-01-06 04:46:03.688915: Epoch time: 124.09 s\n",
      "2024-01-06 04:46:04.663033: \n",
      "2024-01-06 04:46:04.672899: Epoch 719\n",
      "2024-01-06 04:46:04.677067: Current learning rate: 0.00319\n",
      "2024-01-06 04:48:08.810601: train_loss -0.9155\n",
      "2024-01-06 04:48:08.819599: val_loss -0.8397\n",
      "2024-01-06 04:48:08.828600: Pseudo dice [0.8673, 0.9594, 0.9422]\n",
      "2024-01-06 04:48:08.836600: Epoch time: 124.15 s\n",
      "2024-01-06 04:48:09.821939: \n",
      "2024-01-06 04:48:09.828009: Epoch 720\n",
      "2024-01-06 04:48:09.831995: Current learning rate: 0.00318\n",
      "2024-01-06 04:50:13.632019: train_loss -0.9189\n",
      "2024-01-06 04:50:13.640007: val_loss -0.84\n",
      "2024-01-06 04:50:13.645007: Pseudo dice [0.8632, 0.9595, 0.9421]\n",
      "2024-01-06 04:50:13.650008: Epoch time: 123.81 s\n",
      "2024-01-06 04:50:14.625681: \n",
      "2024-01-06 04:50:14.633095: Epoch 721\n",
      "2024-01-06 04:50:14.637113: Current learning rate: 0.00317\n",
      "2024-01-06 04:52:18.698848: train_loss -0.9176\n",
      "2024-01-06 04:52:18.706846: val_loss -0.8326\n",
      "2024-01-06 04:52:18.713848: Pseudo dice [0.8621, 0.9591, 0.9407]\n",
      "2024-01-06 04:52:18.719845: Epoch time: 124.07 s\n",
      "2024-01-06 04:52:19.699577: \n",
      "2024-01-06 04:52:19.705365: Epoch 722\n",
      "2024-01-06 04:52:19.710359: Current learning rate: 0.00316\n",
      "2024-01-06 04:54:23.874515: train_loss -0.9183\n",
      "2024-01-06 04:54:23.881512: val_loss -0.839\n",
      "2024-01-06 04:54:23.889516: Pseudo dice [0.8642, 0.9594, 0.9415]\n",
      "2024-01-06 04:54:23.894512: Epoch time: 124.18 s\n",
      "2024-01-06 04:54:24.872859: \n",
      "2024-01-06 04:54:24.879206: Epoch 723\n",
      "2024-01-06 04:54:24.883275: Current learning rate: 0.00315\n",
      "2024-01-06 04:56:29.161055: train_loss -0.9174\n",
      "2024-01-06 04:56:29.171045: val_loss -0.8249\n",
      "2024-01-06 04:56:29.178042: Pseudo dice [0.8671, 0.9583, 0.9401]\n",
      "2024-01-06 04:56:29.185043: Epoch time: 124.29 s\n",
      "2024-01-06 04:56:30.323280: \n",
      "2024-01-06 04:56:30.334904: Epoch 724\n",
      "2024-01-06 04:56:30.338920: Current learning rate: 0.00314\n",
      "2024-01-06 04:58:34.560773: train_loss -0.9182\n",
      "2024-01-06 04:58:34.569766: val_loss -0.8362\n",
      "2024-01-06 04:58:34.577778: Pseudo dice [0.8646, 0.9585, 0.9405]\n",
      "2024-01-06 04:58:34.584764: Epoch time: 124.24 s\n",
      "2024-01-06 04:58:35.558538: \n",
      "2024-01-06 04:58:35.567538: Epoch 725\n",
      "2024-01-06 04:58:35.572551: Current learning rate: 0.00313\n",
      "2024-01-06 05:00:39.517509: train_loss -0.9204\n",
      "2024-01-06 05:00:39.525506: val_loss -0.8323\n",
      "2024-01-06 05:00:39.530507: Pseudo dice [0.8636, 0.9586, 0.9406]\n",
      "2024-01-06 05:00:39.536507: Epoch time: 123.96 s\n",
      "2024-01-06 05:00:40.518069: \n",
      "2024-01-06 05:00:40.526538: Epoch 726\n",
      "2024-01-06 05:00:40.534585: Current learning rate: 0.00312\n",
      "2024-01-06 05:02:44.899307: train_loss -0.9186\n",
      "2024-01-06 05:02:44.909384: val_loss -0.8324\n",
      "2024-01-06 05:02:44.918311: Pseudo dice [0.8586, 0.9596, 0.941]\n",
      "2024-01-06 05:02:44.925375: Epoch time: 124.38 s\n",
      "2024-01-06 05:02:45.908742: \n",
      "2024-01-06 05:02:45.914943: Epoch 727\n",
      "2024-01-06 05:02:45.919014: Current learning rate: 0.00311\n",
      "2024-01-06 05:04:50.046421: train_loss -0.9189\n",
      "2024-01-06 05:04:50.053420: val_loss -0.8411\n",
      "2024-01-06 05:04:50.061420: Pseudo dice [0.8643, 0.959, 0.941]\n",
      "2024-01-06 05:04:50.067420: Epoch time: 124.14 s\n",
      "2024-01-06 05:04:51.043663: \n",
      "2024-01-06 05:04:51.056860: Epoch 728\n",
      "2024-01-06 05:04:51.061843: Current learning rate: 0.0031\n",
      "2024-01-06 05:06:54.981313: train_loss -0.922\n",
      "2024-01-06 05:06:54.988313: val_loss -0.842\n",
      "2024-01-06 05:06:54.996316: Pseudo dice [0.8625, 0.9601, 0.9432]\n",
      "2024-01-06 05:06:55.005316: Epoch time: 123.94 s\n",
      "2024-01-06 05:06:55.978944: \n",
      "2024-01-06 05:06:55.984945: Epoch 729\n",
      "2024-01-06 05:06:55.989946: Current learning rate: 0.00309\n",
      "2024-01-06 05:09:00.158055: train_loss -0.9187\n",
      "2024-01-06 05:09:00.168055: val_loss -0.8378\n",
      "2024-01-06 05:09:00.177051: Pseudo dice [0.863, 0.9593, 0.9418]\n",
      "2024-01-06 05:09:00.184053: Epoch time: 124.18 s\n",
      "2024-01-06 05:09:01.153922: \n",
      "2024-01-06 05:09:01.159954: Epoch 730\n",
      "2024-01-06 05:09:01.163991: Current learning rate: 0.00308\n",
      "2024-01-06 05:11:05.580755: train_loss -0.918\n",
      "2024-01-06 05:11:05.590755: val_loss -0.8372\n",
      "2024-01-06 05:11:05.599834: Pseudo dice [0.8638, 0.9585, 0.9402]\n",
      "2024-01-06 05:11:05.605758: Epoch time: 124.43 s\n",
      "2024-01-06 05:11:06.577760: \n",
      "2024-01-06 05:11:06.583215: Epoch 731\n",
      "2024-01-06 05:11:06.587297: Current learning rate: 0.00307\n",
      "2024-01-06 05:13:10.973308: train_loss -0.9183\n",
      "2024-01-06 05:13:10.982302: val_loss -0.8261\n",
      "2024-01-06 05:13:10.989830: Pseudo dice [0.8621, 0.958, 0.9397]\n",
      "2024-01-06 05:13:10.996831: Epoch time: 124.4 s\n",
      "2024-01-06 05:13:12.159928: \n",
      "2024-01-06 05:13:12.169019: Epoch 732\n",
      "2024-01-06 05:13:12.174082: Current learning rate: 0.00306\n",
      "2024-01-06 05:15:16.590491: train_loss -0.9191\n",
      "2024-01-06 05:15:16.600483: val_loss -0.8318\n",
      "2024-01-06 05:15:16.611483: Pseudo dice [0.8606, 0.9596, 0.942]\n",
      "2024-01-06 05:15:16.620483: Epoch time: 124.43 s\n",
      "2024-01-06 05:15:17.623866: \n",
      "2024-01-06 05:15:17.633487: Epoch 733\n",
      "2024-01-06 05:15:17.638069: Current learning rate: 0.00305\n",
      "2024-01-06 05:17:21.890778: train_loss -0.9207\n",
      "2024-01-06 05:17:21.899784: val_loss -0.8416\n",
      "2024-01-06 05:17:21.907780: Pseudo dice [0.864, 0.9601, 0.9431]\n",
      "2024-01-06 05:17:21.915778: Epoch time: 124.27 s\n",
      "2024-01-06 05:17:22.902320: \n",
      "2024-01-06 05:17:22.908320: Epoch 734\n",
      "2024-01-06 05:17:22.913312: Current learning rate: 0.00304\n",
      "2024-01-06 05:19:27.527531: train_loss -0.9198\n",
      "2024-01-06 05:19:27.537530: val_loss -0.8399\n",
      "2024-01-06 05:19:27.544531: Pseudo dice [0.866, 0.9591, 0.9416]\n",
      "2024-01-06 05:19:27.552529: Epoch time: 124.63 s\n",
      "2024-01-06 05:19:28.545701: \n",
      "2024-01-06 05:19:28.553208: Epoch 735\n",
      "2024-01-06 05:19:28.559108: Current learning rate: 0.00303\n",
      "2024-01-06 05:21:33.027290: train_loss -0.9186\n",
      "2024-01-06 05:21:33.039613: val_loss -0.83\n",
      "2024-01-06 05:21:33.049931: Pseudo dice [0.8622, 0.9587, 0.9403]\n",
      "2024-01-06 05:21:33.060466: Epoch time: 124.48 s\n",
      "2024-01-06 05:21:34.093539: \n",
      "2024-01-06 05:21:34.099540: Epoch 736\n",
      "2024-01-06 05:21:34.104542: Current learning rate: 0.00302\n",
      "2024-01-06 05:23:38.581720: train_loss -0.9218\n",
      "2024-01-06 05:23:38.590225: val_loss -0.8359\n",
      "2024-01-06 05:23:38.596225: Pseudo dice [0.866, 0.9597, 0.9422]\n",
      "2024-01-06 05:23:38.603227: Epoch time: 124.49 s\n",
      "2024-01-06 05:23:39.589120: \n",
      "2024-01-06 05:23:39.597462: Epoch 737\n",
      "2024-01-06 05:23:39.602467: Current learning rate: 0.00301\n",
      "2024-01-06 05:25:43.833347: train_loss -0.9204\n",
      "2024-01-06 05:25:43.840348: val_loss -0.8306\n",
      "2024-01-06 05:25:43.846854: Pseudo dice [0.8633, 0.9591, 0.9412]\n",
      "2024-01-06 05:25:43.852856: Epoch time: 124.25 s\n",
      "2024-01-06 05:25:44.844016: \n",
      "2024-01-06 05:25:44.851628: Epoch 738\n",
      "2024-01-06 05:25:44.855672: Current learning rate: 0.003\n",
      "2024-01-06 05:27:49.222315: train_loss -0.9223\n",
      "2024-01-06 05:27:49.232332: val_loss -0.8394\n",
      "2024-01-06 05:27:49.241333: Pseudo dice [0.8648, 0.9601, 0.9429]\n",
      "2024-01-06 05:27:49.248316: Epoch time: 124.38 s\n",
      "2024-01-06 05:27:50.395335: \n",
      "2024-01-06 05:27:50.403005: Epoch 739\n",
      "2024-01-06 05:27:50.407589: Current learning rate: 0.00299\n",
      "2024-01-06 05:29:54.922344: train_loss -0.9193\n",
      "2024-01-06 05:29:54.931341: val_loss -0.8409\n",
      "2024-01-06 05:29:54.937350: Pseudo dice [0.867, 0.9601, 0.9434]\n",
      "2024-01-06 05:29:54.950316: Epoch time: 124.53 s\n",
      "2024-01-06 05:29:55.928248: \n",
      "2024-01-06 05:29:55.937553: Epoch 740\n",
      "2024-01-06 05:29:55.946713: Current learning rate: 0.00297\n",
      "2024-01-06 05:32:00.644202: train_loss -0.9166\n",
      "2024-01-06 05:32:00.653203: val_loss -0.8339\n",
      "2024-01-06 05:32:00.661203: Pseudo dice [0.8666, 0.9593, 0.9416]\n",
      "2024-01-06 05:32:00.668203: Epoch time: 124.72 s\n",
      "2024-01-06 05:32:01.649657: \n",
      "2024-01-06 05:32:01.659286: Epoch 741\n",
      "2024-01-06 05:32:01.663346: Current learning rate: 0.00296\n",
      "2024-01-06 05:34:06.365859: train_loss -0.917\n",
      "2024-01-06 05:34:06.374865: val_loss -0.8275\n",
      "2024-01-06 05:34:06.381865: Pseudo dice [0.8538, 0.9562, 0.9364]\n",
      "2024-01-06 05:34:06.387866: Epoch time: 124.72 s\n",
      "2024-01-06 05:34:07.369726: \n",
      "2024-01-06 05:34:07.375725: Epoch 742\n",
      "2024-01-06 05:34:07.380031: Current learning rate: 0.00295\n",
      "2024-01-06 05:36:11.859251: train_loss -0.9184\n",
      "2024-01-06 05:36:11.869249: val_loss -0.8352\n",
      "2024-01-06 05:36:11.878249: Pseudo dice [0.8644, 0.9593, 0.9415]\n",
      "2024-01-06 05:36:11.886266: Epoch time: 124.49 s\n",
      "2024-01-06 05:36:12.893325: \n",
      "2024-01-06 05:36:12.898657: Epoch 743\n",
      "2024-01-06 05:36:12.903724: Current learning rate: 0.00294\n",
      "2024-01-06 05:38:17.639848: train_loss -0.9155\n",
      "2024-01-06 05:38:17.648854: val_loss -0.8339\n",
      "2024-01-06 05:38:17.655857: Pseudo dice [0.8614, 0.9593, 0.9415]\n",
      "2024-01-06 05:38:17.662399: Epoch time: 124.75 s\n",
      "2024-01-06 05:38:18.654291: \n",
      "2024-01-06 05:38:18.660622: Epoch 744\n",
      "2024-01-06 05:38:18.667199: Current learning rate: 0.00293\n",
      "2024-01-06 05:40:23.242910: train_loss -0.9193\n",
      "2024-01-06 05:40:23.251910: val_loss -0.8402\n",
      "2024-01-06 05:40:23.259910: Pseudo dice [0.864, 0.9599, 0.9422]\n",
      "2024-01-06 05:40:23.266908: Epoch time: 124.59 s\n",
      "2024-01-06 05:40:24.235141: \n",
      "2024-01-06 05:40:24.242203: Epoch 745\n",
      "2024-01-06 05:40:24.248217: Current learning rate: 0.00292\n",
      "2024-01-06 05:42:28.613397: train_loss -0.9211\n",
      "2024-01-06 05:42:28.620394: val_loss -0.8357\n",
      "2024-01-06 05:42:28.627400: Pseudo dice [0.8619, 0.9593, 0.9413]\n",
      "2024-01-06 05:42:28.633426: Epoch time: 124.38 s\n",
      "2024-01-06 05:42:29.626884: \n",
      "2024-01-06 05:42:29.631954: Epoch 746\n",
      "2024-01-06 05:42:29.636954: Current learning rate: 0.00291\n",
      "2024-01-06 05:44:34.329387: train_loss -0.9205\n",
      "2024-01-06 05:44:34.337391: val_loss -0.836\n",
      "2024-01-06 05:44:34.345389: Pseudo dice [0.8636, 0.96, 0.9423]\n",
      "2024-01-06 05:44:34.351897: Epoch time: 124.7 s\n",
      "2024-01-06 05:44:35.661721: \n",
      "2024-01-06 05:44:35.669665: Epoch 747\n",
      "2024-01-06 05:44:35.674780: Current learning rate: 0.0029\n",
      "2024-01-06 05:46:40.194849: train_loss -0.9196\n",
      "2024-01-06 05:46:40.205850: val_loss -0.8444\n",
      "2024-01-06 05:46:40.214851: Pseudo dice [0.8675, 0.96, 0.9424]\n",
      "2024-01-06 05:46:40.223857: Epoch time: 124.53 s\n",
      "2024-01-06 05:46:41.245120: \n",
      "2024-01-06 05:46:41.254176: Epoch 748\n",
      "2024-01-06 05:46:41.259055: Current learning rate: 0.00289\n",
      "2024-01-06 05:48:45.874723: train_loss -0.9202\n",
      "2024-01-06 05:48:45.884236: val_loss -0.8367\n",
      "2024-01-06 05:48:45.891236: Pseudo dice [0.8664, 0.9595, 0.9411]\n",
      "2024-01-06 05:48:45.898236: Epoch time: 124.63 s\n",
      "2024-01-06 05:48:46.927263: \n",
      "2024-01-06 05:48:46.934819: Epoch 749\n",
      "2024-01-06 05:48:46.939833: Current learning rate: 0.00288\n",
      "2024-01-06 05:50:51.366277: train_loss -0.9235\n",
      "2024-01-06 05:50:51.375279: val_loss -0.8401\n",
      "2024-01-06 05:50:51.383277: Pseudo dice [0.8665, 0.9597, 0.9419]\n",
      "2024-01-06 05:50:51.391284: Epoch time: 124.44 s\n",
      "2024-01-06 05:50:52.634274: \n",
      "2024-01-06 05:50:52.643480: Epoch 750\n",
      "2024-01-06 05:50:52.648539: Current learning rate: 0.00287\n",
      "2024-01-06 05:52:57.373654: train_loss -0.9208\n",
      "2024-01-06 05:52:57.382645: val_loss -0.8342\n",
      "2024-01-06 05:52:57.390656: Pseudo dice [0.8591, 0.9589, 0.9404]\n",
      "2024-01-06 05:52:57.398649: Epoch time: 124.74 s\n",
      "2024-01-06 05:52:58.383189: \n",
      "2024-01-06 05:52:58.400308: Epoch 751\n",
      "2024-01-06 05:52:58.406864: Current learning rate: 0.00286\n",
      "2024-01-06 05:55:03.105681: train_loss -0.9218\n",
      "2024-01-06 05:55:03.117685: val_loss -0.8371\n",
      "2024-01-06 05:55:03.126682: Pseudo dice [0.8607, 0.9598, 0.9424]\n",
      "2024-01-06 05:55:03.134688: Epoch time: 124.72 s\n",
      "2024-01-06 05:55:04.166689: \n",
      "2024-01-06 05:55:04.172704: Epoch 752\n",
      "2024-01-06 05:55:04.180752: Current learning rate: 0.00285\n",
      "2024-01-06 05:57:08.756926: train_loss -0.92\n",
      "2024-01-06 05:57:08.766431: val_loss -0.839\n",
      "2024-01-06 05:57:08.774439: Pseudo dice [0.8635, 0.9593, 0.9412]\n",
      "2024-01-06 05:57:08.782439: Epoch time: 124.59 s\n",
      "2024-01-06 05:57:09.774579: \n",
      "2024-01-06 05:57:09.786313: Epoch 753\n",
      "2024-01-06 05:57:09.791304: Current learning rate: 0.00284\n",
      "2024-01-06 05:59:14.519133: train_loss -0.9208\n",
      "2024-01-06 05:59:14.529133: val_loss -0.8299\n",
      "2024-01-06 05:59:14.538142: Pseudo dice [0.8632, 0.9587, 0.94]\n",
      "2024-01-06 05:59:14.546134: Epoch time: 124.75 s\n",
      "2024-01-06 05:59:15.550160: \n",
      "2024-01-06 05:59:15.557244: Epoch 754\n",
      "2024-01-06 05:59:15.562225: Current learning rate: 0.00283\n",
      "2024-01-06 06:01:20.143824: train_loss -0.9218\n",
      "2024-01-06 06:01:20.151823: val_loss -0.8384\n",
      "2024-01-06 06:01:20.157334: Pseudo dice [0.862, 0.96, 0.9428]\n",
      "2024-01-06 06:01:20.167338: Epoch time: 124.59 s\n",
      "2024-01-06 06:01:21.306489: \n",
      "2024-01-06 06:01:21.317653: Epoch 755\n",
      "2024-01-06 06:01:21.321732: Current learning rate: 0.00282\n",
      "2024-01-06 06:03:26.068321: train_loss -0.9192\n",
      "2024-01-06 06:03:26.078323: val_loss -0.8355\n",
      "2024-01-06 06:03:26.086321: Pseudo dice [0.863, 0.96, 0.9423]\n",
      "2024-01-06 06:03:26.091321: Epoch time: 124.76 s\n",
      "2024-01-06 06:03:27.063945: \n",
      "2024-01-06 06:03:27.070015: Epoch 756\n",
      "2024-01-06 06:03:27.075012: Current learning rate: 0.00281\n",
      "2024-01-06 06:05:31.684095: train_loss -0.9227\n",
      "2024-01-06 06:05:31.692098: val_loss -0.8372\n",
      "2024-01-06 06:05:31.698605: Pseudo dice [0.8609, 0.9595, 0.9416]\n",
      "2024-01-06 06:05:31.704605: Epoch time: 124.62 s\n",
      "2024-01-06 06:05:32.686280: \n",
      "2024-01-06 06:05:32.694910: Epoch 757\n",
      "2024-01-06 06:05:32.699419: Current learning rate: 0.0028\n",
      "2024-01-06 06:07:37.541637: train_loss -0.9195\n",
      "2024-01-06 06:07:37.550632: val_loss -0.8384\n",
      "2024-01-06 06:07:37.556632: Pseudo dice [0.863, 0.9593, 0.9417]\n",
      "2024-01-06 06:07:37.562638: Epoch time: 124.86 s\n",
      "2024-01-06 06:07:38.541404: \n",
      "2024-01-06 06:07:38.550612: Epoch 758\n",
      "2024-01-06 06:07:38.555195: Current learning rate: 0.00279\n",
      "2024-01-06 06:09:43.246410: train_loss -0.923\n",
      "2024-01-06 06:09:43.255410: val_loss -0.8395\n",
      "2024-01-06 06:09:43.263851: Pseudo dice [0.8619, 0.9608, 0.9432]\n",
      "2024-01-06 06:09:43.271850: Epoch time: 124.71 s\n",
      "2024-01-06 06:09:44.256647: \n",
      "2024-01-06 06:09:44.267768: Epoch 759\n",
      "2024-01-06 06:09:44.272695: Current learning rate: 0.00278\n",
      "2024-01-06 06:11:49.019007: train_loss -0.9198\n",
      "2024-01-06 06:11:49.029004: val_loss -0.844\n",
      "2024-01-06 06:11:49.038551: Pseudo dice [0.8627, 0.9592, 0.9412]\n",
      "2024-01-06 06:11:49.043550: Epoch time: 124.76 s\n",
      "2024-01-06 06:11:50.017769: \n",
      "2024-01-06 06:11:50.023707: Epoch 760\n",
      "2024-01-06 06:11:50.028708: Current learning rate: 0.00277\n",
      "2024-01-06 06:13:54.643909: train_loss -0.921\n",
      "2024-01-06 06:13:54.654904: val_loss -0.8388\n",
      "2024-01-06 06:13:54.663908: Pseudo dice [0.8632, 0.9603, 0.943]\n",
      "2024-01-06 06:13:54.669905: Epoch time: 124.63 s\n",
      "2024-01-06 06:13:55.645632: \n",
      "2024-01-06 06:13:55.654811: Epoch 761\n",
      "2024-01-06 06:13:55.659346: Current learning rate: 0.00276\n",
      "2024-01-06 06:16:00.345237: train_loss -0.9229\n",
      "2024-01-06 06:16:00.355239: val_loss -0.8374\n",
      "2024-01-06 06:16:00.364241: Pseudo dice [0.8637, 0.9591, 0.9414]\n",
      "2024-01-06 06:16:00.370247: Epoch time: 124.7 s\n",
      "2024-01-06 06:16:01.501064: \n",
      "2024-01-06 06:16:01.510141: Epoch 762\n",
      "2024-01-06 06:16:01.515153: Current learning rate: 0.00275\n",
      "2024-01-06 06:18:06.292241: train_loss -0.9177\n",
      "2024-01-06 06:18:06.302245: val_loss -0.8396\n",
      "2024-01-06 06:18:06.310251: Pseudo dice [0.8639, 0.9593, 0.9418]\n",
      "2024-01-06 06:18:06.317255: Epoch time: 124.79 s\n",
      "2024-01-06 06:18:07.315867: \n",
      "2024-01-06 06:18:07.320868: Epoch 763\n",
      "2024-01-06 06:18:07.325386: Current learning rate: 0.00274\n",
      "2024-01-06 06:20:12.314191: train_loss -0.9195\n",
      "2024-01-06 06:20:12.324193: val_loss -0.8364\n",
      "2024-01-06 06:20:12.335202: Pseudo dice [0.8667, 0.9592, 0.9416]\n",
      "2024-01-06 06:20:12.344196: Epoch time: 125.0 s\n",
      "2024-01-06 06:20:13.342700: \n",
      "2024-01-06 06:20:13.351299: Epoch 764\n",
      "2024-01-06 06:20:13.357182: Current learning rate: 0.00273\n",
      "2024-01-06 06:22:17.799042: train_loss -0.9231\n",
      "2024-01-06 06:22:17.810965: val_loss -0.8328\n",
      "2024-01-06 06:22:17.819473: Pseudo dice [0.8586, 0.9586, 0.9407]\n",
      "2024-01-06 06:22:17.825473: Epoch time: 124.46 s\n",
      "2024-01-06 06:22:18.821527: \n",
      "2024-01-06 06:22:18.828599: Epoch 765\n",
      "2024-01-06 06:22:18.833588: Current learning rate: 0.00272\n",
      "2024-01-06 06:24:23.462130: train_loss -0.9227\n",
      "2024-01-06 06:24:23.472130: val_loss -0.8359\n",
      "2024-01-06 06:24:23.481638: Pseudo dice [0.8627, 0.9597, 0.9419]\n",
      "2024-01-06 06:24:23.488645: Epoch time: 124.64 s\n",
      "2024-01-06 06:24:24.490335: \n",
      "2024-01-06 06:24:24.499953: Epoch 766\n",
      "2024-01-06 06:24:24.505030: Current learning rate: 0.00271\n",
      "2024-01-06 06:26:29.144957: train_loss -0.9213\n",
      "2024-01-06 06:26:29.152955: val_loss -0.8275\n",
      "2024-01-06 06:26:29.158957: Pseudo dice [0.8639, 0.9598, 0.9424]\n",
      "2024-01-06 06:26:29.166957: Epoch time: 124.66 s\n",
      "2024-01-06 06:26:30.162961: \n",
      "2024-01-06 06:26:30.172113: Epoch 767\n",
      "2024-01-06 06:26:30.177111: Current learning rate: 0.0027\n",
      "2024-01-06 06:28:34.922748: train_loss -0.92\n",
      "2024-01-06 06:28:34.929748: val_loss -0.8416\n",
      "2024-01-06 06:28:34.935748: Pseudo dice [0.8662, 0.9595, 0.9418]\n",
      "2024-01-06 06:28:34.941750: Epoch time: 124.76 s\n",
      "2024-01-06 06:28:35.937490: \n",
      "2024-01-06 06:28:35.942482: Epoch 768\n",
      "2024-01-06 06:28:35.947426: Current learning rate: 0.00268\n",
      "2024-01-06 06:30:40.819229: train_loss -0.9231\n",
      "2024-01-06 06:30:40.829219: val_loss -0.8383\n",
      "2024-01-06 06:30:40.836218: Pseudo dice [0.8627, 0.9598, 0.9422]\n",
      "2024-01-06 06:30:40.842218: Epoch time: 124.88 s\n",
      "2024-01-06 06:30:41.991475: \n",
      "2024-01-06 06:30:42.002173: Epoch 769\n",
      "2024-01-06 06:30:42.007173: Current learning rate: 0.00267\n",
      "2024-01-06 06:32:46.945126: train_loss -0.9209\n",
      "2024-01-06 06:32:46.954138: val_loss -0.8342\n",
      "2024-01-06 06:32:46.963135: Pseudo dice [0.8629, 0.9596, 0.9418]\n",
      "2024-01-06 06:32:46.971645: Epoch time: 124.95 s\n",
      "2024-01-06 06:32:48.038250: \n",
      "2024-01-06 06:32:48.044477: Epoch 770\n",
      "2024-01-06 06:32:48.049558: Current learning rate: 0.00266\n",
      "2024-01-06 06:34:52.408431: train_loss -0.9227\n",
      "2024-01-06 06:34:52.417431: val_loss -0.8343\n",
      "2024-01-06 06:34:52.424944: Pseudo dice [0.8609, 0.9596, 0.9418]\n",
      "2024-01-06 06:34:52.433944: Epoch time: 124.37 s\n",
      "2024-01-06 06:34:53.425451: \n",
      "2024-01-06 06:34:53.474238: Epoch 771\n",
      "2024-01-06 06:34:53.481824: Current learning rate: 0.00265\n",
      "2024-01-06 06:36:58.212332: train_loss -0.9226\n",
      "2024-01-06 06:36:58.223330: val_loss -0.8341\n",
      "2024-01-06 06:36:58.229330: Pseudo dice [0.8616, 0.9595, 0.9416]\n",
      "2024-01-06 06:36:58.238333: Epoch time: 124.79 s\n",
      "2024-01-06 06:36:59.267211: \n",
      "2024-01-06 06:36:59.272745: Epoch 772\n",
      "2024-01-06 06:36:59.278743: Current learning rate: 0.00264\n",
      "2024-01-06 06:39:03.936304: train_loss -0.9221\n",
      "2024-01-06 06:39:03.945312: val_loss -0.8335\n",
      "2024-01-06 06:39:03.952312: Pseudo dice [0.8643, 0.9597, 0.9422]\n",
      "2024-01-06 06:39:03.958306: Epoch time: 124.67 s\n",
      "2024-01-06 06:39:04.947051: \n",
      "2024-01-06 06:39:04.959805: Epoch 773\n",
      "2024-01-06 06:39:04.963822: Current learning rate: 0.00263\n",
      "2024-01-06 06:41:09.767583: train_loss -0.9192\n",
      "2024-01-06 06:41:09.775594: val_loss -0.8398\n",
      "2024-01-06 06:41:09.783317: Pseudo dice [0.8644, 0.9605, 0.9432]\n",
      "2024-01-06 06:41:09.790317: Epoch time: 124.82 s\n",
      "2024-01-06 06:41:10.783044: \n",
      "2024-01-06 06:41:10.789104: Epoch 774\n",
      "2024-01-06 06:41:10.794047: Current learning rate: 0.00262\n",
      "2024-01-06 06:43:15.313083: train_loss -0.9226\n",
      "2024-01-06 06:43:15.321085: val_loss -0.8336\n",
      "2024-01-06 06:43:15.329098: Pseudo dice [0.8652, 0.9596, 0.9418]\n",
      "2024-01-06 06:43:15.335083: Epoch time: 124.53 s\n",
      "2024-01-06 06:43:16.351578: \n",
      "2024-01-06 06:43:16.363646: Epoch 775\n",
      "2024-01-06 06:43:16.368583: Current learning rate: 0.00261\n",
      "2024-01-06 06:45:21.261034: train_loss -0.9203\n",
      "2024-01-06 06:45:21.270034: val_loss -0.8384\n",
      "2024-01-06 06:45:21.277034: Pseudo dice [0.8627, 0.9594, 0.9422]\n",
      "2024-01-06 06:45:21.284043: Epoch time: 124.91 s\n",
      "2024-01-06 06:45:22.275102: \n",
      "2024-01-06 06:45:22.281902: Epoch 776\n",
      "2024-01-06 06:45:22.286480: Current learning rate: 0.0026\n",
      "2024-01-06 06:47:26.959152: train_loss -0.924\n",
      "2024-01-06 06:47:26.968874: val_loss -0.8412\n",
      "2024-01-06 06:47:26.977879: Pseudo dice [0.864, 0.9596, 0.942]\n",
      "2024-01-06 06:47:26.984878: Epoch time: 124.69 s\n",
      "2024-01-06 06:47:28.149010: \n",
      "2024-01-06 06:47:28.155366: Epoch 777\n",
      "2024-01-06 06:47:28.160135: Current learning rate: 0.00259\n",
      "2024-01-06 06:49:32.754570: train_loss -0.9222\n",
      "2024-01-06 06:49:32.763092: val_loss -0.8327\n",
      "2024-01-06 06:49:32.771162: Pseudo dice [0.8597, 0.9598, 0.942]\n",
      "2024-01-06 06:49:32.781098: Epoch time: 124.61 s\n",
      "2024-01-06 06:49:33.763321: \n",
      "2024-01-06 06:49:33.769528: Epoch 778\n",
      "2024-01-06 06:49:33.774541: Current learning rate: 0.00258\n",
      "2024-01-06 06:51:38.273296: train_loss -0.9229\n",
      "2024-01-06 06:51:38.282296: val_loss -0.838\n",
      "2024-01-06 06:51:38.290298: Pseudo dice [0.8605, 0.96, 0.943]\n",
      "2024-01-06 06:51:38.296306: Epoch time: 124.51 s\n",
      "2024-01-06 06:51:39.285251: \n",
      "2024-01-06 06:51:39.294488: Epoch 779\n",
      "2024-01-06 06:51:39.299580: Current learning rate: 0.00257\n",
      "2024-01-06 06:53:44.087523: train_loss -0.9228\n",
      "2024-01-06 06:53:44.097535: val_loss -0.8387\n",
      "2024-01-06 06:53:44.105524: Pseudo dice [0.8617, 0.9603, 0.9429]\n",
      "2024-01-06 06:53:44.112532: Epoch time: 124.8 s\n",
      "2024-01-06 06:53:45.108897: \n",
      "2024-01-06 06:53:45.118422: Epoch 780\n",
      "2024-01-06 06:53:45.123508: Current learning rate: 0.00256\n",
      "2024-01-06 06:55:49.892780: train_loss -0.9235\n",
      "2024-01-06 06:55:49.902784: val_loss -0.8384\n",
      "2024-01-06 06:55:49.911286: Pseudo dice [0.8627, 0.9598, 0.9424]\n",
      "2024-01-06 06:55:49.917290: Epoch time: 124.78 s\n",
      "2024-01-06 06:55:50.930107: \n",
      "2024-01-06 06:55:50.939245: Epoch 781\n",
      "2024-01-06 06:55:50.944251: Current learning rate: 0.00255\n",
      "2024-01-06 06:57:55.647246: train_loss -0.9233\n",
      "2024-01-06 06:57:55.657249: val_loss -0.8349\n",
      "2024-01-06 06:57:55.666758: Pseudo dice [0.86, 0.9602, 0.9425]\n",
      "2024-01-06 06:57:55.675756: Epoch time: 124.72 s\n",
      "2024-01-06 06:57:56.705260: \n",
      "2024-01-06 06:57:56.714673: Epoch 782\n",
      "2024-01-06 06:57:56.718737: Current learning rate: 0.00254\n",
      "2024-01-06 07:00:01.966556: train_loss -0.9226\n",
      "2024-01-06 07:00:01.978548: val_loss -0.8355\n",
      "2024-01-06 07:00:01.991547: Pseudo dice [0.8619, 0.9591, 0.9414]\n",
      "2024-01-06 07:00:01.998547: Epoch time: 125.26 s\n",
      "2024-01-06 07:00:02.987222: \n",
      "2024-01-06 07:00:02.995199: Epoch 783\n",
      "2024-01-06 07:00:03.000208: Current learning rate: 0.00253\n",
      "2024-01-06 07:02:07.739377: train_loss -0.9204\n",
      "2024-01-06 07:02:07.751375: val_loss -0.8335\n",
      "2024-01-06 07:02:07.759374: Pseudo dice [0.8636, 0.9595, 0.942]\n",
      "2024-01-06 07:02:07.767375: Epoch time: 124.75 s\n",
      "2024-01-06 07:02:08.931517: \n",
      "2024-01-06 07:02:08.940509: Epoch 784\n",
      "2024-01-06 07:02:08.944518: Current learning rate: 0.00252\n",
      "2024-01-06 07:04:13.602742: train_loss -0.9222\n",
      "2024-01-06 07:04:13.611748: val_loss -0.843\n",
      "2024-01-06 07:04:13.618737: Pseudo dice [0.8614, 0.9594, 0.9418]\n",
      "2024-01-06 07:04:13.624737: Epoch time: 124.67 s\n",
      "2024-01-06 07:04:14.606639: \n",
      "2024-01-06 07:04:14.614777: Epoch 785\n",
      "2024-01-06 07:04:14.621802: Current learning rate: 0.00251\n",
      "2024-01-06 07:06:19.403116: train_loss -0.9213\n",
      "2024-01-06 07:06:19.415113: val_loss -0.8433\n",
      "2024-01-06 07:06:19.423625: Pseudo dice [0.8632, 0.9593, 0.9417]\n",
      "2024-01-06 07:06:19.431621: Epoch time: 124.8 s\n",
      "2024-01-06 07:06:20.467521: \n",
      "2024-01-06 07:06:20.477168: Epoch 786\n",
      "2024-01-06 07:06:20.482326: Current learning rate: 0.0025\n",
      "2024-01-06 07:08:25.322112: train_loss -0.9218\n",
      "2024-01-06 07:08:25.330119: val_loss -0.8399\n",
      "2024-01-06 07:08:25.338116: Pseudo dice [0.8632, 0.9597, 0.9423]\n",
      "2024-01-06 07:08:25.343623: Epoch time: 124.86 s\n",
      "2024-01-06 07:08:26.350811: \n",
      "2024-01-06 07:08:26.356867: Epoch 787\n",
      "2024-01-06 07:08:26.366962: Current learning rate: 0.00249\n",
      "2024-01-06 07:10:30.845885: train_loss -0.9222\n",
      "2024-01-06 07:10:30.856884: val_loss -0.8397\n",
      "2024-01-06 07:10:30.862885: Pseudo dice [0.8615, 0.9591, 0.9408]\n",
      "2024-01-06 07:10:30.868886: Epoch time: 124.5 s\n",
      "2024-01-06 07:10:31.870975: \n",
      "2024-01-06 07:10:31.876511: Epoch 788\n",
      "2024-01-06 07:10:31.880583: Current learning rate: 0.00248\n",
      "2024-01-06 07:12:36.560197: train_loss -0.9237\n",
      "2024-01-06 07:12:36.569706: val_loss -0.8385\n",
      "2024-01-06 07:12:36.577705: Pseudo dice [0.861, 0.9593, 0.9417]\n",
      "2024-01-06 07:12:36.586126: Epoch time: 124.69 s\n",
      "2024-01-06 07:12:37.625941: \n",
      "2024-01-06 07:12:37.635784: Epoch 789\n",
      "2024-01-06 07:12:37.640844: Current learning rate: 0.00247\n",
      "2024-01-06 07:14:42.398200: train_loss -0.9198\n",
      "2024-01-06 07:14:42.409209: val_loss -0.8366\n",
      "2024-01-06 07:14:42.417735: Pseudo dice [0.8615, 0.9595, 0.9418]\n",
      "2024-01-06 07:14:42.424247: Epoch time: 124.77 s\n",
      "2024-01-06 07:14:43.421693: \n",
      "2024-01-06 07:14:43.431171: Epoch 790\n",
      "2024-01-06 07:14:43.436245: Current learning rate: 0.00245\n",
      "2024-01-06 07:16:47.790387: train_loss -0.9259\n",
      "2024-01-06 07:16:47.800387: val_loss -0.8341\n",
      "2024-01-06 07:16:47.809387: Pseudo dice [0.8595, 0.9592, 0.9415]\n",
      "2024-01-06 07:16:47.815388: Epoch time: 124.37 s\n",
      "2024-01-06 07:16:48.810075: \n",
      "2024-01-06 07:16:48.818302: Epoch 791\n",
      "2024-01-06 07:16:48.822305: Current learning rate: 0.00244\n",
      "2024-01-06 07:18:53.270353: train_loss -0.9238\n",
      "2024-01-06 07:18:53.279362: val_loss -0.8377\n",
      "2024-01-06 07:18:53.288353: Pseudo dice [0.8624, 0.9604, 0.9427]\n",
      "2024-01-06 07:18:53.295355: Epoch time: 124.46 s\n",
      "2024-01-06 07:18:54.461363: \n",
      "2024-01-06 07:18:54.473747: Epoch 792\n",
      "2024-01-06 07:18:54.478834: Current learning rate: 0.00243\n",
      "2024-01-06 07:20:59.088809: train_loss -0.9219\n",
      "2024-01-06 07:20:59.097842: val_loss -0.8419\n",
      "2024-01-06 07:20:59.105839: Pseudo dice [0.8637, 0.9597, 0.942]\n",
      "2024-01-06 07:20:59.113335: Epoch time: 124.63 s\n",
      "2024-01-06 07:21:00.105679: \n",
      "2024-01-06 07:21:00.112990: Epoch 793\n",
      "2024-01-06 07:21:00.119003: Current learning rate: 0.00242\n",
      "2024-01-06 07:23:04.769128: train_loss -0.9246\n",
      "2024-01-06 07:23:04.780640: val_loss -0.8334\n",
      "2024-01-06 07:23:04.791639: Pseudo dice [0.8604, 0.9598, 0.9422]\n",
      "2024-01-06 07:23:04.801639: Epoch time: 124.66 s\n",
      "2024-01-06 07:23:05.802043: \n",
      "2024-01-06 07:23:05.810423: Epoch 794\n",
      "2024-01-06 07:23:05.818481: Current learning rate: 0.00241\n",
      "2024-01-06 07:25:10.235137: train_loss -0.9245\n",
      "2024-01-06 07:25:10.246137: val_loss -0.8369\n",
      "2024-01-06 07:25:10.253137: Pseudo dice [0.8624, 0.9599, 0.9428]\n",
      "2024-01-06 07:25:10.261137: Epoch time: 124.43 s\n",
      "2024-01-06 07:25:11.259437: \n",
      "2024-01-06 07:25:11.267310: Epoch 795\n",
      "2024-01-06 07:25:11.275251: Current learning rate: 0.0024\n",
      "2024-01-06 07:27:16.004427: train_loss -0.9242\n",
      "2024-01-06 07:27:16.014428: val_loss -0.8367\n",
      "2024-01-06 07:27:16.022427: Pseudo dice [0.8627, 0.9591, 0.9417]\n",
      "2024-01-06 07:27:16.027428: Epoch time: 124.75 s\n",
      "2024-01-06 07:27:17.027443: \n",
      "2024-01-06 07:27:17.033451: Epoch 796\n",
      "2024-01-06 07:27:17.037458: Current learning rate: 0.00239\n",
      "2024-01-06 07:29:21.922139: train_loss -0.9193\n",
      "2024-01-06 07:29:21.934144: val_loss -0.8331\n",
      "2024-01-06 07:29:21.944148: Pseudo dice [0.8592, 0.9599, 0.9423]\n",
      "2024-01-06 07:29:21.949655: Epoch time: 124.9 s\n",
      "2024-01-06 07:29:22.955686: \n",
      "2024-01-06 07:29:22.964355: Epoch 797\n",
      "2024-01-06 07:29:22.969422: Current learning rate: 0.00238\n",
      "2024-01-06 07:31:27.968326: train_loss -0.918\n",
      "2024-01-06 07:31:27.977319: val_loss -0.8348\n",
      "2024-01-06 07:31:27.986321: Pseudo dice [0.8589, 0.9597, 0.9424]\n",
      "2024-01-06 07:31:27.992321: Epoch time: 125.01 s\n",
      "2024-01-06 07:31:28.994015: \n",
      "2024-01-06 07:31:29.000466: Epoch 798\n",
      "2024-01-06 07:31:29.004545: Current learning rate: 0.00237\n",
      "2024-01-06 07:33:33.616575: train_loss -0.9228\n",
      "2024-01-06 07:33:33.626584: val_loss -0.8295\n",
      "2024-01-06 07:33:33.632582: Pseudo dice [0.861, 0.9596, 0.9418]\n",
      "2024-01-06 07:33:33.639585: Epoch time: 124.62 s\n",
      "2024-01-06 07:33:34.800281: \n",
      "2024-01-06 07:33:34.809087: Epoch 799\n",
      "2024-01-06 07:33:34.813675: Current learning rate: 0.00236\n",
      "2024-01-06 07:35:39.452765: train_loss -0.9237\n",
      "2024-01-06 07:35:39.460757: val_loss -0.8407\n",
      "2024-01-06 07:35:39.467757: Pseudo dice [0.8628, 0.9593, 0.9417]\n",
      "2024-01-06 07:35:39.475765: Epoch time: 124.65 s\n",
      "2024-01-06 07:35:40.811778: \n",
      "2024-01-06 07:35:40.820781: Epoch 800\n",
      "2024-01-06 07:35:40.829812: Current learning rate: 0.00235\n",
      "2024-01-06 07:37:45.573529: train_loss -0.9223\n",
      "2024-01-06 07:37:45.581528: val_loss -0.8347\n",
      "2024-01-06 07:37:45.588042: Pseudo dice [0.8648, 0.9603, 0.9425]\n",
      "2024-01-06 07:37:45.594042: Epoch time: 124.76 s\n",
      "2024-01-06 07:37:46.595736: \n",
      "2024-01-06 07:37:46.604105: Epoch 801\n",
      "2024-01-06 07:37:46.609163: Current learning rate: 0.00234\n",
      "2024-01-06 07:39:51.300882: train_loss -0.9204\n",
      "2024-01-06 07:39:51.308882: val_loss -0.8292\n",
      "2024-01-06 07:39:51.314882: Pseudo dice [0.8581, 0.9589, 0.9408]\n",
      "2024-01-06 07:39:51.321881: Epoch time: 124.71 s\n",
      "2024-01-06 07:39:52.316034: \n",
      "2024-01-06 07:39:52.325725: Epoch 802\n",
      "2024-01-06 07:39:52.330747: Current learning rate: 0.00233\n",
      "2024-01-06 07:41:56.886674: train_loss -0.9219\n",
      "2024-01-06 07:41:56.895673: val_loss -0.8304\n",
      "2024-01-06 07:41:56.901673: Pseudo dice [0.8642, 0.9582, 0.9406]\n",
      "2024-01-06 07:41:56.907673: Epoch time: 124.57 s\n",
      "2024-01-06 07:41:57.895828: \n",
      "2024-01-06 07:41:57.902056: Epoch 803\n",
      "2024-01-06 07:41:57.906053: Current learning rate: 0.00232\n",
      "2024-01-06 07:44:02.531399: train_loss -0.9205\n",
      "2024-01-06 07:44:02.539400: val_loss -0.8337\n",
      "2024-01-06 07:44:02.547400: Pseudo dice [0.8585, 0.9593, 0.9411]\n",
      "2024-01-06 07:44:02.553402: Epoch time: 124.64 s\n",
      "2024-01-06 07:44:03.556963: \n",
      "2024-01-06 07:44:03.568178: Epoch 804\n",
      "2024-01-06 07:44:03.573237: Current learning rate: 0.00231\n",
      "2024-01-06 07:46:08.371449: train_loss -0.9215\n",
      "2024-01-06 07:46:08.379451: val_loss -0.8343\n",
      "2024-01-06 07:46:08.387451: Pseudo dice [0.8626, 0.9595, 0.942]\n",
      "2024-01-06 07:46:08.394453: Epoch time: 124.82 s\n",
      "2024-01-06 07:46:09.377183: \n",
      "2024-01-06 07:46:09.385261: Epoch 805\n",
      "2024-01-06 07:46:09.392261: Current learning rate: 0.0023\n",
      "2024-01-06 07:48:13.829346: train_loss -0.9235\n",
      "2024-01-06 07:48:13.837348: val_loss -0.8304\n",
      "2024-01-06 07:48:13.845345: Pseudo dice [0.8584, 0.9598, 0.9423]\n",
      "2024-01-06 07:48:13.850921: Epoch time: 124.45 s\n",
      "2024-01-06 07:48:15.013819: \n",
      "2024-01-06 07:48:15.027730: Epoch 806\n",
      "2024-01-06 07:48:15.031763: Current learning rate: 0.00229\n",
      "2024-01-06 07:50:19.544104: train_loss -0.9214\n",
      "2024-01-06 07:50:19.552103: val_loss -0.8404\n",
      "2024-01-06 07:50:19.560102: Pseudo dice [0.8638, 0.9591, 0.9413]\n",
      "2024-01-06 07:50:19.566103: Epoch time: 124.53 s\n",
      "2024-01-06 07:50:20.548758: \n",
      "2024-01-06 07:50:20.554826: Epoch 807\n",
      "2024-01-06 07:50:20.559837: Current learning rate: 0.00228\n",
      "2024-01-06 07:52:25.284753: train_loss -0.9225\n",
      "2024-01-06 07:52:25.293754: val_loss -0.8367\n",
      "2024-01-06 07:52:25.301754: Pseudo dice [0.8638, 0.9595, 0.9417]\n",
      "2024-01-06 07:52:25.308753: Epoch time: 124.74 s\n",
      "2024-01-06 07:52:26.311982: \n",
      "2024-01-06 07:52:26.318069: Epoch 808\n",
      "2024-01-06 07:52:26.322046: Current learning rate: 0.00226\n",
      "2024-01-06 07:54:30.967961: train_loss -0.9244\n",
      "2024-01-06 07:54:30.975960: val_loss -0.8395\n",
      "2024-01-06 07:54:30.981959: Pseudo dice [0.8664, 0.9599, 0.9422]\n",
      "2024-01-06 07:54:30.988962: Epoch time: 124.66 s\n",
      "2024-01-06 07:54:31.988720: \n",
      "2024-01-06 07:54:31.995974: Epoch 809\n",
      "2024-01-06 07:54:32.000898: Current learning rate: 0.00225\n",
      "2024-01-06 07:56:36.522670: train_loss -0.9247\n",
      "2024-01-06 07:56:36.531682: val_loss -0.8414\n",
      "2024-01-06 07:56:36.539678: Pseudo dice [0.8635, 0.96, 0.9428]\n",
      "2024-01-06 07:56:36.546671: Epoch time: 124.53 s\n",
      "2024-01-06 07:56:37.550204: \n",
      "2024-01-06 07:56:37.558271: Epoch 810\n",
      "2024-01-06 07:56:37.563341: Current learning rate: 0.00224\n",
      "2024-01-06 07:58:42.316328: train_loss -0.9228\n",
      "2024-01-06 07:58:42.325328: val_loss -0.8405\n",
      "2024-01-06 07:58:42.333838: Pseudo dice [0.8592, 0.9598, 0.9421]\n",
      "2024-01-06 07:58:42.343283: Epoch time: 124.77 s\n",
      "2024-01-06 07:58:43.383470: \n",
      "2024-01-06 07:58:43.389478: Epoch 811\n",
      "2024-01-06 07:58:43.394478: Current learning rate: 0.00223\n",
      "2024-01-06 08:00:48.176523: train_loss -0.9219\n",
      "2024-01-06 08:00:48.185523: val_loss -0.8379\n",
      "2024-01-06 08:00:48.192523: Pseudo dice [0.861, 0.9589, 0.9411]\n",
      "2024-01-06 08:00:48.199530: Epoch time: 124.79 s\n",
      "2024-01-06 08:00:49.201462: \n",
      "2024-01-06 08:00:49.207461: Epoch 812\n",
      "2024-01-06 08:00:49.212623: Current learning rate: 0.00222\n",
      "2024-01-06 08:02:53.844101: train_loss -0.9221\n",
      "2024-01-06 08:02:53.856096: val_loss -0.8319\n",
      "2024-01-06 08:02:53.862097: Pseudo dice [0.8586, 0.9591, 0.941]\n",
      "2024-01-06 08:02:53.869097: Epoch time: 124.64 s\n",
      "2024-01-06 08:02:55.031877: \n",
      "2024-01-06 08:02:55.044693: Epoch 813\n",
      "2024-01-06 08:02:55.049673: Current learning rate: 0.00221\n",
      "2024-01-06 08:04:59.763338: train_loss -0.922\n",
      "2024-01-06 08:04:59.773337: val_loss -0.8428\n",
      "2024-01-06 08:04:59.780340: Pseudo dice [0.8569, 0.9596, 0.942]\n",
      "2024-01-06 08:04:59.785857: Epoch time: 124.73 s\n",
      "2024-01-06 08:05:00.771314: \n",
      "2024-01-06 08:05:00.780594: Epoch 814\n",
      "2024-01-06 08:05:00.787329: Current learning rate: 0.0022\n",
      "2024-01-06 08:07:05.438995: train_loss -0.9218\n",
      "2024-01-06 08:07:05.448001: val_loss -0.8389\n",
      "2024-01-06 08:07:05.453999: Pseudo dice [0.8654, 0.9596, 0.942]\n",
      "2024-01-06 08:07:05.459999: Epoch time: 124.67 s\n",
      "2024-01-06 08:07:06.452482: \n",
      "2024-01-06 08:07:06.464525: Epoch 815\n",
      "2024-01-06 08:07:06.469592: Current learning rate: 0.00219\n",
      "2024-01-06 08:09:10.921701: train_loss -0.9242\n",
      "2024-01-06 08:09:10.930702: val_loss -0.8369\n",
      "2024-01-06 08:09:10.938710: Pseudo dice [0.8651, 0.9599, 0.942]\n",
      "2024-01-06 08:09:10.944709: Epoch time: 124.47 s\n",
      "2024-01-06 08:09:11.981838: \n",
      "2024-01-06 08:09:11.987688: Epoch 816\n",
      "2024-01-06 08:09:11.991770: Current learning rate: 0.00218\n",
      "2024-01-06 08:11:16.485867: train_loss -0.9256\n",
      "2024-01-06 08:11:16.493864: val_loss -0.8327\n",
      "2024-01-06 08:11:16.501869: Pseudo dice [0.8629, 0.9591, 0.9414]\n",
      "2024-01-06 08:11:16.508873: Epoch time: 124.51 s\n",
      "2024-01-06 08:11:17.511873: \n",
      "2024-01-06 08:11:17.521368: Epoch 817\n",
      "2024-01-06 08:11:17.525387: Current learning rate: 0.00217\n",
      "2024-01-06 08:13:22.303033: train_loss -0.9219\n",
      "2024-01-06 08:13:22.311033: val_loss -0.8352\n",
      "2024-01-06 08:13:22.318554: Pseudo dice [0.8581, 0.9585, 0.9407]\n",
      "2024-01-06 08:13:22.324564: Epoch time: 124.79 s\n",
      "2024-01-06 08:13:23.332940: \n",
      "2024-01-06 08:13:23.343372: Epoch 818\n",
      "2024-01-06 08:13:23.349350: Current learning rate: 0.00216\n",
      "2024-01-06 08:15:28.186903: train_loss -0.9227\n",
      "2024-01-06 08:15:28.196908: val_loss -0.8396\n",
      "2024-01-06 08:15:28.205425: Pseudo dice [0.8611, 0.9608, 0.9438]\n",
      "2024-01-06 08:15:28.214418: Epoch time: 124.85 s\n",
      "2024-01-06 08:15:29.218604: \n",
      "2024-01-06 08:15:29.226543: Epoch 819\n",
      "2024-01-06 08:15:29.231546: Current learning rate: 0.00215\n",
      "2024-01-06 08:17:34.173001: train_loss -0.9201\n",
      "2024-01-06 08:17:34.182006: val_loss -0.8395\n",
      "2024-01-06 08:17:34.190007: Pseudo dice [0.8604, 0.9597, 0.943]\n",
      "2024-01-06 08:17:34.198234: Epoch time: 124.96 s\n",
      "2024-01-06 08:17:35.132184: \n",
      "2024-01-06 08:17:35.145175: Epoch 820\n",
      "2024-01-06 08:17:35.149809: Current learning rate: 0.00214\n",
      "2024-01-06 08:19:39.649463: train_loss -0.9254\n",
      "2024-01-06 08:19:39.657468: val_loss -0.8365\n",
      "2024-01-06 08:19:39.663465: Pseudo dice [0.8615, 0.9599, 0.9423]\n",
      "2024-01-06 08:19:39.668467: Epoch time: 124.52 s\n",
      "2024-01-06 08:19:40.782471: \n",
      "2024-01-06 08:19:40.788471: Epoch 821\n",
      "2024-01-06 08:19:40.792495: Current learning rate: 0.00213\n",
      "2024-01-06 08:21:45.272129: train_loss -0.9241\n",
      "2024-01-06 08:21:45.281128: val_loss -0.8379\n",
      "2024-01-06 08:21:45.287129: Pseudo dice [0.8607, 0.9595, 0.942]\n",
      "2024-01-06 08:21:45.293128: Epoch time: 124.49 s\n",
      "2024-01-06 08:21:46.235842: \n",
      "2024-01-06 08:21:46.244111: Epoch 822\n",
      "2024-01-06 08:21:46.249107: Current learning rate: 0.00212\n",
      "2024-01-06 08:23:50.746259: train_loss -0.9242\n",
      "2024-01-06 08:23:50.753767: val_loss -0.8373\n",
      "2024-01-06 08:23:50.762271: Pseudo dice [0.8647, 0.9599, 0.9419]\n",
      "2024-01-06 08:23:50.772275: Epoch time: 124.51 s\n",
      "2024-01-06 08:23:51.712268: \n",
      "2024-01-06 08:23:51.721164: Epoch 823\n",
      "2024-01-06 08:23:51.725242: Current learning rate: 0.0021\n",
      "2024-01-06 08:25:56.255244: train_loss -0.922\n",
      "2024-01-06 08:25:56.265248: val_loss -0.8382\n",
      "2024-01-06 08:25:56.274244: Pseudo dice [0.8639, 0.9598, 0.942]\n",
      "2024-01-06 08:25:56.281416: Epoch time: 124.54 s\n",
      "2024-01-06 08:25:57.211595: \n",
      "2024-01-06 08:25:57.217587: Epoch 824\n",
      "2024-01-06 08:25:57.221595: Current learning rate: 0.00209\n",
      "2024-01-06 08:28:01.832757: train_loss -0.9246\n",
      "2024-01-06 08:28:01.840757: val_loss -0.8346\n",
      "2024-01-06 08:28:01.852757: Pseudo dice [0.8609, 0.9594, 0.9413]\n",
      "2024-01-06 08:28:01.860759: Epoch time: 124.62 s\n",
      "2024-01-06 08:28:02.790245: \n",
      "2024-01-06 08:28:02.800844: Epoch 825\n",
      "2024-01-06 08:28:02.805206: Current learning rate: 0.00208\n",
      "2024-01-06 08:30:07.202257: train_loss -0.9234\n",
      "2024-01-06 08:30:07.211258: val_loss -0.8372\n",
      "2024-01-06 08:30:07.217260: Pseudo dice [0.8649, 0.9598, 0.9419]\n",
      "2024-01-06 08:30:07.224260: Epoch time: 124.41 s\n",
      "2024-01-06 08:30:08.163507: \n",
      "2024-01-06 08:30:08.174592: Epoch 826\n",
      "2024-01-06 08:30:08.179568: Current learning rate: 0.00207\n",
      "2024-01-06 08:32:12.682070: train_loss -0.9247\n",
      "2024-01-06 08:32:12.691582: val_loss -0.8382\n",
      "2024-01-06 08:32:12.698583: Pseudo dice [0.8631, 0.9596, 0.9417]\n",
      "2024-01-06 08:32:12.706582: Epoch time: 124.52 s\n",
      "2024-01-06 08:32:13.639477: \n",
      "2024-01-06 08:32:13.648754: Epoch 827\n",
      "2024-01-06 08:32:13.653813: Current learning rate: 0.00206\n",
      "2024-01-06 08:34:18.134207: train_loss -0.926\n",
      "2024-01-06 08:34:18.142221: val_loss -0.8374\n",
      "2024-01-06 08:34:18.149742: Pseudo dice [0.8611, 0.9599, 0.9425]\n",
      "2024-01-06 08:34:18.155738: Epoch time: 124.5 s\n",
      "2024-01-06 08:34:19.098325: \n",
      "2024-01-06 08:34:19.104316: Epoch 828\n",
      "2024-01-06 08:34:19.109507: Current learning rate: 0.00205\n",
      "2024-01-06 08:36:23.987803: train_loss -0.923\n",
      "2024-01-06 08:36:23.995802: val_loss -0.8356\n",
      "2024-01-06 08:36:24.004312: Pseudo dice [0.8588, 0.9601, 0.9425]\n",
      "2024-01-06 08:36:24.011316: Epoch time: 124.89 s\n",
      "2024-01-06 08:36:25.098821: \n",
      "2024-01-06 08:36:25.113826: Epoch 829\n",
      "2024-01-06 08:36:25.118892: Current learning rate: 0.00204\n",
      "2024-01-06 08:38:29.864079: train_loss -0.923\n",
      "2024-01-06 08:38:29.872079: val_loss -0.8402\n",
      "2024-01-06 08:38:29.880079: Pseudo dice [0.8618, 0.9599, 0.9423]\n",
      "2024-01-06 08:38:29.887080: Epoch time: 124.76 s\n",
      "2024-01-06 08:38:30.831760: \n",
      "2024-01-06 08:38:30.840045: Epoch 830\n",
      "2024-01-06 08:38:30.845116: Current learning rate: 0.00203\n",
      "2024-01-06 08:40:35.306119: train_loss -0.9251\n",
      "2024-01-06 08:40:35.318115: val_loss -0.8398\n",
      "2024-01-06 08:40:35.325119: Pseudo dice [0.8583, 0.9599, 0.9419]\n",
      "2024-01-06 08:40:35.333131: Epoch time: 124.48 s\n",
      "2024-01-06 08:40:36.270375: \n",
      "2024-01-06 08:40:36.276443: Epoch 831\n",
      "2024-01-06 08:40:36.281436: Current learning rate: 0.00202\n",
      "2024-01-06 08:42:40.937057: train_loss -0.9229\n",
      "2024-01-06 08:42:40.946057: val_loss -0.8383\n",
      "2024-01-06 08:42:40.955057: Pseudo dice [0.8645, 0.959, 0.9409]\n",
      "2024-01-06 08:42:40.962057: Epoch time: 124.67 s\n",
      "2024-01-06 08:42:41.913390: \n",
      "2024-01-06 08:42:41.919461: Epoch 832\n",
      "2024-01-06 08:42:41.925479: Current learning rate: 0.00201\n",
      "2024-01-06 08:44:46.463137: train_loss -0.9263\n",
      "2024-01-06 08:44:46.471137: val_loss -0.8291\n",
      "2024-01-06 08:44:46.477140: Pseudo dice [0.8612, 0.959, 0.9412]\n",
      "2024-01-06 08:44:46.485139: Epoch time: 124.55 s\n",
      "2024-01-06 08:44:47.432689: \n",
      "2024-01-06 08:44:47.438363: Epoch 833\n",
      "2024-01-06 08:44:47.443371: Current learning rate: 0.002\n",
      "2024-01-06 08:46:51.941112: train_loss -0.9271\n",
      "2024-01-06 08:46:51.950115: val_loss -0.8362\n",
      "2024-01-06 08:46:51.958116: Pseudo dice [0.8609, 0.9605, 0.9431]\n",
      "2024-01-06 08:46:51.964117: Epoch time: 124.51 s\n",
      "2024-01-06 08:46:52.895927: \n",
      "2024-01-06 08:46:52.909150: Epoch 834\n",
      "2024-01-06 08:46:52.914161: Current learning rate: 0.00199\n",
      "2024-01-06 08:48:57.732578: train_loss -0.9223\n",
      "2024-01-06 08:48:57.740576: val_loss -0.8362\n",
      "2024-01-06 08:48:57.746579: Pseudo dice [0.8594, 0.9602, 0.9423]\n",
      "2024-01-06 08:48:57.753575: Epoch time: 124.84 s\n",
      "2024-01-06 08:48:58.736082: \n",
      "2024-01-06 08:48:58.747198: Epoch 835\n",
      "2024-01-06 08:48:58.756186: Current learning rate: 0.00198\n",
      "2024-01-06 08:51:03.395272: train_loss -0.9251\n",
      "2024-01-06 08:51:03.402261: val_loss -0.8362\n",
      "2024-01-06 08:51:03.408260: Pseudo dice [0.862, 0.9597, 0.9422]\n",
      "2024-01-06 08:51:03.416259: Epoch time: 124.66 s\n",
      "2024-01-06 08:51:04.348391: \n",
      "2024-01-06 08:51:04.353739: Epoch 836\n",
      "2024-01-06 08:51:04.358739: Current learning rate: 0.00196\n",
      "2024-01-06 08:53:09.269486: train_loss -0.9212\n",
      "2024-01-06 08:53:09.277483: val_loss -0.8408\n",
      "2024-01-06 08:53:09.286476: Pseudo dice [0.8647, 0.9607, 0.9436]\n",
      "2024-01-06 08:53:09.292475: Epoch time: 124.92 s\n",
      "2024-01-06 08:53:10.419777: \n",
      "2024-01-06 08:53:10.425648: Epoch 837\n",
      "2024-01-06 08:53:10.430671: Current learning rate: 0.00195\n",
      "2024-01-06 08:55:15.052843: train_loss -0.9247\n",
      "2024-01-06 08:55:15.062249: val_loss -0.8388\n",
      "2024-01-06 08:55:15.070249: Pseudo dice [0.862, 0.9604, 0.9431]\n",
      "2024-01-06 08:55:15.077253: Epoch time: 124.63 s\n",
      "2024-01-06 08:55:16.012171: \n",
      "2024-01-06 08:55:16.018259: Epoch 838\n",
      "2024-01-06 08:55:16.022881: Current learning rate: 0.00194\n",
      "2024-01-06 08:57:20.772016: train_loss -0.9244\n",
      "2024-01-06 08:57:20.781019: val_loss -0.8345\n",
      "2024-01-06 08:57:20.790016: Pseudo dice [0.862, 0.9598, 0.9423]\n",
      "2024-01-06 08:57:20.798017: Epoch time: 124.76 s\n",
      "2024-01-06 08:57:21.792177: \n",
      "2024-01-06 08:57:21.797604: Epoch 839\n",
      "2024-01-06 08:57:21.802938: Current learning rate: 0.00193\n",
      "2024-01-06 08:59:26.350859: train_loss -0.9247\n",
      "2024-01-06 08:59:26.359860: val_loss -0.8325\n",
      "2024-01-06 08:59:26.367859: Pseudo dice [0.8606, 0.959, 0.9406]\n",
      "2024-01-06 08:59:26.375861: Epoch time: 124.56 s\n",
      "2024-01-06 08:59:27.357339: \n",
      "2024-01-06 08:59:27.363783: Epoch 840\n",
      "2024-01-06 08:59:27.368872: Current learning rate: 0.00192\n",
      "2024-01-06 09:01:32.033824: train_loss -0.9247\n",
      "2024-01-06 09:01:32.043831: val_loss -0.835\n",
      "2024-01-06 09:01:32.051335: Pseudo dice [0.8631, 0.9597, 0.942]\n",
      "2024-01-06 09:01:32.057335: Epoch time: 124.68 s\n",
      "2024-01-06 09:01:33.032230: \n",
      "2024-01-06 09:01:33.040245: Epoch 841\n",
      "2024-01-06 09:01:33.045253: Current learning rate: 0.00191\n",
      "2024-01-06 09:03:37.847365: train_loss -0.9217\n",
      "2024-01-06 09:03:37.855361: val_loss -0.8351\n",
      "2024-01-06 09:03:37.863369: Pseudo dice [0.8593, 0.9594, 0.9413]\n",
      "2024-01-06 09:03:37.869360: Epoch time: 124.82 s\n",
      "2024-01-06 09:03:38.799077: \n",
      "2024-01-06 09:03:38.805318: Epoch 842\n",
      "2024-01-06 09:03:38.809904: Current learning rate: 0.0019\n",
      "2024-01-06 09:05:43.365794: train_loss -0.9229\n",
      "2024-01-06 09:05:43.373301: val_loss -0.8384\n",
      "2024-01-06 09:05:43.382303: Pseudo dice [0.8637, 0.9595, 0.9414]\n",
      "2024-01-06 09:05:43.390302: Epoch time: 124.57 s\n",
      "2024-01-06 09:05:44.343381: \n",
      "2024-01-06 09:05:44.356283: Epoch 843\n",
      "2024-01-06 09:05:44.360368: Current learning rate: 0.00189\n",
      "2024-01-06 09:07:48.926836: train_loss -0.9232\n",
      "2024-01-06 09:07:48.936837: val_loss -0.8335\n",
      "2024-01-06 09:07:48.943837: Pseudo dice [0.861, 0.9596, 0.9417]\n",
      "2024-01-06 09:07:48.948837: Epoch time: 124.58 s\n",
      "2024-01-06 09:07:50.044370: \n",
      "2024-01-06 09:07:50.050875: Epoch 844\n",
      "2024-01-06 09:07:50.055448: Current learning rate: 0.00188\n",
      "2024-01-06 09:09:54.642444: train_loss -0.9253\n",
      "2024-01-06 09:09:54.653443: val_loss -0.8366\n",
      "2024-01-06 09:09:54.660459: Pseudo dice [0.8634, 0.9594, 0.9417]\n",
      "2024-01-06 09:09:54.667452: Epoch time: 124.6 s\n",
      "2024-01-06 09:09:55.628121: \n",
      "2024-01-06 09:09:55.639163: Epoch 845\n",
      "2024-01-06 09:09:55.643156: Current learning rate: 0.00187\n",
      "2024-01-06 09:12:00.407739: train_loss -0.9224\n",
      "2024-01-06 09:12:00.417745: val_loss -0.8363\n",
      "2024-01-06 09:12:00.426746: Pseudo dice [0.8606, 0.9596, 0.9422]\n",
      "2024-01-06 09:12:00.436256: Epoch time: 124.78 s\n",
      "2024-01-06 09:12:01.423773: \n",
      "2024-01-06 09:12:01.432657: Epoch 846\n",
      "2024-01-06 09:12:01.437657: Current learning rate: 0.00186\n",
      "2024-01-06 09:14:06.163487: train_loss -0.9212\n",
      "2024-01-06 09:14:06.174483: val_loss -0.8364\n",
      "2024-01-06 09:14:06.182491: Pseudo dice [0.8617, 0.9597, 0.9423]\n",
      "2024-01-06 09:14:06.189489: Epoch time: 124.74 s\n",
      "2024-01-06 09:14:07.173429: \n",
      "2024-01-06 09:14:07.183700: Epoch 847\n",
      "2024-01-06 09:14:07.187707: Current learning rate: 0.00185\n",
      "2024-01-06 09:16:11.669412: train_loss -0.9246\n",
      "2024-01-06 09:16:11.677413: val_loss -0.8308\n",
      "2024-01-06 09:16:11.685411: Pseudo dice [0.86, 0.9596, 0.9418]\n",
      "2024-01-06 09:16:11.693413: Epoch time: 124.5 s\n",
      "2024-01-06 09:16:12.618506: \n",
      "2024-01-06 09:16:12.624626: Epoch 848\n",
      "2024-01-06 09:16:12.628708: Current learning rate: 0.00184\n",
      "2024-01-06 09:18:17.353455: train_loss -0.9238\n",
      "2024-01-06 09:18:17.363453: val_loss -0.8403\n",
      "2024-01-06 09:18:17.371454: Pseudo dice [0.8615, 0.9591, 0.9411]\n",
      "2024-01-06 09:18:17.380458: Epoch time: 124.74 s\n",
      "2024-01-06 09:18:18.349572: \n",
      "2024-01-06 09:18:18.356827: Epoch 849\n",
      "2024-01-06 09:18:18.361918: Current learning rate: 0.00182\n",
      "2024-01-06 09:20:22.717315: train_loss -0.9244\n",
      "2024-01-06 09:20:22.726313: val_loss -0.8308\n",
      "2024-01-06 09:20:22.735314: Pseudo dice [0.8607, 0.9589, 0.9402]\n",
      "2024-01-06 09:20:22.744316: Epoch time: 124.37 s\n",
      "2024-01-06 09:20:23.966371: \n",
      "2024-01-06 09:20:23.979760: Epoch 850\n",
      "2024-01-06 09:20:23.991904: Current learning rate: 0.00181\n",
      "2024-01-06 09:22:28.535225: train_loss -0.9255\n",
      "2024-01-06 09:22:28.543213: val_loss -0.8383\n",
      "2024-01-06 09:22:28.552724: Pseudo dice [0.8647, 0.9594, 0.9418]\n",
      "2024-01-06 09:22:28.560726: Epoch time: 124.57 s\n",
      "2024-01-06 09:22:29.525104: \n",
      "2024-01-06 09:22:29.537959: Epoch 851\n",
      "2024-01-06 09:22:29.542901: Current learning rate: 0.0018\n",
      "2024-01-06 09:24:34.051460: train_loss -0.9253\n",
      "2024-01-06 09:24:34.059459: val_loss -0.8341\n",
      "2024-01-06 09:24:34.067458: Pseudo dice [0.8616, 0.9592, 0.942]\n",
      "2024-01-06 09:24:34.074458: Epoch time: 124.53 s\n",
      "2024-01-06 09:24:35.161722: \n",
      "2024-01-06 09:24:35.170497: Epoch 852\n",
      "2024-01-06 09:24:35.175573: Current learning rate: 0.00179\n",
      "2024-01-06 09:26:39.444236: train_loss -0.925\n",
      "2024-01-06 09:26:39.453241: val_loss -0.8354\n",
      "2024-01-06 09:26:39.459305: Pseudo dice [0.8595, 0.9591, 0.9409]\n",
      "2024-01-06 09:26:39.465237: Epoch time: 124.28 s\n",
      "2024-01-06 09:26:40.423415: \n",
      "2024-01-06 09:26:40.429655: Epoch 853\n",
      "2024-01-06 09:26:40.437740: Current learning rate: 0.00178\n",
      "2024-01-06 09:28:45.115534: train_loss -0.925\n",
      "2024-01-06 09:28:45.123533: val_loss -0.8357\n",
      "2024-01-06 09:28:45.131542: Pseudo dice [0.862, 0.959, 0.9415]\n",
      "2024-01-06 09:28:45.139534: Epoch time: 124.69 s\n",
      "2024-01-06 09:28:46.083117: \n",
      "2024-01-06 09:28:46.089184: Epoch 854\n",
      "2024-01-06 09:28:46.094184: Current learning rate: 0.00177\n",
      "2024-01-06 09:30:50.556895: train_loss -0.924\n",
      "2024-01-06 09:30:50.564893: val_loss -0.8365\n",
      "2024-01-06 09:30:50.570892: Pseudo dice [0.8661, 0.9597, 0.9421]\n",
      "2024-01-06 09:30:50.577892: Epoch time: 124.47 s\n",
      "2024-01-06 09:30:51.544808: \n",
      "2024-01-06 09:30:51.552472: Epoch 855\n",
      "2024-01-06 09:30:51.558555: Current learning rate: 0.00176\n",
      "2024-01-06 09:32:55.919215: train_loss -0.9248\n",
      "2024-01-06 09:32:55.927209: val_loss -0.8369\n",
      "2024-01-06 09:32:55.934209: Pseudo dice [0.8626, 0.9589, 0.9414]\n",
      "2024-01-06 09:32:55.942209: Epoch time: 124.38 s\n",
      "2024-01-06 09:32:56.862910: \n",
      "2024-01-06 09:32:56.869430: Epoch 856\n",
      "2024-01-06 09:32:56.874113: Current learning rate: 0.00175\n",
      "2024-01-06 09:35:01.676427: train_loss -0.9253\n",
      "2024-01-06 09:35:01.685426: val_loss -0.8358\n",
      "2024-01-06 09:35:01.691426: Pseudo dice [0.8625, 0.9597, 0.9416]\n",
      "2024-01-06 09:35:01.696428: Epoch time: 124.81 s\n",
      "2024-01-06 09:35:02.651856: \n",
      "2024-01-06 09:35:02.663612: Epoch 857\n",
      "2024-01-06 09:35:02.671595: Current learning rate: 0.00174\n",
      "2024-01-06 09:37:07.086884: train_loss -0.9254\n",
      "2024-01-06 09:37:07.094884: val_loss -0.8369\n",
      "2024-01-06 09:37:07.100894: Pseudo dice [0.8644, 0.9597, 0.942]\n",
      "2024-01-06 09:37:07.106893: Epoch time: 124.44 s\n",
      "2024-01-06 09:37:08.033659: \n",
      "2024-01-06 09:37:08.042740: Epoch 858\n",
      "2024-01-06 09:37:08.047672: Current learning rate: 0.00173\n",
      "2024-01-06 09:39:12.583992: train_loss -0.9271\n",
      "2024-01-06 09:39:12.592993: val_loss -0.8353\n",
      "2024-01-06 09:39:12.600996: Pseudo dice [0.8618, 0.9597, 0.9422]\n",
      "2024-01-06 09:39:12.607993: Epoch time: 124.55 s\n",
      "2024-01-06 09:39:13.555474: \n",
      "2024-01-06 09:39:13.563491: Epoch 859\n",
      "2024-01-06 09:39:13.569001: Current learning rate: 0.00172\n",
      "2024-01-06 09:41:18.288603: train_loss -0.9247\n",
      "2024-01-06 09:41:18.298603: val_loss -0.8313\n",
      "2024-01-06 09:41:18.306604: Pseudo dice [0.8631, 0.9581, 0.9396]\n",
      "2024-01-06 09:41:18.314604: Epoch time: 124.73 s\n",
      "2024-01-06 09:41:19.414761: \n",
      "2024-01-06 09:41:19.426285: Epoch 860\n",
      "2024-01-06 09:41:19.433299: Current learning rate: 0.0017\n",
      "2024-01-06 09:43:24.002349: train_loss -0.9235\n",
      "2024-01-06 09:43:24.014342: val_loss -0.8361\n",
      "2024-01-06 09:43:24.021846: Pseudo dice [0.8637, 0.9595, 0.9421]\n",
      "2024-01-06 09:43:24.029849: Epoch time: 124.59 s\n",
      "2024-01-06 09:43:24.978841: \n",
      "2024-01-06 09:43:24.991282: Epoch 861\n",
      "2024-01-06 09:43:24.996345: Current learning rate: 0.00169\n",
      "2024-01-06 09:45:29.748142: train_loss -0.9242\n",
      "2024-01-06 09:45:29.756143: val_loss -0.832\n",
      "2024-01-06 09:45:29.763143: Pseudo dice [0.864, 0.959, 0.941]\n",
      "2024-01-06 09:45:29.770145: Epoch time: 124.77 s\n",
      "2024-01-06 09:45:30.674740: \n",
      "2024-01-06 09:45:30.682398: Epoch 862\n",
      "2024-01-06 09:45:30.687356: Current learning rate: 0.00168\n",
      "2024-01-06 09:47:35.185944: train_loss -0.9243\n",
      "2024-01-06 09:47:35.195453: val_loss -0.8361\n",
      "2024-01-06 09:47:35.201453: Pseudo dice [0.8612, 0.9592, 0.9415]\n",
      "2024-01-06 09:47:35.207453: Epoch time: 124.51 s\n",
      "2024-01-06 09:47:36.175556: \n",
      "2024-01-06 09:47:36.186481: Epoch 863\n",
      "2024-01-06 09:47:36.193664: Current learning rate: 0.00167\n",
      "2024-01-06 09:49:40.903275: train_loss -0.9246\n",
      "2024-01-06 09:49:40.911281: val_loss -0.8369\n",
      "2024-01-06 09:49:40.919280: Pseudo dice [0.8599, 0.9593, 0.9415]\n",
      "2024-01-06 09:49:40.926281: Epoch time: 124.73 s\n",
      "2024-01-06 09:49:41.860572: \n",
      "2024-01-06 09:49:41.871545: Epoch 864\n",
      "2024-01-06 09:49:41.877480: Current learning rate: 0.00166\n",
      "2024-01-06 09:51:46.703588: train_loss -0.9243\n",
      "2024-01-06 09:51:46.714089: val_loss -0.8356\n",
      "2024-01-06 09:51:46.722093: Pseudo dice [0.8623, 0.9589, 0.9408]\n",
      "2024-01-06 09:51:46.729093: Epoch time: 124.84 s\n",
      "2024-01-06 09:51:47.678985: \n",
      "2024-01-06 09:51:47.684969: Epoch 865\n",
      "2024-01-06 09:51:47.689049: Current learning rate: 0.00165\n",
      "2024-01-06 09:53:52.414432: train_loss -0.9245\n",
      "2024-01-06 09:53:52.423434: val_loss -0.8376\n",
      "2024-01-06 09:53:52.430443: Pseudo dice [0.8616, 0.9598, 0.942]\n",
      "2024-01-06 09:53:52.438436: Epoch time: 124.74 s\n",
      "2024-01-06 09:53:53.375411: \n",
      "2024-01-06 09:53:53.385506: Epoch 866\n",
      "2024-01-06 09:53:53.391040: Current learning rate: 0.00164\n",
      "2024-01-06 09:55:58.086579: train_loss -0.9261\n",
      "2024-01-06 09:55:58.097577: val_loss -0.8324\n",
      "2024-01-06 09:55:58.103592: Pseudo dice [0.8611, 0.9589, 0.9409]\n",
      "2024-01-06 09:55:58.111580: Epoch time: 124.71 s\n",
      "2024-01-06 09:55:59.063675: \n",
      "2024-01-06 09:55:59.071745: Epoch 867\n",
      "2024-01-06 09:55:59.077933: Current learning rate: 0.00163\n",
      "2024-01-06 09:58:04.059335: train_loss -0.9225\n",
      "2024-01-06 09:58:04.068334: val_loss -0.8318\n",
      "2024-01-06 09:58:04.076338: Pseudo dice [0.863, 0.96, 0.9423]\n",
      "2024-01-06 09:58:04.084339: Epoch time: 125.0 s\n",
      "2024-01-06 09:58:05.175508: \n",
      "2024-01-06 09:58:05.185167: Epoch 868\n",
      "2024-01-06 09:58:05.189747: Current learning rate: 0.00162\n",
      "2024-01-06 10:00:10.336091: train_loss -0.9231\n",
      "2024-01-06 10:00:10.347598: val_loss -0.8392\n",
      "2024-01-06 10:00:10.356599: Pseudo dice [0.8589, 0.9597, 0.9423]\n",
      "2024-01-06 10:00:10.365598: Epoch time: 125.16 s\n",
      "2024-01-06 10:00:11.328223: \n",
      "2024-01-06 10:00:11.335282: Epoch 869\n",
      "2024-01-06 10:00:11.340932: Current learning rate: 0.00161\n",
      "2024-01-06 10:02:16.185147: train_loss -0.9253\n",
      "2024-01-06 10:02:16.195149: val_loss -0.8378\n",
      "2024-01-06 10:02:16.203151: Pseudo dice [0.861, 0.9599, 0.9423]\n",
      "2024-01-06 10:02:16.216147: Epoch time: 124.86 s\n",
      "2024-01-06 10:02:17.178876: \n",
      "2024-01-06 10:02:17.184389: Epoch 870\n",
      "2024-01-06 10:02:17.189405: Current learning rate: 0.00159\n",
      "2024-01-06 10:04:22.340422: train_loss -0.9254\n",
      "2024-01-06 10:04:22.349423: val_loss -0.839\n",
      "2024-01-06 10:04:22.357422: Pseudo dice [0.8619, 0.9598, 0.9422]\n",
      "2024-01-06 10:04:22.364422: Epoch time: 125.16 s\n",
      "2024-01-06 10:04:23.379797: \n",
      "2024-01-06 10:04:23.391969: Epoch 871\n",
      "2024-01-06 10:04:23.396049: Current learning rate: 0.00158\n",
      "2024-01-06 10:06:28.229838: train_loss -0.9262\n",
      "2024-01-06 10:06:28.238839: val_loss -0.8339\n",
      "2024-01-06 10:06:28.247838: Pseudo dice [0.8618, 0.9594, 0.9412]\n",
      "2024-01-06 10:06:28.255841: Epoch time: 124.85 s\n",
      "2024-01-06 10:06:29.187162: \n",
      "2024-01-06 10:06:29.192826: Epoch 872\n",
      "2024-01-06 10:06:29.197898: Current learning rate: 0.00157\n",
      "2024-01-06 10:08:34.362424: train_loss -0.924\n",
      "2024-01-06 10:08:34.371426: val_loss -0.8363\n",
      "2024-01-06 10:08:34.380425: Pseudo dice [0.8622, 0.9591, 0.9414]\n",
      "2024-01-06 10:08:34.386423: Epoch time: 125.18 s\n",
      "2024-01-06 10:08:35.341572: \n",
      "2024-01-06 10:08:35.350090: Epoch 873\n",
      "2024-01-06 10:08:35.355617: Current learning rate: 0.00156\n",
      "2024-01-06 10:10:40.288377: train_loss -0.925\n",
      "2024-01-06 10:10:40.298879: val_loss -0.8384\n",
      "2024-01-06 10:10:40.305885: Pseudo dice [0.8635, 0.9596, 0.9421]\n",
      "2024-01-06 10:10:40.313879: Epoch time: 124.95 s\n",
      "2024-01-06 10:10:41.258649: \n",
      "2024-01-06 10:10:41.268313: Epoch 874\n",
      "2024-01-06 10:10:41.272375: Current learning rate: 0.00155\n",
      "2024-01-06 10:12:46.016283: train_loss -0.9271\n",
      "2024-01-06 10:12:46.026284: val_loss -0.834\n",
      "2024-01-06 10:12:46.033278: Pseudo dice [0.8616, 0.9591, 0.941]\n",
      "2024-01-06 10:12:46.040277: Epoch time: 124.76 s\n",
      "2024-01-06 10:12:47.011211: \n",
      "2024-01-06 10:12:47.024045: Epoch 875\n",
      "2024-01-06 10:12:47.029099: Current learning rate: 0.00154\n",
      "2024-01-06 10:14:52.041045: train_loss -0.9236\n",
      "2024-01-06 10:14:52.049224: val_loss -0.8377\n",
      "2024-01-06 10:14:52.056225: Pseudo dice [0.862, 0.9601, 0.943]\n",
      "2024-01-06 10:14:52.061232: Epoch time: 125.03 s\n",
      "2024-01-06 10:14:53.159252: \n",
      "2024-01-06 10:14:53.166368: Epoch 876\n",
      "2024-01-06 10:14:53.170943: Current learning rate: 0.00153\n",
      "2024-01-06 10:16:58.786781: train_loss -0.9204\n",
      "2024-01-06 10:16:58.796780: val_loss -0.8424\n",
      "2024-01-06 10:16:58.804780: Pseudo dice [0.8639, 0.9592, 0.9418]\n",
      "2024-01-06 10:16:58.812779: Epoch time: 125.63 s\n",
      "2024-01-06 10:16:59.783338: \n",
      "2024-01-06 10:16:59.794594: Epoch 877\n",
      "2024-01-06 10:16:59.798529: Current learning rate: 0.00152\n",
      "2024-01-06 10:19:04.978371: train_loss -0.9236\n",
      "2024-01-06 10:19:04.986368: val_loss -0.8369\n",
      "2024-01-06 10:19:04.994361: Pseudo dice [0.8595, 0.9602, 0.9428]\n",
      "2024-01-06 10:19:05.000367: Epoch time: 125.2 s\n",
      "2024-01-06 10:19:05.935834: \n",
      "2024-01-06 10:19:05.945493: Epoch 878\n",
      "2024-01-06 10:19:05.957535: Current learning rate: 0.00151\n",
      "2024-01-06 10:21:11.103660: train_loss -0.9222\n",
      "2024-01-06 10:21:11.113660: val_loss -0.8366\n",
      "2024-01-06 10:21:11.120660: Pseudo dice [0.8566, 0.9597, 0.9423]\n",
      "2024-01-06 10:21:11.128665: Epoch time: 125.17 s\n",
      "2024-01-06 10:21:12.093549: \n",
      "2024-01-06 10:21:12.099541: Epoch 879\n",
      "2024-01-06 10:21:12.103550: Current learning rate: 0.00149\n",
      "2024-01-06 10:23:17.088299: train_loss -0.926\n",
      "2024-01-06 10:23:17.096294: val_loss -0.8435\n",
      "2024-01-06 10:23:17.104295: Pseudo dice [0.8612, 0.9596, 0.9422]\n",
      "2024-01-06 10:23:17.111289: Epoch time: 125.0 s\n",
      "2024-01-06 10:23:18.054197: \n",
      "2024-01-06 10:23:18.060264: Epoch 880\n",
      "2024-01-06 10:23:18.066226: Current learning rate: 0.00148\n",
      "2024-01-06 10:25:23.209413: train_loss -0.9236\n",
      "2024-01-06 10:25:23.218414: val_loss -0.8358\n",
      "2024-01-06 10:25:23.225418: Pseudo dice [0.8574, 0.9582, 0.9401]\n",
      "2024-01-06 10:25:23.232419: Epoch time: 125.16 s\n",
      "2024-01-06 10:25:24.196075: \n",
      "2024-01-06 10:25:24.209286: Epoch 881\n",
      "2024-01-06 10:25:24.213733: Current learning rate: 0.00147\n",
      "2024-01-06 10:27:29.193752: train_loss -0.9261\n",
      "2024-01-06 10:27:29.206752: val_loss -0.8359\n",
      "2024-01-06 10:27:29.213771: Pseudo dice [0.8599, 0.9596, 0.9423]\n",
      "2024-01-06 10:27:29.220766: Epoch time: 125.0 s\n",
      "2024-01-06 10:27:30.156967: \n",
      "2024-01-06 10:27:30.166197: Epoch 882\n",
      "2024-01-06 10:27:30.171201: Current learning rate: 0.00146\n",
      "2024-01-06 10:29:35.129942: train_loss -0.9252\n",
      "2024-01-06 10:29:35.138943: val_loss -0.8362\n",
      "2024-01-06 10:29:35.146944: Pseudo dice [0.8608, 0.9601, 0.9431]\n",
      "2024-01-06 10:29:35.153943: Epoch time: 124.97 s\n",
      "2024-01-06 10:29:36.114172: \n",
      "2024-01-06 10:29:36.122378: Epoch 883\n",
      "2024-01-06 10:29:36.125953: Current learning rate: 0.00145\n",
      "2024-01-06 10:31:41.065353: train_loss -0.9272\n",
      "2024-01-06 10:31:41.076347: val_loss -0.8375\n",
      "2024-01-06 10:31:41.084899: Pseudo dice [0.8583, 0.9604, 0.9429]\n",
      "2024-01-06 10:31:41.090899: Epoch time: 124.95 s\n",
      "2024-01-06 10:31:42.177411: \n",
      "2024-01-06 10:31:42.185391: Epoch 884\n",
      "2024-01-06 10:31:42.192447: Current learning rate: 0.00144\n",
      "2024-01-06 10:33:47.478915: train_loss -0.9255\n",
      "2024-01-06 10:33:47.488915: val_loss -0.8368\n",
      "2024-01-06 10:33:47.496915: Pseudo dice [0.8636, 0.959, 0.9412]\n",
      "2024-01-06 10:33:47.505912: Epoch time: 125.3 s\n",
      "2024-01-06 10:33:48.478295: \n",
      "2024-01-06 10:33:48.484884: Epoch 885\n",
      "2024-01-06 10:33:48.488941: Current learning rate: 0.00143\n",
      "2024-01-06 10:35:53.461858: train_loss -0.925\n",
      "2024-01-06 10:35:53.471859: val_loss -0.8334\n",
      "2024-01-06 10:35:53.478860: Pseudo dice [0.8628, 0.9594, 0.9418]\n",
      "2024-01-06 10:35:53.484860: Epoch time: 124.98 s\n",
      "2024-01-06 10:35:54.419217: \n",
      "2024-01-06 10:35:54.429158: Epoch 886\n",
      "2024-01-06 10:35:54.433523: Current learning rate: 0.00142\n",
      "2024-01-06 10:37:59.807573: train_loss -0.9237\n",
      "2024-01-06 10:37:59.815578: val_loss -0.8355\n",
      "2024-01-06 10:37:59.829391: Pseudo dice [0.8597, 0.9596, 0.9426]\n",
      "2024-01-06 10:37:59.836370: Epoch time: 125.39 s\n",
      "2024-01-06 10:38:00.866702: \n",
      "2024-01-06 10:38:00.879717: Epoch 887\n",
      "2024-01-06 10:38:00.884258: Current learning rate: 0.00141\n",
      "2024-01-06 10:40:06.745725: train_loss -0.9253\n",
      "2024-01-06 10:40:06.753731: val_loss -0.8363\n",
      "2024-01-06 10:40:06.758731: Pseudo dice [0.8603, 0.9598, 0.9418]\n",
      "2024-01-06 10:40:06.764934: Epoch time: 125.88 s\n",
      "2024-01-06 10:40:08.000904: \n",
      "2024-01-06 10:40:08.010905: Epoch 888\n",
      "2024-01-06 10:40:08.014890: Current learning rate: 0.00139\n",
      "2024-01-06 10:42:14.374468: train_loss -0.9266\n",
      "2024-01-06 10:42:14.383477: val_loss -0.8364\n",
      "2024-01-06 10:42:14.391466: Pseudo dice [0.8628, 0.9594, 0.942]\n",
      "2024-01-06 10:42:14.396467: Epoch time: 126.37 s\n",
      "2024-01-06 10:42:15.591032: \n",
      "2024-01-06 10:42:15.599422: Epoch 889\n",
      "2024-01-06 10:42:15.611451: Current learning rate: 0.00138\n",
      "2024-01-06 10:44:21.942508: train_loss -0.926\n",
      "2024-01-06 10:44:21.950509: val_loss -0.8407\n",
      "2024-01-06 10:44:21.959521: Pseudo dice [0.8658, 0.9594, 0.9417]\n",
      "2024-01-06 10:44:21.967541: Epoch time: 126.35 s\n",
      "2024-01-06 10:44:23.115676: \n",
      "2024-01-06 10:44:23.122676: Epoch 890\n",
      "2024-01-06 10:44:23.128758: Current learning rate: 0.00137\n",
      "2024-01-06 10:46:29.404417: train_loss -0.9231\n",
      "2024-01-06 10:46:29.411416: val_loss -0.8338\n",
      "2024-01-06 10:46:29.416415: Pseudo dice [0.8593, 0.9596, 0.9424]\n",
      "2024-01-06 10:46:29.421420: Epoch time: 126.29 s\n",
      "2024-01-06 10:46:30.521786: \n",
      "2024-01-06 10:46:30.530663: Epoch 891\n",
      "2024-01-06 10:46:30.538415: Current learning rate: 0.00136\n",
      "2024-01-06 10:48:36.612575: train_loss -0.9236\n",
      "2024-01-06 10:48:36.624101: val_loss -0.836\n",
      "2024-01-06 10:48:36.634101: Pseudo dice [0.8609, 0.9598, 0.9424]\n",
      "2024-01-06 10:48:36.643091: Epoch time: 126.09 s\n",
      "2024-01-06 10:48:37.997400: \n",
      "2024-01-06 10:48:38.003946: Epoch 892\n",
      "2024-01-06 10:48:38.008954: Current learning rate: 0.00135\n",
      "2024-01-06 10:50:43.864095: train_loss -0.9249\n",
      "2024-01-06 10:50:43.871639: val_loss -0.8317\n",
      "2024-01-06 10:50:43.879689: Pseudo dice [0.8629, 0.9593, 0.9414]\n",
      "2024-01-06 10:50:43.887676: Epoch time: 125.87 s\n",
      "2024-01-06 10:50:44.944204: \n",
      "2024-01-06 10:50:44.949767: Epoch 893\n",
      "2024-01-06 10:50:44.958773: Current learning rate: 0.00134\n",
      "2024-01-06 10:52:50.882228: train_loss -0.927\n",
      "2024-01-06 10:52:50.893234: val_loss -0.8321\n",
      "2024-01-06 10:52:50.900229: Pseudo dice [0.8604, 0.9593, 0.9416]\n",
      "2024-01-06 10:52:50.906239: Epoch time: 125.94 s\n",
      "2024-01-06 10:52:51.987745: \n",
      "2024-01-06 10:52:51.994748: Epoch 894\n",
      "2024-01-06 10:52:52.000746: Current learning rate: 0.00133\n",
      "2024-01-06 10:54:57.907571: train_loss -0.926\n",
      "2024-01-06 10:54:57.914578: val_loss -0.8373\n",
      "2024-01-06 10:54:57.921078: Pseudo dice [0.8622, 0.96, 0.9426]\n",
      "2024-01-06 10:54:57.926083: Epoch time: 125.92 s\n",
      "2024-01-06 10:54:59.062482: \n",
      "2024-01-06 10:54:59.067394: Epoch 895\n",
      "2024-01-06 10:54:59.074459: Current learning rate: 0.00132\n",
      "2024-01-06 10:57:04.687071: train_loss -0.9279\n",
      "2024-01-06 10:57:04.697072: val_loss -0.836\n",
      "2024-01-06 10:57:04.703080: Pseudo dice [0.8602, 0.9596, 0.9421]\n",
      "2024-01-06 10:57:04.710077: Epoch time: 125.63 s\n",
      "2024-01-06 10:57:06.057177: \n",
      "2024-01-06 10:57:06.065252: Epoch 896\n",
      "2024-01-06 10:57:06.073236: Current learning rate: 0.0013\n",
      "2024-01-06 10:59:11.545894: train_loss -0.9259\n",
      "2024-01-06 10:59:11.555892: val_loss -0.8321\n",
      "2024-01-06 10:59:11.564882: Pseudo dice [0.8585, 0.9588, 0.941]\n",
      "2024-01-06 10:59:11.571895: Epoch time: 125.49 s\n",
      "2024-01-06 10:59:12.656149: \n",
      "2024-01-06 10:59:12.668175: Epoch 897\n",
      "2024-01-06 10:59:12.673152: Current learning rate: 0.00129\n",
      "2024-01-06 11:01:18.267903: train_loss -0.9293\n",
      "2024-01-06 11:01:18.275903: val_loss -0.8319\n",
      "2024-01-06 11:01:18.284412: Pseudo dice [0.862, 0.9587, 0.9415]\n",
      "2024-01-06 11:01:18.294413: Epoch time: 125.61 s\n",
      "2024-01-06 11:01:19.279450: \n",
      "2024-01-06 11:01:19.285971: Epoch 898\n",
      "2024-01-06 11:01:19.290596: Current learning rate: 0.00128\n",
      "2024-01-06 11:03:27.370697: train_loss -0.9266\n",
      "2024-01-06 11:03:27.379695: val_loss -0.8348\n",
      "2024-01-06 11:03:27.386717: Pseudo dice [0.8627, 0.96, 0.9426]\n",
      "2024-01-06 11:03:27.395710: Epoch time: 128.09 s\n",
      "2024-01-06 11:03:28.468739: \n",
      "2024-01-06 11:03:28.474812: Epoch 899\n",
      "2024-01-06 11:03:28.480913: Current learning rate: 0.00127\n",
      "2024-01-06 11:05:36.420880: train_loss -0.9243\n",
      "2024-01-06 11:05:36.437970: val_loss -0.834\n",
      "2024-01-06 11:05:36.449256: Pseudo dice [0.8611, 0.9597, 0.9421]\n",
      "2024-01-06 11:05:36.460786: Epoch time: 127.95 s\n",
      "2024-01-06 11:05:38.505637: \n",
      "2024-01-06 11:05:38.511169: Epoch 900\n",
      "2024-01-06 11:05:38.516832: Current learning rate: 0.00126\n",
      "2024-01-06 11:07:46.714568: train_loss -0.9257\n",
      "2024-01-06 11:07:46.724568: val_loss -0.8354\n",
      "2024-01-06 11:07:46.733568: Pseudo dice [0.8585, 0.9599, 0.9424]\n",
      "2024-01-06 11:07:46.743568: Epoch time: 128.21 s\n",
      "2024-01-06 11:07:48.146656: \n",
      "2024-01-06 11:07:48.157716: Epoch 901\n",
      "2024-01-06 11:07:48.164693: Current learning rate: 0.00125\n",
      "2024-01-06 11:09:56.361104: train_loss -0.9273\n",
      "2024-01-06 11:09:56.373103: val_loss -0.8317\n",
      "2024-01-06 11:09:56.383103: Pseudo dice [0.8611, 0.959, 0.9413]\n",
      "2024-01-06 11:09:56.392221: Epoch time: 128.22 s\n",
      "2024-01-06 11:09:57.532809: \n",
      "2024-01-06 11:09:57.542831: Epoch 902\n",
      "2024-01-06 11:09:57.550835: Current learning rate: 0.00124\n",
      "2024-01-06 11:12:06.001935: train_loss -0.9225\n",
      "2024-01-06 11:12:06.015452: val_loss -0.8381\n",
      "2024-01-06 11:12:06.024449: Pseudo dice [0.8581, 0.9596, 0.9423]\n",
      "2024-01-06 11:12:06.034453: Epoch time: 128.47 s\n",
      "2024-01-06 11:12:07.096606: \n",
      "2024-01-06 11:12:07.103678: Epoch 903\n",
      "2024-01-06 11:12:07.110149: Current learning rate: 0.00122\n",
      "2024-01-06 11:14:13.510411: train_loss -0.9272\n",
      "2024-01-06 11:14:13.521413: val_loss -0.8337\n",
      "2024-01-06 11:14:13.531412: Pseudo dice [0.8587, 0.959, 0.9414]\n",
      "2024-01-06 11:14:13.537411: Epoch time: 126.41 s\n",
      "2024-01-06 11:14:14.498530: \n",
      "2024-01-06 11:14:14.506013: Epoch 904\n",
      "2024-01-06 11:14:14.511005: Current learning rate: 0.00121\n",
      "2024-01-06 11:16:25.006580: train_loss -0.9251\n",
      "2024-01-06 11:16:25.029580: val_loss -0.8366\n",
      "2024-01-06 11:16:25.047748: Pseudo dice [0.8614, 0.9598, 0.9427]\n",
      "2024-01-06 11:16:25.068876: Epoch time: 130.51 s\n",
      "2024-01-06 11:16:26.785392: \n",
      "2024-01-06 11:16:26.793398: Epoch 905\n",
      "2024-01-06 11:16:26.800393: Current learning rate: 0.0012\n",
      "2024-01-06 11:18:34.072850: train_loss -0.9271\n",
      "2024-01-06 11:18:34.082366: val_loss -0.8362\n",
      "2024-01-06 11:18:34.089366: Pseudo dice [0.8629, 0.9598, 0.9429]\n",
      "2024-01-06 11:18:34.105275: Epoch time: 127.29 s\n",
      "2024-01-06 11:18:35.074506: \n",
      "2024-01-06 11:18:35.083627: Epoch 906\n",
      "2024-01-06 11:18:35.088654: Current learning rate: 0.00119\n",
      "2024-01-06 11:20:42.816466: train_loss -0.9258\n",
      "2024-01-06 11:20:42.826468: val_loss -0.8342\n",
      "2024-01-06 11:20:42.836472: Pseudo dice [0.8616, 0.9602, 0.943]\n",
      "2024-01-06 11:20:42.843472: Epoch time: 127.74 s\n",
      "2024-01-06 11:20:44.156018: \n",
      "2024-01-06 11:20:44.164016: Epoch 907\n",
      "2024-01-06 11:20:44.170022: Current learning rate: 0.00118\n",
      "2024-01-06 11:22:51.389039: train_loss -0.9256\n",
      "2024-01-06 11:22:51.399035: val_loss -0.8334\n",
      "2024-01-06 11:22:51.408544: Pseudo dice [0.8593, 0.9591, 0.9417]\n",
      "2024-01-06 11:22:51.416544: Epoch time: 127.24 s\n",
      "2024-01-06 11:22:52.885206: \n",
      "2024-01-06 11:22:52.895211: Epoch 908\n",
      "2024-01-06 11:22:52.908236: Current learning rate: 0.00117\n",
      "2024-01-06 11:25:00.434267: train_loss -0.9287\n",
      "2024-01-06 11:25:00.448270: val_loss -0.8352\n",
      "2024-01-06 11:25:00.459267: Pseudo dice [0.86, 0.9592, 0.9411]\n",
      "2024-01-06 11:25:00.467270: Epoch time: 127.55 s\n",
      "2024-01-06 11:25:01.890764: \n",
      "2024-01-06 11:25:01.899457: Epoch 909\n",
      "2024-01-06 11:25:01.908524: Current learning rate: 0.00116\n",
      "2024-01-06 11:27:09.450845: train_loss -0.9251\n",
      "2024-01-06 11:27:09.462844: val_loss -0.8358\n",
      "2024-01-06 11:27:09.473850: Pseudo dice [0.8593, 0.9601, 0.9423]\n",
      "2024-01-06 11:27:09.483846: Epoch time: 127.56 s\n",
      "2024-01-06 11:27:10.564054: \n",
      "2024-01-06 11:27:10.570053: Epoch 910\n",
      "2024-01-06 11:27:10.576052: Current learning rate: 0.00115\n",
      "2024-01-06 11:29:18.086301: train_loss -0.9237\n",
      "2024-01-06 11:29:18.098301: val_loss -0.8332\n",
      "2024-01-06 11:29:18.117696: Pseudo dice [0.859, 0.9597, 0.9422]\n",
      "2024-01-06 11:29:18.125212: Epoch time: 127.52 s\n",
      "2024-01-06 11:29:19.314068: \n",
      "2024-01-06 11:29:19.326175: Epoch 911\n",
      "2024-01-06 11:29:19.331181: Current learning rate: 0.00113\n",
      "2024-01-06 11:31:26.362442: train_loss -0.9278\n",
      "2024-01-06 11:31:26.373439: val_loss -0.8326\n",
      "2024-01-06 11:31:26.384443: Pseudo dice [0.8582, 0.9596, 0.9419]\n",
      "2024-01-06 11:31:26.392442: Epoch time: 127.05 s\n",
      "2024-01-06 11:31:27.376489: \n",
      "2024-01-06 11:31:27.383480: Epoch 912\n",
      "2024-01-06 11:31:27.394239: Current learning rate: 0.00112\n",
      "2024-01-06 11:33:36.031589: train_loss -0.9262\n",
      "2024-01-06 11:33:36.045110: val_loss -0.8339\n",
      "2024-01-06 11:33:36.056139: Pseudo dice [0.8582, 0.9595, 0.9419]\n",
      "2024-01-06 11:33:36.067663: Epoch time: 128.66 s\n",
      "2024-01-06 11:33:37.706743: \n",
      "2024-01-06 11:33:37.716265: Epoch 913\n",
      "2024-01-06 11:33:37.730314: Current learning rate: 0.00111\n",
      "2024-01-06 11:35:45.752379: train_loss -0.9268\n",
      "2024-01-06 11:35:45.767397: val_loss -0.834\n",
      "2024-01-06 11:35:45.779315: Pseudo dice [0.8605, 0.96, 0.9422]\n",
      "2024-01-06 11:35:45.792504: Epoch time: 128.05 s\n",
      "2024-01-06 11:35:47.139168: \n",
      "2024-01-06 11:35:47.146679: Epoch 914\n",
      "2024-01-06 11:35:47.152705: Current learning rate: 0.0011\n",
      "2024-01-06 11:37:54.808306: train_loss -0.9279\n",
      "2024-01-06 11:37:54.822308: val_loss -0.8344\n",
      "2024-01-06 11:37:54.830308: Pseudo dice [0.8594, 0.96, 0.9424]\n",
      "2024-01-06 11:37:54.840315: Epoch time: 127.67 s\n",
      "2024-01-06 11:37:55.829242: \n",
      "2024-01-06 11:37:55.836230: Epoch 915\n",
      "2024-01-06 11:37:55.842232: Current learning rate: 0.00109\n",
      "2024-01-06 11:40:03.437175: train_loss -0.9254\n",
      "2024-01-06 11:40:03.447175: val_loss -0.8391\n",
      "2024-01-06 11:40:03.455175: Pseudo dice [0.862, 0.96, 0.9431]\n",
      "2024-01-06 11:40:03.463175: Epoch time: 127.61 s\n",
      "2024-01-06 11:40:04.815509: \n",
      "2024-01-06 11:40:04.822418: Epoch 916\n",
      "2024-01-06 11:40:04.827361: Current learning rate: 0.00108\n",
      "2024-01-06 11:42:13.220984: train_loss -0.9278\n",
      "2024-01-06 11:42:13.230984: val_loss -0.8336\n",
      "2024-01-06 11:42:13.238984: Pseudo dice [0.8589, 0.9591, 0.9416]\n",
      "2024-01-06 11:42:13.245984: Epoch time: 128.41 s\n",
      "2024-01-06 11:42:14.529819: \n",
      "2024-01-06 11:42:14.537672: Epoch 917\n",
      "2024-01-06 11:42:14.543996: Current learning rate: 0.00106\n",
      "2024-01-06 11:44:22.742491: train_loss -0.9262\n",
      "2024-01-06 11:44:22.751490: val_loss -0.8352\n",
      "2024-01-06 11:44:22.757490: Pseudo dice [0.8598, 0.9595, 0.9418]\n",
      "2024-01-06 11:44:22.766992: Epoch time: 128.21 s\n",
      "2024-01-06 11:44:23.957954: \n",
      "2024-01-06 11:44:23.963826: Epoch 918\n",
      "2024-01-06 11:44:23.968845: Current learning rate: 0.00105\n",
      "2024-01-06 11:46:31.752153: train_loss -0.9267\n",
      "2024-01-06 11:46:31.760654: val_loss -0.8371\n",
      "2024-01-06 11:46:31.770535: Pseudo dice [0.8573, 0.9597, 0.942]\n",
      "2024-01-06 11:46:31.781528: Epoch time: 127.8 s\n",
      "2024-01-06 11:46:33.032545: \n",
      "2024-01-06 11:46:33.039439: Epoch 919\n",
      "2024-01-06 11:46:33.045501: Current learning rate: 0.00104\n",
      "2024-01-06 11:48:45.950638: train_loss -0.9257\n",
      "2024-01-06 11:48:46.006352: val_loss -0.8359\n",
      "2024-01-06 11:48:46.058106: Pseudo dice [0.8579, 0.9604, 0.9431]\n",
      "2024-01-06 11:48:46.108098: Epoch time: 132.92 s\n",
      "2024-01-06 11:48:48.480471: \n",
      "2024-01-06 11:48:48.492264: Epoch 920\n",
      "2024-01-06 11:48:48.503763: Current learning rate: 0.00103\n",
      "2024-01-06 11:50:57.319277: train_loss -0.9271\n",
      "2024-01-06 11:50:57.332808: val_loss -0.8372\n",
      "2024-01-06 11:50:57.345783: Pseudo dice [0.8624, 0.9597, 0.9428]\n",
      "2024-01-06 11:50:57.359575: Epoch time: 128.84 s\n",
      "2024-01-06 11:50:58.690658: \n",
      "2024-01-06 11:50:58.698515: Epoch 921\n",
      "2024-01-06 11:50:58.706431: Current learning rate: 0.00102\n",
      "2024-01-06 11:53:08.739117: train_loss -0.9251\n",
      "2024-01-06 11:53:08.753821: val_loss -0.8345\n",
      "2024-01-06 11:53:08.767881: Pseudo dice [0.861, 0.9593, 0.9416]\n",
      "2024-01-06 11:53:08.780269: Epoch time: 130.05 s\n",
      "2024-01-06 11:53:10.218060: \n",
      "2024-01-06 11:53:10.229116: Epoch 922\n",
      "2024-01-06 11:53:10.239642: Current learning rate: 0.00101\n",
      "2024-01-06 11:55:18.457754: train_loss -0.927\n",
      "2024-01-06 11:55:18.468692: val_loss -0.8335\n",
      "2024-01-06 11:55:18.475693: Pseudo dice [0.8591, 0.959, 0.9414]\n",
      "2024-01-06 11:55:18.484694: Epoch time: 128.24 s\n",
      "2024-01-06 11:55:19.682814: \n",
      "2024-01-06 11:55:19.688842: Epoch 923\n",
      "2024-01-06 11:55:19.693843: Current learning rate: 0.001\n",
      "2024-01-06 11:57:27.777591: train_loss -0.9297\n",
      "2024-01-06 11:57:27.785591: val_loss -0.8354\n",
      "2024-01-06 11:57:27.793591: Pseudo dice [0.8615, 0.9596, 0.9422]\n",
      "2024-01-06 11:57:27.800602: Epoch time: 128.1 s\n",
      "2024-01-06 11:57:29.134540: \n",
      "2024-01-06 11:57:29.140480: Epoch 924\n",
      "2024-01-06 11:57:29.146540: Current learning rate: 0.00098\n",
      "2024-01-06 11:59:36.166576: train_loss -0.924\n",
      "2024-01-06 11:59:36.178579: val_loss -0.832\n",
      "2024-01-06 11:59:36.186587: Pseudo dice [0.858, 0.9598, 0.9419]\n",
      "2024-01-06 11:59:36.198602: Epoch time: 127.03 s\n",
      "2024-01-06 11:59:37.461928: \n",
      "2024-01-06 11:59:37.468930: Epoch 925\n",
      "2024-01-06 11:59:37.472987: Current learning rate: 0.00097\n",
      "2024-01-06 12:01:45.019235: train_loss -0.9272\n",
      "2024-01-06 12:01:45.027235: val_loss -0.8337\n",
      "2024-01-06 12:01:45.034242: Pseudo dice [0.8624, 0.9593, 0.942]\n",
      "2024-01-06 12:01:45.041235: Epoch time: 127.56 s\n",
      "2024-01-06 12:01:46.328829: \n",
      "2024-01-06 12:01:46.335912: Epoch 926\n",
      "2024-01-06 12:01:46.341968: Current learning rate: 0.00096\n",
      "2024-01-06 12:03:54.233340: train_loss -0.9284\n",
      "2024-01-06 12:03:54.243817: val_loss -0.828\n",
      "2024-01-06 12:03:54.252825: Pseudo dice [0.8628, 0.9594, 0.9422]\n",
      "2024-01-06 12:03:54.259720: Epoch time: 127.91 s\n",
      "2024-01-06 12:03:55.600383: \n",
      "2024-01-06 12:03:55.606294: Epoch 927\n",
      "2024-01-06 12:03:55.613558: Current learning rate: 0.00095\n",
      "2024-01-06 12:06:03.857308: train_loss -0.9269\n",
      "2024-01-06 12:06:03.870308: val_loss -0.8336\n",
      "2024-01-06 12:06:03.882307: Pseudo dice [0.8624, 0.9601, 0.9431]\n",
      "2024-01-06 12:06:03.889549: Epoch time: 128.26 s\n",
      "2024-01-06 12:06:05.297046: \n",
      "2024-01-06 12:06:05.304135: Epoch 928\n",
      "2024-01-06 12:06:05.309120: Current learning rate: 0.00094\n",
      "2024-01-06 12:08:13.123014: train_loss -0.9268\n",
      "2024-01-06 12:08:13.131015: val_loss -0.8365\n",
      "2024-01-06 12:08:13.138016: Pseudo dice [0.8621, 0.9594, 0.9421]\n",
      "2024-01-06 12:08:13.143017: Epoch time: 127.83 s\n",
      "2024-01-06 12:08:14.271122: \n",
      "2024-01-06 12:08:14.277124: Epoch 929\n",
      "2024-01-06 12:08:14.282179: Current learning rate: 0.00092\n",
      "2024-01-06 12:10:22.461223: train_loss -0.9279\n",
      "2024-01-06 12:10:22.473223: val_loss -0.8345\n",
      "2024-01-06 12:10:22.484224: Pseudo dice [0.861, 0.9594, 0.9418]\n",
      "2024-01-06 12:10:22.491234: Epoch time: 128.19 s\n",
      "2024-01-06 12:10:23.805409: \n",
      "2024-01-06 12:10:23.815408: Epoch 930\n",
      "2024-01-06 12:10:23.823409: Current learning rate: 0.00091\n",
      "2024-01-06 12:12:32.524678: train_loss -0.9261\n",
      "2024-01-06 12:12:32.537676: val_loss -0.8315\n",
      "2024-01-06 12:12:32.551056: Pseudo dice [0.8592, 0.9593, 0.942]\n",
      "2024-01-06 12:12:32.560057: Epoch time: 128.72 s\n",
      "2024-01-06 12:12:33.836601: \n",
      "2024-01-06 12:12:33.843602: Epoch 931\n",
      "2024-01-06 12:12:33.851645: Current learning rate: 0.0009\n",
      "2024-01-06 12:14:41.358300: train_loss -0.9262\n",
      "2024-01-06 12:14:41.370304: val_loss -0.8324\n",
      "2024-01-06 12:14:41.378307: Pseudo dice [0.8585, 0.9595, 0.9418]\n",
      "2024-01-06 12:14:41.387306: Epoch time: 127.52 s\n",
      "2024-01-06 12:14:42.810786: \n",
      "2024-01-06 12:14:42.817780: Epoch 932\n",
      "2024-01-06 12:14:42.823777: Current learning rate: 0.00089\n",
      "2024-01-06 12:16:50.258348: train_loss -0.9271\n",
      "2024-01-06 12:16:50.267858: val_loss -0.8357\n",
      "2024-01-06 12:16:50.272856: Pseudo dice [0.8599, 0.9594, 0.9419]\n",
      "2024-01-06 12:16:50.278858: Epoch time: 127.45 s\n",
      "2024-01-06 12:16:51.392693: \n",
      "2024-01-06 12:16:51.400693: Epoch 933\n",
      "2024-01-06 12:16:51.405692: Current learning rate: 0.00088\n",
      "2024-01-06 12:18:59.564241: train_loss -0.9266\n",
      "2024-01-06 12:18:59.574238: val_loss -0.8343\n",
      "2024-01-06 12:18:59.583241: Pseudo dice [0.8596, 0.9596, 0.9419]\n",
      "2024-01-06 12:18:59.591386: Epoch time: 128.17 s\n",
      "2024-01-06 12:19:00.823162: \n",
      "2024-01-06 12:19:00.829162: Epoch 934\n",
      "2024-01-06 12:19:00.834185: Current learning rate: 0.00087\n",
      "2024-01-06 12:21:07.719344: train_loss -0.9258\n",
      "2024-01-06 12:21:07.730343: val_loss -0.8377\n",
      "2024-01-06 12:21:07.737345: Pseudo dice [0.8594, 0.9599, 0.9427]\n",
      "2024-01-06 12:21:07.743349: Epoch time: 126.9 s\n",
      "2024-01-06 12:21:08.927510: \n",
      "2024-01-06 12:21:08.934505: Epoch 935\n",
      "2024-01-06 12:21:08.944510: Current learning rate: 0.00085\n",
      "2024-01-06 12:23:15.801320: train_loss -0.9259\n",
      "2024-01-06 12:23:15.811319: val_loss -0.8326\n",
      "2024-01-06 12:23:15.821321: Pseudo dice [0.8608, 0.9599, 0.9429]\n",
      "2024-01-06 12:23:15.829324: Epoch time: 126.88 s\n",
      "2024-01-06 12:23:16.939877: \n",
      "2024-01-06 12:23:16.947078: Epoch 936\n",
      "2024-01-06 12:23:16.954165: Current learning rate: 0.00084\n",
      "2024-01-06 12:25:23.544891: train_loss -0.9286\n",
      "2024-01-06 12:25:23.552890: val_loss -0.8361\n",
      "2024-01-06 12:25:23.559904: Pseudo dice [0.8609, 0.9598, 0.9423]\n",
      "2024-01-06 12:25:23.564896: Epoch time: 126.61 s\n",
      "2024-01-06 12:25:24.668592: \n",
      "2024-01-06 12:25:24.676597: Epoch 937\n",
      "2024-01-06 12:25:24.681595: Current learning rate: 0.00083\n",
      "2024-01-06 12:27:31.339234: train_loss -0.9271\n",
      "2024-01-06 12:27:31.348243: val_loss -0.8396\n",
      "2024-01-06 12:27:31.356236: Pseudo dice [0.8586, 0.9604, 0.9433]\n",
      "2024-01-06 12:27:31.362236: Epoch time: 126.67 s\n",
      "2024-01-06 12:27:32.501606: \n",
      "2024-01-06 12:27:32.508124: Epoch 938\n",
      "2024-01-06 12:27:32.513120: Current learning rate: 0.00082\n",
      "2024-01-06 12:29:39.647531: train_loss -0.9266\n",
      "2024-01-06 12:29:39.660042: val_loss -0.8372\n",
      "2024-01-06 12:29:39.672046: Pseudo dice [0.861, 0.9594, 0.9419]\n",
      "2024-01-06 12:29:39.682555: Epoch time: 127.15 s\n",
      "2024-01-06 12:29:40.899337: \n",
      "2024-01-06 12:29:40.905931: Epoch 939\n",
      "2024-01-06 12:29:40.910996: Current learning rate: 0.00081\n",
      "2024-01-06 12:31:48.648325: train_loss -0.9259\n",
      "2024-01-06 12:31:48.661841: val_loss -0.8322\n",
      "2024-01-06 12:31:48.669839: Pseudo dice [0.86, 0.9593, 0.9416]\n",
      "2024-01-06 12:31:48.677839: Epoch time: 127.75 s\n",
      "2024-01-06 12:31:50.156436: \n",
      "2024-01-06 12:31:50.162351: Epoch 940\n",
      "2024-01-06 12:31:50.168432: Current learning rate: 0.00079\n",
      "2024-01-06 12:33:57.169398: train_loss -0.9257\n",
      "2024-01-06 12:33:57.183400: val_loss -0.8374\n",
      "2024-01-06 12:33:57.190402: Pseudo dice [0.861, 0.9592, 0.9414]\n",
      "2024-01-06 12:33:57.199407: Epoch time: 127.01 s\n",
      "2024-01-06 12:33:58.326389: \n",
      "2024-01-06 12:33:58.335386: Epoch 941\n",
      "2024-01-06 12:33:58.340387: Current learning rate: 0.00078\n",
      "2024-01-06 12:36:06.171451: train_loss -0.9279\n",
      "2024-01-06 12:36:06.180951: val_loss -0.8357\n",
      "2024-01-06 12:36:06.187951: Pseudo dice [0.8584, 0.9604, 0.9429]\n",
      "2024-01-06 12:36:06.194950: Epoch time: 127.85 s\n",
      "2024-01-06 12:36:07.463140: \n",
      "2024-01-06 12:36:07.470286: Epoch 942\n",
      "2024-01-06 12:36:07.475939: Current learning rate: 0.00077\n",
      "2024-01-06 12:38:15.507521: train_loss -0.9278\n",
      "2024-01-06 12:38:15.520396: val_loss -0.8382\n",
      "2024-01-06 12:38:15.529920: Pseudo dice [0.8596, 0.9593, 0.9418]\n",
      "2024-01-06 12:38:15.541142: Epoch time: 128.05 s\n",
      "2024-01-06 12:38:16.572440: \n",
      "2024-01-06 12:38:16.581430: Epoch 943\n",
      "2024-01-06 12:38:16.587120: Current learning rate: 0.00076\n",
      "2024-01-06 12:40:24.706421: train_loss -0.9282\n",
      "2024-01-06 12:40:24.715421: val_loss -0.8348\n",
      "2024-01-06 12:40:24.725552: Pseudo dice [0.8576, 0.9598, 0.9423]\n",
      "2024-01-06 12:40:24.731554: Epoch time: 128.14 s\n",
      "2024-01-06 12:40:25.951867: \n",
      "2024-01-06 12:40:25.959019: Epoch 944\n",
      "2024-01-06 12:40:25.966028: Current learning rate: 0.00075\n",
      "2024-01-06 12:42:33.727417: train_loss -0.9268\n",
      "2024-01-06 12:42:33.739419: val_loss -0.837\n",
      "2024-01-06 12:42:33.749416: Pseudo dice [0.8598, 0.9601, 0.9425]\n",
      "2024-01-06 12:42:33.758439: Epoch time: 127.78 s\n",
      "2024-01-06 12:42:34.998000: \n",
      "2024-01-06 12:42:35.005991: Epoch 945\n",
      "2024-01-06 12:42:35.012424: Current learning rate: 0.00074\n",
      "2024-01-06 12:44:43.326621: train_loss -0.9256\n",
      "2024-01-06 12:44:43.336621: val_loss -0.8341\n",
      "2024-01-06 12:44:43.342620: Pseudo dice [0.8579, 0.9591, 0.9416]\n",
      "2024-01-06 12:44:43.347627: Epoch time: 128.33 s\n",
      "2024-01-06 12:44:44.529794: \n",
      "2024-01-06 12:44:44.538370: Epoch 946\n",
      "2024-01-06 12:44:44.543443: Current learning rate: 0.00072\n",
      "2024-01-06 12:46:51.261860: train_loss -0.9284\n",
      "2024-01-06 12:46:51.272855: val_loss -0.8336\n",
      "2024-01-06 12:46:51.279855: Pseudo dice [0.8591, 0.9592, 0.9416]\n",
      "2024-01-06 12:46:51.288855: Epoch time: 126.73 s\n",
      "2024-01-06 12:46:52.263183: \n",
      "2024-01-06 12:46:52.270848: Epoch 947\n",
      "2024-01-06 12:46:52.275909: Current learning rate: 0.00071\n",
      "2024-01-06 12:48:58.621476: train_loss -0.9273\n",
      "2024-01-06 12:48:58.631477: val_loss -0.8372\n",
      "2024-01-06 12:48:58.640478: Pseudo dice [0.8591, 0.9599, 0.9423]\n",
      "2024-01-06 12:48:58.649475: Epoch time: 126.36 s\n",
      "2024-01-06 12:48:59.819933: \n",
      "2024-01-06 12:48:59.831331: Epoch 948\n",
      "2024-01-06 12:48:59.836874: Current learning rate: 0.0007\n",
      "2024-01-06 12:51:08.047482: train_loss -0.9246\n",
      "2024-01-06 12:51:08.060997: val_loss -0.8404\n",
      "2024-01-06 12:51:08.070995: Pseudo dice [0.86, 0.9598, 0.9426]\n",
      "2024-01-06 12:51:08.079995: Epoch time: 128.23 s\n",
      "2024-01-06 12:51:09.177105: \n",
      "2024-01-06 12:51:09.187100: Epoch 949\n",
      "2024-01-06 12:51:09.196100: Current learning rate: 0.00069\n",
      "2024-01-06 12:53:16.009206: train_loss -0.9284\n",
      "2024-01-06 12:53:16.019207: val_loss -0.8421\n",
      "2024-01-06 12:53:16.029205: Pseudo dice [0.8591, 0.9602, 0.943]\n",
      "2024-01-06 12:53:16.035211: Epoch time: 126.83 s\n",
      "2024-01-06 12:53:17.364002: \n",
      "2024-01-06 12:53:17.371652: Epoch 950\n",
      "2024-01-06 12:53:17.378262: Current learning rate: 0.00067\n",
      "2024-01-06 12:55:24.687970: train_loss -0.9278\n",
      "2024-01-06 12:55:24.700977: val_loss -0.8396\n",
      "2024-01-06 12:55:24.709972: Pseudo dice [0.8606, 0.9598, 0.9424]\n",
      "2024-01-06 12:55:24.718479: Epoch time: 127.33 s\n",
      "2024-01-06 12:55:25.839818: \n",
      "2024-01-06 12:55:25.850096: Epoch 951\n",
      "2024-01-06 12:55:25.854425: Current learning rate: 0.00066\n",
      "2024-01-06 12:57:33.065770: train_loss -0.9296\n",
      "2024-01-06 12:57:33.072770: val_loss -0.8363\n",
      "2024-01-06 12:57:33.080769: Pseudo dice [0.86, 0.9598, 0.9427]\n",
      "2024-01-06 12:57:33.086770: Epoch time: 127.23 s\n",
      "2024-01-06 12:57:34.042293: \n",
      "2024-01-06 12:57:34.049196: Epoch 952\n",
      "2024-01-06 12:57:34.055221: Current learning rate: 0.00065\n",
      "2024-01-06 12:59:40.689390: train_loss -0.9308\n",
      "2024-01-06 12:59:40.700393: val_loss -0.8405\n",
      "2024-01-06 12:59:40.709398: Pseudo dice [0.8609, 0.9598, 0.9424]\n",
      "2024-01-06 12:59:40.716921: Epoch time: 126.65 s\n",
      "2024-01-06 12:59:41.743863: \n",
      "2024-01-06 12:59:41.757406: Epoch 953\n",
      "2024-01-06 12:59:41.762473: Current learning rate: 0.00064\n",
      "2024-01-06 13:01:48.453508: train_loss -0.9275\n",
      "2024-01-06 13:01:48.464507: val_loss -0.8403\n",
      "2024-01-06 13:01:48.472512: Pseudo dice [0.8583, 0.96, 0.9432]\n",
      "2024-01-06 13:01:48.488197: Epoch time: 126.71 s\n",
      "2024-01-06 13:01:49.675711: \n",
      "2024-01-06 13:01:49.683281: Epoch 954\n",
      "2024-01-06 13:01:49.689975: Current learning rate: 0.00063\n",
      "2024-01-06 13:03:58.474992: train_loss -0.9275\n",
      "2024-01-06 13:03:58.486501: val_loss -0.8378\n",
      "2024-01-06 13:03:58.496501: Pseudo dice [0.8617, 0.9599, 0.9428]\n",
      "2024-01-06 13:03:58.507504: Epoch time: 128.8 s\n",
      "2024-01-06 13:03:59.568436: \n",
      "2024-01-06 13:03:59.574502: Epoch 955\n",
      "2024-01-06 13:03:59.579437: Current learning rate: 0.00061\n",
      "2024-01-06 13:06:05.956195: train_loss -0.9281\n",
      "2024-01-06 13:06:05.966194: val_loss -0.8356\n",
      "2024-01-06 13:06:05.975196: Pseudo dice [0.8614, 0.9589, 0.9412]\n",
      "2024-01-06 13:06:05.981195: Epoch time: 126.39 s\n",
      "2024-01-06 13:06:07.139831: \n",
      "2024-01-06 13:06:07.146338: Epoch 956\n",
      "2024-01-06 13:06:07.151397: Current learning rate: 0.0006\n",
      "2024-01-06 13:08:14.131750: train_loss -0.9257\n",
      "2024-01-06 13:08:14.142742: val_loss -0.8347\n",
      "2024-01-06 13:08:14.149742: Pseudo dice [0.8584, 0.9598, 0.9423]\n",
      "2024-01-06 13:08:14.156739: Epoch time: 126.99 s\n",
      "2024-01-06 13:08:15.155173: \n",
      "2024-01-06 13:08:15.165806: Epoch 957\n",
      "2024-01-06 13:08:15.169825: Current learning rate: 0.00059\n",
      "2024-01-06 13:10:22.034882: train_loss -0.9286\n",
      "2024-01-06 13:10:22.046879: val_loss -0.8374\n",
      "2024-01-06 13:10:22.057643: Pseudo dice [0.8608, 0.9598, 0.9427]\n",
      "2024-01-06 13:10:22.066641: Epoch time: 126.88 s\n",
      "2024-01-06 13:10:23.102025: \n",
      "2024-01-06 13:10:23.108106: Epoch 958\n",
      "2024-01-06 13:10:23.113186: Current learning rate: 0.00058\n",
      "2024-01-06 13:12:30.123775: train_loss -0.9248\n",
      "2024-01-06 13:12:30.133316: val_loss -0.8381\n",
      "2024-01-06 13:12:30.143320: Pseudo dice [0.8616, 0.9596, 0.9422]\n",
      "2024-01-06 13:12:30.152833: Epoch time: 127.02 s\n",
      "2024-01-06 13:12:31.166816: \n",
      "2024-01-06 13:12:31.172802: Epoch 959\n",
      "2024-01-06 13:12:31.178952: Current learning rate: 0.00056\n",
      "2024-01-06 13:14:38.335254: train_loss -0.9273\n",
      "2024-01-06 13:14:38.342766: val_loss -0.836\n",
      "2024-01-06 13:14:38.351765: Pseudo dice [0.8603, 0.9595, 0.9421]\n",
      "2024-01-06 13:14:38.359764: Epoch time: 127.17 s\n",
      "2024-01-06 13:14:39.321552: \n",
      "2024-01-06 13:14:39.330494: Epoch 960\n",
      "2024-01-06 13:14:39.365015: Current learning rate: 0.00055\n",
      "2024-01-06 13:16:45.977886: train_loss -0.9291\n",
      "2024-01-06 13:16:45.992887: val_loss -0.8359\n",
      "2024-01-06 13:16:46.002892: Pseudo dice [0.8604, 0.9591, 0.9419]\n",
      "2024-01-06 13:16:46.009395: Epoch time: 126.66 s\n",
      "2024-01-06 13:16:47.054430: \n",
      "2024-01-06 13:16:47.060418: Epoch 961\n",
      "2024-01-06 13:16:47.066427: Current learning rate: 0.00054\n",
      "2024-01-06 13:18:54.576412: train_loss -0.927\n",
      "2024-01-06 13:18:54.587019: val_loss -0.8405\n",
      "2024-01-06 13:18:54.597623: Pseudo dice [0.8605, 0.96, 0.9425]\n",
      "2024-01-06 13:18:54.607716: Epoch time: 127.52 s\n",
      "2024-01-06 13:18:55.841331: \n",
      "2024-01-06 13:18:55.850051: Epoch 962\n",
      "2024-01-06 13:18:55.856856: Current learning rate: 0.00053\n",
      "2024-01-06 13:21:02.927965: train_loss -0.9261\n",
      "2024-01-06 13:21:02.943468: val_loss -0.8359\n",
      "2024-01-06 13:21:02.953469: Pseudo dice [0.8598, 0.9595, 0.9422]\n",
      "2024-01-06 13:21:02.961984: Epoch time: 127.09 s\n",
      "2024-01-06 13:21:04.339954: \n",
      "2024-01-06 13:21:04.346969: Epoch 963\n",
      "2024-01-06 13:21:04.352974: Current learning rate: 0.00051\n",
      "2024-01-06 13:23:11.740402: train_loss -0.9283\n",
      "2024-01-06 13:23:11.748405: val_loss -0.8375\n",
      "2024-01-06 13:23:11.756406: Pseudo dice [0.8618, 0.9599, 0.9426]\n",
      "2024-01-06 13:23:11.764404: Epoch time: 127.4 s\n",
      "2024-01-06 13:23:13.298543: \n",
      "2024-01-06 13:23:13.304534: Epoch 964\n",
      "2024-01-06 13:23:13.309542: Current learning rate: 0.0005\n",
      "2024-01-06 13:25:19.816747: train_loss -0.9272\n",
      "2024-01-06 13:25:19.826747: val_loss -0.8401\n",
      "2024-01-06 13:25:19.834746: Pseudo dice [0.8615, 0.9593, 0.9417]\n",
      "2024-01-06 13:25:19.842754: Epoch time: 126.52 s\n",
      "2024-01-06 13:25:21.039485: \n",
      "2024-01-06 13:25:21.045928: Epoch 965\n",
      "2024-01-06 13:25:21.050909: Current learning rate: 0.00049\n",
      "2024-01-06 13:27:27.569287: train_loss -0.9284\n",
      "2024-01-06 13:27:27.582290: val_loss -0.8362\n",
      "2024-01-06 13:27:27.593289: Pseudo dice [0.8614, 0.9598, 0.9425]\n",
      "2024-01-06 13:27:27.615150: Epoch time: 126.53 s\n",
      "2024-01-06 13:27:28.914078: \n",
      "2024-01-06 13:27:28.920025: Epoch 966\n",
      "2024-01-06 13:27:28.926017: Current learning rate: 0.00048\n",
      "2024-01-06 13:29:37.179132: train_loss -0.9268\n",
      "2024-01-06 13:29:37.194177: val_loss -0.8397\n",
      "2024-01-06 13:29:37.203177: Pseudo dice [0.862, 0.9598, 0.943]\n",
      "2024-01-06 13:29:37.211330: Epoch time: 128.27 s\n",
      "2024-01-06 13:29:38.641418: \n",
      "2024-01-06 13:29:38.647950: Epoch 967\n",
      "2024-01-06 13:29:38.653949: Current learning rate: 0.00046\n",
      "2024-01-06 13:31:46.216705: train_loss -0.9286\n",
      "2024-01-06 13:31:46.228711: val_loss -0.8363\n",
      "2024-01-06 13:31:46.238222: Pseudo dice [0.8602, 0.9595, 0.9421]\n",
      "2024-01-06 13:31:46.248223: Epoch time: 127.58 s\n",
      "2024-01-06 13:31:47.752934: \n",
      "2024-01-06 13:31:47.759951: Epoch 968\n",
      "2024-01-06 13:31:47.764936: Current learning rate: 0.00045\n",
      "2024-01-06 13:33:56.455167: train_loss -0.9291\n",
      "2024-01-06 13:33:56.465690: val_loss -0.8385\n",
      "2024-01-06 13:33:56.471685: Pseudo dice [0.8613, 0.9599, 0.9427]\n",
      "2024-01-06 13:33:56.479204: Epoch time: 128.71 s\n",
      "2024-01-06 13:33:57.736754: \n",
      "2024-01-06 13:33:57.743406: Epoch 969\n",
      "2024-01-06 13:33:57.748700: Current learning rate: 0.00044\n",
      "2024-01-06 13:36:04.314874: train_loss -0.9279\n",
      "2024-01-06 13:36:04.325226: val_loss -0.8345\n",
      "2024-01-06 13:36:04.333744: Pseudo dice [0.8607, 0.9593, 0.9421]\n",
      "2024-01-06 13:36:04.342339: Epoch time: 126.58 s\n",
      "2024-01-06 13:36:05.601760: \n",
      "2024-01-06 13:36:05.607756: Epoch 970\n",
      "2024-01-06 13:36:05.612574: Current learning rate: 0.00043\n",
      "2024-01-06 13:38:14.355920: train_loss -0.9266\n",
      "2024-01-06 13:38:14.368444: val_loss -0.8346\n",
      "2024-01-06 13:38:14.378964: Pseudo dice [0.8607, 0.9596, 0.942]\n",
      "2024-01-06 13:38:14.389488: Epoch time: 128.76 s\n",
      "2024-01-06 13:38:15.758245: \n",
      "2024-01-06 13:38:15.766281: Epoch 971\n",
      "2024-01-06 13:38:15.774805: Current learning rate: 0.00041\n",
      "2024-01-06 13:40:25.131319: train_loss -0.9266\n",
      "2024-01-06 13:40:25.144933: val_loss -0.8331\n",
      "2024-01-06 13:40:25.157227: Pseudo dice [0.8595, 0.9597, 0.942]\n",
      "2024-01-06 13:40:25.168226: Epoch time: 129.38 s\n",
      "2024-01-06 13:40:27.270355: \n",
      "2024-01-06 13:40:27.282561: Epoch 972\n",
      "2024-01-06 13:40:27.289792: Current learning rate: 0.0004\n",
      "2024-01-06 13:42:36.058921: train_loss -0.928\n",
      "2024-01-06 13:42:36.070444: val_loss -0.8339\n",
      "2024-01-06 13:42:36.081484: Pseudo dice [0.8582, 0.9598, 0.9426]\n",
      "2024-01-06 13:42:36.089994: Epoch time: 128.79 s\n",
      "2024-01-06 13:42:37.590114: \n",
      "2024-01-06 13:42:37.598114: Epoch 973\n",
      "2024-01-06 13:42:37.602624: Current learning rate: 0.00039\n",
      "2024-01-06 13:44:44.376567: train_loss -0.9285\n",
      "2024-01-06 13:44:44.387568: val_loss -0.8327\n",
      "2024-01-06 13:44:44.395569: Pseudo dice [0.8578, 0.9592, 0.9414]\n",
      "2024-01-06 13:44:44.401567: Epoch time: 126.79 s\n",
      "2024-01-06 13:44:45.795831: \n",
      "2024-01-06 13:44:45.804831: Epoch 974\n",
      "2024-01-06 13:44:45.809828: Current learning rate: 0.00037\n",
      "2024-01-06 13:46:52.745238: train_loss -0.927\n",
      "2024-01-06 13:46:52.752238: val_loss -0.8324\n",
      "2024-01-06 13:46:52.759242: Pseudo dice [0.8596, 0.9595, 0.9419]\n",
      "2024-01-06 13:46:52.765238: Epoch time: 126.95 s\n",
      "2024-01-06 13:46:53.965314: \n",
      "2024-01-06 13:46:53.972314: Epoch 975\n",
      "2024-01-06 13:46:53.978446: Current learning rate: 0.00036\n",
      "2024-01-06 13:49:00.537140: train_loss -0.9265\n",
      "2024-01-06 13:49:00.548141: val_loss -0.835\n",
      "2024-01-06 13:49:00.554140: Pseudo dice [0.8602, 0.9597, 0.9423]\n",
      "2024-01-06 13:49:00.559140: Epoch time: 126.57 s\n",
      "2024-01-06 13:49:01.766912: \n",
      "2024-01-06 13:49:01.772982: Epoch 976\n",
      "2024-01-06 13:49:01.781251: Current learning rate: 0.00035\n",
      "2024-01-06 13:51:07.947718: train_loss -0.93\n",
      "2024-01-06 13:51:07.955723: val_loss -0.8356\n",
      "2024-01-06 13:51:07.963723: Pseudo dice [0.8599, 0.96, 0.9424]\n",
      "2024-01-06 13:51:07.970236: Epoch time: 126.18 s\n",
      "2024-01-06 13:51:09.138428: \n",
      "2024-01-06 13:51:09.147353: Epoch 977\n",
      "2024-01-06 13:51:09.152095: Current learning rate: 0.00034\n",
      "2024-01-06 13:53:16.000870: train_loss -0.9289\n",
      "2024-01-06 13:53:16.010871: val_loss -0.8379\n",
      "2024-01-06 13:53:16.017872: Pseudo dice [0.8604, 0.9602, 0.9429]\n",
      "2024-01-06 13:53:16.025933: Epoch time: 126.87 s\n",
      "2024-01-06 13:53:17.265236: \n",
      "2024-01-06 13:53:17.271236: Epoch 978\n",
      "2024-01-06 13:53:17.276236: Current learning rate: 0.00032\n",
      "2024-01-06 13:55:24.121141: train_loss -0.9287\n",
      "2024-01-06 13:55:24.131139: val_loss -0.8362\n",
      "2024-01-06 13:55:24.139140: Pseudo dice [0.8603, 0.9598, 0.9422]\n",
      "2024-01-06 13:55:24.148139: Epoch time: 126.86 s\n",
      "2024-01-06 13:55:25.416868: \n",
      "2024-01-06 13:55:25.424000: Epoch 979\n",
      "2024-01-06 13:55:25.428751: Current learning rate: 0.00031\n",
      "2024-01-06 13:57:31.785519: train_loss -0.928\n",
      "2024-01-06 13:57:31.795526: val_loss -0.8342\n",
      "2024-01-06 13:57:31.803526: Pseudo dice [0.8592, 0.9597, 0.942]\n",
      "2024-01-06 13:57:31.812037: Epoch time: 126.37 s\n",
      "2024-01-06 13:57:32.995118: \n",
      "2024-01-06 13:57:33.005071: Epoch 980\n",
      "2024-01-06 13:57:33.009725: Current learning rate: 0.0003\n",
      "2024-01-06 13:59:38.921616: train_loss -0.9308\n",
      "2024-01-06 13:59:38.932213: val_loss -0.8327\n",
      "2024-01-06 13:59:38.939213: Pseudo dice [0.8608, 0.9594, 0.942]\n",
      "2024-01-06 13:59:38.947226: Epoch time: 125.93 s\n",
      "2024-01-06 13:59:40.055205: \n",
      "2024-01-06 13:59:40.065945: Epoch 981\n",
      "2024-01-06 13:59:40.074955: Current learning rate: 0.00028\n",
      "2024-01-06 14:01:45.963485: train_loss -0.9284\n",
      "2024-01-06 14:01:45.972473: val_loss -0.8357\n",
      "2024-01-06 14:01:45.978474: Pseudo dice [0.8591, 0.9596, 0.9422]\n",
      "2024-01-06 14:01:45.987022: Epoch time: 125.91 s\n",
      "2024-01-06 14:01:47.109741: \n",
      "2024-01-06 14:01:47.115666: Epoch 982\n",
      "2024-01-06 14:01:47.120421: Current learning rate: 0.00027\n",
      "2024-01-06 14:03:52.982501: train_loss -0.9311\n",
      "2024-01-06 14:03:52.989502: val_loss -0.8363\n",
      "2024-01-06 14:03:52.995501: Pseudo dice [0.8602, 0.9597, 0.9421]\n",
      "2024-01-06 14:03:53.003500: Epoch time: 125.87 s\n",
      "2024-01-06 14:03:54.094184: \n",
      "2024-01-06 14:03:54.100179: Epoch 983\n",
      "2024-01-06 14:03:54.105178: Current learning rate: 0.00026\n",
      "2024-01-06 14:06:00.117064: train_loss -0.9287\n",
      "2024-01-06 14:06:00.127063: val_loss -0.8356\n",
      "2024-01-06 14:06:00.136063: Pseudo dice [0.8599, 0.9592, 0.9419]\n",
      "2024-01-06 14:06:00.142063: Epoch time: 126.02 s\n",
      "2024-01-06 14:06:01.381619: \n",
      "2024-01-06 14:06:01.387974: Epoch 984\n",
      "2024-01-06 14:06:01.394974: Current learning rate: 0.00024\n",
      "2024-01-06 14:08:07.903501: train_loss -0.9266\n",
      "2024-01-06 14:08:07.911503: val_loss -0.8397\n",
      "2024-01-06 14:08:07.917539: Pseudo dice [0.8614, 0.96, 0.9425]\n",
      "2024-01-06 14:08:07.926044: Epoch time: 126.52 s\n",
      "2024-01-06 14:08:09.097374: \n",
      "2024-01-06 14:08:09.110314: Epoch 985\n",
      "2024-01-06 14:08:09.115332: Current learning rate: 0.00023\n",
      "2024-01-06 14:10:15.001863: train_loss -0.9281\n",
      "2024-01-06 14:10:15.009863: val_loss -0.8348\n",
      "2024-01-06 14:10:15.016862: Pseudo dice [0.861, 0.9594, 0.9416]\n",
      "2024-01-06 14:10:15.022863: Epoch time: 125.91 s\n",
      "2024-01-06 14:10:16.177515: \n",
      "2024-01-06 14:10:16.184337: Epoch 986\n",
      "2024-01-06 14:10:16.189282: Current learning rate: 0.00021\n",
      "2024-01-06 14:12:22.150332: train_loss -0.9295\n",
      "2024-01-06 14:12:22.161334: val_loss -0.8369\n",
      "2024-01-06 14:12:22.172339: Pseudo dice [0.8608, 0.9594, 0.942]\n",
      "2024-01-06 14:12:22.181854: Epoch time: 125.97 s\n",
      "2024-01-06 14:12:23.587539: \n",
      "2024-01-06 14:12:23.596530: Epoch 987\n",
      "2024-01-06 14:12:23.606474: Current learning rate: 0.0002\n",
      "2024-01-06 14:14:29.309684: train_loss -0.9293\n",
      "2024-01-06 14:14:29.317684: val_loss -0.8345\n",
      "2024-01-06 14:14:29.326692: Pseudo dice [0.8595, 0.9594, 0.9416]\n",
      "2024-01-06 14:14:29.333696: Epoch time: 125.72 s\n",
      "2024-01-06 14:14:30.565935: \n",
      "2024-01-06 14:14:30.573936: Epoch 988\n",
      "2024-01-06 14:14:30.580937: Current learning rate: 0.00019\n",
      "2024-01-06 14:16:37.078528: train_loss -0.9278\n",
      "2024-01-06 14:16:37.089528: val_loss -0.838\n",
      "2024-01-06 14:16:37.097544: Pseudo dice [0.8616, 0.9602, 0.9429]\n",
      "2024-01-06 14:16:37.102542: Epoch time: 126.51 s\n",
      "2024-01-06 14:16:38.244497: \n",
      "2024-01-06 14:16:38.250489: Epoch 989\n",
      "2024-01-06 14:16:38.254497: Current learning rate: 0.00017\n",
      "2024-01-06 14:18:44.261603: train_loss -0.9275\n",
      "2024-01-06 14:18:44.273610: val_loss -0.8358\n",
      "2024-01-06 14:18:44.284128: Pseudo dice [0.8602, 0.9596, 0.9424]\n",
      "2024-01-06 14:18:44.295127: Epoch time: 126.02 s\n",
      "2024-01-06 14:18:45.723113: \n",
      "2024-01-06 14:18:45.729370: Epoch 990\n",
      "2024-01-06 14:18:45.739374: Current learning rate: 0.00016\n",
      "2024-01-06 14:20:52.253512: train_loss -0.9286\n",
      "2024-01-06 14:20:52.261513: val_loss -0.8335\n",
      "2024-01-06 14:20:52.269026: Pseudo dice [0.86, 0.9596, 0.942]\n",
      "2024-01-06 14:20:52.275028: Epoch time: 126.53 s\n",
      "2024-01-06 14:20:53.412338: \n",
      "2024-01-06 14:20:53.418457: Epoch 991\n",
      "2024-01-06 14:20:53.423459: Current learning rate: 0.00014\n",
      "2024-01-06 14:22:59.190888: train_loss -0.9284\n",
      "2024-01-06 14:22:59.199904: val_loss -0.8371\n",
      "2024-01-06 14:22:59.204924: Pseudo dice [0.8599, 0.96, 0.9426]\n",
      "2024-01-06 14:22:59.214412: Epoch time: 125.78 s\n",
      "2024-01-06 14:23:00.394509: \n",
      "2024-01-06 14:23:00.400246: Epoch 992\n",
      "2024-01-06 14:23:00.407319: Current learning rate: 0.00013\n",
      "2024-01-06 14:25:08.157252: train_loss -0.9286\n",
      "2024-01-06 14:25:08.171289: val_loss -0.8347\n",
      "2024-01-06 14:25:08.183820: Pseudo dice [0.8597, 0.9594, 0.9416]\n",
      "2024-01-06 14:25:08.191333: Epoch time: 127.76 s\n",
      "2024-01-06 14:25:09.612621: \n",
      "2024-01-06 14:25:09.621143: Epoch 993\n",
      "2024-01-06 14:25:09.626665: Current learning rate: 0.00011\n",
      "2024-01-06 14:27:18.184284: train_loss -0.9285\n",
      "2024-01-06 14:27:18.197284: val_loss -0.8367\n",
      "2024-01-06 14:27:18.209796: Pseudo dice [0.8612, 0.9599, 0.9426]\n",
      "2024-01-06 14:27:18.219796: Epoch time: 128.57 s\n",
      "2024-01-06 14:27:19.850469: \n",
      "2024-01-06 14:27:19.861470: Epoch 994\n",
      "2024-01-06 14:27:19.868480: Current learning rate: 0.0001\n",
      "2024-01-06 14:29:27.280094: train_loss -0.9273\n",
      "2024-01-06 14:29:27.296228: val_loss -0.8371\n",
      "2024-01-06 14:29:27.308834: Pseudo dice [0.861, 0.9594, 0.9424]\n",
      "2024-01-06 14:29:27.319712: Epoch time: 127.43 s\n",
      "2024-01-06 14:29:29.203104: \n",
      "2024-01-06 14:29:29.209850: Epoch 995\n",
      "2024-01-06 14:29:29.214848: Current learning rate: 8e-05\n",
      "2024-01-06 14:31:37.616004: train_loss -0.9307\n",
      "2024-01-06 14:31:37.626007: val_loss -0.8362\n",
      "2024-01-06 14:31:37.634007: Pseudo dice [0.8612, 0.9594, 0.9418]\n",
      "2024-01-06 14:31:37.640518: Epoch time: 128.41 s\n",
      "2024-01-06 14:31:39.016281: \n",
      "2024-01-06 14:31:39.023289: Epoch 996\n",
      "2024-01-06 14:31:39.029285: Current learning rate: 7e-05\n",
      "2024-01-06 14:33:47.642566: train_loss -0.9304\n",
      "2024-01-06 14:33:47.652062: val_loss -0.8374\n",
      "2024-01-06 14:33:47.660069: Pseudo dice [0.8603, 0.9594, 0.9421]\n",
      "2024-01-06 14:33:47.669580: Epoch time: 128.63 s\n",
      "2024-01-06 14:33:48.668608: \n",
      "2024-01-06 14:33:48.674762: Epoch 997\n",
      "2024-01-06 14:33:48.680280: Current learning rate: 5e-05\n",
      "2024-01-06 14:35:57.936071: train_loss -0.9315\n",
      "2024-01-06 14:35:57.948593: val_loss -0.8384\n",
      "2024-01-06 14:35:57.960104: Pseudo dice [0.8611, 0.9598, 0.9426]\n",
      "2024-01-06 14:35:57.969620: Epoch time: 129.27 s\n",
      "2024-01-06 14:35:59.204717: \n",
      "2024-01-06 14:35:59.213253: Epoch 998\n",
      "2024-01-06 14:35:59.220267: Current learning rate: 4e-05\n",
      "2024-01-06 14:38:08.677491: train_loss -0.9234\n",
      "2024-01-06 14:38:08.690488: val_loss -0.837\n",
      "2024-01-06 14:38:08.700487: Pseudo dice [0.8615, 0.9595, 0.9421]\n",
      "2024-01-06 14:38:08.711489: Epoch time: 129.47 s\n",
      "2024-01-06 14:38:09.926680: \n",
      "2024-01-06 14:38:09.933680: Epoch 999\n",
      "2024-01-06 14:38:09.939681: Current learning rate: 2e-05\n",
      "2024-01-06 14:40:17.447376: train_loss -0.9287\n",
      "2024-01-06 14:40:17.463633: val_loss -0.8364\n",
      "2024-01-06 14:40:17.479632: Pseudo dice [0.8608, 0.9596, 0.9424]\n",
      "2024-01-06 14:40:17.495639: Epoch time: 127.52 s\n",
      "2024-01-06 14:40:20.016622: Training done.\n",
      "2024-01-06 14:40:20.062201: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-06 14:40:20.074204: The split file contains 5 splits.\n",
      "2024-01-06 14:40:20.081200: Desired fold for training: 3\n",
      "2024-01-06 14:40:20.089202: This split has 8 training and 2 validation cases.\n",
      "2024-01-06 14:40:20.096201: predicting case_1\n",
      "2024-01-06 14:40:22.038422: predicting case_8\n",
      "2024-01-06 14:40:33.725587: Validation complete\n",
      "2024-01-06 14:40:33.732238: Mean Validation Dice:  0.9264936556324637\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-06 14:40:34.573893: unpacking dataset...\n",
      "2024-01-06 14:40:34.842373: unpacking done...\n",
      "2024-01-06 14:40:34.848326: do_dummy_2d_data_aug: False\n",
      "2024-01-06 14:40:34.852326: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-06 14:40:34.855916: The split file contains 5 splits.\n",
      "2024-01-06 14:40:34.859923: Desired fold for training: 4\n",
      "2024-01-06 14:40:34.863260: This split has 8 training and 2 validation cases.\n",
      "2024-01-06 14:40:34.898215: Unable to plot network architecture:\n",
      "2024-01-06 14:40:34.902213: No module named 'hiddenlayer'\n",
      "2024-01-06 14:40:34.934642: \n",
      "2024-01-06 14:40:34.939642: Epoch 0\n",
      "2024-01-06 14:40:34.943642: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-06 14:42:59.703888: train_loss -0.4071\n",
      "2024-01-06 14:42:59.716883: val_loss -0.6826\n",
      "2024-01-06 14:42:59.728816: Pseudo dice [0.8135, 0.9087, 0.8971]\n",
      "2024-01-06 14:42:59.740823: Epoch time: 144.77 s\n",
      "2024-01-06 14:42:59.752896: Yayy! New best EMA pseudo Dice: 0.8731\n",
      "2024-01-06 14:43:01.454256: \n",
      "2024-01-06 14:43:01.461278: Epoch 1\n",
      "2024-01-06 14:43:01.466284: Current learning rate: 0.00999\n",
      "2024-01-06 14:45:07.494374: train_loss -0.6965\n",
      "2024-01-06 14:45:07.503374: val_loss -0.7359\n",
      "2024-01-06 14:45:07.509376: Pseudo dice [0.8365, 0.9183, 0.9035]\n",
      "2024-01-06 14:45:07.517382: Epoch time: 126.04 s\n",
      "2024-01-06 14:45:07.524379: Yayy! New best EMA pseudo Dice: 0.8744\n",
      "2024-01-06 14:45:08.755719: \n",
      "2024-01-06 14:45:08.761714: Epoch 2\n",
      "2024-01-06 14:45:08.765714: Current learning rate: 0.00998\n",
      "2024-01-06 14:47:14.732882: train_loss -0.7436\n",
      "2024-01-06 14:47:14.739882: val_loss -0.7548\n",
      "2024-01-06 14:47:14.746884: Pseudo dice [0.8399, 0.9238, 0.9078]\n",
      "2024-01-06 14:47:14.753881: Epoch time: 125.98 s\n",
      "2024-01-06 14:47:14.760881: Yayy! New best EMA pseudo Dice: 0.876\n",
      "2024-01-06 14:47:16.268101: \n",
      "2024-01-06 14:47:16.273101: Epoch 3\n",
      "2024-01-06 14:47:16.278093: Current learning rate: 0.00997\n",
      "2024-01-06 14:49:22.730216: train_loss -0.7712\n",
      "2024-01-06 14:49:22.739224: val_loss -0.7693\n",
      "2024-01-06 14:49:22.746222: Pseudo dice [0.8481, 0.9287, 0.9126]\n",
      "2024-01-06 14:49:22.751728: Epoch time: 126.46 s\n",
      "2024-01-06 14:49:22.758733: Yayy! New best EMA pseudo Dice: 0.878\n",
      "2024-01-06 14:49:24.036540: \n",
      "2024-01-06 14:49:24.041597: Epoch 4\n",
      "2024-01-06 14:49:24.046544: Current learning rate: 0.00996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\apps\\nnunet\\nnunetv2_runner.py:957\u001b[0m, in \u001b[0;36mnnUNetV2Runner.run\u001b[1;34m(self, run_convert_dataset, run_plan_and_process, run_train, run_find_best_configuration, run_predict_ensemble_postprocessing)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplan_and_process()\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_train:\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_find_best_configuration:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_best_configuration()\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\apps\\nnunet\\nnunetv2_runner.py:589\u001b[0m, in \u001b[0;36mnnUNetV2Runner.train\u001b[1;34m(self, configs, gpu_id_for_all, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m ensure_tuple(configs):\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_folds):\n\u001b[1;32m--> 589\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_single_model(config\u001b[38;5;241m=\u001b[39mcfg, fold\u001b[38;5;241m=\u001b[39m_fold, gpu_id\u001b[38;5;241m=\u001b[39mgpu_id_for_all, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\apps\\nnunet\\nnunetv2_runner.py:538\u001b[0m, in \u001b[0;36mnnUNetV2Runner.train_single_model\u001b[1;34m(self, config, fold, gpu_id, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnnunetv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_training\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpu_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gpu_id) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 538\u001b[0m     run_training(\n\u001b[0;32m    539\u001b[0m         dataset_name_or_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name_or_id,\n\u001b[0;32m    540\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    541\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m    542\u001b[0m         trainer_class_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_class_name,\n\u001b[0;32m    543\u001b[0m         export_validation_probabilities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_validation_probabilities,\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    545\u001b[0m     )\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     run_training(\n\u001b[0;32m    548\u001b[0m         dataset_name_or_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name_or_id,\n\u001b[0;32m    549\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\run\\run_training.py:204\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(dataset_name_or_id, configuration, fold, trainer_class_name, plans_identifier, pretrained_weights, num_gpus, use_compressed_data, export_validation_probabilities, continue_training, only_run_validation, disable_checkpointing, val_with_best, device)\u001b[0m\n\u001b[0;32m    201\u001b[0m     cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_run_validation:\n\u001b[1;32m--> 204\u001b[0m     \u001b[43mnnunet_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_with_best:\n\u001b[0;32m    207\u001b[0m     nnunet_trainer\u001b[38;5;241m.\u001b[39mload_checkpoint(join(nnunet_trainer\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py:1242\u001b[0m, in \u001b[0;36mnnUNetTrainer.run_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1240\u001b[0m train_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations_per_epoch):\n\u001b[1;32m-> 1242\u001b[0m     train_outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_train_epoch_end(train_outputs)\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py:891\u001b[0m, in \u001b[0;36mnnUNetTrainer.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[0;32m    890\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 315\u001b[0m     retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[1;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 220\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\torch\\optim\\sgd.py:293\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[1;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    291\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m momentum \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    296\u001b[0m     bufs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-10 09:07:51,758 - INFO - number of GPUs is 1, device ids are (0,)\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:07:54.850124: unpacking dataset...\n",
      "2024-01-10 09:07:55.237515: unpacking done...\n",
      "2024-01-10 09:07:55.244206: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:07:55.249739: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:07:55.257744: The split file contains 5 splits.\n",
      "2024-01-10 09:07:55.261748: Desired fold for training: 0\n",
      "2024-01-10 09:07:55.265759: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:07:55.319387: Unable to plot network architecture:\n",
      "2024-01-10 09:07:55.323385: No module named 'hiddenlayer'\n",
      "2024-01-10 09:07:55.743830: Training done.\n",
      "2024-01-10 09:07:55.784878: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:07:55.793879: The split file contains 5 splits.\n",
      "2024-01-10 09:07:55.801878: Desired fold for training: 0\n",
      "2024-01-10 09:07:55.808880: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:07:55.815878: predicting case_0\n",
      "2024-01-10 09:08:00.302614: predicting case_7\n",
      "2024-01-10 09:08:11.345051: Validation complete\n",
      "2024-01-10 09:08:11.350055: Mean Validation Dice:  0.9383910601885285\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:08:12.765655: unpacking dataset...\n",
      "2024-01-10 09:08:13.029180: unpacking done...\n",
      "2024-01-10 09:08:13.035180: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:08:13.039757: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:13.043763: The split file contains 5 splits.\n",
      "2024-01-10 09:08:13.047772: Desired fold for training: 1\n",
      "2024-01-10 09:08:13.052039: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:13.090263: Unable to plot network architecture:\n",
      "2024-01-10 09:08:13.094262: No module named 'hiddenlayer'\n",
      "2024-01-10 09:08:13.478305: Training done.\n",
      "2024-01-10 09:08:13.518681: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:13.528688: The split file contains 5 splits.\n",
      "2024-01-10 09:08:13.535688: Desired fold for training: 1\n",
      "2024-01-10 09:08:13.543690: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:13.550694: predicting case_3\n",
      "2024-01-10 09:08:16.468706: predicting case_9\n",
      "2024-01-10 09:08:28.030509: Validation complete\n",
      "2024-01-10 09:08:28.035182: Mean Validation Dice:  0.9380037343185083\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:08:29.345884: unpacking dataset...\n",
      "2024-01-10 09:08:29.588669: unpacking done...\n",
      "2024-01-10 09:08:29.594669: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:08:29.598677: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:29.602876: The split file contains 5 splits.\n",
      "2024-01-10 09:08:29.606311: Desired fold for training: 2\n",
      "2024-01-10 09:08:29.609382: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:29.654603: Unable to plot network architecture:\n",
      "2024-01-10 09:08:29.659612: No module named 'hiddenlayer'\n",
      "2024-01-10 09:08:30.003587: Training done.\n",
      "2024-01-10 09:08:30.032568: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:30.037571: The split file contains 5 splits.\n",
      "2024-01-10 09:08:30.043568: Desired fold for training: 2\n",
      "2024-01-10 09:08:30.047572: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:30.053568: predicting case_4\n",
      "2024-01-10 09:08:32.879580: predicting case_6\n",
      "2024-01-10 09:08:41.984052: Validation complete\n",
      "2024-01-10 09:08:41.990052: Mean Validation Dice:  0.9314290294423321\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:08:43.269121: unpacking dataset...\n",
      "2024-01-10 09:08:43.517348: unpacking done...\n",
      "2024-01-10 09:08:43.523348: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:08:43.527358: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:43.531347: The split file contains 5 splits.\n",
      "2024-01-10 09:08:43.534356: Desired fold for training: 3\n",
      "2024-01-10 09:08:43.538359: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:43.561356: Unable to plot network architecture:\n",
      "2024-01-10 09:08:43.566373: No module named 'hiddenlayer'\n",
      "2024-01-10 09:08:43.934245: Training done.\n",
      "2024-01-10 09:08:43.972252: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:43.980253: The split file contains 5 splits.\n",
      "2024-01-10 09:08:43.987309: Desired fold for training: 3\n",
      "2024-01-10 09:08:43.994445: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:44.002463: predicting case_1\n",
      "2024-01-10 09:08:45.808526: predicting case_8\n",
      "2024-01-10 09:08:56.774180: Validation complete\n",
      "2024-01-10 09:08:56.779691: Mean Validation Dice:  0.926492664679912\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 128, 160], 'median_image_size_in_voxels': [115.0, 139.0, 144.5], 'spacing': [1.5, 0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:08:58.071212: unpacking dataset...\n",
      "2024-01-10 09:08:58.319273: unpacking done...\n",
      "2024-01-10 09:08:58.325148: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:08:58.329474: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:58.333473: The split file contains 5 splits.\n",
      "2024-01-10 09:08:58.336479: Desired fold for training: 4\n",
      "2024-01-10 09:08:58.339478: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:58.387864: Unable to plot network architecture:\n",
      "2024-01-10 09:08:58.392385: No module named 'hiddenlayer'\n",
      "2024-01-10 09:08:58.860134: Training done.\n",
      "2024-01-10 09:08:58.886199: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:08:58.891713: The split file contains 5 splits.\n",
      "2024-01-10 09:08:58.896958: Desired fold for training: 4\n",
      "2024-01-10 09:08:58.901957: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:08:58.907954: predicting case_2\n",
      "2024-01-10 09:09:01.668934: predicting case_5\n",
      "2024-01-10 09:09:11.743895: Validation complete\n",
      "2024-01-10 09:09:11.749934: Mean Validation Dice:  0.9286150704223748\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': [160, 160], 'median_image_size_in_voxels': [139.0, 144.5], 'spacing': [0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:09:12.760186: unpacking dataset...\n",
      "2024-01-10 09:09:13.012959: unpacking done...\n",
      "2024-01-10 09:09:13.017958: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:09:13.021966: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:09:13.025959: The split file contains 5 splits.\n",
      "2024-01-10 09:09:13.028966: Desired fold for training: 0\n",
      "2024-01-10 09:09:13.032958: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:09:13.077881: Unable to plot network architecture:\n",
      "2024-01-10 09:09:13.081872: No module named 'hiddenlayer'\n",
      "2024-01-10 09:09:13.353481: Training done.\n",
      "2024-01-10 09:09:13.381477: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:09:13.388482: The split file contains 5 splits.\n",
      "2024-01-10 09:09:13.394482: Desired fold for training: 0\n",
      "2024-01-10 09:09:13.399987: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:09:13.405995: predicting case_0\n",
      "2024-01-10 09:09:16.821543: predicting case_7\n",
      "2024-01-10 09:09:27.694454: Validation complete\n",
      "2024-01-10 09:09:27.699454: Mean Validation Dice:  0.9390641955813007\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': [160, 160], 'median_image_size_in_voxels': [139.0, 144.5], 'spacing': [0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:09:28.687130: unpacking dataset...\n",
      "2024-01-10 09:09:28.922226: unpacking done...\n",
      "2024-01-10 09:09:28.928225: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:09:28.933241: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:09:28.938224: The split file contains 5 splits.\n",
      "2024-01-10 09:09:28.942235: Desired fold for training: 1\n",
      "2024-01-10 09:09:28.947224: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:09:28.983599: Unable to plot network architecture:\n",
      "2024-01-10 09:09:28.988905: No module named 'hiddenlayer'\n",
      "2024-01-10 09:09:29.255967: Training done.\n",
      "2024-01-10 09:09:29.282668: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:09:29.288669: The split file contains 5 splits.\n",
      "2024-01-10 09:09:29.293670: Desired fold for training: 1\n",
      "2024-01-10 09:09:29.298675: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:09:29.303672: predicting case_3\n",
      "2024-01-10 09:09:31.877030: predicting case_9\n",
      "2024-01-10 09:09:43.887418: Validation complete\n",
      "2024-01-10 09:09:43.892410: Mean Validation Dice:  0.9373409791466285\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': [160, 160], 'median_image_size_in_voxels': [139.0, 144.5], 'spacing': [0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 09:09:44.885427: unpacking dataset...\n",
      "2024-01-10 09:09:45.133332: unpacking done...\n",
      "2024-01-10 09:09:45.139146: do_dummy_2d_data_aug: False\n",
      "2024-01-10 09:09:45.143144: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 09:09:45.147144: The split file contains 5 splits.\n",
      "2024-01-10 09:09:45.150367: Desired fold for training: 2\n",
      "2024-01-10 09:09:45.154382: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 09:09:45.197947: Unable to plot network architecture:\n",
      "2024-01-10 09:09:45.202947: No module named 'hiddenlayer'\n",
      "2024-01-10 09:09:45.224540: \n",
      "2024-01-10 09:09:45.228553: Epoch 450\n",
      "2024-01-10 09:09:45.232547: Current learning rate: 0.00584\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-10 09:10:37.245686: train_loss -0.9428\n",
      "2024-01-10 09:10:37.286685: val_loss -0.8\n",
      "2024-01-10 09:10:37.295687: Pseudo dice [0.9225, 0.9289, 0.937]\n",
      "2024-01-10 09:10:37.304037: Epoch time: 52.02 s\n",
      "2024-01-10 09:10:38.428550: \n",
      "2024-01-10 09:10:38.436407: Epoch 451\n",
      "2024-01-10 09:10:38.441405: Current learning rate: 0.00583\n",
      "2024-01-10 09:11:17.757131: train_loss -0.9421\n",
      "2024-01-10 09:11:17.766644: val_loss -0.8042\n",
      "2024-01-10 09:11:17.775643: Pseudo dice [0.9223, 0.9296, 0.9376]\n",
      "2024-01-10 09:11:17.784641: Epoch time: 39.33 s\n",
      "2024-01-10 09:11:18.971978: \n",
      "2024-01-10 09:11:18.976978: Epoch 452\n",
      "2024-01-10 09:11:18.980974: Current learning rate: 0.00582\n",
      "2024-01-10 09:11:58.617032: train_loss -0.9426\n",
      "2024-01-10 09:11:58.652032: val_loss -0.7963\n",
      "2024-01-10 09:11:58.660033: Pseudo dice [0.9229, 0.9271, 0.9353]\n",
      "2024-01-10 09:11:58.668048: Epoch time: 39.65 s\n",
      "2024-01-10 09:11:59.913766: \n",
      "2024-01-10 09:11:59.919760: Epoch 453\n",
      "2024-01-10 09:11:59.926757: Current learning rate: 0.00581\n",
      "2024-01-10 09:12:39.863860: train_loss -0.9418\n",
      "2024-01-10 09:12:39.872865: val_loss -0.8054\n",
      "2024-01-10 09:12:39.880866: Pseudo dice [0.9223, 0.9303, 0.9379]\n",
      "2024-01-10 09:12:39.887865: Epoch time: 39.95 s\n",
      "2024-01-10 09:12:41.210875: \n",
      "2024-01-10 09:12:41.215879: Epoch 454\n",
      "2024-01-10 09:12:41.220869: Current learning rate: 0.0058\n",
      "2024-01-10 09:13:21.279394: train_loss -0.943\n",
      "2024-01-10 09:13:21.289395: val_loss -0.8074\n",
      "2024-01-10 09:13:21.297397: Pseudo dice [0.9245, 0.932, 0.94]\n",
      "2024-01-10 09:13:21.305401: Epoch time: 40.07 s\n",
      "2024-01-10 09:13:22.465176: \n",
      "2024-01-10 09:13:22.470161: Epoch 455\n",
      "2024-01-10 09:13:22.474174: Current learning rate: 0.00579\n",
      "2024-01-10 09:14:03.129452: train_loss -0.9427\n",
      "2024-01-10 09:14:03.138966: val_loss -0.8056\n",
      "2024-01-10 09:14:03.147965: Pseudo dice [0.9236, 0.9306, 0.9379]\n",
      "2024-01-10 09:14:03.158451: Epoch time: 40.67 s\n",
      "2024-01-10 09:14:04.755703: \n",
      "2024-01-10 09:14:04.760971: Epoch 456\n",
      "2024-01-10 09:14:04.765939: Current learning rate: 0.00578\n",
      "2024-01-10 09:14:45.454673: train_loss -0.944\n",
      "2024-01-10 09:14:45.460186: val_loss -0.8001\n",
      "2024-01-10 09:14:45.467187: Pseudo dice [0.9225, 0.9285, 0.9357]\n",
      "2024-01-10 09:14:45.473189: Epoch time: 40.7 s\n",
      "2024-01-10 09:14:46.936034: \n",
      "2024-01-10 09:14:46.941190: Epoch 457\n",
      "2024-01-10 09:14:46.946203: Current learning rate: 0.00577\n",
      "2024-01-10 09:15:27.569293: train_loss -0.9434\n",
      "2024-01-10 09:15:27.579292: val_loss -0.8082\n",
      "2024-01-10 09:15:27.587811: Pseudo dice [0.9237, 0.9317, 0.9404]\n",
      "2024-01-10 09:15:27.595812: Epoch time: 40.63 s\n",
      "2024-01-10 09:15:29.048435: \n",
      "2024-01-10 09:15:29.059442: Epoch 458\n",
      "2024-01-10 09:15:29.067427: Current learning rate: 0.00576\n",
      "2024-01-10 09:16:10.439010: train_loss -0.9426\n",
      "2024-01-10 09:16:10.448112: val_loss -0.7992\n",
      "2024-01-10 09:16:10.455689: Pseudo dice [0.9242, 0.9292, 0.9362]\n",
      "2024-01-10 09:16:10.463834: Epoch time: 41.39 s\n",
      "2024-01-10 09:16:12.115669: \n",
      "2024-01-10 09:16:12.120841: Epoch 459\n",
      "2024-01-10 09:16:12.125894: Current learning rate: 0.00575\n",
      "2024-01-10 09:16:55.622050: train_loss -0.9426\n",
      "2024-01-10 09:16:55.631883: val_loss -0.7994\n",
      "2024-01-10 09:16:55.640066: Pseudo dice [0.9187, 0.9286, 0.9362]\n",
      "2024-01-10 09:16:55.648627: Epoch time: 43.51 s\n",
      "2024-01-10 09:16:57.129240: \n",
      "2024-01-10 09:16:57.134258: Epoch 460\n",
      "2024-01-10 09:16:57.142237: Current learning rate: 0.00574\n",
      "2024-01-10 09:17:39.977798: train_loss -0.9412\n",
      "2024-01-10 09:17:39.987746: val_loss -0.7994\n",
      "2024-01-10 09:17:39.996265: Pseudo dice [0.9201, 0.9318, 0.9388]\n",
      "2024-01-10 09:17:40.004781: Epoch time: 42.85 s\n",
      "2024-01-10 09:17:41.925375: \n",
      "2024-01-10 09:17:41.932056: Epoch 461\n",
      "2024-01-10 09:17:41.937052: Current learning rate: 0.00573\n",
      "2024-01-10 09:18:26.385968: train_loss -0.9423\n",
      "2024-01-10 09:18:26.395993: val_loss -0.8025\n",
      "2024-01-10 09:18:26.405653: Pseudo dice [0.9221, 0.9307, 0.9383]\n",
      "2024-01-10 09:18:26.435212: Epoch time: 44.46 s\n",
      "2024-01-10 09:18:27.974523: \n",
      "2024-01-10 09:18:27.980053: Epoch 462\n",
      "2024-01-10 09:18:27.985049: Current learning rate: 0.00572\n",
      "2024-01-10 09:19:11.316139: train_loss -0.9423\n",
      "2024-01-10 09:19:11.332034: val_loss -0.7986\n",
      "2024-01-10 09:19:11.343551: Pseudo dice [0.9208, 0.9285, 0.9365]\n",
      "2024-01-10 09:19:11.359327: Epoch time: 43.34 s\n",
      "2024-01-10 09:19:13.094253: \n",
      "2024-01-10 09:19:13.100557: Epoch 463\n",
      "2024-01-10 09:19:13.104578: Current learning rate: 0.00571\n",
      "2024-01-10 09:19:56.526842: train_loss -0.9433\n",
      "2024-01-10 09:19:56.537400: val_loss -0.7944\n",
      "2024-01-10 09:19:56.544921: Pseudo dice [0.9204, 0.9277, 0.935]\n",
      "2024-01-10 09:19:56.553440: Epoch time: 43.43 s\n",
      "2024-01-10 09:19:58.144947: \n",
      "2024-01-10 09:19:58.149992: Epoch 464\n",
      "2024-01-10 09:19:58.155061: Current learning rate: 0.0057\n",
      "2024-01-10 09:20:39.433055: train_loss -0.943\n",
      "2024-01-10 09:20:39.440590: val_loss -0.7986\n",
      "2024-01-10 09:20:39.450114: Pseudo dice [0.9231, 0.9297, 0.9377]\n",
      "2024-01-10 09:20:39.460644: Epoch time: 41.29 s\n",
      "2024-01-10 09:20:40.922064: \n",
      "2024-01-10 09:20:40.928581: Epoch 465\n",
      "2024-01-10 09:20:40.933599: Current learning rate: 0.0057\n",
      "2024-01-10 09:21:22.481421: train_loss -0.9435\n",
      "2024-01-10 09:21:22.487867: val_loss -0.796\n",
      "2024-01-10 09:21:22.495231: Pseudo dice [0.9179, 0.9288, 0.9363]\n",
      "2024-01-10 09:21:22.503282: Epoch time: 41.56 s\n",
      "2024-01-10 09:21:24.007555: \n",
      "2024-01-10 09:21:24.014910: Epoch 466\n",
      "2024-01-10 09:21:24.018887: Current learning rate: 0.00569\n",
      "2024-01-10 09:22:05.422698: train_loss -0.9432\n",
      "2024-01-10 09:22:05.467697: val_loss -0.804\n",
      "2024-01-10 09:22:05.477073: Pseudo dice [0.9204, 0.9302, 0.9392]\n",
      "2024-01-10 09:22:05.487072: Epoch time: 41.42 s\n",
      "2024-01-10 09:22:07.007651: \n",
      "2024-01-10 09:22:07.012439: Epoch 467\n",
      "2024-01-10 09:22:07.016382: Current learning rate: 0.00568\n",
      "2024-01-10 09:22:48.357991: train_loss -0.9435\n",
      "2024-01-10 09:22:48.366368: val_loss -0.7967\n",
      "2024-01-10 09:22:48.371881: Pseudo dice [0.9221, 0.9286, 0.9368]\n",
      "2024-01-10 09:22:48.377331: Epoch time: 41.35 s\n",
      "2024-01-10 09:22:49.793045: \n",
      "2024-01-10 09:22:49.798114: Epoch 468\n",
      "2024-01-10 09:22:49.802832: Current learning rate: 0.00567\n",
      "2024-01-10 09:23:31.903347: train_loss -0.9446\n",
      "2024-01-10 09:23:31.913760: val_loss -0.8013\n",
      "2024-01-10 09:23:31.923176: Pseudo dice [0.9224, 0.9294, 0.9376]\n",
      "2024-01-10 09:23:31.931244: Epoch time: 42.11 s\n",
      "2024-01-10 09:23:33.200769: \n",
      "2024-01-10 09:23:33.211879: Epoch 469\n",
      "2024-01-10 09:23:33.215876: Current learning rate: 0.00566\n",
      "2024-01-10 09:24:14.216352: train_loss -0.9431\n",
      "2024-01-10 09:24:14.227349: val_loss -0.7978\n",
      "2024-01-10 09:24:14.236348: Pseudo dice [0.9205, 0.928, 0.9356]\n",
      "2024-01-10 09:24:14.273359: Epoch time: 41.02 s\n",
      "2024-01-10 09:24:15.608633: \n",
      "2024-01-10 09:24:15.615219: Epoch 470\n",
      "2024-01-10 09:24:15.621352: Current learning rate: 0.00565\n",
      "2024-01-10 09:24:55.939969: train_loss -0.9429\n",
      "2024-01-10 09:24:55.948968: val_loss -0.7984\n",
      "2024-01-10 09:24:55.958971: Pseudo dice [0.9215, 0.9285, 0.9371]\n",
      "2024-01-10 09:24:55.993978: Epoch time: 40.33 s\n",
      "2024-01-10 09:24:57.147920: \n",
      "2024-01-10 09:24:57.153912: Epoch 471\n",
      "2024-01-10 09:24:57.157926: Current learning rate: 0.00564\n",
      "2024-01-10 09:25:38.537486: train_loss -0.943\n",
      "2024-01-10 09:25:38.554488: val_loss -0.8015\n",
      "2024-01-10 09:25:38.571502: Pseudo dice [0.9206, 0.9286, 0.9378]\n",
      "2024-01-10 09:25:38.585487: Epoch time: 41.39 s\n",
      "2024-01-10 09:25:39.843626: \n",
      "2024-01-10 09:25:39.851473: Epoch 472\n",
      "2024-01-10 09:25:39.855542: Current learning rate: 0.00563\n",
      "2024-01-10 09:26:21.435112: train_loss -0.944\n",
      "2024-01-10 09:26:21.471626: val_loss -0.7945\n",
      "2024-01-10 09:26:21.480622: Pseudo dice [0.9191, 0.9271, 0.9357]\n",
      "2024-01-10 09:26:21.488622: Epoch time: 41.59 s\n",
      "2024-01-10 09:26:22.667425: \n",
      "2024-01-10 09:26:22.672517: Epoch 473\n",
      "2024-01-10 09:26:22.677498: Current learning rate: 0.00562\n",
      "2024-01-10 09:27:03.772063: train_loss -0.9437\n",
      "2024-01-10 09:27:03.810061: val_loss -0.8081\n",
      "2024-01-10 09:27:03.822437: Pseudo dice [0.9242, 0.9305, 0.938]\n",
      "2024-01-10 09:27:03.834434: Epoch time: 41.11 s\n",
      "2024-01-10 09:27:05.620842: \n",
      "2024-01-10 09:27:05.626843: Epoch 474\n",
      "2024-01-10 09:27:05.630850: Current learning rate: 0.00561\n",
      "2024-01-10 09:27:47.610126: train_loss -0.9435\n",
      "2024-01-10 09:27:47.649122: val_loss -0.7964\n",
      "2024-01-10 09:27:47.661132: Pseudo dice [0.9228, 0.928, 0.9365]\n",
      "2024-01-10 09:27:47.670130: Epoch time: 41.99 s\n",
      "2024-01-10 09:27:49.014913: \n",
      "2024-01-10 09:27:49.019909: Epoch 475\n",
      "2024-01-10 09:27:49.024909: Current learning rate: 0.0056\n",
      "2024-01-10 09:28:30.399921: train_loss -0.9435\n",
      "2024-01-10 09:28:30.408921: val_loss -0.8072\n",
      "2024-01-10 09:28:30.420921: Pseudo dice [0.9217, 0.9324, 0.9399]\n",
      "2024-01-10 09:28:30.433921: Epoch time: 41.39 s\n",
      "2024-01-10 09:28:31.748430: \n",
      "2024-01-10 09:28:31.766398: Epoch 476\n",
      "2024-01-10 09:28:31.770411: Current learning rate: 0.00559\n",
      "2024-01-10 09:29:11.962826: train_loss -0.944\n",
      "2024-01-10 09:29:11.970827: val_loss -0.7962\n",
      "2024-01-10 09:29:11.979830: Pseudo dice [0.9236, 0.9287, 0.9369]\n",
      "2024-01-10 09:29:11.986826: Epoch time: 40.22 s\n",
      "2024-01-10 09:29:13.181212: \n",
      "2024-01-10 09:29:13.188456: Epoch 477\n",
      "2024-01-10 09:29:13.196536: Current learning rate: 0.00558\n",
      "2024-01-10 09:29:53.527511: train_loss -0.9437\n",
      "2024-01-10 09:29:53.536514: val_loss -0.8014\n",
      "2024-01-10 09:29:53.550689: Pseudo dice [0.9206, 0.9279, 0.937]\n",
      "2024-01-10 09:29:53.558688: Epoch time: 40.35 s\n",
      "2024-01-10 09:29:54.791052: \n",
      "2024-01-10 09:29:54.797042: Epoch 478\n",
      "2024-01-10 09:29:54.801041: Current learning rate: 0.00557\n",
      "2024-01-10 09:30:35.080434: train_loss -0.944\n",
      "2024-01-10 09:30:35.089947: val_loss -0.8026\n",
      "2024-01-10 09:30:35.097946: Pseudo dice [0.9212, 0.9309, 0.9388]\n",
      "2024-01-10 09:30:35.105945: Epoch time: 40.29 s\n",
      "2024-01-10 09:30:36.441828: \n",
      "2024-01-10 09:30:36.450950: Epoch 479\n",
      "2024-01-10 09:30:36.455966: Current learning rate: 0.00556\n",
      "2024-01-10 09:31:16.756936: train_loss -0.9443\n",
      "2024-01-10 09:31:16.789941: val_loss -0.8025\n",
      "2024-01-10 09:31:16.800447: Pseudo dice [0.9227, 0.9288, 0.9364]\n",
      "2024-01-10 09:31:16.808456: Epoch time: 40.32 s\n",
      "2024-01-10 09:31:17.990057: \n",
      "2024-01-10 09:31:17.995131: Epoch 480\n",
      "2024-01-10 09:31:17.999115: Current learning rate: 0.00555\n",
      "2024-01-10 09:31:58.163685: train_loss -0.944\n",
      "2024-01-10 09:31:58.174684: val_loss -0.8064\n",
      "2024-01-10 09:31:58.182685: Pseudo dice [0.9258, 0.9309, 0.9384]\n",
      "2024-01-10 09:31:58.190683: Epoch time: 40.17 s\n",
      "2024-01-10 09:31:59.430797: \n",
      "2024-01-10 09:31:59.438653: Epoch 481\n",
      "2024-01-10 09:31:59.447620: Current learning rate: 0.00554\n",
      "2024-01-10 09:32:39.804296: train_loss -0.9438\n",
      "2024-01-10 09:32:39.815299: val_loss -0.8065\n",
      "2024-01-10 09:32:39.824308: Pseudo dice [0.9269, 0.9302, 0.9392]\n",
      "2024-01-10 09:32:39.831309: Epoch time: 40.38 s\n",
      "2024-01-10 09:32:41.051759: \n",
      "2024-01-10 09:32:41.058759: Epoch 482\n",
      "2024-01-10 09:32:41.066756: Current learning rate: 0.00553\n",
      "2024-01-10 09:33:21.219393: train_loss -0.944\n",
      "2024-01-10 09:33:21.226913: val_loss -0.8014\n",
      "2024-01-10 09:33:21.236067: Pseudo dice [0.9216, 0.9294, 0.9367]\n",
      "2024-01-10 09:33:21.243069: Epoch time: 40.17 s\n",
      "2024-01-10 09:33:22.426399: \n",
      "2024-01-10 09:33:22.431752: Epoch 483\n",
      "2024-01-10 09:33:22.435759: Current learning rate: 0.00552\n",
      "2024-01-10 09:34:02.237699: train_loss -0.943\n",
      "2024-01-10 09:34:02.246708: val_loss -0.7903\n",
      "2024-01-10 09:34:02.255700: Pseudo dice [0.9184, 0.9279, 0.9359]\n",
      "2024-01-10 09:34:02.263700: Epoch time: 39.81 s\n",
      "2024-01-10 09:34:03.636477: \n",
      "2024-01-10 09:34:03.644053: Epoch 484\n",
      "2024-01-10 09:34:03.648191: Current learning rate: 0.00551\n",
      "2024-01-10 09:34:43.446351: train_loss -0.9429\n",
      "2024-01-10 09:34:43.456350: val_loss -0.7986\n",
      "2024-01-10 09:34:43.486356: Pseudo dice [0.9257, 0.927, 0.9351]\n",
      "2024-01-10 09:34:43.493356: Epoch time: 39.81 s\n",
      "2024-01-10 09:34:44.655593: \n",
      "2024-01-10 09:34:44.660594: Epoch 485\n",
      "2024-01-10 09:34:44.664593: Current learning rate: 0.0055\n",
      "2024-01-10 09:35:24.479768: train_loss -0.9422\n",
      "2024-01-10 09:35:24.488768: val_loss -0.8046\n",
      "2024-01-10 09:35:24.495775: Pseudo dice [0.9201, 0.93, 0.9369]\n",
      "2024-01-10 09:35:24.503776: Epoch time: 39.83 s\n",
      "2024-01-10 09:35:25.651341: \n",
      "2024-01-10 09:35:25.656407: Epoch 486\n",
      "2024-01-10 09:35:25.660428: Current learning rate: 0.00549\n",
      "2024-01-10 09:36:05.450348: train_loss -0.9422\n",
      "2024-01-10 09:36:05.477344: val_loss -0.8096\n",
      "2024-01-10 09:36:05.486346: Pseudo dice [0.9241, 0.9322, 0.9385]\n",
      "2024-01-10 09:36:05.493345: Epoch time: 39.8 s\n",
      "2024-01-10 09:36:06.671806: \n",
      "2024-01-10 09:36:06.679866: Epoch 487\n",
      "2024-01-10 09:36:06.683866: Current learning rate: 0.00548\n",
      "2024-01-10 09:36:46.483706: train_loss -0.944\n",
      "2024-01-10 09:36:46.492707: val_loss -0.7992\n",
      "2024-01-10 09:36:46.519706: Pseudo dice [0.9185, 0.9286, 0.9365]\n",
      "2024-01-10 09:36:46.527710: Epoch time: 39.81 s\n",
      "2024-01-10 09:36:47.724834: \n",
      "2024-01-10 09:36:47.729844: Epoch 488\n",
      "2024-01-10 09:36:47.733844: Current learning rate: 0.00547\n",
      "2024-01-10 09:37:27.637721: train_loss -0.9442\n",
      "2024-01-10 09:37:27.646720: val_loss -0.8019\n",
      "2024-01-10 09:37:27.653731: Pseudo dice [0.925, 0.9289, 0.9379]\n",
      "2024-01-10 09:37:27.661729: Epoch time: 39.91 s\n",
      "2024-01-10 09:37:28.868361: \n",
      "2024-01-10 09:37:28.873365: Epoch 489\n",
      "2024-01-10 09:37:28.877361: Current learning rate: 0.00546\n",
      "2024-01-10 09:38:08.672784: train_loss -0.944\n",
      "2024-01-10 09:38:08.685786: val_loss -0.8011\n",
      "2024-01-10 09:38:08.693301: Pseudo dice [0.9224, 0.9294, 0.9373]\n",
      "2024-01-10 09:38:08.700299: Epoch time: 39.81 s\n",
      "2024-01-10 09:38:09.889566: \n",
      "2024-01-10 09:38:09.893870: Epoch 490\n",
      "2024-01-10 09:38:09.897816: Current learning rate: 0.00546\n",
      "2024-01-10 09:38:49.663030: train_loss -0.9434\n",
      "2024-01-10 09:38:49.686030: val_loss -0.7979\n",
      "2024-01-10 09:38:49.695037: Pseudo dice [0.9197, 0.9291, 0.9374]\n",
      "2024-01-10 09:38:49.704555: Epoch time: 39.78 s\n",
      "2024-01-10 09:38:51.002494: \n",
      "2024-01-10 09:38:51.009795: Epoch 491\n",
      "2024-01-10 09:38:51.013814: Current learning rate: 0.00545\n",
      "2024-01-10 09:39:30.804483: train_loss -0.9427\n",
      "2024-01-10 09:39:30.813489: val_loss -0.7956\n",
      "2024-01-10 09:39:30.822000: Pseudo dice [0.9178, 0.9303, 0.9381]\n",
      "2024-01-10 09:39:30.831003: Epoch time: 39.8 s\n",
      "2024-01-10 09:39:32.009230: \n",
      "2024-01-10 09:39:32.013627: Epoch 492\n",
      "2024-01-10 09:39:32.017646: Current learning rate: 0.00544\n",
      "2024-01-10 09:40:11.752805: train_loss -0.9442\n",
      "2024-01-10 09:40:11.763806: val_loss -0.7963\n",
      "2024-01-10 09:40:11.797805: Pseudo dice [0.9227, 0.927, 0.9358]\n",
      "2024-01-10 09:40:11.804805: Epoch time: 39.74 s\n",
      "2024-01-10 09:40:12.947400: \n",
      "2024-01-10 09:40:12.957597: Epoch 493\n",
      "2024-01-10 09:40:12.967515: Current learning rate: 0.00543\n",
      "2024-01-10 09:40:52.935425: train_loss -0.9439\n",
      "2024-01-10 09:40:52.946932: val_loss -0.7946\n",
      "2024-01-10 09:40:52.978939: Pseudo dice [0.9222, 0.9278, 0.9361]\n",
      "2024-01-10 09:40:52.987938: Epoch time: 39.99 s\n",
      "2024-01-10 09:40:54.131440: \n",
      "2024-01-10 09:40:54.136509: Epoch 494\n",
      "2024-01-10 09:40:54.140511: Current learning rate: 0.00542\n",
      "2024-01-10 09:41:33.783242: train_loss -0.9441\n",
      "2024-01-10 09:41:33.793240: val_loss -0.7969\n",
      "2024-01-10 09:41:33.802241: Pseudo dice [0.9189, 0.9291, 0.9371]\n",
      "2024-01-10 09:41:33.811242: Epoch time: 39.65 s\n",
      "2024-01-10 09:41:35.047622: \n",
      "2024-01-10 09:41:35.052625: Epoch 495\n",
      "2024-01-10 09:41:35.057234: Current learning rate: 0.00541\n",
      "2024-01-10 09:42:14.769284: train_loss -0.9447\n",
      "2024-01-10 09:42:14.779802: val_loss -0.7946\n",
      "2024-01-10 09:42:14.790802: Pseudo dice [0.9224, 0.9279, 0.9356]\n",
      "2024-01-10 09:42:14.799802: Epoch time: 39.72 s\n",
      "2024-01-10 09:42:16.017254: \n",
      "2024-01-10 09:42:16.022254: Epoch 496\n",
      "2024-01-10 09:42:16.026512: Current learning rate: 0.0054\n",
      "2024-01-10 09:42:55.916258: train_loss -0.9434\n",
      "2024-01-10 09:42:55.926258: val_loss -0.7976\n",
      "2024-01-10 09:42:55.934256: Pseudo dice [0.922, 0.9286, 0.9358]\n",
      "2024-01-10 09:42:55.943257: Epoch time: 39.9 s\n",
      "2024-01-10 09:42:57.105510: \n",
      "2024-01-10 09:42:57.113506: Epoch 497\n",
      "2024-01-10 09:42:57.117503: Current learning rate: 0.00539\n",
      "2024-01-10 09:43:36.938178: train_loss -0.9442\n",
      "2024-01-10 09:43:36.947180: val_loss -0.7935\n",
      "2024-01-10 09:43:36.956184: Pseudo dice [0.9185, 0.9289, 0.9383]\n",
      "2024-01-10 09:43:36.964184: Epoch time: 39.83 s\n",
      "2024-01-10 09:43:38.149835: \n",
      "2024-01-10 09:43:38.158612: Epoch 498\n",
      "2024-01-10 09:43:38.162686: Current learning rate: 0.00538\n",
      "2024-01-10 09:44:18.005985: train_loss -0.9441\n",
      "2024-01-10 09:44:18.017986: val_loss -0.8061\n",
      "2024-01-10 09:44:18.025994: Pseudo dice [0.9247, 0.9304, 0.939]\n",
      "2024-01-10 09:44:18.032990: Epoch time: 39.86 s\n",
      "2024-01-10 09:44:19.239305: \n",
      "2024-01-10 09:44:19.247307: Epoch 499\n",
      "2024-01-10 09:44:19.253366: Current learning rate: 0.00537\n",
      "2024-01-10 09:44:58.958007: train_loss -0.9443\n",
      "2024-01-10 09:44:58.967003: val_loss -0.8064\n",
      "2024-01-10 09:44:58.976007: Pseudo dice [0.9253, 0.9314, 0.9393]\n",
      "2024-01-10 09:44:59.004005: Epoch time: 39.72 s\n",
      "2024-01-10 09:45:00.389439: \n",
      "2024-01-10 09:45:00.394443: Epoch 500\n",
      "2024-01-10 09:45:00.398439: Current learning rate: 0.00536\n",
      "2024-01-10 09:45:40.143695: train_loss -0.9439\n",
      "2024-01-10 09:45:40.152694: val_loss -0.7944\n",
      "2024-01-10 09:45:40.160696: Pseudo dice [0.9208, 0.9283, 0.9358]\n",
      "2024-01-10 09:45:40.167698: Epoch time: 39.75 s\n",
      "2024-01-10 09:45:41.369780: \n",
      "2024-01-10 09:45:41.374426: Epoch 501\n",
      "2024-01-10 09:45:41.379433: Current learning rate: 0.00535\n",
      "2024-01-10 09:46:21.127116: train_loss -0.9441\n",
      "2024-01-10 09:46:21.137118: val_loss -0.7954\n",
      "2024-01-10 09:46:21.145119: Pseudo dice [0.9226, 0.9277, 0.9359]\n",
      "2024-01-10 09:46:21.153122: Epoch time: 39.76 s\n",
      "2024-01-10 09:46:22.355053: \n",
      "2024-01-10 09:46:22.360131: Epoch 502\n",
      "2024-01-10 09:46:22.367135: Current learning rate: 0.00534\n",
      "2024-01-10 09:47:02.130629: train_loss -0.9442\n",
      "2024-01-10 09:47:02.139629: val_loss -0.8\n",
      "2024-01-10 09:47:02.148629: Pseudo dice [0.9207, 0.9298, 0.9381]\n",
      "2024-01-10 09:47:02.156727: Epoch time: 39.78 s\n",
      "2024-01-10 09:47:03.418042: \n",
      "2024-01-10 09:47:03.426271: Epoch 503\n",
      "2024-01-10 09:47:03.432282: Current learning rate: 0.00533\n",
      "2024-01-10 09:47:43.063791: train_loss -0.9441\n",
      "2024-01-10 09:47:43.074795: val_loss -0.8035\n",
      "2024-01-10 09:47:43.083793: Pseudo dice [0.9238, 0.9289, 0.9377]\n",
      "2024-01-10 09:47:43.092791: Epoch time: 39.65 s\n",
      "2024-01-10 09:47:44.271226: \n",
      "2024-01-10 09:47:44.276221: Epoch 504\n",
      "2024-01-10 09:47:44.280432: Current learning rate: 0.00532\n",
      "2024-01-10 09:48:24.015229: train_loss -0.9445\n",
      "2024-01-10 09:48:24.045745: val_loss -0.7928\n",
      "2024-01-10 09:48:24.052745: Pseudo dice [0.9218, 0.9259, 0.9345]\n",
      "2024-01-10 09:48:24.059745: Epoch time: 39.75 s\n",
      "2024-01-10 09:48:25.237661: \n",
      "2024-01-10 09:48:25.246396: Epoch 505\n",
      "2024-01-10 09:48:25.256402: Current learning rate: 0.00531\n",
      "2024-01-10 09:49:04.994833: train_loss -0.9448\n",
      "2024-01-10 09:49:05.005354: val_loss -0.803\n",
      "2024-01-10 09:49:05.034075: Pseudo dice [0.9221, 0.9296, 0.9375]\n",
      "2024-01-10 09:49:05.042587: Epoch time: 39.76 s\n",
      "2024-01-10 09:49:06.213689: \n",
      "2024-01-10 09:49:06.218750: Epoch 506\n",
      "2024-01-10 09:49:06.223294: Current learning rate: 0.0053\n",
      "2024-01-10 09:49:46.067527: train_loss -0.9444\n",
      "2024-01-10 09:49:46.077038: val_loss -0.7966\n",
      "2024-01-10 09:49:46.085037: Pseudo dice [0.9181, 0.9286, 0.9372]\n",
      "2024-01-10 09:49:46.115038: Epoch time: 39.85 s\n",
      "2024-01-10 09:49:47.266101: \n",
      "2024-01-10 09:49:47.271102: Epoch 507\n",
      "2024-01-10 09:49:47.277646: Current learning rate: 0.00529\n",
      "2024-01-10 09:50:26.959890: train_loss -0.9449\n",
      "2024-01-10 09:50:26.968888: val_loss -0.8039\n",
      "2024-01-10 09:50:26.977894: Pseudo dice [0.9206, 0.9296, 0.9378]\n",
      "2024-01-10 09:50:26.985893: Epoch time: 39.69 s\n",
      "2024-01-10 09:50:28.301852: \n",
      "2024-01-10 09:50:28.311000: Epoch 508\n",
      "2024-01-10 09:50:28.318087: Current learning rate: 0.00528\n",
      "2024-01-10 09:51:08.008254: train_loss -0.9444\n",
      "2024-01-10 09:51:08.018267: val_loss -0.8007\n",
      "2024-01-10 09:51:08.046773: Pseudo dice [0.9236, 0.9302, 0.9374]\n",
      "2024-01-10 09:51:08.055773: Epoch time: 39.71 s\n",
      "2024-01-10 09:51:09.210224: \n",
      "2024-01-10 09:51:09.216765: Epoch 509\n",
      "2024-01-10 09:51:09.220340: Current learning rate: 0.00527\n",
      "2024-01-10 09:51:48.892804: train_loss -0.9449\n",
      "2024-01-10 09:51:48.899806: val_loss -0.7979\n",
      "2024-01-10 09:51:48.906805: Pseudo dice [0.9212, 0.9298, 0.9373]\n",
      "2024-01-10 09:51:48.914805: Epoch time: 39.68 s\n",
      "2024-01-10 09:51:50.111739: \n",
      "2024-01-10 09:51:50.117742: Epoch 510\n",
      "2024-01-10 09:51:50.122738: Current learning rate: 0.00526\n",
      "2024-01-10 09:52:29.852110: train_loss -0.9451\n",
      "2024-01-10 09:52:29.886635: val_loss -0.8023\n",
      "2024-01-10 09:52:29.894633: Pseudo dice [0.9261, 0.9295, 0.9384]\n",
      "2024-01-10 09:52:29.924635: Epoch time: 39.74 s\n",
      "2024-01-10 09:52:31.085160: \n",
      "2024-01-10 09:52:31.090229: Epoch 511\n",
      "2024-01-10 09:52:31.094219: Current learning rate: 0.00525\n",
      "2024-01-10 09:53:10.875489: train_loss -0.945\n",
      "2024-01-10 09:53:10.887003: val_loss -0.8003\n",
      "2024-01-10 09:53:10.897003: Pseudo dice [0.9258, 0.9291, 0.9371]\n",
      "2024-01-10 09:53:10.905003: Epoch time: 39.79 s\n",
      "2024-01-10 09:53:12.083420: \n",
      "2024-01-10 09:53:12.088407: Epoch 512\n",
      "2024-01-10 09:53:12.092414: Current learning rate: 0.00524\n",
      "2024-01-10 09:53:51.791216: train_loss -0.944\n",
      "2024-01-10 09:53:51.800763: val_loss -0.7957\n",
      "2024-01-10 09:53:51.807756: Pseudo dice [0.924, 0.9285, 0.9369]\n",
      "2024-01-10 09:53:51.815753: Epoch time: 39.71 s\n",
      "2024-01-10 09:53:53.140265: \n",
      "2024-01-10 09:53:53.145266: Epoch 513\n",
      "2024-01-10 09:53:53.149265: Current learning rate: 0.00523\n",
      "2024-01-10 09:54:32.815882: train_loss -0.9449\n",
      "2024-01-10 09:54:32.844881: val_loss -0.7997\n",
      "2024-01-10 09:54:32.852882: Pseudo dice [0.9239, 0.928, 0.9357]\n",
      "2024-01-10 09:54:32.863883: Epoch time: 39.68 s\n",
      "2024-01-10 09:54:34.042836: \n",
      "2024-01-10 09:54:34.051714: Epoch 514\n",
      "2024-01-10 09:54:34.055712: Current learning rate: 0.00522\n",
      "2024-01-10 09:55:13.796260: train_loss -0.9446\n",
      "2024-01-10 09:55:13.808259: val_loss -0.7905\n",
      "2024-01-10 09:55:13.816298: Pseudo dice [0.9171, 0.9262, 0.9351]\n",
      "2024-01-10 09:55:13.852814: Epoch time: 39.75 s\n",
      "2024-01-10 09:55:15.048595: \n",
      "2024-01-10 09:55:15.053914: Epoch 515\n",
      "2024-01-10 09:55:15.058913: Current learning rate: 0.00521\n",
      "2024-01-10 09:55:54.763320: train_loss -0.9446\n",
      "2024-01-10 09:55:54.773316: val_loss -0.8\n",
      "2024-01-10 09:55:54.781313: Pseudo dice [0.9233, 0.9284, 0.9363]\n",
      "2024-01-10 09:55:54.813313: Epoch time: 39.72 s\n",
      "2024-01-10 09:55:56.012926: \n",
      "2024-01-10 09:55:56.022847: Epoch 516\n",
      "2024-01-10 09:55:56.026910: Current learning rate: 0.0052\n",
      "2024-01-10 09:56:35.740588: train_loss -0.9446\n",
      "2024-01-10 09:56:35.750589: val_loss -0.8005\n",
      "2024-01-10 09:56:35.759593: Pseudo dice [0.9235, 0.9301, 0.9375]\n",
      "2024-01-10 09:56:35.767593: Epoch time: 39.73 s\n",
      "2024-01-10 09:56:37.007326: \n",
      "2024-01-10 09:56:37.012943: Epoch 517\n",
      "2024-01-10 09:56:37.017267: Current learning rate: 0.00519\n",
      "2024-01-10 09:57:16.692727: train_loss -0.9444\n",
      "2024-01-10 09:57:16.703726: val_loss -0.7925\n",
      "2024-01-10 09:57:16.737727: Pseudo dice [0.9226, 0.9274, 0.9353]\n",
      "2024-01-10 09:57:16.747727: Epoch time: 39.69 s\n",
      "2024-01-10 09:57:18.032512: \n",
      "2024-01-10 09:57:18.040576: Epoch 518\n",
      "2024-01-10 09:57:18.044578: Current learning rate: 0.00518\n",
      "2024-01-10 09:57:57.765853: train_loss -0.9442\n",
      "2024-01-10 09:57:57.775853: val_loss -0.7935\n",
      "2024-01-10 09:57:57.783854: Pseudo dice [0.9168, 0.9279, 0.9363]\n",
      "2024-01-10 09:57:57.791861: Epoch time: 39.73 s\n",
      "2024-01-10 09:57:58.930213: \n",
      "2024-01-10 09:57:58.935214: Epoch 519\n",
      "2024-01-10 09:57:58.939214: Current learning rate: 0.00518\n",
      "2024-01-10 09:58:38.607935: train_loss -0.9443\n",
      "2024-01-10 09:58:38.617471: val_loss -0.8011\n",
      "2024-01-10 09:58:38.624483: Pseudo dice [0.9211, 0.9306, 0.9378]\n",
      "2024-01-10 09:58:38.632470: Epoch time: 39.68 s\n",
      "2024-01-10 09:58:39.852471: \n",
      "2024-01-10 09:58:39.862596: Epoch 520\n",
      "2024-01-10 09:58:39.868653: Current learning rate: 0.00517\n",
      "2024-01-10 09:59:19.540184: train_loss -0.9446\n",
      "2024-01-10 09:59:19.550191: val_loss -0.7981\n",
      "2024-01-10 09:59:19.558710: Pseudo dice [0.9213, 0.9298, 0.9377]\n",
      "2024-01-10 09:59:19.586717: Epoch time: 39.69 s\n",
      "2024-01-10 09:59:20.759811: \n",
      "2024-01-10 09:59:20.768804: Epoch 521\n",
      "2024-01-10 09:59:20.774808: Current learning rate: 0.00516\n",
      "2024-01-10 10:00:00.490742: train_loss -0.9447\n",
      "2024-01-10 10:00:00.499745: val_loss -0.7977\n",
      "2024-01-10 10:00:00.505749: Pseudo dice [0.9201, 0.9307, 0.9376]\n",
      "2024-01-10 10:00:00.512751: Epoch time: 39.73 s\n",
      "2024-01-10 10:00:01.673138: \n",
      "2024-01-10 10:00:01.677570: Epoch 522\n",
      "2024-01-10 10:00:01.682566: Current learning rate: 0.00515\n",
      "2024-01-10 10:00:41.462131: train_loss -0.9447\n",
      "2024-01-10 10:00:41.469124: val_loss -0.803\n",
      "2024-01-10 10:00:41.477124: Pseudo dice [0.9212, 0.9305, 0.9387]\n",
      "2024-01-10 10:00:41.483121: Epoch time: 39.79 s\n",
      "2024-01-10 10:00:42.670100: \n",
      "2024-01-10 10:00:42.678378: Epoch 523\n",
      "2024-01-10 10:00:42.682535: Current learning rate: 0.00514\n",
      "2024-01-10 10:01:22.606820: train_loss -0.9446\n",
      "2024-01-10 10:01:22.616821: val_loss -0.7955\n",
      "2024-01-10 10:01:22.625828: Pseudo dice [0.9224, 0.9294, 0.9367]\n",
      "2024-01-10 10:01:22.634338: Epoch time: 39.94 s\n",
      "2024-01-10 10:01:23.783718: \n",
      "2024-01-10 10:01:23.791793: Epoch 524\n",
      "2024-01-10 10:01:23.795782: Current learning rate: 0.00513\n",
      "2024-01-10 10:02:03.481473: train_loss -0.945\n",
      "2024-01-10 10:02:03.488474: val_loss -0.8012\n",
      "2024-01-10 10:02:03.495762: Pseudo dice [0.9217, 0.9296, 0.9382]\n",
      "2024-01-10 10:02:03.503762: Epoch time: 39.7 s\n",
      "2024-01-10 10:02:04.714378: \n",
      "2024-01-10 10:02:04.718434: Epoch 525\n",
      "2024-01-10 10:02:04.722440: Current learning rate: 0.00512\n",
      "2024-01-10 10:02:44.345913: train_loss -0.9453\n",
      "2024-01-10 10:02:44.385425: val_loss -0.7973\n",
      "2024-01-10 10:02:44.394425: Pseudo dice [0.9226, 0.928, 0.9349]\n",
      "2024-01-10 10:02:44.403426: Epoch time: 39.63 s\n",
      "2024-01-10 10:02:45.580316: \n",
      "2024-01-10 10:02:45.589493: Epoch 526\n",
      "2024-01-10 10:02:45.595960: Current learning rate: 0.00511\n",
      "2024-01-10 10:03:25.268555: train_loss -0.9448\n",
      "2024-01-10 10:03:25.299572: val_loss -0.794\n",
      "2024-01-10 10:03:25.308947: Pseudo dice [0.9229, 0.928, 0.9354]\n",
      "2024-01-10 10:03:25.317947: Epoch time: 39.69 s\n",
      "2024-01-10 10:03:26.515021: \n",
      "2024-01-10 10:03:26.520525: Epoch 527\n",
      "2024-01-10 10:03:26.525041: Current learning rate: 0.0051\n",
      "2024-01-10 10:04:06.300904: train_loss -0.9447\n",
      "2024-01-10 10:04:06.336906: val_loss -0.7964\n",
      "2024-01-10 10:04:06.346908: Pseudo dice [0.92, 0.9287, 0.9368]\n",
      "2024-01-10 10:04:06.355906: Epoch time: 39.79 s\n",
      "2024-01-10 10:04:07.662651: \n",
      "2024-01-10 10:04:07.671520: Epoch 528\n",
      "2024-01-10 10:04:07.675022: Current learning rate: 0.00509\n",
      "2024-01-10 10:04:47.336248: train_loss -0.9456\n",
      "2024-01-10 10:04:47.369250: val_loss -0.8062\n",
      "2024-01-10 10:04:47.377485: Pseudo dice [0.9246, 0.9294, 0.937]\n",
      "2024-01-10 10:04:47.383491: Epoch time: 39.67 s\n",
      "2024-01-10 10:04:48.539395: \n",
      "2024-01-10 10:04:48.547153: Epoch 529\n",
      "2024-01-10 10:04:48.551237: Current learning rate: 0.00508\n",
      "2024-01-10 10:05:28.193732: train_loss -0.9452\n",
      "2024-01-10 10:05:28.203733: val_loss -0.7984\n",
      "2024-01-10 10:05:28.237338: Pseudo dice [0.9212, 0.9284, 0.9361]\n",
      "2024-01-10 10:05:28.247340: Epoch time: 39.66 s\n",
      "2024-01-10 10:05:29.398205: \n",
      "2024-01-10 10:05:29.403209: Epoch 530\n",
      "2024-01-10 10:05:29.408231: Current learning rate: 0.00507\n",
      "2024-01-10 10:06:09.191406: train_loss -0.946\n",
      "2024-01-10 10:06:09.199405: val_loss -0.799\n",
      "2024-01-10 10:06:09.205412: Pseudo dice [0.9252, 0.9293, 0.9373]\n",
      "2024-01-10 10:06:09.211414: Epoch time: 39.79 s\n",
      "2024-01-10 10:06:10.358405: \n",
      "2024-01-10 10:06:10.364401: Epoch 531\n",
      "2024-01-10 10:06:10.368401: Current learning rate: 0.00506\n",
      "2024-01-10 10:06:50.139081: train_loss -0.9455\n",
      "2024-01-10 10:06:50.169621: val_loss -0.8048\n",
      "2024-01-10 10:06:50.178135: Pseudo dice [0.9252, 0.9303, 0.9378]\n",
      "2024-01-10 10:06:50.186133: Epoch time: 39.78 s\n",
      "2024-01-10 10:06:51.362728: \n",
      "2024-01-10 10:06:51.370239: Epoch 532\n",
      "2024-01-10 10:06:51.373874: Current learning rate: 0.00505\n",
      "2024-01-10 10:07:31.085435: train_loss -0.946\n",
      "2024-01-10 10:07:31.094954: val_loss -0.8055\n",
      "2024-01-10 10:07:31.102954: Pseudo dice [0.9202, 0.9309, 0.9387]\n",
      "2024-01-10 10:07:31.110955: Epoch time: 39.72 s\n",
      "2024-01-10 10:07:32.435005: \n",
      "2024-01-10 10:07:32.439998: Epoch 533\n",
      "2024-01-10 10:07:32.443999: Current learning rate: 0.00504\n",
      "2024-01-10 10:08:12.223635: train_loss -0.9449\n",
      "2024-01-10 10:08:12.232635: val_loss -0.7994\n",
      "2024-01-10 10:08:12.240635: Pseudo dice [0.9251, 0.9286, 0.9361]\n",
      "2024-01-10 10:08:12.248636: Epoch time: 39.79 s\n",
      "2024-01-10 10:08:13.453156: \n",
      "2024-01-10 10:08:13.461156: Epoch 534\n",
      "2024-01-10 10:08:13.469716: Current learning rate: 0.00503\n",
      "2024-01-10 10:08:53.171299: train_loss -0.945\n",
      "2024-01-10 10:08:53.181298: val_loss -0.8009\n",
      "2024-01-10 10:08:53.219529: Pseudo dice [0.9227, 0.9305, 0.937]\n",
      "2024-01-10 10:08:53.228629: Epoch time: 39.72 s\n",
      "2024-01-10 10:08:54.453232: \n",
      "2024-01-10 10:08:54.458227: Epoch 535\n",
      "2024-01-10 10:08:54.463233: Current learning rate: 0.00502\n",
      "2024-01-10 10:09:34.199229: train_loss -0.9458\n",
      "2024-01-10 10:09:34.208543: val_loss -0.7944\n",
      "2024-01-10 10:09:34.215543: Pseudo dice [0.9235, 0.9268, 0.9349]\n",
      "2024-01-10 10:09:34.221543: Epoch time: 39.75 s\n",
      "2024-01-10 10:09:35.362530: \n",
      "2024-01-10 10:09:35.371142: Epoch 536\n",
      "2024-01-10 10:09:35.375162: Current learning rate: 0.00501\n",
      "2024-01-10 10:10:15.065033: train_loss -0.9453\n",
      "2024-01-10 10:10:15.073541: val_loss -0.7904\n",
      "2024-01-10 10:10:15.079541: Pseudo dice [0.9214, 0.9263, 0.934]\n",
      "2024-01-10 10:10:15.086543: Epoch time: 39.7 s\n",
      "2024-01-10 10:10:16.278124: \n",
      "2024-01-10 10:10:16.283130: Epoch 537\n",
      "2024-01-10 10:10:16.289124: Current learning rate: 0.005\n",
      "2024-01-10 10:10:56.077439: train_loss -0.9463\n",
      "2024-01-10 10:10:56.086960: val_loss -0.7992\n",
      "2024-01-10 10:10:56.095960: Pseudo dice [0.9211, 0.9291, 0.9362]\n",
      "2024-01-10 10:10:56.125971: Epoch time: 39.8 s\n",
      "2024-01-10 10:10:57.270874: \n",
      "2024-01-10 10:10:57.278600: Epoch 538\n",
      "2024-01-10 10:10:57.283607: Current learning rate: 0.00499\n",
      "2024-01-10 10:11:37.065312: train_loss -0.9457\n",
      "2024-01-10 10:11:37.075315: val_loss -0.8021\n",
      "2024-01-10 10:11:37.083315: Pseudo dice [0.9206, 0.9303, 0.9379]\n",
      "2024-01-10 10:11:37.091318: Epoch time: 39.8 s\n",
      "2024-01-10 10:11:38.270330: \n",
      "2024-01-10 10:11:38.276270: Epoch 539\n",
      "2024-01-10 10:11:38.280266: Current learning rate: 0.00498\n",
      "2024-01-10 10:12:18.029958: train_loss -0.9443\n",
      "2024-01-10 10:12:18.036956: val_loss -0.7929\n",
      "2024-01-10 10:12:18.044959: Pseudo dice [0.9222, 0.9269, 0.9361]\n",
      "2024-01-10 10:12:18.051960: Epoch time: 39.76 s\n",
      "2024-01-10 10:12:19.291044: \n",
      "2024-01-10 10:12:19.299181: Epoch 540\n",
      "2024-01-10 10:12:19.303172: Current learning rate: 0.00497\n",
      "2024-01-10 10:12:59.054219: train_loss -0.9432\n",
      "2024-01-10 10:12:59.063627: val_loss -0.8026\n",
      "2024-01-10 10:12:59.071628: Pseudo dice [0.9277, 0.9291, 0.9378]\n",
      "2024-01-10 10:12:59.082632: Epoch time: 39.77 s\n",
      "2024-01-10 10:13:00.248959: \n",
      "2024-01-10 10:13:00.254027: Epoch 541\n",
      "2024-01-10 10:13:00.258047: Current learning rate: 0.00496\n",
      "2024-01-10 10:13:39.980640: train_loss -0.9446\n",
      "2024-01-10 10:13:39.989856: val_loss -0.8032\n",
      "2024-01-10 10:13:39.998857: Pseudo dice [0.9219, 0.9296, 0.9377]\n",
      "2024-01-10 10:13:40.005852: Epoch time: 39.73 s\n",
      "2024-01-10 10:13:41.196897: \n",
      "2024-01-10 10:13:41.202384: Epoch 542\n",
      "2024-01-10 10:13:41.207316: Current learning rate: 0.00495\n",
      "2024-01-10 10:14:20.875691: train_loss -0.9451\n",
      "2024-01-10 10:14:20.886161: val_loss -0.7985\n",
      "2024-01-10 10:14:20.919156: Pseudo dice [0.9206, 0.9285, 0.9354]\n",
      "2024-01-10 10:14:20.930157: Epoch time: 39.68 s\n",
      "2024-01-10 10:14:22.075863: \n",
      "2024-01-10 10:14:22.081920: Epoch 543\n",
      "2024-01-10 10:14:22.085912: Current learning rate: 0.00494\n",
      "2024-01-10 10:15:01.879738: train_loss -0.9449\n",
      "2024-01-10 10:15:01.910748: val_loss -0.8046\n",
      "2024-01-10 10:15:01.919259: Pseudo dice [0.9259, 0.9303, 0.9385]\n",
      "2024-01-10 10:15:01.929257: Epoch time: 39.8 s\n",
      "2024-01-10 10:15:03.124674: \n",
      "2024-01-10 10:15:03.129736: Epoch 544\n",
      "2024-01-10 10:15:03.134306: Current learning rate: 0.00493\n",
      "2024-01-10 10:15:42.917455: train_loss -0.9456\n",
      "2024-01-10 10:15:42.927949: val_loss -0.8034\n",
      "2024-01-10 10:15:42.965475: Pseudo dice [0.9263, 0.929, 0.9376]\n",
      "2024-01-10 10:15:42.974480: Epoch time: 39.79 s\n",
      "2024-01-10 10:15:44.131357: \n",
      "2024-01-10 10:15:44.140279: Epoch 545\n",
      "2024-01-10 10:15:44.144433: Current learning rate: 0.00492\n",
      "2024-01-10 10:16:23.793467: train_loss -0.9445\n",
      "2024-01-10 10:16:23.830468: val_loss -0.795\n",
      "2024-01-10 10:16:23.840477: Pseudo dice [0.9231, 0.9289, 0.9362]\n",
      "2024-01-10 10:16:23.849473: Epoch time: 39.66 s\n",
      "2024-01-10 10:16:25.034550: \n",
      "2024-01-10 10:16:25.039555: Epoch 546\n",
      "2024-01-10 10:16:25.043554: Current learning rate: 0.00491\n",
      "2024-01-10 10:17:04.757998: train_loss -0.9446\n",
      "2024-01-10 10:17:04.767012: val_loss -0.8033\n",
      "2024-01-10 10:17:04.773548: Pseudo dice [0.924, 0.929, 0.9367]\n",
      "2024-01-10 10:17:04.780528: Epoch time: 39.72 s\n",
      "2024-01-10 10:17:05.955008: \n",
      "2024-01-10 10:17:05.962723: Epoch 547\n",
      "2024-01-10 10:17:05.967727: Current learning rate: 0.0049\n",
      "2024-01-10 10:17:45.706494: train_loss -0.945\n",
      "2024-01-10 10:17:45.739496: val_loss -0.8034\n",
      "2024-01-10 10:17:45.749032: Pseudo dice [0.9221, 0.931, 0.9385]\n",
      "2024-01-10 10:17:45.758032: Epoch time: 39.75 s\n",
      "2024-01-10 10:17:46.949694: \n",
      "2024-01-10 10:17:46.955696: Epoch 548\n",
      "2024-01-10 10:17:46.959696: Current learning rate: 0.00489\n",
      "2024-01-10 10:18:26.808256: train_loss -0.9456\n",
      "2024-01-10 10:18:26.819259: val_loss -0.804\n",
      "2024-01-10 10:18:26.830258: Pseudo dice [0.9256, 0.9284, 0.9376]\n",
      "2024-01-10 10:18:26.839258: Epoch time: 39.86 s\n",
      "2024-01-10 10:18:28.035185: \n",
      "2024-01-10 10:18:28.043424: Epoch 549\n",
      "2024-01-10 10:18:28.047490: Current learning rate: 0.00488\n",
      "2024-01-10 10:19:07.814923: train_loss -0.9454\n",
      "2024-01-10 10:19:07.824438: val_loss -0.7962\n",
      "2024-01-10 10:19:07.852438: Pseudo dice [0.9254, 0.9269, 0.9356]\n",
      "2024-01-10 10:19:07.861435: Epoch time: 39.78 s\n",
      "2024-01-10 10:19:09.252660: \n",
      "2024-01-10 10:19:09.259736: Epoch 550\n",
      "2024-01-10 10:19:09.266722: Current learning rate: 0.00487\n",
      "2024-01-10 10:19:48.962091: train_loss -0.9455\n",
      "2024-01-10 10:19:48.972093: val_loss -0.7959\n",
      "2024-01-10 10:19:49.003089: Pseudo dice [0.9199, 0.9296, 0.9373]\n",
      "2024-01-10 10:19:49.013091: Epoch time: 39.71 s\n",
      "2024-01-10 10:19:50.157688: \n",
      "2024-01-10 10:19:50.167725: Epoch 551\n",
      "2024-01-10 10:19:50.171565: Current learning rate: 0.00486\n",
      "2024-01-10 10:20:29.966515: train_loss -0.9457\n",
      "2024-01-10 10:20:30.003518: val_loss -0.7974\n",
      "2024-01-10 10:20:30.011515: Pseudo dice [0.9228, 0.9286, 0.9365]\n",
      "2024-01-10 10:20:30.019516: Epoch time: 39.81 s\n",
      "2024-01-10 10:20:31.193416: \n",
      "2024-01-10 10:20:31.203527: Epoch 552\n",
      "2024-01-10 10:20:31.213461: Current learning rate: 0.00485\n",
      "2024-01-10 10:21:10.935908: train_loss -0.9463\n",
      "2024-01-10 10:21:10.946907: val_loss -0.7973\n",
      "2024-01-10 10:21:10.955911: Pseudo dice [0.9227, 0.9278, 0.936]\n",
      "2024-01-10 10:21:10.963911: Epoch time: 39.74 s\n",
      "2024-01-10 10:21:12.286432: \n",
      "2024-01-10 10:21:12.295281: Epoch 553\n",
      "2024-01-10 10:21:12.298906: Current learning rate: 0.00484\n",
      "2024-01-10 10:21:51.950506: train_loss -0.9461\n",
      "2024-01-10 10:21:51.981509: val_loss -0.7993\n",
      "2024-01-10 10:21:51.991514: Pseudo dice [0.9262, 0.928, 0.9358]\n",
      "2024-01-10 10:21:51.998514: Epoch time: 39.67 s\n",
      "2024-01-10 10:21:53.240546: \n",
      "2024-01-10 10:21:53.245546: Epoch 554\n",
      "2024-01-10 10:21:53.249546: Current learning rate: 0.00484\n",
      "2024-01-10 10:22:32.933105: train_loss -0.9461\n",
      "2024-01-10 10:22:32.942115: val_loss -0.8066\n",
      "2024-01-10 10:22:32.949104: Pseudo dice [0.922, 0.9311, 0.9379]\n",
      "2024-01-10 10:22:32.957106: Epoch time: 39.69 s\n",
      "2024-01-10 10:22:34.164543: \n",
      "2024-01-10 10:22:34.174546: Epoch 555\n",
      "2024-01-10 10:22:34.178544: Current learning rate: 0.00483\n",
      "2024-01-10 10:23:14.008314: train_loss -0.9452\n",
      "2024-01-10 10:23:14.020312: val_loss -0.8002\n",
      "2024-01-10 10:23:14.028315: Pseudo dice [0.9216, 0.9287, 0.936]\n",
      "2024-01-10 10:23:14.036313: Epoch time: 39.85 s\n",
      "2024-01-10 10:23:15.178091: \n",
      "2024-01-10 10:23:15.182977: Epoch 556\n",
      "2024-01-10 10:23:15.187184: Current learning rate: 0.00482\n",
      "2024-01-10 10:23:55.061539: train_loss -0.9453\n",
      "2024-01-10 10:23:55.071549: val_loss -0.7975\n",
      "2024-01-10 10:23:55.079547: Pseudo dice [0.9223, 0.9286, 0.9368]\n",
      "2024-01-10 10:23:55.088063: Epoch time: 39.88 s\n",
      "2024-01-10 10:23:56.240043: \n",
      "2024-01-10 10:23:56.244784: Epoch 557\n",
      "2024-01-10 10:23:56.248842: Current learning rate: 0.00481\n",
      "2024-01-10 10:24:36.040482: train_loss -0.9462\n",
      "2024-01-10 10:24:36.050484: val_loss -0.7971\n",
      "2024-01-10 10:24:36.058482: Pseudo dice [0.9218, 0.9285, 0.9363]\n",
      "2024-01-10 10:24:36.065483: Epoch time: 39.8 s\n",
      "2024-01-10 10:24:37.386577: \n",
      "2024-01-10 10:24:37.391568: Epoch 558\n",
      "2024-01-10 10:24:37.395577: Current learning rate: 0.0048\n",
      "2024-01-10 10:25:17.033481: train_loss -0.945\n",
      "2024-01-10 10:25:17.044484: val_loss -0.7995\n",
      "2024-01-10 10:25:17.052482: Pseudo dice [0.9229, 0.9288, 0.9358]\n",
      "2024-01-10 10:25:17.060481: Epoch time: 39.65 s\n",
      "2024-01-10 10:25:18.230838: \n",
      "2024-01-10 10:25:18.236836: Epoch 559\n",
      "2024-01-10 10:25:18.242836: Current learning rate: 0.00479\n",
      "2024-01-10 10:25:57.977173: train_loss -0.947\n",
      "2024-01-10 10:25:57.986172: val_loss -0.7983\n",
      "2024-01-10 10:25:57.994174: Pseudo dice [0.9226, 0.9284, 0.9351]\n",
      "2024-01-10 10:25:58.002174: Epoch time: 39.75 s\n",
      "2024-01-10 10:25:59.174193: \n",
      "2024-01-10 10:25:59.180181: Epoch 560\n",
      "2024-01-10 10:25:59.184192: Current learning rate: 0.00478\n",
      "2024-01-10 10:26:39.001114: train_loss -0.9466\n",
      "2024-01-10 10:26:39.010114: val_loss -0.8022\n",
      "2024-01-10 10:26:39.017115: Pseudo dice [0.9246, 0.9287, 0.9362]\n",
      "2024-01-10 10:26:39.025119: Epoch time: 39.83 s\n",
      "2024-01-10 10:26:40.206634: \n",
      "2024-01-10 10:26:40.211702: Epoch 561\n",
      "2024-01-10 10:26:40.215703: Current learning rate: 0.00477\n",
      "2024-01-10 10:27:19.897051: train_loss -0.9462\n",
      "2024-01-10 10:27:19.908051: val_loss -0.7975\n",
      "2024-01-10 10:27:19.916050: Pseudo dice [0.9211, 0.9304, 0.9376]\n",
      "2024-01-10 10:27:19.927051: Epoch time: 39.69 s\n",
      "2024-01-10 10:27:21.106400: \n",
      "2024-01-10 10:27:21.115920: Epoch 562\n",
      "2024-01-10 10:27:21.121917: Current learning rate: 0.00476\n",
      "2024-01-10 10:28:00.813678: train_loss -0.9465\n",
      "2024-01-10 10:28:00.849680: val_loss -0.7932\n",
      "2024-01-10 10:28:00.857687: Pseudo dice [0.9199, 0.93, 0.9378]\n",
      "2024-01-10 10:28:00.866691: Epoch time: 39.71 s\n",
      "2024-01-10 10:28:02.054766: \n",
      "2024-01-10 10:28:02.060404: Epoch 563\n",
      "2024-01-10 10:28:02.063983: Current learning rate: 0.00475\n",
      "2024-01-10 10:28:42.004172: train_loss -0.9468\n",
      "2024-01-10 10:28:42.030171: val_loss -0.7991\n",
      "2024-01-10 10:28:42.039171: Pseudo dice [0.9214, 0.9281, 0.9364]\n",
      "2024-01-10 10:28:42.046173: Epoch time: 39.95 s\n",
      "2024-01-10 10:28:43.196374: \n",
      "2024-01-10 10:28:43.201192: Epoch 564\n",
      "2024-01-10 10:28:43.204601: Current learning rate: 0.00474\n",
      "2024-01-10 10:29:22.872816: train_loss -0.9466\n",
      "2024-01-10 10:29:22.884815: val_loss -0.7906\n",
      "2024-01-10 10:29:22.914349: Pseudo dice [0.9249, 0.9267, 0.934]\n",
      "2024-01-10 10:29:22.922349: Epoch time: 39.68 s\n",
      "2024-01-10 10:29:24.107757: \n",
      "2024-01-10 10:29:24.115115: Epoch 565\n",
      "2024-01-10 10:29:24.120123: Current learning rate: 0.00473\n",
      "2024-01-10 10:30:03.992424: train_loss -0.9463\n",
      "2024-01-10 10:30:04.023432: val_loss -0.7987\n",
      "2024-01-10 10:30:04.032946: Pseudo dice [0.9233, 0.9289, 0.9363]\n",
      "2024-01-10 10:30:04.041946: Epoch time: 39.89 s\n",
      "2024-01-10 10:30:05.225735: \n",
      "2024-01-10 10:30:05.234189: Epoch 566\n",
      "2024-01-10 10:30:05.238131: Current learning rate: 0.00472\n",
      "2024-01-10 10:30:44.866560: train_loss -0.9463\n",
      "2024-01-10 10:30:44.877561: val_loss -0.7948\n",
      "2024-01-10 10:30:44.885559: Pseudo dice [0.9244, 0.9292, 0.9366]\n",
      "2024-01-10 10:30:44.894562: Epoch time: 39.64 s\n",
      "2024-01-10 10:30:46.067785: \n",
      "2024-01-10 10:30:46.076408: Epoch 567\n",
      "2024-01-10 10:30:46.080400: Current learning rate: 0.00471\n",
      "2024-01-10 10:31:25.889569: train_loss -0.9466\n",
      "2024-01-10 10:31:25.920569: val_loss -0.7992\n",
      "2024-01-10 10:31:25.928568: Pseudo dice [0.924, 0.9275, 0.9364]\n",
      "2024-01-10 10:31:25.935569: Epoch time: 39.82 s\n",
      "2024-01-10 10:31:27.145576: \n",
      "2024-01-10 10:31:27.153586: Epoch 568\n",
      "2024-01-10 10:31:27.158596: Current learning rate: 0.0047\n",
      "2024-01-10 10:32:06.971548: train_loss -0.9457\n",
      "2024-01-10 10:32:06.980546: val_loss -0.7998\n",
      "2024-01-10 10:32:07.010062: Pseudo dice [0.9238, 0.9296, 0.938]\n",
      "2024-01-10 10:32:07.019062: Epoch time: 39.83 s\n",
      "2024-01-10 10:32:08.175358: \n",
      "2024-01-10 10:32:08.185893: Epoch 569\n",
      "2024-01-10 10:32:08.189906: Current learning rate: 0.00469\n",
      "2024-01-10 10:32:48.003138: train_loss -0.9457\n",
      "2024-01-10 10:32:48.011137: val_loss -0.8054\n",
      "2024-01-10 10:32:48.019138: Pseudo dice [0.9251, 0.9315, 0.9388]\n",
      "2024-01-10 10:32:48.027138: Epoch time: 39.83 s\n",
      "2024-01-10 10:32:49.251424: \n",
      "2024-01-10 10:32:49.259475: Epoch 570\n",
      "2024-01-10 10:32:49.265504: Current learning rate: 0.00468\n",
      "2024-01-10 10:33:29.059533: train_loss -0.9455\n",
      "2024-01-10 10:33:29.068533: val_loss -0.8036\n",
      "2024-01-10 10:33:29.076533: Pseudo dice [0.9255, 0.9295, 0.9373]\n",
      "2024-01-10 10:33:29.082533: Epoch time: 39.81 s\n",
      "2024-01-10 10:33:30.293972: \n",
      "2024-01-10 10:33:30.301463: Epoch 571\n",
      "2024-01-10 10:33:30.306505: Current learning rate: 0.00467\n",
      "2024-01-10 10:34:10.003337: train_loss -0.9459\n",
      "2024-01-10 10:34:10.015342: val_loss -0.7994\n",
      "2024-01-10 10:34:10.023904: Pseudo dice [0.9227, 0.9306, 0.9376]\n",
      "2024-01-10 10:34:10.032419: Epoch time: 39.71 s\n",
      "2024-01-10 10:34:11.230223: \n",
      "2024-01-10 10:34:11.234880: Epoch 572\n",
      "2024-01-10 10:34:11.238911: Current learning rate: 0.00466\n",
      "2024-01-10 10:34:50.936239: train_loss -0.9465\n",
      "2024-01-10 10:34:50.946754: val_loss -0.8\n",
      "2024-01-10 10:34:50.955754: Pseudo dice [0.9224, 0.928, 0.9368]\n",
      "2024-01-10 10:34:50.963762: Epoch time: 39.71 s\n",
      "2024-01-10 10:34:52.168151: \n",
      "2024-01-10 10:34:52.173097: Epoch 573\n",
      "2024-01-10 10:34:52.178097: Current learning rate: 0.00465\n",
      "2024-01-10 10:35:32.086232: train_loss -0.946\n",
      "2024-01-10 10:35:32.096234: val_loss -0.7994\n",
      "2024-01-10 10:35:32.126279: Pseudo dice [0.9259, 0.9274, 0.9365]\n",
      "2024-01-10 10:35:32.135279: Epoch time: 39.92 s\n",
      "2024-01-10 10:35:33.307835: \n",
      "2024-01-10 10:35:33.315761: Epoch 574\n",
      "2024-01-10 10:35:33.320759: Current learning rate: 0.00464\n",
      "2024-01-10 10:36:12.986404: train_loss -0.946\n",
      "2024-01-10 10:36:12.995411: val_loss -0.7991\n",
      "2024-01-10 10:36:13.004920: Pseudo dice [0.9212, 0.929, 0.9373]\n",
      "2024-01-10 10:36:13.012921: Epoch time: 39.68 s\n",
      "2024-01-10 10:36:14.210531: \n",
      "2024-01-10 10:36:14.216532: Epoch 575\n",
      "2024-01-10 10:36:14.221523: Current learning rate: 0.00463\n",
      "2024-01-10 10:36:53.913092: train_loss -0.9458\n",
      "2024-01-10 10:36:53.924605: val_loss -0.8038\n",
      "2024-01-10 10:36:53.933605: Pseudo dice [0.9242, 0.9311, 0.9389]\n",
      "2024-01-10 10:36:53.942605: Epoch time: 39.7 s\n",
      "2024-01-10 10:36:55.137201: \n",
      "2024-01-10 10:36:55.142207: Epoch 576\n",
      "2024-01-10 10:36:55.146211: Current learning rate: 0.00462\n",
      "2024-01-10 10:37:34.898815: train_loss -0.9467\n",
      "2024-01-10 10:37:34.908815: val_loss -0.802\n",
      "2024-01-10 10:37:34.917814: Pseudo dice [0.9238, 0.9296, 0.9373]\n",
      "2024-01-10 10:37:34.950335: Epoch time: 39.76 s\n",
      "2024-01-10 10:37:36.158603: \n",
      "2024-01-10 10:37:36.163598: Epoch 577\n",
      "2024-01-10 10:37:36.168952: Current learning rate: 0.00461\n",
      "2024-01-10 10:38:15.775537: train_loss -0.9455\n",
      "2024-01-10 10:38:15.785520: val_loss -0.8022\n",
      "2024-01-10 10:38:15.793851: Pseudo dice [0.9202, 0.9284, 0.9366]\n",
      "2024-01-10 10:38:15.824853: Epoch time: 39.62 s\n",
      "2024-01-10 10:38:17.152007: \n",
      "2024-01-10 10:38:17.160119: Epoch 578\n",
      "2024-01-10 10:38:17.170123: Current learning rate: 0.0046\n",
      "2024-01-10 10:38:56.814486: train_loss -0.9461\n",
      "2024-01-10 10:38:56.822487: val_loss -0.8008\n",
      "2024-01-10 10:38:56.828490: Pseudo dice [0.9227, 0.9295, 0.9376]\n",
      "2024-01-10 10:38:56.836484: Epoch time: 39.66 s\n",
      "2024-01-10 10:38:58.045112: \n",
      "2024-01-10 10:38:58.053825: Epoch 579\n",
      "2024-01-10 10:38:58.058202: Current learning rate: 0.00459\n",
      "2024-01-10 10:39:37.645838: train_loss -0.946\n",
      "2024-01-10 10:39:37.655840: val_loss -0.7997\n",
      "2024-01-10 10:39:37.686830: Pseudo dice [0.9237, 0.9281, 0.9372]\n",
      "2024-01-10 10:39:37.696840: Epoch time: 39.6 s\n",
      "2024-01-10 10:39:38.887436: \n",
      "2024-01-10 10:39:38.899322: Epoch 580\n",
      "2024-01-10 10:39:38.910285: Current learning rate: 0.00458\n",
      "2024-01-10 10:40:18.624763: train_loss -0.9461\n",
      "2024-01-10 10:40:18.634768: val_loss -0.8046\n",
      "2024-01-10 10:40:18.641768: Pseudo dice [0.9233, 0.9303, 0.9384]\n",
      "2024-01-10 10:40:18.649282: Epoch time: 39.74 s\n",
      "2024-01-10 10:40:19.869293: \n",
      "2024-01-10 10:40:19.877118: Epoch 581\n",
      "2024-01-10 10:40:19.882127: Current learning rate: 0.00457\n",
      "2024-01-10 10:40:59.549847: train_loss -0.9456\n",
      "2024-01-10 10:40:59.557846: val_loss -0.8035\n",
      "2024-01-10 10:40:59.567359: Pseudo dice [0.9241, 0.9287, 0.9373]\n",
      "2024-01-10 10:40:59.575358: Epoch time: 39.68 s\n",
      "2024-01-10 10:41:00.776645: \n",
      "2024-01-10 10:41:00.781645: Epoch 582\n",
      "2024-01-10 10:41:00.785651: Current learning rate: 0.00456\n",
      "2024-01-10 10:41:40.553597: train_loss -0.9462\n",
      "2024-01-10 10:41:40.562597: val_loss -0.7912\n",
      "2024-01-10 10:41:40.570600: Pseudo dice [0.9171, 0.9278, 0.936]\n",
      "2024-01-10 10:41:40.578600: Epoch time: 39.78 s\n",
      "2024-01-10 10:41:41.915469: \n",
      "2024-01-10 10:41:41.920461: Epoch 583\n",
      "2024-01-10 10:41:41.925468: Current learning rate: 0.00455\n",
      "2024-01-10 10:42:21.661792: train_loss -0.9449\n",
      "2024-01-10 10:42:21.694808: val_loss -0.8097\n",
      "2024-01-10 10:42:21.704877: Pseudo dice [0.9234, 0.9312, 0.9398]\n",
      "2024-01-10 10:42:21.711871: Epoch time: 39.75 s\n",
      "2024-01-10 10:42:22.945934: \n",
      "2024-01-10 10:42:22.950933: Epoch 584\n",
      "2024-01-10 10:42:22.954933: Current learning rate: 0.00454\n",
      "2024-01-10 10:43:02.609484: train_loss -0.9449\n",
      "2024-01-10 10:43:02.641481: val_loss -0.7978\n",
      "2024-01-10 10:43:02.652481: Pseudo dice [0.9223, 0.9296, 0.9369]\n",
      "2024-01-10 10:43:02.659480: Epoch time: 39.66 s\n",
      "2024-01-10 10:43:03.832178: \n",
      "2024-01-10 10:43:03.843624: Epoch 585\n",
      "2024-01-10 10:43:03.847694: Current learning rate: 0.00453\n",
      "2024-01-10 10:43:43.540204: train_loss -0.9453\n",
      "2024-01-10 10:43:43.548204: val_loss -0.8042\n",
      "2024-01-10 10:43:43.556204: Pseudo dice [0.926, 0.9294, 0.9371]\n",
      "2024-01-10 10:43:43.562203: Epoch time: 39.71 s\n",
      "2024-01-10 10:43:44.775543: \n",
      "2024-01-10 10:43:44.784878: Epoch 586\n",
      "2024-01-10 10:43:44.788893: Current learning rate: 0.00452\n",
      "2024-01-10 10:44:24.524798: train_loss -0.9454\n",
      "2024-01-10 10:44:24.535804: val_loss -0.8005\n",
      "2024-01-10 10:44:24.544316: Pseudo dice [0.924, 0.9296, 0.9366]\n",
      "2024-01-10 10:44:24.576316: Epoch time: 39.75 s\n",
      "2024-01-10 10:44:25.744019: \n",
      "2024-01-10 10:44:25.751516: Epoch 587\n",
      "2024-01-10 10:44:25.756441: Current learning rate: 0.00451\n",
      "2024-01-10 10:45:05.377773: train_loss -0.9457\n",
      "2024-01-10 10:45:05.386776: val_loss -0.8039\n",
      "2024-01-10 10:45:05.395776: Pseudo dice [0.9223, 0.9304, 0.9387]\n",
      "2024-01-10 10:45:05.402776: Epoch time: 39.63 s\n",
      "2024-01-10 10:45:06.764894: \n",
      "2024-01-10 10:45:06.776893: Epoch 588\n",
      "2024-01-10 10:45:06.784894: Current learning rate: 0.0045\n",
      "2024-01-10 10:45:46.405023: train_loss -0.9468\n",
      "2024-01-10 10:45:46.413023: val_loss -0.8039\n",
      "2024-01-10 10:45:46.419023: Pseudo dice [0.9223, 0.9298, 0.9385]\n",
      "2024-01-10 10:45:46.427032: Epoch time: 39.64 s\n",
      "2024-01-10 10:45:47.626226: \n",
      "2024-01-10 10:45:47.631223: Epoch 589\n",
      "2024-01-10 10:45:47.635225: Current learning rate: 0.00449\n",
      "2024-01-10 10:46:27.341727: train_loss -0.9471\n",
      "2024-01-10 10:46:27.350727: val_loss -0.8004\n",
      "2024-01-10 10:46:27.410855: Pseudo dice [0.9231, 0.9286, 0.9374]\n",
      "2024-01-10 10:46:27.418846: Epoch time: 39.72 s\n",
      "2024-01-10 10:46:28.586803: \n",
      "2024-01-10 10:46:28.592320: Epoch 590\n",
      "2024-01-10 10:46:28.598325: Current learning rate: 0.00448\n",
      "2024-01-10 10:47:08.443459: train_loss -0.9458\n",
      "2024-01-10 10:47:08.452460: val_loss -0.7998\n",
      "2024-01-10 10:47:08.460467: Pseudo dice [0.9228, 0.9289, 0.9364]\n",
      "2024-01-10 10:47:08.468458: Epoch time: 39.86 s\n",
      "2024-01-10 10:47:09.657546: \n",
      "2024-01-10 10:47:09.662540: Epoch 591\n",
      "2024-01-10 10:47:09.667546: Current learning rate: 0.00447\n",
      "2024-01-10 10:47:49.375646: train_loss -0.9478\n",
      "2024-01-10 10:47:49.383646: val_loss -0.7967\n",
      "2024-01-10 10:47:49.394157: Pseudo dice [0.9212, 0.9289, 0.9371]\n",
      "2024-01-10 10:47:49.428164: Epoch time: 39.72 s\n",
      "2024-01-10 10:47:50.647643: \n",
      "2024-01-10 10:47:50.652920: Epoch 592\n",
      "2024-01-10 10:47:50.657004: Current learning rate: 0.00446\n",
      "2024-01-10 10:48:30.287291: train_loss -0.947\n",
      "2024-01-10 10:48:30.298291: val_loss -0.8012\n",
      "2024-01-10 10:48:30.306288: Pseudo dice [0.9202, 0.9311, 0.9382]\n",
      "2024-01-10 10:48:30.314289: Epoch time: 39.64 s\n",
      "2024-01-10 10:48:31.531743: \n",
      "2024-01-10 10:48:31.537334: Epoch 593\n",
      "2024-01-10 10:48:31.541347: Current learning rate: 0.00445\n",
      "2024-01-10 10:49:11.397671: train_loss -0.9474\n",
      "2024-01-10 10:49:11.433673: val_loss -0.8044\n",
      "2024-01-10 10:49:11.442672: Pseudo dice [0.9273, 0.9306, 0.9371]\n",
      "2024-01-10 10:49:11.450676: Epoch time: 39.87 s\n",
      "2024-01-10 10:49:12.643595: \n",
      "2024-01-10 10:49:12.648595: Epoch 594\n",
      "2024-01-10 10:49:12.652612: Current learning rate: 0.00444\n",
      "2024-01-10 10:49:52.382501: train_loss -0.9475\n",
      "2024-01-10 10:49:52.413506: val_loss -0.7989\n",
      "2024-01-10 10:49:52.421507: Pseudo dice [0.9256, 0.93, 0.9382]\n",
      "2024-01-10 10:49:52.429507: Epoch time: 39.74 s\n",
      "2024-01-10 10:49:53.594522: \n",
      "2024-01-10 10:49:53.603601: Epoch 595\n",
      "2024-01-10 10:49:53.607533: Current learning rate: 0.00443\n",
      "2024-01-10 10:50:33.367954: train_loss -0.9472\n",
      "2024-01-10 10:50:33.375951: val_loss -0.7999\n",
      "2024-01-10 10:50:33.383958: Pseudo dice [0.9193, 0.9302, 0.938]\n",
      "2024-01-10 10:50:33.391957: Epoch time: 39.77 s\n",
      "2024-01-10 10:50:34.589336: \n",
      "2024-01-10 10:50:34.594152: Epoch 596\n",
      "2024-01-10 10:50:34.599548: Current learning rate: 0.00442\n",
      "2024-01-10 10:51:14.403603: train_loss -0.9463\n",
      "2024-01-10 10:51:14.411606: val_loss -0.801\n",
      "2024-01-10 10:51:14.441118: Pseudo dice [0.9218, 0.9305, 0.9378]\n",
      "2024-01-10 10:51:14.450119: Epoch time: 39.82 s\n",
      "2024-01-10 10:51:15.643642: \n",
      "2024-01-10 10:51:15.650643: Epoch 597\n",
      "2024-01-10 10:51:15.654573: Current learning rate: 0.00441\n",
      "2024-01-10 10:51:55.353018: train_loss -0.9473\n",
      "2024-01-10 10:51:55.363012: val_loss -0.7962\n",
      "2024-01-10 10:51:55.369011: Pseudo dice [0.9219, 0.93, 0.939]\n",
      "2024-01-10 10:51:55.378011: Epoch time: 39.71 s\n",
      "2024-01-10 10:51:56.604887: \n",
      "2024-01-10 10:51:56.613880: Epoch 598\n",
      "2024-01-10 10:51:56.617887: Current learning rate: 0.0044\n",
      "2024-01-10 10:52:36.470695: train_loss -0.9469\n",
      "2024-01-10 10:52:36.479694: val_loss -0.8027\n",
      "2024-01-10 10:52:36.486694: Pseudo dice [0.926, 0.9313, 0.9396]\n",
      "2024-01-10 10:52:36.517693: Epoch time: 39.87 s\n",
      "2024-01-10 10:52:37.725124: \n",
      "2024-01-10 10:52:37.732803: Epoch 599\n",
      "2024-01-10 10:52:37.739823: Current learning rate: 0.00439\n",
      "2024-01-10 10:53:17.487203: train_loss -0.947\n",
      "2024-01-10 10:53:17.499202: val_loss -0.8043\n",
      "2024-01-10 10:53:17.506200: Pseudo dice [0.9222, 0.9308, 0.9389]\n",
      "2024-01-10 10:53:17.514207: Epoch time: 39.76 s\n",
      "2024-01-10 10:53:18.959495: \n",
      "2024-01-10 10:53:18.967495: Epoch 600\n",
      "2024-01-10 10:53:18.971495: Current learning rate: 0.00438\n",
      "2024-01-10 10:53:58.690471: train_loss -0.9466\n",
      "2024-01-10 10:53:58.699000: val_loss -0.7969\n",
      "2024-01-10 10:53:58.708000: Pseudo dice [0.9223, 0.9277, 0.9358]\n",
      "2024-01-10 10:53:58.716097: Epoch time: 39.73 s\n",
      "2024-01-10 10:53:59.979915: \n",
      "2024-01-10 10:53:59.984915: Epoch 601\n",
      "2024-01-10 10:53:59.989907: Current learning rate: 0.00437\n",
      "2024-01-10 10:54:39.640166: train_loss -0.9478\n",
      "2024-01-10 10:54:39.649165: val_loss -0.7935\n",
      "2024-01-10 10:54:39.657173: Pseudo dice [0.9237, 0.9275, 0.937]\n",
      "2024-01-10 10:54:39.663173: Epoch time: 39.66 s\n",
      "2024-01-10 10:54:40.892466: \n",
      "2024-01-10 10:54:40.898469: Epoch 602\n",
      "2024-01-10 10:54:40.904468: Current learning rate: 0.00436\n",
      "2024-01-10 10:55:20.714950: train_loss -0.9471\n",
      "2024-01-10 10:55:20.723957: val_loss -0.7994\n",
      "2024-01-10 10:55:20.732464: Pseudo dice [0.9253, 0.9292, 0.9369]\n",
      "2024-01-10 10:55:20.740472: Epoch time: 39.82 s\n",
      "2024-01-10 10:55:22.038250: \n",
      "2024-01-10 10:55:22.043927: Epoch 603\n",
      "2024-01-10 10:55:22.051260: Current learning rate: 0.00435\n",
      "2024-01-10 10:56:01.995171: train_loss -0.947\n",
      "2024-01-10 10:56:02.005175: val_loss -0.7942\n",
      "2024-01-10 10:56:02.015171: Pseudo dice [0.924, 0.9272, 0.935]\n",
      "2024-01-10 10:56:02.024170: Epoch time: 39.96 s\n",
      "2024-01-10 10:56:03.205207: \n",
      "2024-01-10 10:56:03.214229: Epoch 604\n",
      "2024-01-10 10:56:03.222237: Current learning rate: 0.00434\n",
      "2024-01-10 10:56:42.951941: train_loss -0.9477\n",
      "2024-01-10 10:56:42.961945: val_loss -0.7971\n",
      "2024-01-10 10:56:42.968954: Pseudo dice [0.9208, 0.9283, 0.9361]\n",
      "2024-01-10 10:56:42.976456: Epoch time: 39.75 s\n",
      "2024-01-10 10:56:44.207056: \n",
      "2024-01-10 10:56:44.213352: Epoch 605\n",
      "2024-01-10 10:56:44.218433: Current learning rate: 0.00433\n",
      "2024-01-10 10:57:23.942378: train_loss -0.9475\n",
      "2024-01-10 10:57:23.953377: val_loss -0.7973\n",
      "2024-01-10 10:57:23.961377: Pseudo dice [0.9282, 0.9271, 0.9346]\n",
      "2024-01-10 10:57:23.969377: Epoch time: 39.74 s\n",
      "2024-01-10 10:57:25.159915: \n",
      "2024-01-10 10:57:25.168566: Epoch 606\n",
      "2024-01-10 10:57:25.172722: Current learning rate: 0.00432\n",
      "2024-01-10 10:58:04.830312: train_loss -0.9477\n",
      "2024-01-10 10:58:04.838311: val_loss -0.7961\n",
      "2024-01-10 10:58:04.845313: Pseudo dice [0.9244, 0.93, 0.9382]\n",
      "2024-01-10 10:58:04.853311: Epoch time: 39.67 s\n",
      "2024-01-10 10:58:06.091403: \n",
      "2024-01-10 10:58:06.098193: Epoch 607\n",
      "2024-01-10 10:58:06.102188: Current learning rate: 0.00431\n",
      "2024-01-10 10:58:45.759839: train_loss -0.9476\n",
      "2024-01-10 10:58:45.769828: val_loss -0.7975\n",
      "2024-01-10 10:58:45.803835: Pseudo dice [0.9215, 0.9285, 0.9361]\n",
      "2024-01-10 10:58:45.812341: Epoch time: 39.67 s\n",
      "2024-01-10 10:58:47.167596: \n",
      "2024-01-10 10:58:47.172599: Epoch 608\n",
      "2024-01-10 10:58:47.178596: Current learning rate: 0.0043\n",
      "2024-01-10 10:59:26.859796: train_loss -0.9471\n",
      "2024-01-10 10:59:26.868737: val_loss -0.7966\n",
      "2024-01-10 10:59:26.876736: Pseudo dice [0.9216, 0.9288, 0.9375]\n",
      "2024-01-10 10:59:26.883736: Epoch time: 39.69 s\n",
      "2024-01-10 10:59:28.079979: \n",
      "2024-01-10 10:59:28.085978: Epoch 609\n",
      "2024-01-10 10:59:28.089976: Current learning rate: 0.00429\n",
      "2024-01-10 11:00:07.820081: train_loss -0.9475\n",
      "2024-01-10 11:00:07.829086: val_loss -0.798\n",
      "2024-01-10 11:00:07.837089: Pseudo dice [0.9265, 0.9294, 0.937]\n",
      "2024-01-10 11:00:07.845602: Epoch time: 39.74 s\n",
      "2024-01-10 11:00:09.068668: \n",
      "2024-01-10 11:00:09.074189: Epoch 610\n",
      "2024-01-10 11:00:09.077763: Current learning rate: 0.00429\n",
      "2024-01-10 11:00:48.875113: train_loss -0.9485\n",
      "2024-01-10 11:00:48.906110: val_loss -0.7931\n",
      "2024-01-10 11:00:48.916110: Pseudo dice [0.9248, 0.9295, 0.9367]\n",
      "2024-01-10 11:00:48.925118: Epoch time: 39.81 s\n",
      "2024-01-10 11:00:50.133364: \n",
      "2024-01-10 11:00:50.141592: Epoch 611\n",
      "2024-01-10 11:00:50.151169: Current learning rate: 0.00428\n",
      "2024-01-10 11:01:29.845498: train_loss -0.947\n",
      "2024-01-10 11:01:29.854503: val_loss -0.7947\n",
      "2024-01-10 11:01:29.860502: Pseudo dice [0.9204, 0.928, 0.9357]\n",
      "2024-01-10 11:01:29.867502: Epoch time: 39.71 s\n",
      "2024-01-10 11:01:31.060838: \n",
      "2024-01-10 11:01:31.065840: Epoch 612\n",
      "2024-01-10 11:01:31.070909: Current learning rate: 0.00427\n",
      "2024-01-10 11:02:10.935629: train_loss -0.9474\n",
      "2024-01-10 11:02:10.943956: val_loss -0.7979\n",
      "2024-01-10 11:02:10.951953: Pseudo dice [0.9229, 0.9288, 0.9366]\n",
      "2024-01-10 11:02:10.959954: Epoch time: 39.88 s\n",
      "2024-01-10 11:02:12.280992: \n",
      "2024-01-10 11:02:12.291816: Epoch 613\n",
      "2024-01-10 11:02:12.298903: Current learning rate: 0.00426\n",
      "2024-01-10 11:02:52.072667: train_loss -0.9475\n",
      "2024-01-10 11:02:52.082665: val_loss -0.7994\n",
      "2024-01-10 11:02:52.091670: Pseudo dice [0.9237, 0.9288, 0.937]\n",
      "2024-01-10 11:02:52.098671: Epoch time: 39.79 s\n",
      "2024-01-10 11:02:53.296451: \n",
      "2024-01-10 11:02:53.301394: Epoch 614\n",
      "2024-01-10 11:02:53.305883: Current learning rate: 0.00425\n",
      "2024-01-10 11:03:33.010122: train_loss -0.9478\n",
      "2024-01-10 11:03:33.019630: val_loss -0.7929\n",
      "2024-01-10 11:03:33.045634: Pseudo dice [0.9262, 0.9272, 0.935]\n",
      "2024-01-10 11:03:33.054631: Epoch time: 39.71 s\n",
      "2024-01-10 11:03:34.269924: \n",
      "2024-01-10 11:03:34.278677: Epoch 615\n",
      "2024-01-10 11:03:34.282664: Current learning rate: 0.00424\n",
      "2024-01-10 11:04:14.064477: train_loss -0.9478\n",
      "2024-01-10 11:04:14.073478: val_loss -0.801\n",
      "2024-01-10 11:04:14.080479: Pseudo dice [0.9215, 0.9285, 0.9367]\n",
      "2024-01-10 11:04:14.106478: Epoch time: 39.8 s\n",
      "2024-01-10 11:04:15.295291: \n",
      "2024-01-10 11:04:15.300291: Epoch 616\n",
      "2024-01-10 11:04:15.304291: Current learning rate: 0.00423\n",
      "2024-01-10 11:04:55.075558: train_loss -0.9479\n",
      "2024-01-10 11:04:55.085555: val_loss -0.8016\n",
      "2024-01-10 11:04:55.093554: Pseudo dice [0.9222, 0.9292, 0.9374]\n",
      "2024-01-10 11:04:55.100558: Epoch time: 39.78 s\n",
      "2024-01-10 11:04:56.337314: \n",
      "2024-01-10 11:04:56.342713: Epoch 617\n",
      "2024-01-10 11:04:56.347115: Current learning rate: 0.00422\n",
      "2024-01-10 11:05:36.022441: train_loss -0.9476\n",
      "2024-01-10 11:05:36.033438: val_loss -0.7939\n",
      "2024-01-10 11:05:36.040440: Pseudo dice [0.9231, 0.9279, 0.9366]\n",
      "2024-01-10 11:05:36.072983: Epoch time: 39.69 s\n",
      "2024-01-10 11:05:37.287925: \n",
      "2024-01-10 11:05:37.292994: Epoch 618\n",
      "2024-01-10 11:05:37.296988: Current learning rate: 0.00421\n",
      "2024-01-10 11:06:17.231877: train_loss -0.9471\n",
      "2024-01-10 11:06:17.241874: val_loss -0.7967\n",
      "2024-01-10 11:06:17.250875: Pseudo dice [0.9234, 0.9292, 0.9374]\n",
      "2024-01-10 11:06:17.259874: Epoch time: 39.94 s\n",
      "2024-01-10 11:06:18.439892: \n",
      "2024-01-10 11:06:18.448521: Epoch 619\n",
      "2024-01-10 11:06:18.455591: Current learning rate: 0.0042\n",
      "2024-01-10 11:06:58.199155: train_loss -0.9482\n",
      "2024-01-10 11:06:58.206436: val_loss -0.8003\n",
      "2024-01-10 11:06:58.215617: Pseudo dice [0.9237, 0.9297, 0.9367]\n",
      "2024-01-10 11:06:58.223621: Epoch time: 39.76 s\n",
      "2024-01-10 11:06:59.385576: \n",
      "2024-01-10 11:06:59.390269: Epoch 620\n",
      "2024-01-10 11:06:59.394269: Current learning rate: 0.00419\n",
      "2024-01-10 11:07:39.190927: train_loss -0.9478\n",
      "2024-01-10 11:07:39.220926: val_loss -0.7947\n",
      "2024-01-10 11:07:39.229054: Pseudo dice [0.9227, 0.9291, 0.9375]\n",
      "2024-01-10 11:07:39.238562: Epoch time: 39.81 s\n",
      "2024-01-10 11:07:40.405375: \n",
      "2024-01-10 11:07:40.412572: Epoch 621\n",
      "2024-01-10 11:07:40.420586: Current learning rate: 0.00418\n",
      "2024-01-10 11:08:20.129797: train_loss -0.9479\n",
      "2024-01-10 11:08:20.139805: val_loss -0.8048\n",
      "2024-01-10 11:08:20.148316: Pseudo dice [0.9241, 0.9304, 0.9377]\n",
      "2024-01-10 11:08:20.155323: Epoch time: 39.73 s\n",
      "2024-01-10 11:08:21.348809: \n",
      "2024-01-10 11:08:21.356866: Epoch 622\n",
      "2024-01-10 11:08:21.365816: Current learning rate: 0.00417\n",
      "2024-01-10 11:09:01.074346: train_loss -0.9476\n",
      "2024-01-10 11:09:01.082346: val_loss -0.7995\n",
      "2024-01-10 11:09:01.089347: Pseudo dice [0.924, 0.9297, 0.9373]\n",
      "2024-01-10 11:09:01.095346: Epoch time: 39.73 s\n",
      "2024-01-10 11:09:02.450394: \n",
      "2024-01-10 11:09:02.456390: Epoch 623\n",
      "2024-01-10 11:09:02.459930: Current learning rate: 0.00416\n",
      "2024-01-10 11:09:42.283529: train_loss -0.9479\n",
      "2024-01-10 11:09:42.293535: val_loss -0.8012\n",
      "2024-01-10 11:09:42.301541: Pseudo dice [0.9241, 0.9288, 0.9367]\n",
      "2024-01-10 11:09:42.329537: Epoch time: 39.83 s\n",
      "2024-01-10 11:09:43.563392: \n",
      "2024-01-10 11:09:43.568766: Epoch 624\n",
      "2024-01-10 11:09:43.572176: Current learning rate: 0.00415\n",
      "2024-01-10 11:10:23.362987: train_loss -0.9474\n",
      "2024-01-10 11:10:23.371986: val_loss -0.7988\n",
      "2024-01-10 11:10:23.379987: Pseudo dice [0.9227, 0.9285, 0.9371]\n",
      "2024-01-10 11:10:23.387996: Epoch time: 39.8 s\n",
      "2024-01-10 11:10:24.588371: \n",
      "2024-01-10 11:10:24.593436: Epoch 625\n",
      "2024-01-10 11:10:24.597437: Current learning rate: 0.00414\n",
      "2024-01-10 11:11:04.327825: train_loss -0.9476\n",
      "2024-01-10 11:11:04.337816: val_loss -0.8016\n",
      "2024-01-10 11:11:04.344816: Pseudo dice [0.9243, 0.9294, 0.9378]\n",
      "2024-01-10 11:11:04.351816: Epoch time: 39.74 s\n",
      "2024-01-10 11:11:05.572916: \n",
      "2024-01-10 11:11:05.579453: Epoch 626\n",
      "2024-01-10 11:11:05.584387: Current learning rate: 0.00413\n",
      "2024-01-10 11:11:45.367459: train_loss -0.948\n",
      "2024-01-10 11:11:45.376461: val_loss -0.801\n",
      "2024-01-10 11:11:45.382461: Pseudo dice [0.9232, 0.9295, 0.9388]\n",
      "2024-01-10 11:11:45.390459: Epoch time: 39.8 s\n",
      "2024-01-10 11:11:46.571219: \n",
      "2024-01-10 11:11:46.577215: Epoch 627\n",
      "2024-01-10 11:11:46.581213: Current learning rate: 0.00412\n",
      "2024-01-10 11:12:26.303095: train_loss -0.9481\n",
      "2024-01-10 11:12:26.311093: val_loss -0.7994\n",
      "2024-01-10 11:12:26.317094: Pseudo dice [0.9222, 0.9307, 0.9389]\n",
      "2024-01-10 11:12:26.345094: Epoch time: 39.73 s\n",
      "2024-01-10 11:12:27.549479: \n",
      "2024-01-10 11:12:27.556284: Epoch 628\n",
      "2024-01-10 11:12:27.563867: Current learning rate: 0.00411\n",
      "2024-01-10 11:13:08.895710: train_loss -0.9479\n",
      "2024-01-10 11:13:08.906214: val_loss -0.7978\n",
      "2024-01-10 11:13:08.916219: Pseudo dice [0.9235, 0.9286, 0.9362]\n",
      "2024-01-10 11:13:08.926221: Epoch time: 41.35 s\n",
      "2024-01-10 11:13:10.366271: \n",
      "2024-01-10 11:13:10.374265: Epoch 629\n",
      "2024-01-10 11:13:10.379264: Current learning rate: 0.0041\n",
      "2024-01-10 11:13:52.016817: train_loss -0.9475\n",
      "2024-01-10 11:13:52.027824: val_loss -0.8012\n",
      "2024-01-10 11:13:52.037082: Pseudo dice [0.9198, 0.9301, 0.9374]\n",
      "2024-01-10 11:13:52.047609: Epoch time: 41.65 s\n",
      "2024-01-10 11:13:53.408957: \n",
      "2024-01-10 11:13:53.413953: Epoch 630\n",
      "2024-01-10 11:13:53.418957: Current learning rate: 0.00409\n",
      "2024-01-10 11:14:35.391330: train_loss -0.948\n",
      "2024-01-10 11:14:35.402331: val_loss -0.8078\n",
      "2024-01-10 11:14:35.410330: Pseudo dice [0.9241, 0.9327, 0.9402]\n",
      "2024-01-10 11:14:35.446330: Epoch time: 41.98 s\n",
      "2024-01-10 11:14:36.777515: \n",
      "2024-01-10 11:14:36.782525: Epoch 631\n",
      "2024-01-10 11:14:36.786531: Current learning rate: 0.00408\n",
      "2024-01-10 11:15:17.515060: train_loss -0.9481\n",
      "2024-01-10 11:15:17.525066: val_loss -0.8054\n",
      "2024-01-10 11:15:17.534062: Pseudo dice [0.9232, 0.9303, 0.9379]\n",
      "2024-01-10 11:15:17.542580: Epoch time: 40.74 s\n",
      "2024-01-10 11:15:18.761353: \n",
      "2024-01-10 11:15:18.772436: Epoch 632\n",
      "2024-01-10 11:15:18.778471: Current learning rate: 0.00407\n",
      "2024-01-10 11:16:00.611352: train_loss -0.9476\n",
      "2024-01-10 11:16:00.621866: val_loss -0.7959\n",
      "2024-01-10 11:16:00.658975: Pseudo dice [0.9212, 0.9287, 0.9361]\n",
      "2024-01-10 11:16:00.668413: Epoch time: 41.85 s\n",
      "2024-01-10 11:16:01.875134: \n",
      "2024-01-10 11:16:01.880136: Epoch 633\n",
      "2024-01-10 11:16:01.884651: Current learning rate: 0.00406\n",
      "2024-01-10 11:16:44.563318: train_loss -0.9479\n",
      "2024-01-10 11:16:44.585677: val_loss -0.8015\n",
      "2024-01-10 11:16:44.640895: Pseudo dice [0.9242, 0.9284, 0.9365]\n",
      "2024-01-10 11:16:44.659769: Epoch time: 42.69 s\n",
      "2024-01-10 11:16:46.108812: \n",
      "2024-01-10 11:16:46.114594: Epoch 634\n",
      "2024-01-10 11:16:46.119593: Current learning rate: 0.00405\n",
      "2024-01-10 11:17:29.439827: train_loss -0.9481\n",
      "2024-01-10 11:17:29.450233: val_loss -0.8031\n",
      "2024-01-10 11:17:29.461269: Pseudo dice [0.923, 0.9305, 0.9384]\n",
      "2024-01-10 11:17:29.470930: Epoch time: 43.33 s\n",
      "2024-01-10 11:17:30.969660: \n",
      "2024-01-10 11:17:30.984663: Epoch 635\n",
      "2024-01-10 11:17:30.999652: Current learning rate: 0.00404\n",
      "2024-01-10 11:18:14.535540: train_loss -0.9483\n",
      "2024-01-10 11:18:14.546605: val_loss -0.7994\n",
      "2024-01-10 11:18:14.553975: Pseudo dice [0.9254, 0.9294, 0.9368]\n",
      "2024-01-10 11:18:14.563989: Epoch time: 43.57 s\n",
      "2024-01-10 11:18:16.113542: \n",
      "2024-01-10 11:18:16.119520: Epoch 636\n",
      "2024-01-10 11:18:16.123541: Current learning rate: 0.00403\n",
      "2024-01-10 11:18:58.546459: train_loss -0.947\n",
      "2024-01-10 11:18:58.554458: val_loss -0.8009\n",
      "2024-01-10 11:18:58.561403: Pseudo dice [0.9206, 0.93, 0.9379]\n",
      "2024-01-10 11:18:58.567487: Epoch time: 42.43 s\n",
      "2024-01-10 11:19:00.054122: \n",
      "2024-01-10 11:19:00.065574: Epoch 637\n",
      "2024-01-10 11:19:00.073512: Current learning rate: 0.00402\n",
      "2024-01-10 11:19:41.075784: train_loss -0.9483\n",
      "2024-01-10 11:19:41.086783: val_loss -0.798\n",
      "2024-01-10 11:19:41.092783: Pseudo dice [0.922, 0.9296, 0.9378]\n",
      "2024-01-10 11:19:41.101781: Epoch time: 41.02 s\n",
      "2024-01-10 11:19:42.705244: \n",
      "2024-01-10 11:19:42.710170: Epoch 638\n",
      "2024-01-10 11:19:42.716258: Current learning rate: 0.00401\n",
      "2024-01-10 11:20:23.740455: train_loss -0.9489\n",
      "2024-01-10 11:20:23.752978: val_loss -0.7963\n",
      "2024-01-10 11:20:23.761986: Pseudo dice [0.924, 0.9291, 0.9372]\n",
      "2024-01-10 11:20:23.769499: Epoch time: 41.04 s\n",
      "2024-01-10 11:20:25.178269: \n",
      "2024-01-10 11:20:25.184268: Epoch 639\n",
      "2024-01-10 11:20:25.189260: Current learning rate: 0.004\n",
      "2024-01-10 11:21:06.240492: train_loss -0.9472\n",
      "2024-01-10 11:21:06.250493: val_loss -0.7951\n",
      "2024-01-10 11:21:06.258492: Pseudo dice [0.9198, 0.9283, 0.9358]\n",
      "2024-01-10 11:21:06.266496: Epoch time: 41.06 s\n",
      "2024-01-10 11:21:07.766374: \n",
      "2024-01-10 11:21:07.772390: Epoch 640\n",
      "2024-01-10 11:21:07.776390: Current learning rate: 0.00399\n",
      "2024-01-10 11:21:50.708024: train_loss -0.9474\n",
      "2024-01-10 11:21:50.716335: val_loss -0.7999\n",
      "2024-01-10 11:21:50.725836: Pseudo dice [0.9205, 0.929, 0.9373]\n",
      "2024-01-10 11:21:50.732837: Epoch time: 42.94 s\n",
      "2024-01-10 11:21:52.176637: \n",
      "2024-01-10 11:21:52.181203: Epoch 641\n",
      "2024-01-10 11:21:52.186264: Current learning rate: 0.00398\n",
      "2024-01-10 11:22:33.706040: train_loss -0.9481\n",
      "2024-01-10 11:22:33.748333: val_loss -0.7955\n",
      "2024-01-10 11:22:33.760853: Pseudo dice [0.9237, 0.9277, 0.9364]\n",
      "2024-01-10 11:22:33.803230: Epoch time: 41.53 s\n",
      "2024-01-10 11:22:35.466349: \n",
      "2024-01-10 11:22:35.472356: Epoch 642\n",
      "2024-01-10 11:22:35.477359: Current learning rate: 0.00397\n",
      "2024-01-10 11:23:18.089043: train_loss -0.9483\n",
      "2024-01-10 11:23:18.130042: val_loss -0.8018\n",
      "2024-01-10 11:23:18.142044: Pseudo dice [0.9201, 0.9299, 0.938]\n",
      "2024-01-10 11:23:18.154043: Epoch time: 42.62 s\n",
      "2024-01-10 11:23:19.723024: \n",
      "2024-01-10 11:23:19.729029: Epoch 643\n",
      "2024-01-10 11:23:19.734034: Current learning rate: 0.00396\n",
      "2024-01-10 11:24:03.122007: train_loss -0.9486\n",
      "2024-01-10 11:24:03.157760: val_loss -0.7992\n",
      "2024-01-10 11:24:03.172509: Pseudo dice [0.9206, 0.9295, 0.9383]\n",
      "2024-01-10 11:24:03.184749: Epoch time: 43.4 s\n",
      "2024-01-10 11:24:05.036063: \n",
      "2024-01-10 11:24:05.041576: Epoch 644\n",
      "2024-01-10 11:24:05.047593: Current learning rate: 0.00395\n",
      "2024-01-10 11:24:50.019404: train_loss -0.9483\n",
      "2024-01-10 11:24:50.030316: val_loss -0.7986\n",
      "2024-01-10 11:24:50.037320: Pseudo dice [0.9238, 0.9295, 0.9376]\n",
      "2024-01-10 11:24:50.048773: Epoch time: 44.98 s\n",
      "2024-01-10 11:24:51.621200: \n",
      "2024-01-10 11:24:51.626301: Epoch 645\n",
      "2024-01-10 11:24:51.631643: Current learning rate: 0.00394\n",
      "2024-01-10 11:25:35.377339: train_loss -0.9483\n",
      "2024-01-10 11:25:35.424147: val_loss -0.7972\n",
      "2024-01-10 11:25:35.433654: Pseudo dice [0.923, 0.929, 0.9376]\n",
      "2024-01-10 11:25:35.442656: Epoch time: 43.76 s\n",
      "2024-01-10 11:25:36.955067: \n",
      "2024-01-10 11:25:36.962060: Epoch 646\n",
      "2024-01-10 11:25:36.969081: Current learning rate: 0.00393\n",
      "2024-01-10 11:26:23.420321: train_loss -0.9477\n",
      "2024-01-10 11:26:23.434871: val_loss -0.7968\n",
      "2024-01-10 11:26:23.470730: Pseudo dice [0.923, 0.9287, 0.9372]\n",
      "2024-01-10 11:26:23.482718: Epoch time: 46.47 s\n",
      "2024-01-10 11:26:25.086303: \n",
      "2024-01-10 11:26:25.091310: Epoch 647\n",
      "2024-01-10 11:26:25.096313: Current learning rate: 0.00392\n",
      "2024-01-10 11:27:09.826343: train_loss -0.9477\n",
      "2024-01-10 11:27:09.837371: val_loss -0.8025\n",
      "2024-01-10 11:27:09.847881: Pseudo dice [0.9228, 0.9304, 0.9382]\n",
      "2024-01-10 11:27:09.858403: Epoch time: 44.74 s\n",
      "2024-01-10 11:27:11.633616: \n",
      "2024-01-10 11:27:11.642071: Epoch 648\n",
      "2024-01-10 11:27:11.653655: Current learning rate: 0.00391\n",
      "2024-01-10 11:27:56.984863: train_loss -0.9486\n",
      "2024-01-10 11:27:57.041045: val_loss -0.7962\n",
      "2024-01-10 11:27:57.050567: Pseudo dice [0.9246, 0.9278, 0.9356]\n",
      "2024-01-10 11:27:57.062150: Epoch time: 45.35 s\n",
      "2024-01-10 11:27:58.671995: \n",
      "2024-01-10 11:27:58.677520: Epoch 649\n",
      "2024-01-10 11:27:58.682526: Current learning rate: 0.0039\n",
      "2024-01-10 11:28:40.597190: train_loss -0.948\n",
      "2024-01-10 11:28:40.606699: val_loss -0.7989\n",
      "2024-01-10 11:28:40.617551: Pseudo dice [0.9198, 0.9295, 0.9363]\n",
      "2024-01-10 11:28:40.624565: Epoch time: 41.93 s\n",
      "2024-01-10 11:28:42.406628: \n",
      "2024-01-10 11:28:42.411644: Epoch 650\n",
      "2024-01-10 11:28:42.417168: Current learning rate: 0.00389\n",
      "2024-01-10 11:29:24.054404: train_loss -0.9488\n",
      "2024-01-10 11:29:24.069465: val_loss -0.7986\n",
      "2024-01-10 11:29:24.085040: Pseudo dice [0.9245, 0.9296, 0.9379]\n",
      "2024-01-10 11:29:24.093558: Epoch time: 41.65 s\n",
      "2024-01-10 11:29:25.680760: \n",
      "2024-01-10 11:29:25.686336: Epoch 651\n",
      "2024-01-10 11:29:25.691394: Current learning rate: 0.00388\n",
      "2024-01-10 11:30:07.465198: train_loss -0.9489\n",
      "2024-01-10 11:30:07.473594: val_loss -0.798\n",
      "2024-01-10 11:30:07.480851: Pseudo dice [0.9235, 0.929, 0.9371]\n",
      "2024-01-10 11:30:07.491885: Epoch time: 41.79 s\n",
      "2024-01-10 11:30:09.339192: \n",
      "2024-01-10 11:30:09.345189: Epoch 652\n",
      "2024-01-10 11:30:09.351705: Current learning rate: 0.00387\n",
      "2024-01-10 11:30:52.799957: train_loss -0.948\n",
      "2024-01-10 11:30:52.811468: val_loss -0.7974\n",
      "2024-01-10 11:30:52.820365: Pseudo dice [0.9213, 0.9286, 0.9364]\n",
      "2024-01-10 11:30:52.859639: Epoch time: 43.46 s\n",
      "2024-01-10 11:30:54.586978: \n",
      "2024-01-10 11:30:54.593983: Epoch 653\n",
      "2024-01-10 11:30:54.598514: Current learning rate: 0.00386\n",
      "2024-01-10 11:31:37.565646: train_loss -0.9481\n",
      "2024-01-10 11:31:37.579175: val_loss -0.7996\n",
      "2024-01-10 11:31:37.590759: Pseudo dice [0.9249, 0.929, 0.9373]\n",
      "2024-01-10 11:31:37.606172: Epoch time: 42.98 s\n",
      "2024-01-10 11:31:39.553810: \n",
      "2024-01-10 11:31:39.559293: Epoch 654\n",
      "2024-01-10 11:31:39.564297: Current learning rate: 0.00385\n",
      "2024-01-10 11:32:25.294521: train_loss -0.9487\n",
      "2024-01-10 11:32:25.304040: val_loss -0.799\n",
      "2024-01-10 11:32:25.314488: Pseudo dice [0.9202, 0.9297, 0.9379]\n",
      "2024-01-10 11:32:25.329297: Epoch time: 45.74 s\n",
      "2024-01-10 11:32:27.140294: \n",
      "2024-01-10 11:32:27.151836: Epoch 655\n",
      "2024-01-10 11:32:27.162911: Current learning rate: 0.00384\n",
      "2024-01-10 11:33:11.338131: train_loss -0.9486\n",
      "2024-01-10 11:33:11.348133: val_loss -0.801\n",
      "2024-01-10 11:33:11.357130: Pseudo dice [0.9242, 0.9293, 0.9379]\n",
      "2024-01-10 11:33:11.365140: Epoch time: 44.2 s\n",
      "2024-01-10 11:33:13.041559: \n",
      "2024-01-10 11:33:13.046556: Epoch 656\n",
      "2024-01-10 11:33:13.051559: Current learning rate: 0.00383\n",
      "2024-01-10 11:33:56.800946: train_loss -0.9496\n",
      "2024-01-10 11:33:56.813879: val_loss -0.796\n",
      "2024-01-10 11:33:56.825592: Pseudo dice [0.9215, 0.9289, 0.937]\n",
      "2024-01-10 11:33:56.838126: Epoch time: 43.76 s\n",
      "2024-01-10 11:33:58.366442: \n",
      "2024-01-10 11:33:58.371443: Epoch 657\n",
      "2024-01-10 11:33:58.375977: Current learning rate: 0.00382\n",
      "2024-01-10 11:34:43.411640: train_loss -0.948\n",
      "2024-01-10 11:34:43.424202: val_loss -0.7973\n",
      "2024-01-10 11:34:43.436838: Pseudo dice [0.9217, 0.9288, 0.9377]\n",
      "2024-01-10 11:34:43.449341: Epoch time: 45.05 s\n",
      "2024-01-10 11:34:45.639929: \n",
      "2024-01-10 11:34:45.645439: Epoch 658\n",
      "2024-01-10 11:34:45.651439: Current learning rate: 0.00381\n",
      "2024-01-10 11:35:30.818580: train_loss -0.9486\n",
      "2024-01-10 11:35:30.832232: val_loss -0.7928\n",
      "2024-01-10 11:35:30.847275: Pseudo dice [0.9214, 0.9282, 0.9356]\n",
      "2024-01-10 11:35:30.858401: Epoch time: 45.18 s\n",
      "2024-01-10 11:35:32.774175: \n",
      "2024-01-10 11:35:32.780179: Epoch 659\n",
      "2024-01-10 11:35:32.785688: Current learning rate: 0.0038\n",
      "2024-01-10 11:36:17.618922: train_loss -0.9489\n",
      "2024-01-10 11:36:17.627922: val_loss -0.7956\n",
      "2024-01-10 11:36:17.636117: Pseudo dice [0.9251, 0.9291, 0.9375]\n",
      "2024-01-10 11:36:17.643220: Epoch time: 44.85 s\n",
      "2024-01-10 11:36:19.284490: \n",
      "2024-01-10 11:36:19.290490: Epoch 660\n",
      "2024-01-10 11:36:19.296044: Current learning rate: 0.00379\n",
      "2024-01-10 11:37:01.810502: train_loss -0.9486\n",
      "2024-01-10 11:37:01.820494: val_loss -0.7959\n",
      "2024-01-10 11:37:01.854016: Pseudo dice [0.9229, 0.9265, 0.9349]\n",
      "2024-01-10 11:37:01.861016: Epoch time: 42.53 s\n",
      "2024-01-10 11:37:03.439491: \n",
      "2024-01-10 11:37:03.444918: Epoch 661\n",
      "2024-01-10 11:37:03.449946: Current learning rate: 0.00378\n",
      "2024-01-10 11:37:47.205108: train_loss -0.949\n",
      "2024-01-10 11:37:47.220798: val_loss -0.8032\n",
      "2024-01-10 11:37:47.232306: Pseudo dice [0.9245, 0.9299, 0.939]\n",
      "2024-01-10 11:37:47.239816: Epoch time: 43.77 s\n",
      "2024-01-10 11:37:48.932465: \n",
      "2024-01-10 11:37:48.937464: Epoch 662\n",
      "2024-01-10 11:37:48.942465: Current learning rate: 0.00377\n",
      "2024-01-10 11:38:32.131094: train_loss -0.9482\n",
      "2024-01-10 11:38:32.138094: val_loss -0.7975\n",
      "2024-01-10 11:38:32.146095: Pseudo dice [0.9205, 0.9292, 0.9385]\n",
      "2024-01-10 11:38:32.182102: Epoch time: 43.2 s\n",
      "2024-01-10 11:38:33.732324: \n",
      "2024-01-10 11:38:33.737324: Epoch 663\n",
      "2024-01-10 11:38:33.742320: Current learning rate: 0.00376\n",
      "2024-01-10 11:39:17.211097: train_loss -0.9483\n",
      "2024-01-10 11:39:17.246611: val_loss -0.7947\n",
      "2024-01-10 11:39:17.256610: Pseudo dice [0.923, 0.9287, 0.9366]\n",
      "2024-01-10 11:39:17.269612: Epoch time: 43.48 s\n",
      "2024-01-10 11:39:19.197849: \n",
      "2024-01-10 11:39:19.206867: Epoch 664\n",
      "2024-01-10 11:39:19.212871: Current learning rate: 0.00375\n",
      "2024-01-10 11:40:02.144770: train_loss -0.9493\n",
      "2024-01-10 11:40:02.156489: val_loss -0.7959\n",
      "2024-01-10 11:40:02.197397: Pseudo dice [0.927, 0.9285, 0.9367]\n",
      "2024-01-10 11:40:02.207401: Epoch time: 42.95 s\n",
      "2024-01-10 11:40:03.834216: \n",
      "2024-01-10 11:40:03.841237: Epoch 665\n",
      "2024-01-10 11:40:03.848232: Current learning rate: 0.00374\n",
      "2024-01-10 11:40:45.331903: train_loss -0.9491\n",
      "2024-01-10 11:40:45.343903: val_loss -0.789\n",
      "2024-01-10 11:40:45.370616: Pseudo dice [0.9191, 0.9277, 0.936]\n",
      "2024-01-10 11:40:45.384621: Epoch time: 41.5 s\n",
      "2024-01-10 11:40:46.954108: \n",
      "2024-01-10 11:40:46.960116: Epoch 666\n",
      "2024-01-10 11:40:46.966116: Current learning rate: 0.00373\n",
      "2024-01-10 11:41:29.139849: train_loss -0.9491\n",
      "2024-01-10 11:41:29.149105: val_loss -0.7997\n",
      "2024-01-10 11:41:29.158483: Pseudo dice [0.9248, 0.9286, 0.9372]\n",
      "2024-01-10 11:41:29.164484: Epoch time: 42.19 s\n",
      "2024-01-10 11:41:30.653412: \n",
      "2024-01-10 11:41:30.658412: Epoch 667\n",
      "2024-01-10 11:41:30.662413: Current learning rate: 0.00372\n",
      "2024-01-10 11:42:13.719508: train_loss -0.948\n",
      "2024-01-10 11:42:13.728505: val_loss -0.802\n",
      "2024-01-10 11:42:13.736688: Pseudo dice [0.9278, 0.9287, 0.937]\n",
      "2024-01-10 11:42:13.743687: Epoch time: 43.07 s\n",
      "2024-01-10 11:42:15.230847: \n",
      "2024-01-10 11:42:15.236851: Epoch 668\n",
      "2024-01-10 11:42:15.242229: Current learning rate: 0.00371\n",
      "2024-01-10 11:42:58.357529: train_loss -0.9487\n",
      "2024-01-10 11:42:58.366528: val_loss -0.7926\n",
      "2024-01-10 11:42:58.373758: Pseudo dice [0.9232, 0.9267, 0.9351]\n",
      "2024-01-10 11:42:58.382283: Epoch time: 43.13 s\n",
      "2024-01-10 11:42:59.927419: \n",
      "2024-01-10 11:42:59.933414: Epoch 669\n",
      "2024-01-10 11:42:59.938425: Current learning rate: 0.0037\n",
      "2024-01-10 11:43:42.319142: train_loss -0.9494\n",
      "2024-01-10 11:43:42.334145: val_loss -0.8021\n",
      "2024-01-10 11:43:42.355148: Pseudo dice [0.9232, 0.9303, 0.9392]\n",
      "2024-01-10 11:43:42.369139: Epoch time: 42.39 s\n",
      "2024-01-10 11:43:44.016592: \n",
      "2024-01-10 11:43:44.023597: Epoch 670\n",
      "2024-01-10 11:43:44.028598: Current learning rate: 0.00369\n",
      "2024-01-10 11:44:26.557676: train_loss -0.948\n",
      "2024-01-10 11:44:26.566676: val_loss -0.7954\n",
      "2024-01-10 11:44:26.575608: Pseudo dice [0.92, 0.9282, 0.936]\n",
      "2024-01-10 11:44:26.608608: Epoch time: 42.54 s\n",
      "2024-01-10 11:44:28.204240: \n",
      "2024-01-10 11:44:28.212251: Epoch 671\n",
      "2024-01-10 11:44:28.221256: Current learning rate: 0.00368\n",
      "2024-01-10 11:45:10.920149: train_loss -0.9489\n",
      "2024-01-10 11:45:10.927150: val_loss -0.7957\n",
      "2024-01-10 11:45:10.933147: Pseudo dice [0.9226, 0.9288, 0.9364]\n",
      "2024-01-10 11:45:10.940148: Epoch time: 42.72 s\n",
      "2024-01-10 11:45:12.653532: \n",
      "2024-01-10 11:45:12.659361: Epoch 672\n",
      "2024-01-10 11:45:12.664530: Current learning rate: 0.00367\n",
      "2024-01-10 11:45:54.733045: train_loss -0.949\n",
      "2024-01-10 11:45:54.751197: val_loss -0.7964\n",
      "2024-01-10 11:45:54.801591: Pseudo dice [0.9238, 0.9291, 0.9364]\n",
      "2024-01-10 11:45:54.814029: Epoch time: 42.08 s\n",
      "2024-01-10 11:45:56.415213: \n",
      "2024-01-10 11:45:56.421216: Epoch 673\n",
      "2024-01-10 11:45:56.429214: Current learning rate: 0.00366\n",
      "2024-01-10 11:46:39.334191: train_loss -0.9488\n",
      "2024-01-10 11:46:39.344704: val_loss -0.7967\n",
      "2024-01-10 11:46:39.351878: Pseudo dice [0.922, 0.9293, 0.9362]\n",
      "2024-01-10 11:46:39.381387: Epoch time: 42.92 s\n",
      "2024-01-10 11:46:40.826015: \n",
      "2024-01-10 11:46:40.831821: Epoch 674\n",
      "2024-01-10 11:46:40.839576: Current learning rate: 0.00365\n",
      "2024-01-10 11:47:26.619426: train_loss -0.9494\n",
      "2024-01-10 11:47:26.630417: val_loss -0.7979\n",
      "2024-01-10 11:47:26.638927: Pseudo dice [0.9214, 0.9298, 0.938]\n",
      "2024-01-10 11:47:26.668625: Epoch time: 45.79 s\n",
      "2024-01-10 11:47:28.120348: \n",
      "2024-01-10 11:47:28.125427: Epoch 675\n",
      "2024-01-10 11:47:28.130905: Current learning rate: 0.00364\n",
      "2024-01-10 11:48:11.709985: train_loss -0.95\n",
      "2024-01-10 11:48:11.734986: val_loss -0.7951\n",
      "2024-01-10 11:48:11.751983: Pseudo dice [0.9226, 0.9293, 0.9363]\n",
      "2024-01-10 11:48:11.768362: Epoch time: 43.59 s\n",
      "2024-01-10 11:48:13.696856: \n",
      "2024-01-10 11:48:13.701787: Epoch 676\n",
      "2024-01-10 11:48:13.707292: Current learning rate: 0.00363\n",
      "2024-01-10 11:48:55.838328: train_loss -0.9493\n",
      "2024-01-10 11:48:55.846326: val_loss -0.7971\n",
      "2024-01-10 11:48:55.853327: Pseudo dice [0.9178, 0.9292, 0.9382]\n",
      "2024-01-10 11:48:55.858327: Epoch time: 42.14 s\n",
      "2024-01-10 11:48:57.533905: \n",
      "2024-01-10 11:48:57.539663: Epoch 677\n",
      "2024-01-10 11:48:57.545206: Current learning rate: 0.00362\n",
      "2024-01-10 11:49:38.533701: train_loss -0.9498\n",
      "2024-01-10 11:49:38.545209: val_loss -0.7951\n",
      "2024-01-10 11:49:38.554209: Pseudo dice [0.9219, 0.9279, 0.9369]\n",
      "2024-01-10 11:49:38.564209: Epoch time: 41.0 s\n",
      "2024-01-10 11:49:40.074355: \n",
      "2024-01-10 11:49:40.083360: Epoch 678\n",
      "2024-01-10 11:49:40.088346: Current learning rate: 0.00361\n",
      "2024-01-10 11:50:20.814615: train_loss -0.9493\n",
      "2024-01-10 11:50:20.840612: val_loss -0.7991\n",
      "2024-01-10 11:50:20.847611: Pseudo dice [0.9237, 0.9301, 0.938]\n",
      "2024-01-10 11:50:20.853609: Epoch time: 40.74 s\n",
      "2024-01-10 11:50:22.305058: \n",
      "2024-01-10 11:50:22.315134: Epoch 679\n",
      "2024-01-10 11:50:22.320120: Current learning rate: 0.0036\n",
      "2024-01-10 11:51:03.078704: train_loss -0.9494\n",
      "2024-01-10 11:51:03.088713: val_loss -0.8013\n",
      "2024-01-10 11:51:03.096713: Pseudo dice [0.9213, 0.9303, 0.9386]\n",
      "2024-01-10 11:51:03.104227: Epoch time: 40.77 s\n",
      "2024-01-10 11:51:04.651478: \n",
      "2024-01-10 11:51:04.661423: Epoch 680\n",
      "2024-01-10 11:51:04.668500: Current learning rate: 0.00359\n",
      "2024-01-10 11:51:45.586762: train_loss -0.9489\n",
      "2024-01-10 11:51:45.594763: val_loss -0.7977\n",
      "2024-01-10 11:51:45.601766: Pseudo dice [0.9239, 0.9295, 0.9376]\n",
      "2024-01-10 11:51:45.608765: Epoch time: 40.94 s\n",
      "2024-01-10 11:51:46.966847: \n",
      "2024-01-10 11:51:46.972777: Epoch 681\n",
      "2024-01-10 11:51:46.976778: Current learning rate: 0.00358\n",
      "2024-01-10 11:52:27.616424: train_loss -0.9494\n",
      "2024-01-10 11:52:27.627422: val_loss -0.8028\n",
      "2024-01-10 11:52:27.634423: Pseudo dice [0.9243, 0.9311, 0.9394]\n",
      "2024-01-10 11:52:27.641431: Epoch time: 40.65 s\n",
      "2024-01-10 11:52:29.285592: \n",
      "2024-01-10 11:52:29.290329: Epoch 682\n",
      "2024-01-10 11:52:29.297404: Current learning rate: 0.00357\n",
      "2024-01-10 11:53:10.388728: train_loss -0.9498\n",
      "2024-01-10 11:53:10.398728: val_loss -0.8035\n",
      "2024-01-10 11:53:10.405726: Pseudo dice [0.9235, 0.9312, 0.939]\n",
      "2024-01-10 11:53:10.414727: Epoch time: 41.1 s\n",
      "2024-01-10 11:53:11.927016: \n",
      "2024-01-10 11:53:11.936016: Epoch 683\n",
      "2024-01-10 11:53:11.943017: Current learning rate: 0.00356\n",
      "2024-01-10 11:53:52.564899: train_loss -0.9496\n",
      "2024-01-10 11:53:52.572904: val_loss -0.7992\n",
      "2024-01-10 11:53:52.578902: Pseudo dice [0.9206, 0.9297, 0.9378]\n",
      "2024-01-10 11:53:52.586414: Epoch time: 40.64 s\n",
      "2024-01-10 11:53:54.065289: \n",
      "2024-01-10 11:53:54.072292: Epoch 684\n",
      "2024-01-10 11:53:54.076345: Current learning rate: 0.00355\n",
      "2024-01-10 11:54:34.947181: train_loss -0.9495\n",
      "2024-01-10 11:54:34.954312: val_loss -0.7881\n",
      "2024-01-10 11:54:34.959312: Pseudo dice [0.9194, 0.9276, 0.9358]\n",
      "2024-01-10 11:54:34.966313: Epoch time: 40.88 s\n",
      "2024-01-10 11:54:36.404681: \n",
      "2024-01-10 11:54:36.409723: Epoch 685\n",
      "2024-01-10 11:54:36.414728: Current learning rate: 0.00354\n",
      "2024-01-10 11:55:17.206549: train_loss -0.9494\n",
      "2024-01-10 11:55:17.216551: val_loss -0.798\n",
      "2024-01-10 11:55:17.245071: Pseudo dice [0.9231, 0.9287, 0.9375]\n",
      "2024-01-10 11:55:17.253071: Epoch time: 40.8 s\n",
      "2024-01-10 11:55:18.719611: \n",
      "2024-01-10 11:55:18.724809: Epoch 686\n",
      "2024-01-10 11:55:18.728730: Current learning rate: 0.00353\n",
      "2024-01-10 11:55:59.450051: train_loss -0.9497\n",
      "2024-01-10 11:55:59.459048: val_loss -0.8028\n",
      "2024-01-10 11:55:59.466048: Pseudo dice [0.9226, 0.9313, 0.9386]\n",
      "2024-01-10 11:55:59.496051: Epoch time: 40.73 s\n",
      "2024-01-10 11:56:01.036728: \n",
      "2024-01-10 11:56:01.043052: Epoch 687\n",
      "2024-01-10 11:56:01.052563: Current learning rate: 0.00352\n",
      "2024-01-10 11:56:41.726723: train_loss -0.9491\n",
      "2024-01-10 11:56:41.733724: val_loss -0.7997\n",
      "2024-01-10 11:56:41.738724: Pseudo dice [0.9236, 0.9294, 0.937]\n",
      "2024-01-10 11:56:41.744728: Epoch time: 40.69 s\n",
      "2024-01-10 11:56:43.208057: \n",
      "2024-01-10 11:56:43.213129: Epoch 688\n",
      "2024-01-10 11:56:43.218061: Current learning rate: 0.00351\n",
      "2024-01-10 11:57:23.578218: train_loss -0.9489\n",
      "2024-01-10 11:57:23.588221: val_loss -0.7947\n",
      "2024-01-10 11:57:23.596220: Pseudo dice [0.9219, 0.9288, 0.9365]\n",
      "2024-01-10 11:57:23.604219: Epoch time: 40.37 s\n",
      "2024-01-10 11:57:25.142308: \n",
      "2024-01-10 11:57:25.147407: Epoch 689\n",
      "2024-01-10 11:57:25.155421: Current learning rate: 0.0035\n",
      "2024-01-10 11:58:05.527067: train_loss -0.9495\n",
      "2024-01-10 11:58:05.536071: val_loss -0.7953\n",
      "2024-01-10 11:58:05.542073: Pseudo dice [0.9207, 0.9283, 0.9369]\n",
      "2024-01-10 11:58:05.550068: Epoch time: 40.39 s\n",
      "2024-01-10 11:58:06.960155: \n",
      "2024-01-10 11:58:06.966054: Epoch 690\n",
      "2024-01-10 11:58:06.970028: Current learning rate: 0.00349\n",
      "2024-01-10 11:58:47.063093: train_loss -0.9497\n",
      "2024-01-10 11:58:47.069094: val_loss -0.7973\n",
      "2024-01-10 11:58:47.076093: Pseudo dice [0.9225, 0.9288, 0.9363]\n",
      "2024-01-10 11:58:47.081094: Epoch time: 40.1 s\n",
      "2024-01-10 11:58:48.664253: \n",
      "2024-01-10 11:58:48.672318: Epoch 691\n",
      "2024-01-10 11:58:48.679393: Current learning rate: 0.00348\n",
      "2024-01-10 11:59:29.049557: train_loss -0.9499\n",
      "2024-01-10 11:59:29.060559: val_loss -0.8026\n",
      "2024-01-10 11:59:29.066557: Pseudo dice [0.9246, 0.9292, 0.9375]\n",
      "2024-01-10 11:59:29.071560: Epoch time: 40.39 s\n",
      "2024-01-10 11:59:30.778111: \n",
      "2024-01-10 11:59:30.783102: Epoch 692\n",
      "2024-01-10 11:59:30.788108: Current learning rate: 0.00346\n",
      "2024-01-10 12:00:10.987655: train_loss -0.9491\n",
      "2024-01-10 12:00:10.996664: val_loss -0.7991\n",
      "2024-01-10 12:00:11.002656: Pseudo dice [0.922, 0.9287, 0.9362]\n",
      "2024-01-10 12:00:11.007656: Epoch time: 40.21 s\n",
      "2024-01-10 12:00:12.494745: \n",
      "2024-01-10 12:00:12.505828: Epoch 693\n",
      "2024-01-10 12:00:12.514820: Current learning rate: 0.00345\n",
      "2024-01-10 12:00:52.767494: train_loss -0.9497\n",
      "2024-01-10 12:00:52.775497: val_loss -0.7936\n",
      "2024-01-10 12:00:52.782496: Pseudo dice [0.9203, 0.9289, 0.9366]\n",
      "2024-01-10 12:00:52.787496: Epoch time: 40.27 s\n",
      "2024-01-10 12:00:54.235811: \n",
      "2024-01-10 12:00:54.246829: Epoch 694\n",
      "2024-01-10 12:00:54.254842: Current learning rate: 0.00344\n",
      "2024-01-10 12:01:34.470160: train_loss -0.9492\n",
      "2024-01-10 12:01:34.478157: val_loss -0.7994\n",
      "2024-01-10 12:01:34.483156: Pseudo dice [0.924, 0.9298, 0.9373]\n",
      "2024-01-10 12:01:34.490156: Epoch time: 40.24 s\n",
      "2024-01-10 12:01:35.964048: \n",
      "2024-01-10 12:01:35.969049: Epoch 695\n",
      "2024-01-10 12:01:35.973992: Current learning rate: 0.00343\n",
      "2024-01-10 12:02:16.127268: train_loss -0.9494\n",
      "2024-01-10 12:02:16.133777: val_loss -0.7971\n",
      "2024-01-10 12:02:16.141294: Pseudo dice [0.9229, 0.9286, 0.9373]\n",
      "2024-01-10 12:02:16.163290: Epoch time: 40.17 s\n",
      "2024-01-10 12:02:17.568043: \n",
      "2024-01-10 12:02:17.578696: Epoch 696\n",
      "2024-01-10 12:02:17.585768: Current learning rate: 0.00342\n",
      "2024-01-10 12:02:57.714168: train_loss -0.9493\n",
      "2024-01-10 12:02:57.724167: val_loss -0.796\n",
      "2024-01-10 12:02:57.730178: Pseudo dice [0.9215, 0.9285, 0.9371]\n",
      "2024-01-10 12:02:57.763683: Epoch time: 40.15 s\n",
      "2024-01-10 12:02:59.321995: \n",
      "2024-01-10 12:02:59.327330: Epoch 697\n",
      "2024-01-10 12:02:59.331478: Current learning rate: 0.00341\n",
      "2024-01-10 12:03:39.492497: train_loss -0.9497\n",
      "2024-01-10 12:03:39.501498: val_loss -0.7902\n",
      "2024-01-10 12:03:39.506501: Pseudo dice [0.9225, 0.9272, 0.9351]\n",
      "2024-01-10 12:03:39.512499: Epoch time: 40.17 s\n",
      "2024-01-10 12:03:41.064987: \n",
      "2024-01-10 12:03:41.071058: Epoch 698\n",
      "2024-01-10 12:03:41.079064: Current learning rate: 0.0034\n",
      "2024-01-10 12:04:21.086525: train_loss -0.9499\n",
      "2024-01-10 12:04:21.096554: val_loss -0.8003\n",
      "2024-01-10 12:04:21.128049: Pseudo dice [0.9203, 0.9296, 0.9374]\n",
      "2024-01-10 12:04:21.135050: Epoch time: 40.02 s\n",
      "2024-01-10 12:04:22.678726: \n",
      "2024-01-10 12:04:22.683619: Epoch 699\n",
      "2024-01-10 12:04:22.688656: Current learning rate: 0.00339\n",
      "2024-01-10 12:05:02.981555: train_loss -0.9498\n",
      "2024-01-10 12:05:02.988555: val_loss -0.7966\n",
      "2024-01-10 12:05:02.993557: Pseudo dice [0.9242, 0.9287, 0.937]\n",
      "2024-01-10 12:05:02.997557: Epoch time: 40.3 s\n",
      "2024-01-10 12:05:04.681427: \n",
      "2024-01-10 12:05:04.692400: Epoch 700\n",
      "2024-01-10 12:05:04.702375: Current learning rate: 0.00338\n",
      "2024-01-10 12:05:45.011422: train_loss -0.9498\n",
      "2024-01-10 12:05:45.018423: val_loss -0.8003\n",
      "2024-01-10 12:05:45.047949: Pseudo dice [0.9254, 0.9288, 0.9363]\n",
      "2024-01-10 12:05:45.055946: Epoch time: 40.33 s\n",
      "2024-01-10 12:05:46.531186: \n",
      "2024-01-10 12:05:46.538263: Epoch 701\n",
      "2024-01-10 12:05:46.543994: Current learning rate: 0.00337\n",
      "2024-01-10 12:06:27.228602: train_loss -0.9502\n",
      "2024-01-10 12:06:27.238600: val_loss -0.7992\n",
      "2024-01-10 12:06:27.264120: Pseudo dice [0.9248, 0.9297, 0.9383]\n",
      "2024-01-10 12:06:27.270120: Epoch time: 40.7 s\n",
      "2024-01-10 12:06:28.480260: \n",
      "2024-01-10 12:06:28.485260: Epoch 702\n",
      "2024-01-10 12:06:28.489887: Current learning rate: 0.00336\n",
      "2024-01-10 12:07:08.912304: train_loss -0.9501\n",
      "2024-01-10 12:07:08.918304: val_loss -0.7983\n",
      "2024-01-10 12:07:08.924304: Pseudo dice [0.9221, 0.9301, 0.9384]\n",
      "2024-01-10 12:07:08.930310: Epoch time: 40.43 s\n",
      "2024-01-10 12:07:10.334655: \n",
      "2024-01-10 12:07:10.340599: Epoch 703\n",
      "2024-01-10 12:07:10.347656: Current learning rate: 0.00335\n",
      "2024-01-10 12:07:50.972192: train_loss -0.9499\n",
      "2024-01-10 12:07:50.982003: val_loss -0.8032\n",
      "2024-01-10 12:07:50.993094: Pseudo dice [0.9238, 0.9304, 0.9384]\n",
      "2024-01-10 12:07:51.002629: Epoch time: 40.64 s\n",
      "2024-01-10 12:07:52.660367: \n",
      "2024-01-10 12:07:52.666367: Epoch 704\n",
      "2024-01-10 12:07:52.674367: Current learning rate: 0.00334\n",
      "2024-01-10 12:08:34.030127: train_loss -0.9505\n",
      "2024-01-10 12:08:34.039132: val_loss -0.8019\n",
      "2024-01-10 12:08:34.049130: Pseudo dice [0.924, 0.9296, 0.9379]\n",
      "2024-01-10 12:08:34.055968: Epoch time: 41.37 s\n",
      "2024-01-10 12:08:35.577785: \n",
      "2024-01-10 12:08:35.586389: Epoch 705\n",
      "2024-01-10 12:08:35.591398: Current learning rate: 0.00333\n",
      "2024-01-10 12:09:16.471581: train_loss -0.9505\n",
      "2024-01-10 12:09:16.482580: val_loss -0.7986\n",
      "2024-01-10 12:09:16.490580: Pseudo dice [0.9209, 0.9298, 0.9377]\n",
      "2024-01-10 12:09:16.498580: Epoch time: 40.89 s\n",
      "2024-01-10 12:09:17.931630: \n",
      "2024-01-10 12:09:17.938629: Epoch 706\n",
      "2024-01-10 12:09:17.943695: Current learning rate: 0.00332\n",
      "2024-01-10 12:09:58.735795: train_loss -0.9503\n",
      "2024-01-10 12:09:58.743784: val_loss -0.7992\n",
      "2024-01-10 12:09:58.749784: Pseudo dice [0.9211, 0.9301, 0.9378]\n",
      "2024-01-10 12:09:58.754790: Epoch time: 40.81 s\n",
      "2024-01-10 12:10:00.227627: \n",
      "2024-01-10 12:10:00.237631: Epoch 707\n",
      "2024-01-10 12:10:00.242619: Current learning rate: 0.00331\n",
      "2024-01-10 12:10:40.833224: train_loss -0.9496\n",
      "2024-01-10 12:10:40.840226: val_loss -0.8006\n",
      "2024-01-10 12:10:40.867224: Pseudo dice [0.9269, 0.9277, 0.9365]\n",
      "2024-01-10 12:10:40.875225: Epoch time: 40.61 s\n",
      "2024-01-10 12:10:42.366761: \n",
      "2024-01-10 12:10:42.373755: Epoch 708\n",
      "2024-01-10 12:10:42.378764: Current learning rate: 0.0033\n",
      "2024-01-10 12:11:23.066223: train_loss -0.9507\n",
      "2024-01-10 12:11:23.073231: val_loss -0.7972\n",
      "2024-01-10 12:11:23.082225: Pseudo dice [0.9207, 0.9299, 0.9385]\n",
      "2024-01-10 12:11:23.090222: Epoch time: 40.7 s\n",
      "2024-01-10 12:11:24.684445: \n",
      "2024-01-10 12:11:24.696100: Epoch 709\n",
      "2024-01-10 12:11:24.700083: Current learning rate: 0.00329\n",
      "2024-01-10 12:12:05.337309: train_loss -0.9503\n",
      "2024-01-10 12:12:05.343312: val_loss -0.7891\n",
      "2024-01-10 12:12:05.351301: Pseudo dice [0.9199, 0.9268, 0.9344]\n",
      "2024-01-10 12:12:05.357303: Epoch time: 40.65 s\n",
      "2024-01-10 12:12:06.945641: \n",
      "2024-01-10 12:12:06.955580: Epoch 710\n",
      "2024-01-10 12:12:06.962089: Current learning rate: 0.00328\n",
      "2024-01-10 12:12:47.710419: train_loss -0.9507\n",
      "2024-01-10 12:12:47.720422: val_loss -0.7849\n",
      "2024-01-10 12:12:47.727420: Pseudo dice [0.9197, 0.9262, 0.9345]\n",
      "2024-01-10 12:12:47.734421: Epoch time: 40.77 s\n",
      "2024-01-10 12:12:49.228261: \n",
      "2024-01-10 12:12:49.233957: Epoch 711\n",
      "2024-01-10 12:12:49.238977: Current learning rate: 0.00327\n",
      "2024-01-10 12:13:29.883351: train_loss -0.9497\n",
      "2024-01-10 12:13:29.892868: val_loss -0.7993\n",
      "2024-01-10 12:13:29.900863: Pseudo dice [0.9209, 0.9297, 0.9374]\n",
      "2024-01-10 12:13:29.925952: Epoch time: 40.66 s\n",
      "2024-01-10 12:13:31.338176: \n",
      "2024-01-10 12:13:31.344165: Epoch 712\n",
      "2024-01-10 12:13:31.354111: Current learning rate: 0.00326\n",
      "2024-01-10 12:14:11.605699: train_loss -0.9498\n",
      "2024-01-10 12:14:11.615705: val_loss -0.7985\n",
      "2024-01-10 12:14:11.621705: Pseudo dice [0.9213, 0.9305, 0.938]\n",
      "2024-01-10 12:14:11.627211: Epoch time: 40.27 s\n",
      "2024-01-10 12:14:13.088928: \n",
      "2024-01-10 12:14:13.096012: Epoch 713\n",
      "2024-01-10 12:14:13.099988: Current learning rate: 0.00325\n",
      "2024-01-10 12:14:53.344983: train_loss -0.9501\n",
      "2024-01-10 12:14:53.354983: val_loss -0.7905\n",
      "2024-01-10 12:14:53.361982: Pseudo dice [0.9169, 0.9273, 0.9361]\n",
      "2024-01-10 12:14:53.367982: Epoch time: 40.26 s\n",
      "2024-01-10 12:14:54.854548: \n",
      "2024-01-10 12:14:54.860563: Epoch 714\n",
      "2024-01-10 12:14:54.865502: Current learning rate: 0.00324\n",
      "2024-01-10 12:15:35.104148: train_loss -0.9493\n",
      "2024-01-10 12:15:35.111151: val_loss -0.7944\n",
      "2024-01-10 12:15:35.119151: Pseudo dice [0.9188, 0.9285, 0.9368]\n",
      "2024-01-10 12:15:35.126151: Epoch time: 40.25 s\n",
      "2024-01-10 12:15:36.639291: \n",
      "2024-01-10 12:15:36.645290: Epoch 715\n",
      "2024-01-10 12:15:36.649397: Current learning rate: 0.00323\n",
      "2024-01-10 12:16:16.800081: train_loss -0.9502\n",
      "2024-01-10 12:16:16.806784: val_loss -0.7934\n",
      "2024-01-10 12:16:16.815792: Pseudo dice [0.9188, 0.9292, 0.9364]\n",
      "2024-01-10 12:16:16.822784: Epoch time: 40.16 s\n",
      "2024-01-10 12:16:18.428401: \n",
      "2024-01-10 12:16:18.435412: Epoch 716\n",
      "2024-01-10 12:16:18.440495: Current learning rate: 0.00322\n",
      "2024-01-10 12:16:58.846290: train_loss -0.9506\n",
      "2024-01-10 12:16:58.854292: val_loss -0.7872\n",
      "2024-01-10 12:16:58.883296: Pseudo dice [0.9208, 0.9259, 0.9347]\n",
      "2024-01-10 12:16:58.889296: Epoch time: 40.42 s\n",
      "2024-01-10 12:17:00.320648: \n",
      "2024-01-10 12:17:00.330286: Epoch 717\n",
      "2024-01-10 12:17:00.337261: Current learning rate: 0.00321\n",
      "2024-01-10 12:17:40.599931: train_loss -0.9498\n",
      "2024-01-10 12:17:40.605946: val_loss -0.7951\n",
      "2024-01-10 12:17:40.612948: Pseudo dice [0.9226, 0.9285, 0.9368]\n",
      "2024-01-10 12:17:40.618449: Epoch time: 40.28 s\n",
      "2024-01-10 12:17:42.076155: \n",
      "2024-01-10 12:17:42.081219: Epoch 718\n",
      "2024-01-10 12:17:42.086153: Current learning rate: 0.0032\n",
      "2024-01-10 12:18:22.396283: train_loss -0.95\n",
      "2024-01-10 12:18:22.405286: val_loss -0.7917\n",
      "2024-01-10 12:18:22.413284: Pseudo dice [0.9211, 0.9281, 0.9359]\n",
      "2024-01-10 12:18:22.417290: Epoch time: 40.32 s\n",
      "2024-01-10 12:18:23.881499: \n",
      "2024-01-10 12:18:23.890474: Epoch 719\n",
      "2024-01-10 12:18:23.896482: Current learning rate: 0.00319\n",
      "2024-01-10 12:19:04.020501: train_loss -0.9496\n",
      "2024-01-10 12:19:04.028496: val_loss -0.796\n",
      "2024-01-10 12:19:04.033496: Pseudo dice [0.9219, 0.929, 0.9371]\n",
      "2024-01-10 12:19:04.038496: Epoch time: 40.14 s\n",
      "2024-01-10 12:19:05.573051: \n",
      "2024-01-10 12:19:05.578104: Epoch 720\n",
      "2024-01-10 12:19:05.583109: Current learning rate: 0.00318\n",
      "2024-01-10 12:19:45.724258: train_loss -0.9503\n",
      "2024-01-10 12:19:45.733258: val_loss -0.798\n",
      "2024-01-10 12:19:45.740259: Pseudo dice [0.9192, 0.9288, 0.9366]\n",
      "2024-01-10 12:19:45.746260: Epoch time: 40.15 s\n",
      "2024-01-10 12:19:47.340441: \n",
      "2024-01-10 12:19:47.350367: Epoch 721\n",
      "2024-01-10 12:19:47.358441: Current learning rate: 0.00317\n",
      "2024-01-10 12:20:27.783641: train_loss -0.95\n",
      "2024-01-10 12:20:27.790655: val_loss -0.7991\n",
      "2024-01-10 12:20:27.796657: Pseudo dice [0.9227, 0.9306, 0.9389]\n",
      "2024-01-10 12:20:27.802652: Epoch time: 40.44 s\n",
      "2024-01-10 12:20:29.284762: \n",
      "2024-01-10 12:20:29.295219: Epoch 722\n",
      "2024-01-10 12:20:29.302239: Current learning rate: 0.00316\n",
      "2024-01-10 12:21:10.887043: train_loss -0.9507\n",
      "2024-01-10 12:21:10.898044: val_loss -0.7978\n",
      "2024-01-10 12:21:10.903044: Pseudo dice [0.9193, 0.9288, 0.9373]\n",
      "2024-01-10 12:21:10.908044: Epoch time: 41.6 s\n",
      "2024-01-10 12:21:12.386083: \n",
      "2024-01-10 12:21:12.391081: Epoch 723\n",
      "2024-01-10 12:21:12.395095: Current learning rate: 0.00315\n",
      "2024-01-10 12:21:54.812241: train_loss -0.95\n",
      "2024-01-10 12:21:54.821244: val_loss -0.7983\n",
      "2024-01-10 12:21:54.832243: Pseudo dice [0.9218, 0.9294, 0.937]\n",
      "2024-01-10 12:21:54.839242: Epoch time: 42.43 s\n",
      "2024-01-10 12:21:56.312173: \n",
      "2024-01-10 12:21:56.317315: Epoch 724\n",
      "2024-01-10 12:21:56.321304: Current learning rate: 0.00314\n",
      "2024-01-10 12:22:37.822564: train_loss -0.9507\n",
      "2024-01-10 12:22:37.832568: val_loss -0.7978\n",
      "2024-01-10 12:22:37.840564: Pseudo dice [0.9218, 0.9299, 0.9375]\n",
      "2024-01-10 12:22:37.880474: Epoch time: 41.51 s\n",
      "2024-01-10 12:22:39.339006: \n",
      "2024-01-10 12:22:39.348010: Epoch 725\n",
      "2024-01-10 12:22:39.352077: Current learning rate: 0.00313\n",
      "2024-01-10 12:23:23.665555: train_loss -0.9507\n",
      "2024-01-10 12:23:23.673557: val_loss -0.8013\n",
      "2024-01-10 12:23:23.710565: Pseudo dice [0.9224, 0.93, 0.9382]\n",
      "2024-01-10 12:23:23.718562: Epoch time: 44.33 s\n",
      "2024-01-10 12:23:25.379295: \n",
      "2024-01-10 12:23:25.385335: Epoch 726\n",
      "2024-01-10 12:23:25.390352: Current learning rate: 0.00312\n",
      "2024-01-10 12:24:08.694029: train_loss -0.9507\n",
      "2024-01-10 12:24:08.723036: val_loss -0.8014\n",
      "2024-01-10 12:24:08.729027: Pseudo dice [0.9209, 0.9304, 0.939]\n",
      "2024-01-10 12:24:08.734028: Epoch time: 43.32 s\n",
      "2024-01-10 12:24:10.279074: \n",
      "2024-01-10 12:24:10.290545: Epoch 727\n",
      "2024-01-10 12:24:10.296632: Current learning rate: 0.00311\n",
      "2024-01-10 12:24:51.573164: train_loss -0.95\n",
      "2024-01-10 12:24:51.580163: val_loss -0.7952\n",
      "2024-01-10 12:24:51.586162: Pseudo dice [0.917, 0.9298, 0.9374]\n",
      "2024-01-10 12:24:51.592166: Epoch time: 41.3 s\n",
      "2024-01-10 12:24:53.085000: \n",
      "2024-01-10 12:24:53.091020: Epoch 728\n",
      "2024-01-10 12:24:53.096020: Current learning rate: 0.0031\n",
      "2024-01-10 12:25:36.379388: train_loss -0.9509\n",
      "2024-01-10 12:25:36.430906: val_loss -0.799\n",
      "2024-01-10 12:25:36.442416: Pseudo dice [0.9219, 0.9287, 0.9373]\n",
      "2024-01-10 12:25:36.450414: Epoch time: 43.3 s\n",
      "2024-01-10 12:25:38.202467: \n",
      "2024-01-10 12:25:38.208457: Epoch 729\n",
      "2024-01-10 12:25:38.213462: Current learning rate: 0.00309\n",
      "2024-01-10 12:26:22.439226: train_loss -0.9501\n",
      "2024-01-10 12:26:22.458227: val_loss -0.7918\n",
      "2024-01-10 12:26:22.470383: Pseudo dice [0.923, 0.9279, 0.9357]\n",
      "2024-01-10 12:26:22.483404: Epoch time: 44.24 s\n",
      "2024-01-10 12:26:24.244821: \n",
      "2024-01-10 12:26:24.253821: Epoch 730\n",
      "2024-01-10 12:26:24.263829: Current learning rate: 0.00308\n",
      "2024-01-10 12:27:07.588440: train_loss -0.9506\n",
      "2024-01-10 12:27:07.600441: val_loss -0.7938\n",
      "2024-01-10 12:27:07.609445: Pseudo dice [0.923, 0.9282, 0.9363]\n",
      "2024-01-10 12:27:07.616446: Epoch time: 43.34 s\n",
      "2024-01-10 12:27:09.123926: \n",
      "2024-01-10 12:27:09.129956: Epoch 731\n",
      "2024-01-10 12:27:09.134971: Current learning rate: 0.00307\n",
      "2024-01-10 12:27:53.817063: train_loss -0.95\n",
      "2024-01-10 12:27:53.893684: val_loss -0.7989\n",
      "2024-01-10 12:27:53.906744: Pseudo dice [0.9218, 0.9285, 0.9369]\n",
      "2024-01-10 12:27:53.921436: Epoch time: 44.69 s\n",
      "2024-01-10 12:27:56.798902: \n",
      "2024-01-10 12:27:56.804905: Epoch 732\n",
      "2024-01-10 12:27:56.810907: Current learning rate: 0.00306\n",
      "2024-01-10 12:28:39.984620: train_loss -0.9502\n",
      "2024-01-10 12:28:39.991128: val_loss -0.802\n",
      "2024-01-10 12:28:39.998135: Pseudo dice [0.9219, 0.9305, 0.9373]\n",
      "2024-01-10 12:28:40.022160: Epoch time: 43.19 s\n",
      "2024-01-10 12:28:41.598622: \n",
      "2024-01-10 12:28:41.604937: Epoch 733\n",
      "2024-01-10 12:28:41.609010: Current learning rate: 0.00305\n",
      "2024-01-10 12:29:22.779725: train_loss -0.9514\n",
      "2024-01-10 12:29:22.789728: val_loss -0.7924\n",
      "2024-01-10 12:29:22.796729: Pseudo dice [0.9202, 0.9274, 0.9354]\n",
      "2024-01-10 12:29:22.808740: Epoch time: 41.18 s\n",
      "2024-01-10 12:29:24.317639: \n",
      "2024-01-10 12:29:24.325843: Epoch 734\n",
      "2024-01-10 12:29:24.330843: Current learning rate: 0.00304\n",
      "2024-01-10 12:30:05.631145: train_loss -0.9509\n",
      "2024-01-10 12:30:05.641653: val_loss -0.7984\n",
      "2024-01-10 12:30:05.650654: Pseudo dice [0.9204, 0.9275, 0.9345]\n",
      "2024-01-10 12:30:05.687655: Epoch time: 41.31 s\n",
      "2024-01-10 12:30:06.922284: \n",
      "2024-01-10 12:30:06.932293: Epoch 735\n",
      "2024-01-10 12:30:06.941918: Current learning rate: 0.00303\n",
      "2024-01-10 12:30:48.825176: train_loss -0.9515\n",
      "2024-01-10 12:30:48.834177: val_loss -0.7924\n",
      "2024-01-10 12:30:48.868184: Pseudo dice [0.9187, 0.9282, 0.9362]\n",
      "2024-01-10 12:30:48.874695: Epoch time: 41.9 s\n",
      "2024-01-10 12:30:50.231541: \n",
      "2024-01-10 12:30:50.238796: Epoch 736\n",
      "2024-01-10 12:30:50.247861: Current learning rate: 0.00302\n",
      "2024-01-10 12:31:32.110862: train_loss -0.9505\n",
      "2024-01-10 12:31:32.117862: val_loss -0.8063\n",
      "2024-01-10 12:31:32.124868: Pseudo dice [0.9232, 0.9302, 0.9386]\n",
      "2024-01-10 12:31:32.132377: Epoch time: 41.88 s\n",
      "2024-01-10 12:31:33.702328: \n",
      "2024-01-10 12:31:33.712294: Epoch 737\n",
      "2024-01-10 12:31:33.724297: Current learning rate: 0.00301\n",
      "2024-01-10 12:32:15.687924: train_loss -0.9512\n",
      "2024-01-10 12:32:15.696924: val_loss -0.7979\n",
      "2024-01-10 12:32:15.706932: Pseudo dice [0.918, 0.9292, 0.9368]\n",
      "2024-01-10 12:32:15.743448: Epoch time: 41.99 s\n",
      "2024-01-10 12:32:17.223169: \n",
      "2024-01-10 12:32:17.228098: Epoch 738\n",
      "2024-01-10 12:32:17.233009: Current learning rate: 0.003\n",
      "2024-01-10 12:32:59.307765: train_loss -0.9514\n",
      "2024-01-10 12:32:59.321931: val_loss -0.801\n",
      "2024-01-10 12:32:59.376117: Pseudo dice [0.9202, 0.9292, 0.937]\n",
      "2024-01-10 12:32:59.389116: Epoch time: 42.09 s\n",
      "2024-01-10 12:33:01.130026: \n",
      "2024-01-10 12:33:01.136007: Epoch 739\n",
      "2024-01-10 12:33:01.142015: Current learning rate: 0.00299\n",
      "2024-01-10 12:33:43.188472: train_loss -0.9508\n",
      "2024-01-10 12:33:43.231472: val_loss -0.7935\n",
      "2024-01-10 12:33:43.244472: Pseudo dice [0.9209, 0.9286, 0.9363]\n",
      "2024-01-10 12:33:43.254473: Epoch time: 42.06 s\n",
      "2024-01-10 12:33:44.825856: \n",
      "2024-01-10 12:33:44.831928: Epoch 740\n",
      "2024-01-10 12:33:44.836937: Current learning rate: 0.00297\n",
      "2024-01-10 12:34:29.269436: train_loss -0.9498\n",
      "2024-01-10 12:34:29.288445: val_loss -0.7944\n",
      "2024-01-10 12:34:29.306965: Pseudo dice [0.9221, 0.9291, 0.9374]\n",
      "2024-01-10 12:34:29.329157: Epoch time: 44.44 s\n",
      "2024-01-10 12:34:31.488091: \n",
      "2024-01-10 12:34:31.495091: Epoch 741\n",
      "2024-01-10 12:34:31.502307: Current learning rate: 0.00296\n",
      "2024-01-10 12:35:16.818738: train_loss -0.95\n",
      "2024-01-10 12:35:16.826745: val_loss -0.7958\n",
      "2024-01-10 12:35:16.836749: Pseudo dice [0.9229, 0.9276, 0.9357]\n",
      "2024-01-10 12:35:16.876052: Epoch time: 45.33 s\n",
      "2024-01-10 12:35:18.370980: \n",
      "2024-01-10 12:35:18.376667: Epoch 742\n",
      "2024-01-10 12:35:18.380723: Current learning rate: 0.00295\n",
      "2024-01-10 12:35:59.804390: train_loss -0.9509\n",
      "2024-01-10 12:35:59.836905: val_loss -0.7949\n",
      "2024-01-10 12:35:59.845905: Pseudo dice [0.9211, 0.9287, 0.937]\n",
      "2024-01-10 12:35:59.853906: Epoch time: 41.43 s\n",
      "2024-01-10 12:36:01.068978: \n",
      "2024-01-10 12:36:01.080224: Epoch 743\n",
      "2024-01-10 12:36:01.086248: Current learning rate: 0.00294\n",
      "2024-01-10 12:36:43.117218: train_loss -0.9502\n",
      "2024-01-10 12:36:43.127225: val_loss -0.7962\n",
      "2024-01-10 12:36:43.135222: Pseudo dice [0.919, 0.9287, 0.9366]\n",
      "2024-01-10 12:36:43.143221: Epoch time: 42.05 s\n",
      "2024-01-10 12:36:44.481450: \n",
      "2024-01-10 12:36:44.486453: Epoch 744\n",
      "2024-01-10 12:36:44.493452: Current learning rate: 0.00293\n",
      "2024-01-10 12:37:25.625589: train_loss -0.9516\n",
      "2024-01-10 12:37:25.634597: val_loss -0.7971\n",
      "2024-01-10 12:37:25.643588: Pseudo dice [0.9203, 0.9292, 0.9377]\n",
      "2024-01-10 12:37:25.655669: Epoch time: 41.15 s\n",
      "2024-01-10 12:37:27.166221: \n",
      "2024-01-10 12:37:27.174170: Epoch 745\n",
      "2024-01-10 12:37:27.178665: Current learning rate: 0.00292\n",
      "2024-01-10 12:38:08.073837: train_loss -0.9513\n",
      "2024-01-10 12:38:08.083836: val_loss -0.7968\n",
      "2024-01-10 12:38:08.108839: Pseudo dice [0.9214, 0.93, 0.938]\n",
      "2024-01-10 12:38:08.119482: Epoch time: 40.91 s\n",
      "2024-01-10 12:38:09.362106: \n",
      "2024-01-10 12:38:09.367082: Epoch 746\n",
      "2024-01-10 12:38:09.371118: Current learning rate: 0.00291\n",
      "2024-01-10 12:38:50.585534: train_loss -0.9508\n",
      "2024-01-10 12:38:50.598876: val_loss -0.7939\n",
      "2024-01-10 12:38:50.642112: Pseudo dice [0.9201, 0.9286, 0.9367]\n",
      "2024-01-10 12:38:50.653629: Epoch time: 41.22 s\n",
      "2024-01-10 12:38:52.114564: \n",
      "2024-01-10 12:38:52.120554: Epoch 747\n",
      "2024-01-10 12:38:52.124559: Current learning rate: 0.0029\n",
      "2024-01-10 12:39:33.986897: train_loss -0.9506\n",
      "2024-01-10 12:39:33.996897: val_loss -0.7958\n",
      "2024-01-10 12:39:34.006505: Pseudo dice [0.9207, 0.9294, 0.9378]\n",
      "2024-01-10 12:39:34.016407: Epoch time: 41.87 s\n",
      "2024-01-10 12:39:35.648137: \n",
      "2024-01-10 12:39:35.653196: Epoch 748\n",
      "2024-01-10 12:39:35.658164: Current learning rate: 0.00289\n",
      "2024-01-10 12:40:18.016756: train_loss -0.9506\n",
      "2024-01-10 12:40:18.026103: val_loss -0.7976\n",
      "2024-01-10 12:40:18.036668: Pseudo dice [0.9203, 0.9288, 0.9374]\n",
      "2024-01-10 12:40:18.066647: Epoch time: 42.37 s\n",
      "2024-01-10 12:40:19.720430: \n",
      "2024-01-10 12:40:19.725430: Epoch 749\n",
      "2024-01-10 12:40:19.730435: Current learning rate: 0.00288\n",
      "2024-01-10 12:41:01.959133: train_loss -0.9515\n",
      "2024-01-10 12:41:01.966138: val_loss -0.7968\n",
      "2024-01-10 12:41:01.972131: Pseudo dice [0.9192, 0.9293, 0.9382]\n",
      "2024-01-10 12:41:01.978137: Epoch time: 42.24 s\n",
      "2024-01-10 12:41:03.869409: \n",
      "2024-01-10 12:41:03.875475: Epoch 750\n",
      "2024-01-10 12:41:03.880377: Current learning rate: 0.00287\n",
      "2024-01-10 12:41:45.771792: train_loss -0.9517\n",
      "2024-01-10 12:41:45.781790: val_loss -0.7998\n",
      "2024-01-10 12:41:45.790791: Pseudo dice [0.9201, 0.9295, 0.9374]\n",
      "2024-01-10 12:41:45.801795: Epoch time: 41.9 s\n",
      "2024-01-10 12:41:47.478774: \n",
      "2024-01-10 12:41:47.483711: Epoch 751\n",
      "2024-01-10 12:41:47.487718: Current learning rate: 0.00286\n",
      "2024-01-10 12:42:29.273708: train_loss -0.9514\n",
      "2024-01-10 12:42:29.285232: val_loss -0.8013\n",
      "2024-01-10 12:42:29.293756: Pseudo dice [0.9223, 0.9285, 0.9372]\n",
      "2024-01-10 12:42:29.302281: Epoch time: 41.8 s\n",
      "2024-01-10 12:42:30.985770: \n",
      "2024-01-10 12:42:30.991350: Epoch 752\n",
      "2024-01-10 12:42:30.999362: Current learning rate: 0.00285\n",
      "2024-01-10 12:43:13.164549: train_loss -0.9505\n",
      "2024-01-10 12:43:13.174057: val_loss -0.794\n",
      "2024-01-10 12:43:13.184061: Pseudo dice [0.9235, 0.9285, 0.9367]\n",
      "2024-01-10 12:43:13.192059: Epoch time: 42.18 s\n",
      "2024-01-10 12:43:14.662558: \n",
      "2024-01-10 12:43:14.668559: Epoch 753\n",
      "2024-01-10 12:43:14.678537: Current learning rate: 0.00284\n",
      "2024-01-10 12:43:57.001423: train_loss -0.9504\n",
      "2024-01-10 12:43:57.011433: val_loss -0.7966\n",
      "2024-01-10 12:43:57.021107: Pseudo dice [0.922, 0.9288, 0.9367]\n",
      "2024-01-10 12:43:57.065880: Epoch time: 42.34 s\n",
      "2024-01-10 12:43:58.734464: \n",
      "2024-01-10 12:43:58.739465: Epoch 754\n",
      "2024-01-10 12:43:58.744464: Current learning rate: 0.00283\n",
      "2024-01-10 12:44:40.877032: train_loss -0.9505\n",
      "2024-01-10 12:44:40.914041: val_loss -0.8011\n",
      "2024-01-10 12:44:40.925552: Pseudo dice [0.9208, 0.9298, 0.9385]\n",
      "2024-01-10 12:44:40.934553: Epoch time: 42.14 s\n",
      "2024-01-10 12:44:42.266962: \n",
      "2024-01-10 12:44:42.275220: Epoch 755\n",
      "2024-01-10 12:44:42.279734: Current learning rate: 0.00282\n",
      "2024-01-10 12:45:22.843675: train_loss -0.9513\n",
      "2024-01-10 12:45:22.854187: val_loss -0.797\n",
      "2024-01-10 12:45:22.886190: Pseudo dice [0.9202, 0.9299, 0.9383]\n",
      "2024-01-10 12:45:22.898185: Epoch time: 40.58 s\n",
      "2024-01-10 12:45:24.153301: \n",
      "2024-01-10 12:45:24.158302: Epoch 756\n",
      "2024-01-10 12:45:24.162303: Current learning rate: 0.00281\n",
      "2024-01-10 12:46:04.855794: train_loss -0.9516\n",
      "2024-01-10 12:46:04.871336: val_loss -0.7977\n",
      "2024-01-10 12:46:04.914338: Pseudo dice [0.9241, 0.9293, 0.937]\n",
      "2024-01-10 12:46:04.924346: Epoch time: 40.7 s\n",
      "2024-01-10 12:46:06.354710: \n",
      "2024-01-10 12:46:06.362761: Epoch 757\n",
      "2024-01-10 12:46:06.368331: Current learning rate: 0.0028\n",
      "2024-01-10 12:46:47.075028: train_loss -0.9515\n",
      "2024-01-10 12:46:47.085028: val_loss -0.7937\n",
      "2024-01-10 12:46:47.092032: Pseudo dice [0.921, 0.9284, 0.936]\n",
      "2024-01-10 12:46:47.138600: Epoch time: 40.72 s\n",
      "2024-01-10 12:46:48.391598: \n",
      "2024-01-10 12:46:48.401079: Epoch 758\n",
      "2024-01-10 12:46:48.405707: Current learning rate: 0.00279\n",
      "2024-01-10 12:47:30.900385: train_loss -0.9519\n",
      "2024-01-10 12:47:30.911391: val_loss -0.8023\n",
      "2024-01-10 12:47:30.922897: Pseudo dice [0.9226, 0.9305, 0.9386]\n",
      "2024-01-10 12:47:30.932905: Epoch time: 42.51 s\n",
      "2024-01-10 12:47:32.244145: \n",
      "2024-01-10 12:47:32.249145: Epoch 759\n",
      "2024-01-10 12:47:32.254145: Current learning rate: 0.00278\n",
      "2024-01-10 12:48:13.405828: train_loss -0.9521\n",
      "2024-01-10 12:48:13.431835: val_loss -0.8013\n",
      "2024-01-10 12:48:13.439340: Pseudo dice [0.9208, 0.9302, 0.9382]\n",
      "2024-01-10 12:48:13.446341: Epoch time: 41.16 s\n",
      "2024-01-10 12:48:14.646025: \n",
      "2024-01-10 12:48:14.651659: Epoch 760\n",
      "2024-01-10 12:48:14.656668: Current learning rate: 0.00277\n",
      "2024-01-10 12:48:55.945086: train_loss -0.9518\n",
      "2024-01-10 12:48:55.957085: val_loss -0.7914\n",
      "2024-01-10 12:48:55.966604: Pseudo dice [0.923, 0.9274, 0.9367]\n",
      "2024-01-10 12:48:55.980061: Epoch time: 41.3 s\n",
      "2024-01-10 12:48:57.477980: \n",
      "2024-01-10 12:48:57.483700: Epoch 761\n",
      "2024-01-10 12:48:57.488699: Current learning rate: 0.00276\n",
      "2024-01-10 12:49:39.214787: train_loss -0.9514\n",
      "2024-01-10 12:49:39.224790: val_loss -0.7964\n",
      "2024-01-10 12:49:39.233788: Pseudo dice [0.9181, 0.9297, 0.9379]\n",
      "2024-01-10 12:49:39.242788: Epoch time: 41.74 s\n",
      "2024-01-10 12:49:40.579610: \n",
      "2024-01-10 12:49:40.588534: Epoch 762\n",
      "2024-01-10 12:49:40.592526: Current learning rate: 0.00275\n",
      "2024-01-10 12:50:22.043234: train_loss -0.9517\n",
      "2024-01-10 12:50:22.053235: val_loss -0.7935\n",
      "2024-01-10 12:50:22.060236: Pseudo dice [0.9213, 0.9285, 0.9371]\n",
      "2024-01-10 12:50:22.066235: Epoch time: 41.46 s\n",
      "2024-01-10 12:50:23.367063: \n",
      "2024-01-10 12:50:23.372761: Epoch 763\n",
      "2024-01-10 12:50:23.377393: Current learning rate: 0.00274\n",
      "2024-01-10 12:51:04.645535: train_loss -0.9513\n",
      "2024-01-10 12:51:04.653532: val_loss -0.7951\n",
      "2024-01-10 12:51:04.690532: Pseudo dice [0.9227, 0.928, 0.9368]\n",
      "2024-01-10 12:51:04.700669: Epoch time: 41.28 s\n",
      "2024-01-10 12:51:05.946266: \n",
      "2024-01-10 12:51:05.951527: Epoch 764\n",
      "2024-01-10 12:51:05.956518: Current learning rate: 0.00273\n",
      "2024-01-10 12:51:47.671899: train_loss -0.9511\n",
      "2024-01-10 12:51:47.681898: val_loss -0.8\n",
      "2024-01-10 12:51:47.691386: Pseudo dice [0.9236, 0.93, 0.9381]\n",
      "2024-01-10 12:51:47.699383: Epoch time: 41.73 s\n",
      "2024-01-10 12:51:48.987741: \n",
      "2024-01-10 12:51:48.993741: Epoch 765\n",
      "2024-01-10 12:51:48.998752: Current learning rate: 0.00272\n",
      "2024-01-10 12:52:30.502133: train_loss -0.9519\n",
      "2024-01-10 12:52:30.513130: val_loss -0.801\n",
      "2024-01-10 12:52:30.524131: Pseudo dice [0.9221, 0.9296, 0.9383]\n",
      "2024-01-10 12:52:30.569655: Epoch time: 41.52 s\n",
      "2024-01-10 12:52:31.837493: \n",
      "2024-01-10 12:52:31.842489: Epoch 766\n",
      "2024-01-10 12:52:31.847492: Current learning rate: 0.00271\n",
      "2024-01-10 12:53:13.693930: train_loss -0.9516\n",
      "2024-01-10 12:53:13.710927: val_loss -0.7945\n",
      "2024-01-10 12:53:13.721926: Pseudo dice [0.9196, 0.9289, 0.9367]\n",
      "2024-01-10 12:53:13.734927: Epoch time: 41.86 s\n",
      "2024-01-10 12:53:15.199857: \n",
      "2024-01-10 12:53:15.205851: Epoch 767\n",
      "2024-01-10 12:53:15.209848: Current learning rate: 0.0027\n",
      "2024-01-10 12:53:57.814427: train_loss -0.9521\n",
      "2024-01-10 12:53:57.825953: val_loss -0.7942\n",
      "2024-01-10 12:53:57.858822: Pseudo dice [0.9222, 0.9287, 0.9368]\n",
      "2024-01-10 12:53:57.869823: Epoch time: 42.62 s\n",
      "2024-01-10 12:53:59.225990: \n",
      "2024-01-10 12:53:59.230999: Epoch 768\n",
      "2024-01-10 12:53:59.235949: Current learning rate: 0.00268\n",
      "2024-01-10 12:54:41.127355: train_loss -0.9518\n",
      "2024-01-10 12:54:41.170863: val_loss -0.7945\n",
      "2024-01-10 12:54:41.182863: Pseudo dice [0.9198, 0.9285, 0.9368]\n",
      "2024-01-10 12:54:41.192863: Epoch time: 41.9 s\n",
      "2024-01-10 12:54:42.697049: \n",
      "2024-01-10 12:54:42.702602: Epoch 769\n",
      "2024-01-10 12:54:42.707625: Current learning rate: 0.00267\n",
      "2024-01-10 12:55:24.123180: train_loss -0.951\n",
      "2024-01-10 12:55:24.168711: val_loss -0.801\n",
      "2024-01-10 12:55:24.180223: Pseudo dice [0.9214, 0.9299, 0.9383]\n",
      "2024-01-10 12:55:24.213730: Epoch time: 41.43 s\n",
      "2024-01-10 12:55:25.535025: \n",
      "2024-01-10 12:55:25.541013: Epoch 770\n",
      "2024-01-10 12:55:25.550952: Current learning rate: 0.00266\n",
      "2024-01-10 12:56:08.817537: train_loss -0.952\n",
      "2024-01-10 12:56:08.832537: val_loss -0.7964\n",
      "2024-01-10 12:56:08.847270: Pseudo dice [0.9213, 0.9287, 0.9373]\n",
      "2024-01-10 12:56:08.863954: Epoch time: 43.28 s\n",
      "2024-01-10 12:56:10.238050: \n",
      "2024-01-10 12:56:10.243974: Epoch 771\n",
      "2024-01-10 12:56:10.248973: Current learning rate: 0.00265\n",
      "2024-01-10 12:56:51.389947: train_loss -0.9522\n",
      "2024-01-10 12:56:51.403715: val_loss -0.7976\n",
      "2024-01-10 12:56:51.445710: Pseudo dice [0.9206, 0.9295, 0.9377]\n",
      "2024-01-10 12:56:51.455741: Epoch time: 41.15 s\n",
      "2024-01-10 12:56:52.780677: \n",
      "2024-01-10 12:56:52.785980: Epoch 772\n",
      "2024-01-10 12:56:52.791001: Current learning rate: 0.00264\n",
      "2024-01-10 12:57:34.129274: train_loss -0.9516\n",
      "2024-01-10 12:57:34.139790: val_loss -0.8031\n",
      "2024-01-10 12:57:34.173789: Pseudo dice [0.9238, 0.9294, 0.937]\n",
      "2024-01-10 12:57:34.182788: Epoch time: 41.35 s\n",
      "2024-01-10 12:57:35.429390: \n",
      "2024-01-10 12:57:35.438132: Epoch 773\n",
      "2024-01-10 12:57:35.443668: Current learning rate: 0.00263\n",
      "2024-01-10 12:58:17.595838: train_loss -0.9519\n",
      "2024-01-10 12:58:17.607836: val_loss -0.8016\n",
      "2024-01-10 12:58:17.618842: Pseudo dice [0.9216, 0.9304, 0.9379]\n",
      "2024-01-10 12:58:17.651889: Epoch time: 42.17 s\n",
      "2024-01-10 12:58:19.245689: \n",
      "2024-01-10 12:58:19.251689: Epoch 774\n",
      "2024-01-10 12:58:19.256680: Current learning rate: 0.00262\n",
      "2024-01-10 12:59:00.949061: train_loss -0.9512\n",
      "2024-01-10 12:59:00.976068: val_loss -0.7956\n",
      "2024-01-10 12:59:00.984579: Pseudo dice [0.9211, 0.9279, 0.9369]\n",
      "2024-01-10 12:59:00.993582: Epoch time: 41.7 s\n",
      "2024-01-10 12:59:02.276471: \n",
      "2024-01-10 12:59:02.284497: Epoch 775\n",
      "2024-01-10 12:59:02.289412: Current learning rate: 0.00261\n",
      "2024-01-10 12:59:43.754787: train_loss -0.9523\n",
      "2024-01-10 12:59:43.763788: val_loss -0.7908\n",
      "2024-01-10 12:59:43.773787: Pseudo dice [0.9182, 0.9274, 0.936]\n",
      "2024-01-10 12:59:43.784802: Epoch time: 41.48 s\n",
      "2024-01-10 12:59:45.152286: \n",
      "2024-01-10 12:59:45.158234: Epoch 776\n",
      "2024-01-10 12:59:45.162313: Current learning rate: 0.0026\n",
      "2024-01-10 13:00:26.791265: train_loss -0.9519\n",
      "2024-01-10 13:00:26.800264: val_loss -0.7987\n",
      "2024-01-10 13:00:26.809272: Pseudo dice [0.9209, 0.929, 0.9379]\n",
      "2024-01-10 13:00:26.818790: Epoch time: 41.64 s\n",
      "2024-01-10 13:00:28.112781: \n",
      "2024-01-10 13:00:28.117778: Epoch 777\n",
      "2024-01-10 13:00:28.122332: Current learning rate: 0.00259\n",
      "2024-01-10 13:01:09.046661: train_loss -0.9517\n",
      "2024-01-10 13:01:09.057652: val_loss -0.7938\n",
      "2024-01-10 13:01:09.065665: Pseudo dice [0.92, 0.9276, 0.9366]\n",
      "2024-01-10 13:01:09.094663: Epoch time: 40.93 s\n",
      "2024-01-10 13:01:10.497650: \n",
      "2024-01-10 13:01:10.503655: Epoch 778\n",
      "2024-01-10 13:01:10.509650: Current learning rate: 0.00258\n",
      "2024-01-10 13:01:54.436520: train_loss -0.9523\n",
      "2024-01-10 13:01:54.449521: val_loss -0.7967\n",
      "2024-01-10 13:01:54.461525: Pseudo dice [0.9209, 0.9301, 0.9381]\n",
      "2024-01-10 13:01:54.472073: Epoch time: 43.94 s\n",
      "2024-01-10 13:01:56.012283: \n",
      "2024-01-10 13:01:56.018283: Epoch 779\n",
      "2024-01-10 13:01:56.023301: Current learning rate: 0.00257\n",
      "2024-01-10 13:02:39.603282: train_loss -0.9525\n",
      "2024-01-10 13:02:39.616948: val_loss -0.8032\n",
      "2024-01-10 13:02:39.629659: Pseudo dice [0.9263, 0.9301, 0.938]\n",
      "2024-01-10 13:02:39.639794: Epoch time: 43.59 s\n",
      "2024-01-10 13:02:41.368614: \n",
      "2024-01-10 13:02:41.374066: Epoch 780\n",
      "2024-01-10 13:02:41.379079: Current learning rate: 0.00256\n",
      "2024-01-10 13:03:23.948795: train_loss -0.9525\n",
      "2024-01-10 13:03:23.984776: val_loss -0.7948\n",
      "2024-01-10 13:03:23.996773: Pseudo dice [0.9216, 0.9285, 0.9371]\n",
      "2024-01-10 13:03:24.007199: Epoch time: 42.58 s\n",
      "2024-01-10 13:03:25.736591: \n",
      "2024-01-10 13:03:25.742604: Epoch 781\n",
      "2024-01-10 13:03:25.750605: Current learning rate: 0.00255\n",
      "2024-01-10 13:04:08.173144: train_loss -0.9523\n",
      "2024-01-10 13:04:08.186175: val_loss -0.7932\n",
      "2024-01-10 13:04:08.223827: Pseudo dice [0.9211, 0.9277, 0.9363]\n",
      "2024-01-10 13:04:08.231371: Epoch time: 42.44 s\n",
      "2024-01-10 13:04:09.859666: \n",
      "2024-01-10 13:04:09.870388: Epoch 782\n",
      "2024-01-10 13:04:09.875447: Current learning rate: 0.00254\n",
      "2024-01-10 13:04:52.837156: train_loss -0.9527\n",
      "2024-01-10 13:04:52.857741: val_loss -0.8004\n",
      "2024-01-10 13:04:52.865760: Pseudo dice [0.921, 0.9292, 0.9374]\n",
      "2024-01-10 13:04:52.873287: Epoch time: 42.98 s\n",
      "2024-01-10 13:04:54.523147: \n",
      "2024-01-10 13:04:54.532773: Epoch 783\n",
      "2024-01-10 13:04:54.543322: Current learning rate: 0.00253\n",
      "2024-01-10 13:05:37.803352: train_loss -0.952\n",
      "2024-01-10 13:05:37.813614: val_loss -0.7959\n",
      "2024-01-10 13:05:37.821366: Pseudo dice [0.9183, 0.9296, 0.9385]\n",
      "2024-01-10 13:05:37.830803: Epoch time: 43.28 s\n",
      "2024-01-10 13:05:39.677880: \n",
      "2024-01-10 13:05:39.687539: Epoch 784\n",
      "2024-01-10 13:05:39.697252: Current learning rate: 0.00252\n",
      "2024-01-10 13:06:22.939991: train_loss -0.9523\n",
      "2024-01-10 13:06:22.949505: val_loss -0.7933\n",
      "2024-01-10 13:06:22.958015: Pseudo dice [0.9202, 0.9284, 0.9361]\n",
      "2024-01-10 13:06:22.967536: Epoch time: 43.26 s\n",
      "2024-01-10 13:06:24.282290: \n",
      "2024-01-10 13:06:24.289945: Epoch 785\n",
      "2024-01-10 13:06:24.294035: Current learning rate: 0.00251\n",
      "2024-01-10 13:07:06.739920: train_loss -0.9524\n",
      "2024-01-10 13:07:06.747925: val_loss -0.7947\n",
      "2024-01-10 13:07:06.757435: Pseudo dice [0.922, 0.9282, 0.9362]\n",
      "2024-01-10 13:07:06.765435: Epoch time: 42.46 s\n",
      "2024-01-10 13:07:08.370212: \n",
      "2024-01-10 13:07:08.380043: Epoch 786\n",
      "2024-01-10 13:07:08.385096: Current learning rate: 0.0025\n",
      "2024-01-10 13:07:50.073524: train_loss -0.9523\n",
      "2024-01-10 13:07:50.115037: val_loss -0.7938\n",
      "2024-01-10 13:07:50.124097: Pseudo dice [0.9201, 0.9286, 0.9376]\n",
      "2024-01-10 13:07:50.133097: Epoch time: 41.7 s\n",
      "2024-01-10 13:07:51.651571: \n",
      "2024-01-10 13:07:51.657093: Epoch 787\n",
      "2024-01-10 13:07:51.665228: Current learning rate: 0.00249\n",
      "2024-01-10 13:08:35.547823: train_loss -0.9523\n",
      "2024-01-10 13:08:35.556434: val_loss -0.7967\n",
      "2024-01-10 13:08:35.568435: Pseudo dice [0.924, 0.9287, 0.9373]\n",
      "2024-01-10 13:08:35.578926: Epoch time: 43.9 s\n",
      "2024-01-10 13:08:37.535538: \n",
      "2024-01-10 13:08:37.542101: Epoch 788\n",
      "2024-01-10 13:08:37.547505: Current learning rate: 0.00248\n",
      "2024-01-10 13:09:21.282682: train_loss -0.9516\n",
      "2024-01-10 13:09:21.294683: val_loss -0.7962\n",
      "2024-01-10 13:09:21.336444: Pseudo dice [0.9231, 0.9291, 0.9374]\n",
      "2024-01-10 13:09:21.345364: Epoch time: 43.75 s\n",
      "2024-01-10 13:09:22.965799: \n",
      "2024-01-10 13:09:22.973805: Epoch 789\n",
      "2024-01-10 13:09:22.982986: Current learning rate: 0.00247\n",
      "2024-01-10 13:10:05.834789: train_loss -0.9519\n",
      "2024-01-10 13:10:05.845789: val_loss -0.7925\n",
      "2024-01-10 13:10:05.854791: Pseudo dice [0.9205, 0.9283, 0.937]\n",
      "2024-01-10 13:10:05.863790: Epoch time: 42.87 s\n",
      "2024-01-10 13:10:07.470865: \n",
      "2024-01-10 13:10:07.475939: Epoch 790\n",
      "2024-01-10 13:10:07.480886: Current learning rate: 0.00245\n",
      "2024-01-10 13:10:50.897427: train_loss -0.9519\n",
      "2024-01-10 13:10:50.945408: val_loss -0.79\n",
      "2024-01-10 13:10:50.956410: Pseudo dice [0.9229, 0.9276, 0.936]\n",
      "2024-01-10 13:10:50.966408: Epoch time: 43.43 s\n",
      "2024-01-10 13:10:52.607842: \n",
      "2024-01-10 13:10:52.615783: Epoch 791\n",
      "2024-01-10 13:10:52.620877: Current learning rate: 0.00244\n",
      "2024-01-10 13:11:36.370431: train_loss -0.952\n",
      "2024-01-10 13:11:36.384432: val_loss -0.7901\n",
      "2024-01-10 13:11:36.397431: Pseudo dice [0.9202, 0.9285, 0.9368]\n",
      "2024-01-10 13:11:36.438956: Epoch time: 43.76 s\n",
      "2024-01-10 13:11:37.919320: \n",
      "2024-01-10 13:11:37.924322: Epoch 792\n",
      "2024-01-10 13:11:37.930322: Current learning rate: 0.00243\n",
      "2024-01-10 13:12:19.523877: train_loss -0.9527\n",
      "2024-01-10 13:12:19.536878: val_loss -0.7994\n",
      "2024-01-10 13:12:19.574405: Pseudo dice [0.922, 0.9302, 0.9376]\n",
      "2024-01-10 13:12:19.588403: Epoch time: 41.61 s\n",
      "2024-01-10 13:12:21.099657: \n",
      "2024-01-10 13:12:21.106655: Epoch 793\n",
      "2024-01-10 13:12:21.113655: Current learning rate: 0.00242\n",
      "2024-01-10 13:13:02.398485: train_loss -0.9522\n",
      "2024-01-10 13:13:02.412487: val_loss -0.7986\n",
      "2024-01-10 13:13:02.420485: Pseudo dice [0.9207, 0.9304, 0.9391]\n",
      "2024-01-10 13:13:02.427487: Epoch time: 41.3 s\n",
      "2024-01-10 13:13:03.651219: \n",
      "2024-01-10 13:13:03.656210: Epoch 794\n",
      "2024-01-10 13:13:03.660227: Current learning rate: 0.00241\n",
      "2024-01-10 13:13:43.880805: train_loss -0.9522\n",
      "2024-01-10 13:13:43.889804: val_loss -0.7978\n",
      "2024-01-10 13:13:43.898319: Pseudo dice [0.9249, 0.9287, 0.9366]\n",
      "2024-01-10 13:13:43.907316: Epoch time: 40.23 s\n",
      "2024-01-10 13:13:45.110808: \n",
      "2024-01-10 13:13:45.115807: Epoch 795\n",
      "2024-01-10 13:13:45.119871: Current learning rate: 0.0024\n",
      "2024-01-10 13:14:27.794729: train_loss -0.9526\n",
      "2024-01-10 13:14:27.806854: val_loss -0.7974\n",
      "2024-01-10 13:14:27.818020: Pseudo dice [0.9243, 0.9294, 0.9373]\n",
      "2024-01-10 13:14:27.827532: Epoch time: 42.69 s\n",
      "2024-01-10 13:14:29.378063: \n",
      "2024-01-10 13:14:29.384581: Epoch 796\n",
      "2024-01-10 13:14:29.389584: Current learning rate: 0.00239\n",
      "2024-01-10 13:15:11.480123: train_loss -0.9516\n",
      "2024-01-10 13:15:11.517256: val_loss -0.7928\n",
      "2024-01-10 13:15:11.528126: Pseudo dice [0.9238, 0.9274, 0.9361]\n",
      "2024-01-10 13:15:11.537641: Epoch time: 42.1 s\n",
      "2024-01-10 13:15:13.783839: \n",
      "2024-01-10 13:15:13.791589: Epoch 797\n",
      "2024-01-10 13:15:13.796623: Current learning rate: 0.00238\n",
      "2024-01-10 13:15:55.490075: train_loss -0.9524\n",
      "2024-01-10 13:15:55.501076: val_loss -0.7954\n",
      "2024-01-10 13:15:55.510079: Pseudo dice [0.9235, 0.9286, 0.9372]\n",
      "2024-01-10 13:15:55.518076: Epoch time: 41.71 s\n",
      "2024-01-10 13:15:56.997558: \n",
      "2024-01-10 13:15:57.002889: Epoch 798\n",
      "2024-01-10 13:15:57.007405: Current learning rate: 0.00237\n",
      "2024-01-10 13:16:37.854311: train_loss -0.9529\n",
      "2024-01-10 13:16:37.865309: val_loss -0.7976\n",
      "2024-01-10 13:16:37.875306: Pseudo dice [0.9215, 0.9293, 0.9381]\n",
      "2024-01-10 13:16:37.887978: Epoch time: 40.86 s\n",
      "2024-01-10 13:16:39.232154: \n",
      "2024-01-10 13:16:39.247474: Epoch 799\n",
      "2024-01-10 13:16:39.252531: Current learning rate: 0.00236\n",
      "2024-01-10 13:17:20.794849: train_loss -0.9524\n",
      "2024-01-10 13:17:20.807006: val_loss -0.8009\n",
      "2024-01-10 13:17:20.850755: Pseudo dice [0.9217, 0.9293, 0.9368]\n",
      "2024-01-10 13:17:20.863755: Epoch time: 41.56 s\n",
      "2024-01-10 13:17:22.543607: \n",
      "2024-01-10 13:17:22.548689: Epoch 800\n",
      "2024-01-10 13:17:22.553722: Current learning rate: 0.00235\n",
      "2024-01-10 13:18:03.284800: train_loss -0.9525\n",
      "2024-01-10 13:18:03.297798: val_loss -0.7956\n",
      "2024-01-10 13:18:03.306799: Pseudo dice [0.9198, 0.93, 0.9376]\n",
      "2024-01-10 13:18:03.345807: Epoch time: 40.74 s\n",
      "2024-01-10 13:18:04.584048: \n",
      "2024-01-10 13:18:04.589031: Epoch 801\n",
      "2024-01-10 13:18:04.593040: Current learning rate: 0.00234\n",
      "2024-01-10 13:18:50.923024: train_loss -0.9519\n",
      "2024-01-10 13:18:50.931019: val_loss -0.8007\n",
      "2024-01-10 13:18:50.939534: Pseudo dice [0.9222, 0.9304, 0.938]\n",
      "2024-01-10 13:18:50.948766: Epoch time: 46.34 s\n",
      "2024-01-10 13:18:52.793787: \n",
      "2024-01-10 13:18:52.798796: Epoch 802\n",
      "2024-01-10 13:18:52.803788: Current learning rate: 0.00233\n",
      "2024-01-10 13:19:35.700768: train_loss -0.9532\n",
      "2024-01-10 13:19:35.713764: val_loss -0.7902\n",
      "2024-01-10 13:19:35.726766: Pseudo dice [0.9221, 0.9279, 0.9364]\n",
      "2024-01-10 13:19:35.779295: Epoch time: 42.91 s\n",
      "2024-01-10 13:19:37.637820: \n",
      "2024-01-10 13:19:37.642832: Epoch 803\n",
      "2024-01-10 13:19:37.646904: Current learning rate: 0.00232\n",
      "2024-01-10 13:20:19.740539: train_loss -0.9522\n",
      "2024-01-10 13:20:19.769547: val_loss -0.7998\n",
      "2024-01-10 13:20:19.778546: Pseudo dice [0.9247, 0.9292, 0.9374]\n",
      "2024-01-10 13:20:19.787063: Epoch time: 42.1 s\n",
      "2024-01-10 13:20:21.418834: \n",
      "2024-01-10 13:20:21.431839: Epoch 804\n",
      "2024-01-10 13:20:21.444831: Current learning rate: 0.00231\n",
      "2024-01-10 13:21:04.641442: train_loss -0.953\n",
      "2024-01-10 13:21:04.652750: val_loss -0.8014\n",
      "2024-01-10 13:21:04.662872: Pseudo dice [0.9225, 0.9293, 0.9376]\n",
      "2024-01-10 13:21:04.672869: Epoch time: 43.22 s\n",
      "2024-01-10 13:21:06.856496: \n",
      "2024-01-10 13:21:06.869391: Epoch 805\n",
      "2024-01-10 13:21:06.881787: Current learning rate: 0.0023\n",
      "2024-01-10 13:21:49.464247: train_loss -0.9526\n",
      "2024-01-10 13:21:49.477262: val_loss -0.7887\n",
      "2024-01-10 13:21:49.488297: Pseudo dice [0.9239, 0.928, 0.9355]\n",
      "2024-01-10 13:21:49.498300: Epoch time: 42.61 s\n",
      "2024-01-10 13:21:51.011887: \n",
      "2024-01-10 13:21:51.021830: Epoch 806\n",
      "2024-01-10 13:21:51.025834: Current learning rate: 0.00229\n",
      "2024-01-10 13:22:33.933102: train_loss -0.9524\n",
      "2024-01-10 13:22:33.946101: val_loss -0.7988\n",
      "2024-01-10 13:22:33.956099: Pseudo dice [0.9246, 0.9291, 0.9374]\n",
      "2024-01-10 13:22:33.967099: Epoch time: 42.92 s\n",
      "2024-01-10 13:22:35.512717: \n",
      "2024-01-10 13:22:35.517792: Epoch 807\n",
      "2024-01-10 13:22:35.522804: Current learning rate: 0.00228\n",
      "2024-01-10 13:23:19.114186: train_loss -0.9528\n",
      "2024-01-10 13:23:19.125200: val_loss -0.7938\n",
      "2024-01-10 13:23:19.177799: Pseudo dice [0.9219, 0.9287, 0.9367]\n",
      "2024-01-10 13:23:19.185798: Epoch time: 43.6 s\n",
      "2024-01-10 13:23:20.762305: \n",
      "2024-01-10 13:23:20.768167: Epoch 808\n",
      "2024-01-10 13:23:20.773217: Current learning rate: 0.00226\n",
      "2024-01-10 13:24:04.216892: train_loss -0.9524\n",
      "2024-01-10 13:24:04.226362: val_loss -0.7939\n",
      "2024-01-10 13:24:04.238362: Pseudo dice [0.9226, 0.9277, 0.9361]\n",
      "2024-01-10 13:24:04.245361: Epoch time: 43.45 s\n",
      "2024-01-10 13:24:05.778023: \n",
      "2024-01-10 13:24:05.785032: Epoch 809\n",
      "2024-01-10 13:24:05.791033: Current learning rate: 0.00225\n",
      "2024-01-10 13:24:51.757873: train_loss -0.953\n",
      "2024-01-10 13:24:51.764875: val_loss -0.7911\n",
      "2024-01-10 13:24:51.771876: Pseudo dice [0.919, 0.928, 0.9364]\n",
      "2024-01-10 13:24:51.777875: Epoch time: 45.98 s\n",
      "2024-01-10 13:24:53.526034: \n",
      "2024-01-10 13:24:53.535971: Epoch 810\n",
      "2024-01-10 13:24:53.547568: Current learning rate: 0.00224\n",
      "2024-01-10 13:25:39.191338: train_loss -0.9531\n",
      "2024-01-10 13:25:39.204338: val_loss -0.7971\n",
      "2024-01-10 13:25:39.216341: Pseudo dice [0.9216, 0.9292, 0.9373]\n",
      "2024-01-10 13:25:39.226339: Epoch time: 45.67 s\n",
      "2024-01-10 13:25:41.085931: \n",
      "2024-01-10 13:25:41.090989: Epoch 811\n",
      "2024-01-10 13:25:41.095928: Current learning rate: 0.00223\n",
      "2024-01-10 13:26:24.683016: train_loss -0.9529\n",
      "2024-01-10 13:26:24.697642: val_loss -0.7976\n",
      "2024-01-10 13:26:24.710043: Pseudo dice [0.9233, 0.9292, 0.9371]\n",
      "2024-01-10 13:26:24.722150: Epoch time: 43.6 s\n",
      "2024-01-10 13:26:26.525314: \n",
      "2024-01-10 13:26:26.530781: Epoch 812\n",
      "2024-01-10 13:26:26.534781: Current learning rate: 0.00222\n",
      "2024-01-10 13:27:08.560152: train_loss -0.9526\n",
      "2024-01-10 13:27:08.572727: val_loss -0.7947\n",
      "2024-01-10 13:27:08.583232: Pseudo dice [0.9189, 0.929, 0.9374]\n",
      "2024-01-10 13:27:08.593537: Epoch time: 42.04 s\n",
      "2024-01-10 13:27:10.275298: \n",
      "2024-01-10 13:27:10.281299: Epoch 813\n",
      "2024-01-10 13:27:10.286295: Current learning rate: 0.00221\n",
      "2024-01-10 13:27:53.280077: train_loss -0.9529\n",
      "2024-01-10 13:27:53.289079: val_loss -0.7961\n",
      "2024-01-10 13:27:53.324882: Pseudo dice [0.9229, 0.9289, 0.9376]\n",
      "2024-01-10 13:27:53.336992: Epoch time: 43.01 s\n",
      "2024-01-10 13:27:54.928873: \n",
      "2024-01-10 13:27:54.936486: Epoch 814\n",
      "2024-01-10 13:27:54.944486: Current learning rate: 0.0022\n",
      "2024-01-10 13:28:38.360393: train_loss -0.9533\n",
      "2024-01-10 13:28:38.367392: val_loss -0.7951\n",
      "2024-01-10 13:28:38.373391: Pseudo dice [0.923, 0.9276, 0.9363]\n",
      "2024-01-10 13:28:38.379390: Epoch time: 43.43 s\n",
      "2024-01-10 13:28:39.763849: \n",
      "2024-01-10 13:28:39.770851: Epoch 815\n",
      "2024-01-10 13:28:39.776850: Current learning rate: 0.00219\n",
      "2024-01-10 13:29:22.824039: train_loss -0.9533\n",
      "2024-01-10 13:29:22.836044: val_loss -0.7959\n",
      "2024-01-10 13:29:22.845053: Pseudo dice [0.9232, 0.9289, 0.9362]\n",
      "2024-01-10 13:29:22.882560: Epoch time: 43.06 s\n",
      "2024-01-10 13:29:24.258319: \n",
      "2024-01-10 13:29:24.264322: Epoch 816\n",
      "2024-01-10 13:29:24.270321: Current learning rate: 0.00218\n",
      "2024-01-10 13:30:06.298406: train_loss -0.9528\n",
      "2024-01-10 13:30:06.343740: val_loss -0.8023\n",
      "2024-01-10 13:30:06.357742: Pseudo dice [0.9209, 0.9303, 0.9382]\n",
      "2024-01-10 13:30:06.368740: Epoch time: 42.04 s\n",
      "2024-01-10 13:30:08.143630: \n",
      "2024-01-10 13:30:08.150620: Epoch 817\n",
      "2024-01-10 13:30:08.157222: Current learning rate: 0.00217\n",
      "2024-01-10 13:30:49.509048: train_loss -0.9534\n",
      "2024-01-10 13:30:49.520049: val_loss -0.7952\n",
      "2024-01-10 13:30:49.529049: Pseudo dice [0.9228, 0.9294, 0.938]\n",
      "2024-01-10 13:30:49.559055: Epoch time: 41.37 s\n",
      "2024-01-10 13:30:50.756168: \n",
      "2024-01-10 13:30:50.763160: Epoch 818\n",
      "2024-01-10 13:30:50.766704: Current learning rate: 0.00216\n",
      "2024-01-10 13:31:31.681963: train_loss -0.9532\n",
      "2024-01-10 13:31:31.690963: val_loss -0.7933\n",
      "2024-01-10 13:31:31.698969: Pseudo dice [0.9206, 0.9279, 0.9363]\n",
      "2024-01-10 13:31:31.707972: Epoch time: 40.93 s\n",
      "2024-01-10 13:31:33.006984: \n",
      "2024-01-10 13:31:33.017838: Epoch 819\n",
      "2024-01-10 13:31:33.027452: Current learning rate: 0.00215\n",
      "2024-01-10 13:32:13.665341: train_loss -0.9531\n",
      "2024-01-10 13:32:13.675345: val_loss -0.7999\n",
      "2024-01-10 13:32:13.684340: Pseudo dice [0.9248, 0.93, 0.9371]\n",
      "2024-01-10 13:32:13.691340: Epoch time: 40.66 s\n",
      "2024-01-10 13:32:14.895158: \n",
      "2024-01-10 13:32:14.903199: Epoch 820\n",
      "2024-01-10 13:32:14.908206: Current learning rate: 0.00214\n",
      "2024-01-10 13:32:55.933594: train_loss -0.9537\n",
      "2024-01-10 13:32:55.971079: val_loss -0.7986\n",
      "2024-01-10 13:32:55.980079: Pseudo dice [0.9245, 0.9287, 0.9371]\n",
      "2024-01-10 13:32:55.988081: Epoch time: 41.04 s\n",
      "2024-01-10 13:32:57.153516: \n",
      "2024-01-10 13:32:57.158849: Epoch 821\n",
      "2024-01-10 13:32:57.166431: Current learning rate: 0.00213\n",
      "2024-01-10 13:33:38.064720: train_loss -0.9536\n",
      "2024-01-10 13:33:38.075722: val_loss -0.7966\n",
      "2024-01-10 13:33:38.111240: Pseudo dice [0.9222, 0.9291, 0.9377]\n",
      "2024-01-10 13:33:38.122232: Epoch time: 40.91 s\n",
      "2024-01-10 13:33:39.459561: \n",
      "2024-01-10 13:33:39.466577: Epoch 822\n",
      "2024-01-10 13:33:39.472577: Current learning rate: 0.00212\n",
      "2024-01-10 13:34:20.384861: train_loss -0.9533\n",
      "2024-01-10 13:34:20.415892: val_loss -0.7983\n",
      "2024-01-10 13:34:20.426893: Pseudo dice [0.9238, 0.9292, 0.9373]\n",
      "2024-01-10 13:34:20.434892: Epoch time: 40.93 s\n",
      "2024-01-10 13:34:21.718367: \n",
      "2024-01-10 13:34:21.724360: Epoch 823\n",
      "2024-01-10 13:34:21.729360: Current learning rate: 0.0021\n",
      "2024-01-10 13:35:03.125736: train_loss -0.9533\n",
      "2024-01-10 13:35:03.131735: val_loss -0.7989\n",
      "2024-01-10 13:35:03.138738: Pseudo dice [0.9226, 0.9296, 0.9381]\n",
      "2024-01-10 13:35:03.144735: Epoch time: 41.41 s\n",
      "2024-01-10 13:35:04.525003: \n",
      "2024-01-10 13:35:04.530663: Epoch 824\n",
      "2024-01-10 13:35:04.534677: Current learning rate: 0.00209\n",
      "2024-01-10 13:35:45.411763: train_loss -0.9521\n",
      "2024-01-10 13:35:45.420763: val_loss -0.8031\n",
      "2024-01-10 13:35:45.428772: Pseudo dice [0.9259, 0.9293, 0.9374]\n",
      "2024-01-10 13:35:45.437275: Epoch time: 40.89 s\n",
      "2024-01-10 13:35:46.846978: \n",
      "2024-01-10 13:35:46.859131: Epoch 825\n",
      "2024-01-10 13:35:46.863102: Current learning rate: 0.00208\n",
      "2024-01-10 13:36:28.849601: train_loss -0.9528\n",
      "2024-01-10 13:36:28.881444: val_loss -0.8\n",
      "2024-01-10 13:36:28.890444: Pseudo dice [0.9215, 0.93, 0.9375]\n",
      "2024-01-10 13:36:28.897443: Epoch time: 42.01 s\n",
      "2024-01-10 13:36:30.309479: \n",
      "2024-01-10 13:36:30.315472: Epoch 826\n",
      "2024-01-10 13:36:30.320987: Current learning rate: 0.00207\n",
      "2024-01-10 13:37:13.064017: train_loss -0.9543\n",
      "2024-01-10 13:37:13.073526: val_loss -0.7937\n",
      "2024-01-10 13:37:13.080540: Pseudo dice [0.9197, 0.9297, 0.938]\n",
      "2024-01-10 13:37:13.091548: Epoch time: 42.76 s\n",
      "2024-01-10 13:37:14.808713: \n",
      "2024-01-10 13:37:14.820714: Epoch 827\n",
      "2024-01-10 13:37:14.832722: Current learning rate: 0.00206\n",
      "2024-01-10 13:37:57.456624: train_loss -0.9533\n",
      "2024-01-10 13:37:57.499520: val_loss -0.7982\n",
      "2024-01-10 13:37:57.509979: Pseudo dice [0.9219, 0.9295, 0.9367]\n",
      "2024-01-10 13:37:57.520350: Epoch time: 42.65 s\n",
      "2024-01-10 13:37:59.101124: \n",
      "2024-01-10 13:37:59.105698: Epoch 828\n",
      "2024-01-10 13:37:59.110684: Current learning rate: 0.00205\n",
      "2024-01-10 13:38:40.952957: train_loss -0.9535\n",
      "2024-01-10 13:38:40.961957: val_loss -0.792\n",
      "2024-01-10 13:38:40.972956: Pseudo dice [0.9199, 0.9285, 0.9361]\n",
      "2024-01-10 13:38:40.982957: Epoch time: 41.85 s\n",
      "2024-01-10 13:38:42.487688: \n",
      "2024-01-10 13:38:42.494690: Epoch 829\n",
      "2024-01-10 13:38:42.499691: Current learning rate: 0.00204\n",
      "2024-01-10 13:39:24.989274: train_loss -0.9537\n",
      "2024-01-10 13:39:24.999273: val_loss -0.7911\n",
      "2024-01-10 13:39:25.007272: Pseudo dice [0.921, 0.9281, 0.936]\n",
      "2024-01-10 13:39:25.016271: Epoch time: 42.5 s\n",
      "2024-01-10 13:39:26.448437: \n",
      "2024-01-10 13:39:26.454538: Epoch 830\n",
      "2024-01-10 13:39:26.459569: Current learning rate: 0.00203\n",
      "2024-01-10 13:40:08.125663: train_loss -0.9529\n",
      "2024-01-10 13:40:08.139666: val_loss -0.7923\n",
      "2024-01-10 13:40:08.149930: Pseudo dice [0.9236, 0.9272, 0.9364]\n",
      "2024-01-10 13:40:08.160769: Epoch time: 41.68 s\n",
      "2024-01-10 13:40:09.802165: \n",
      "2024-01-10 13:40:09.808397: Epoch 831\n",
      "2024-01-10 13:40:09.816453: Current learning rate: 0.00202\n",
      "2024-01-10 13:40:51.592153: train_loss -0.9537\n",
      "2024-01-10 13:40:51.600155: val_loss -0.7951\n",
      "2024-01-10 13:40:51.608154: Pseudo dice [0.9202, 0.9293, 0.9364]\n",
      "2024-01-10 13:40:51.618234: Epoch time: 41.79 s\n",
      "2024-01-10 13:40:53.042926: \n",
      "2024-01-10 13:40:53.048492: Epoch 832\n",
      "2024-01-10 13:40:53.053253: Current learning rate: 0.00201\n",
      "2024-01-10 13:41:35.295772: train_loss -0.954\n",
      "2024-01-10 13:41:35.310302: val_loss -0.7948\n",
      "2024-01-10 13:41:35.318825: Pseudo dice [0.9212, 0.9281, 0.9368]\n",
      "2024-01-10 13:41:35.327347: Epoch time: 42.25 s\n",
      "2024-01-10 13:41:36.870483: \n",
      "2024-01-10 13:41:36.876603: Epoch 833\n",
      "2024-01-10 13:41:36.881259: Current learning rate: 0.002\n",
      "2024-01-10 13:42:18.698149: train_loss -0.9533\n",
      "2024-01-10 13:42:18.743988: val_loss -0.7919\n",
      "2024-01-10 13:42:18.753980: Pseudo dice [0.9238, 0.9282, 0.9361]\n",
      "2024-01-10 13:42:18.764662: Epoch time: 41.83 s\n",
      "2024-01-10 13:42:20.740139: \n",
      "2024-01-10 13:42:20.749136: Epoch 834\n",
      "2024-01-10 13:42:20.756138: Current learning rate: 0.00199\n",
      "2024-01-10 13:43:02.298746: train_loss -0.9535\n",
      "2024-01-10 13:43:02.306745: val_loss -0.7981\n",
      "2024-01-10 13:43:02.315752: Pseudo dice [0.9209, 0.9292, 0.9369]\n",
      "2024-01-10 13:43:02.324258: Epoch time: 41.56 s\n",
      "2024-01-10 13:43:03.461112: \n",
      "2024-01-10 13:43:03.469187: Epoch 835\n",
      "2024-01-10 13:43:03.475195: Current learning rate: 0.00198\n",
      "2024-01-10 13:43:43.679181: train_loss -0.9533\n",
      "2024-01-10 13:43:43.689178: val_loss -0.7998\n",
      "2024-01-10 13:43:43.697174: Pseudo dice [0.9238, 0.9293, 0.9372]\n",
      "2024-01-10 13:43:43.706173: Epoch time: 40.22 s\n",
      "2024-01-10 13:43:44.888829: \n",
      "2024-01-10 13:43:44.893746: Epoch 836\n",
      "2024-01-10 13:43:44.898753: Current learning rate: 0.00196\n",
      "2024-01-10 13:44:25.214849: train_loss -0.9533\n",
      "2024-01-10 13:44:25.253861: val_loss -0.7966\n",
      "2024-01-10 13:44:25.265000: Pseudo dice [0.9238, 0.9291, 0.937]\n",
      "2024-01-10 13:44:25.276038: Epoch time: 40.33 s\n",
      "2024-01-10 13:44:26.616903: \n",
      "2024-01-10 13:44:26.624062: Epoch 837\n",
      "2024-01-10 13:44:26.631063: Current learning rate: 0.00195\n",
      "2024-01-10 13:45:06.832730: train_loss -0.9534\n",
      "2024-01-10 13:45:06.843735: val_loss -0.7915\n",
      "2024-01-10 13:45:06.874260: Pseudo dice [0.9201, 0.9278, 0.9362]\n",
      "2024-01-10 13:45:06.884260: Epoch time: 40.22 s\n",
      "2024-01-10 13:45:08.083977: \n",
      "2024-01-10 13:45:08.091464: Epoch 838\n",
      "2024-01-10 13:45:08.095542: Current learning rate: 0.00194\n",
      "2024-01-10 13:45:48.557801: train_loss -0.9537\n",
      "2024-01-10 13:45:48.568800: val_loss -0.7957\n",
      "2024-01-10 13:45:48.576809: Pseudo dice [0.9192, 0.9283, 0.9366]\n",
      "2024-01-10 13:45:48.586808: Epoch time: 40.47 s\n",
      "2024-01-10 13:45:49.783346: \n",
      "2024-01-10 13:45:49.789167: Epoch 839\n",
      "2024-01-10 13:45:49.797338: Current learning rate: 0.00193\n",
      "2024-01-10 13:46:30.013813: train_loss -0.9526\n",
      "2024-01-10 13:46:30.045815: val_loss -0.7968\n",
      "2024-01-10 13:46:30.058813: Pseudo dice [0.9191, 0.9285, 0.9364]\n",
      "2024-01-10 13:46:30.070816: Epoch time: 40.23 s\n",
      "2024-01-10 13:46:31.312569: \n",
      "2024-01-10 13:46:31.321628: Epoch 840\n",
      "2024-01-10 13:46:31.325635: Current learning rate: 0.00192\n",
      "2024-01-10 13:47:11.547060: train_loss -0.9528\n",
      "2024-01-10 13:47:11.557060: val_loss -0.7949\n",
      "2024-01-10 13:47:11.586241: Pseudo dice [0.9192, 0.928, 0.9368]\n",
      "2024-01-10 13:47:11.596241: Epoch time: 40.24 s\n",
      "2024-01-10 13:47:12.802029: \n",
      "2024-01-10 13:47:12.810338: Epoch 841\n",
      "2024-01-10 13:47:12.818429: Current learning rate: 0.00191\n",
      "2024-01-10 13:47:53.461204: train_loss -0.9541\n",
      "2024-01-10 13:47:53.471376: val_loss -0.7962\n",
      "2024-01-10 13:47:53.479385: Pseudo dice [0.9196, 0.9298, 0.9376]\n",
      "2024-01-10 13:47:53.488380: Epoch time: 40.66 s\n",
      "2024-01-10 13:47:54.708805: \n",
      "2024-01-10 13:47:54.713798: Epoch 842\n",
      "2024-01-10 13:47:54.717802: Current learning rate: 0.0019\n",
      "2024-01-10 13:48:34.704967: train_loss -0.9539\n",
      "2024-01-10 13:48:34.713969: val_loss -0.7934\n",
      "2024-01-10 13:48:34.722968: Pseudo dice [0.9198, 0.9291, 0.9377]\n",
      "2024-01-10 13:48:34.729969: Epoch time: 40.0 s\n",
      "2024-01-10 13:48:35.877395: \n",
      "2024-01-10 13:48:35.882630: Epoch 843\n",
      "2024-01-10 13:48:35.886612: Current learning rate: 0.00189\n",
      "2024-01-10 13:49:15.440863: train_loss -0.9536\n",
      "2024-01-10 13:49:15.450863: val_loss -0.7981\n",
      "2024-01-10 13:49:15.456870: Pseudo dice [0.9217, 0.9296, 0.9376]\n",
      "2024-01-10 13:49:15.462869: Epoch time: 39.56 s\n",
      "2024-01-10 13:49:16.632240: \n",
      "2024-01-10 13:49:16.640117: Epoch 844\n",
      "2024-01-10 13:49:16.645109: Current learning rate: 0.00188\n",
      "2024-01-10 13:49:56.174153: train_loss -0.9534\n",
      "2024-01-10 13:49:56.183662: val_loss -0.7919\n",
      "2024-01-10 13:49:56.191663: Pseudo dice [0.9209, 0.9288, 0.9366]\n",
      "2024-01-10 13:49:56.197663: Epoch time: 39.54 s\n",
      "2024-01-10 13:49:57.309042: \n",
      "2024-01-10 13:49:57.314040: Epoch 845\n",
      "2024-01-10 13:49:57.318885: Current learning rate: 0.00187\n",
      "2024-01-10 13:50:37.075631: train_loss -0.9528\n",
      "2024-01-10 13:50:37.085639: val_loss -0.7985\n",
      "2024-01-10 13:50:37.094148: Pseudo dice [0.9227, 0.9296, 0.9372]\n",
      "2024-01-10 13:50:37.102155: Epoch time: 39.77 s\n",
      "2024-01-10 13:50:38.331681: \n",
      "2024-01-10 13:50:38.337821: Epoch 846\n",
      "2024-01-10 13:50:38.343716: Current learning rate: 0.00186\n",
      "2024-01-10 13:51:18.188536: train_loss -0.9535\n",
      "2024-01-10 13:51:18.196541: val_loss -0.7927\n",
      "2024-01-10 13:51:18.204541: Pseudo dice [0.9215, 0.9281, 0.9366]\n",
      "2024-01-10 13:51:18.212051: Epoch time: 39.86 s\n",
      "2024-01-10 13:51:19.386927: \n",
      "2024-01-10 13:51:19.393994: Epoch 847\n",
      "2024-01-10 13:51:19.398074: Current learning rate: 0.00185\n",
      "2024-01-10 13:51:59.408492: train_loss -0.9534\n",
      "2024-01-10 13:51:59.417499: val_loss -0.8029\n",
      "2024-01-10 13:51:59.427002: Pseudo dice [0.9235, 0.9289, 0.938]\n",
      "2024-01-10 13:51:59.433007: Epoch time: 40.02 s\n",
      "2024-01-10 13:52:00.623055: \n",
      "2024-01-10 13:52:00.631530: Epoch 848\n",
      "2024-01-10 13:52:00.634942: Current learning rate: 0.00184\n",
      "2024-01-10 13:52:40.483107: train_loss -0.9535\n",
      "2024-01-10 13:52:40.492108: val_loss -0.7864\n",
      "2024-01-10 13:52:40.500109: Pseudo dice [0.9184, 0.9271, 0.9354]\n",
      "2024-01-10 13:52:40.508111: Epoch time: 39.86 s\n",
      "2024-01-10 13:52:41.647277: \n",
      "2024-01-10 13:52:41.656687: Epoch 849\n",
      "2024-01-10 13:52:41.660711: Current learning rate: 0.00182\n",
      "2024-01-10 13:53:21.406077: train_loss -0.9532\n",
      "2024-01-10 13:53:21.415648: val_loss -0.7903\n",
      "2024-01-10 13:53:21.423648: Pseudo dice [0.9221, 0.928, 0.9365]\n",
      "2024-01-10 13:53:21.431649: Epoch time: 39.76 s\n",
      "2024-01-10 13:53:22.884470: \n",
      "2024-01-10 13:53:22.890470: Epoch 850\n",
      "2024-01-10 13:53:22.894465: Current learning rate: 0.00181\n",
      "2024-01-10 13:54:02.667294: train_loss -0.9538\n",
      "2024-01-10 13:54:02.675805: val_loss -0.794\n",
      "2024-01-10 13:54:02.684803: Pseudo dice [0.9209, 0.9287, 0.9378]\n",
      "2024-01-10 13:54:02.692803: Epoch time: 39.78 s\n",
      "2024-01-10 13:54:03.909412: \n",
      "2024-01-10 13:54:03.915353: Epoch 851\n",
      "2024-01-10 13:54:03.919364: Current learning rate: 0.0018\n",
      "2024-01-10 13:54:43.710452: train_loss -0.954\n",
      "2024-01-10 13:54:43.721446: val_loss -0.7944\n",
      "2024-01-10 13:54:43.729447: Pseudo dice [0.9191, 0.929, 0.9371]\n",
      "2024-01-10 13:54:43.736455: Epoch time: 39.8 s\n",
      "2024-01-10 13:54:44.855217: \n",
      "2024-01-10 13:54:44.862929: Epoch 852\n",
      "2024-01-10 13:54:44.866871: Current learning rate: 0.00179\n",
      "2024-01-10 13:55:24.844199: train_loss -0.9535\n",
      "2024-01-10 13:55:24.853200: val_loss -0.7879\n",
      "2024-01-10 13:55:24.861200: Pseudo dice [0.9195, 0.927, 0.9368]\n",
      "2024-01-10 13:55:24.868200: Epoch time: 39.99 s\n",
      "2024-01-10 13:55:26.054393: \n",
      "2024-01-10 13:55:26.063042: Epoch 853\n",
      "2024-01-10 13:55:26.068055: Current learning rate: 0.00178\n",
      "2024-01-10 13:56:05.902769: train_loss -0.9537\n",
      "2024-01-10 13:56:05.911777: val_loss -0.8001\n",
      "2024-01-10 13:56:05.920773: Pseudo dice [0.9218, 0.9291, 0.9378]\n",
      "2024-01-10 13:56:05.929285: Epoch time: 39.85 s\n",
      "2024-01-10 13:56:07.069896: \n",
      "2024-01-10 13:56:07.077536: Epoch 854\n",
      "2024-01-10 13:56:07.083605: Current learning rate: 0.00177\n",
      "2024-01-10 13:56:46.909067: train_loss -0.9539\n",
      "2024-01-10 13:56:46.920082: val_loss -0.7958\n",
      "2024-01-10 13:56:46.953332: Pseudo dice [0.9218, 0.9283, 0.9371]\n",
      "2024-01-10 13:56:46.965330: Epoch time: 39.84 s\n",
      "2024-01-10 13:56:48.109703: \n",
      "2024-01-10 13:56:48.118312: Epoch 855\n",
      "2024-01-10 13:56:48.122388: Current learning rate: 0.00176\n",
      "2024-01-10 13:57:28.122669: train_loss -0.9536\n",
      "2024-01-10 13:57:28.135669: val_loss -0.7945\n",
      "2024-01-10 13:57:28.144669: Pseudo dice [0.9218, 0.9285, 0.9368]\n",
      "2024-01-10 13:57:28.153177: Epoch time: 40.01 s\n",
      "2024-01-10 13:57:29.278935: \n",
      "2024-01-10 13:57:29.283997: Epoch 856\n",
      "2024-01-10 13:57:29.288938: Current learning rate: 0.00175\n",
      "2024-01-10 13:58:09.216882: train_loss -0.9537\n",
      "2024-01-10 13:58:09.227882: val_loss -0.7957\n",
      "2024-01-10 13:58:09.255690: Pseudo dice [0.9197, 0.9295, 0.9369]\n",
      "2024-01-10 13:58:09.263688: Epoch time: 39.94 s\n",
      "2024-01-10 13:58:10.392860: \n",
      "2024-01-10 13:58:10.402691: Epoch 857\n",
      "2024-01-10 13:58:10.406758: Current learning rate: 0.00174\n",
      "2024-01-10 13:58:50.328588: train_loss -0.9545\n",
      "2024-01-10 13:58:50.335589: val_loss -0.7927\n",
      "2024-01-10 13:58:50.342589: Pseudo dice [0.9199, 0.9283, 0.9363]\n",
      "2024-01-10 13:58:50.375594: Epoch time: 39.94 s\n",
      "2024-01-10 13:58:51.512152: \n",
      "2024-01-10 13:58:51.520294: Epoch 858\n",
      "2024-01-10 13:58:51.524295: Current learning rate: 0.00173\n",
      "2024-01-10 13:59:31.276887: train_loss -0.9537\n",
      "2024-01-10 13:59:31.285892: val_loss -0.7937\n",
      "2024-01-10 13:59:31.294890: Pseudo dice [0.9238, 0.9275, 0.9362]\n",
      "2024-01-10 13:59:31.303406: Epoch time: 39.77 s\n",
      "2024-01-10 13:59:32.464882: \n",
      "2024-01-10 13:59:32.471882: Epoch 859\n",
      "2024-01-10 13:59:32.476882: Current learning rate: 0.00172\n",
      "2024-01-10 14:00:12.175756: train_loss -0.9546\n",
      "2024-01-10 14:00:12.183755: val_loss -0.7952\n",
      "2024-01-10 14:00:12.189755: Pseudo dice [0.9223, 0.9282, 0.9373]\n",
      "2024-01-10 14:00:12.195755: Epoch time: 39.71 s\n",
      "2024-01-10 14:00:13.389256: \n",
      "2024-01-10 14:00:13.394257: Epoch 860\n",
      "2024-01-10 14:00:13.398257: Current learning rate: 0.0017\n",
      "2024-01-10 14:00:53.244575: train_loss -0.9544\n",
      "2024-01-10 14:00:53.253574: val_loss -0.7959\n",
      "2024-01-10 14:00:53.278572: Pseudo dice [0.9216, 0.9289, 0.9371]\n",
      "2024-01-10 14:00:53.288573: Epoch time: 39.86 s\n",
      "2024-01-10 14:00:54.404886: \n",
      "2024-01-10 14:00:54.410353: Epoch 861\n",
      "2024-01-10 14:00:54.414501: Current learning rate: 0.00169\n",
      "2024-01-10 14:01:34.317445: train_loss -0.9538\n",
      "2024-01-10 14:01:34.325447: val_loss -0.7978\n",
      "2024-01-10 14:01:34.361974: Pseudo dice [0.921, 0.9292, 0.9381]\n",
      "2024-01-10 14:01:34.369971: Epoch time: 39.91 s\n",
      "2024-01-10 14:01:35.493357: \n",
      "2024-01-10 14:01:35.499357: Epoch 862\n",
      "2024-01-10 14:01:35.504285: Current learning rate: 0.00168\n",
      "2024-01-10 14:02:15.468307: train_loss -0.9545\n",
      "2024-01-10 14:02:15.478305: val_loss -0.7951\n",
      "2024-01-10 14:02:15.490817: Pseudo dice [0.9216, 0.9294, 0.9376]\n",
      "2024-01-10 14:02:15.523818: Epoch time: 39.98 s\n",
      "2024-01-10 14:02:16.670759: \n",
      "2024-01-10 14:02:16.680716: Epoch 863\n",
      "2024-01-10 14:02:16.685229: Current learning rate: 0.00167\n",
      "2024-01-10 14:02:56.455005: train_loss -0.9547\n",
      "2024-01-10 14:02:56.465005: val_loss -0.794\n",
      "2024-01-10 14:02:56.474004: Pseudo dice [0.9241, 0.9282, 0.9361]\n",
      "2024-01-10 14:02:56.482008: Epoch time: 39.79 s\n",
      "2024-01-10 14:02:57.637343: \n",
      "2024-01-10 14:02:57.643997: Epoch 864\n",
      "2024-01-10 14:02:57.650011: Current learning rate: 0.00166\n",
      "2024-01-10 14:03:37.442085: train_loss -0.9545\n",
      "2024-01-10 14:03:37.449082: val_loss -0.7987\n",
      "2024-01-10 14:03:37.481081: Pseudo dice [0.9205, 0.9294, 0.938]\n",
      "2024-01-10 14:03:37.488082: Epoch time: 39.8 s\n",
      "2024-01-10 14:03:38.695198: \n",
      "2024-01-10 14:03:38.706225: Epoch 865\n",
      "2024-01-10 14:03:38.710194: Current learning rate: 0.00165\n",
      "2024-01-10 14:04:18.470810: train_loss -0.9543\n",
      "2024-01-10 14:04:18.483827: val_loss -0.7976\n",
      "2024-01-10 14:04:18.495805: Pseudo dice [0.9197, 0.9294, 0.9373]\n",
      "2024-01-10 14:04:18.504804: Epoch time: 39.78 s\n",
      "2024-01-10 14:04:19.708232: \n",
      "2024-01-10 14:04:19.719299: Epoch 866\n",
      "2024-01-10 14:04:19.723368: Current learning rate: 0.00164\n",
      "2024-01-10 14:04:59.563380: train_loss -0.9549\n",
      "2024-01-10 14:04:59.572376: val_loss -0.7937\n",
      "2024-01-10 14:04:59.581900: Pseudo dice [0.9208, 0.9284, 0.9363]\n",
      "2024-01-10 14:04:59.589900: Epoch time: 39.86 s\n",
      "2024-01-10 14:05:00.773071: \n",
      "2024-01-10 14:05:00.782068: Epoch 867\n",
      "2024-01-10 14:05:00.788157: Current learning rate: 0.00163\n",
      "2024-01-10 14:05:40.604175: train_loss -0.9547\n",
      "2024-01-10 14:05:40.614176: val_loss -0.7934\n",
      "2024-01-10 14:05:40.650182: Pseudo dice [0.9212, 0.9284, 0.9369]\n",
      "2024-01-10 14:05:40.659186: Epoch time: 39.83 s\n",
      "2024-01-10 14:05:41.828830: \n",
      "2024-01-10 14:05:41.833822: Epoch 868\n",
      "2024-01-10 14:05:41.838822: Current learning rate: 0.00162\n",
      "2024-01-10 14:06:21.701720: train_loss -0.9553\n",
      "2024-01-10 14:06:21.710721: val_loss -0.7953\n",
      "2024-01-10 14:06:21.719720: Pseudo dice [0.9214, 0.9284, 0.9366]\n",
      "2024-01-10 14:06:21.743726: Epoch time: 39.87 s\n",
      "2024-01-10 14:06:22.798881: \n",
      "2024-01-10 14:06:22.808560: Epoch 869\n",
      "2024-01-10 14:06:22.814564: Current learning rate: 0.00161\n",
      "2024-01-10 14:07:02.636727: train_loss -0.9552\n",
      "2024-01-10 14:07:02.646726: val_loss -0.7894\n",
      "2024-01-10 14:07:02.677731: Pseudo dice [0.9199, 0.9278, 0.9363]\n",
      "2024-01-10 14:07:02.686736: Epoch time: 39.84 s\n",
      "2024-01-10 14:07:03.829040: \n",
      "2024-01-10 14:07:03.835506: Epoch 870\n",
      "2024-01-10 14:07:03.839590: Current learning rate: 0.00159\n",
      "2024-01-10 14:07:43.675108: train_loss -0.9546\n",
      "2024-01-10 14:07:43.683106: val_loss -0.7933\n",
      "2024-01-10 14:07:43.690106: Pseudo dice [0.9224, 0.9277, 0.936]\n",
      "2024-01-10 14:07:43.698119: Epoch time: 39.85 s\n",
      "2024-01-10 14:07:44.821811: \n",
      "2024-01-10 14:07:44.830408: Epoch 871\n",
      "2024-01-10 14:07:44.835486: Current learning rate: 0.00158\n",
      "2024-01-10 14:08:24.678194: train_loss -0.954\n",
      "2024-01-10 14:08:24.688193: val_loss -0.7913\n",
      "2024-01-10 14:08:24.697193: Pseudo dice [0.9191, 0.9276, 0.9353]\n",
      "2024-01-10 14:08:24.705193: Epoch time: 39.86 s\n",
      "2024-01-10 14:08:25.895256: \n",
      "2024-01-10 14:08:25.903455: Epoch 872\n",
      "2024-01-10 14:08:25.907524: Current learning rate: 0.00157\n",
      "2024-01-10 14:09:05.792114: train_loss -0.9542\n",
      "2024-01-10 14:09:05.803113: val_loss -0.7948\n",
      "2024-01-10 14:09:05.813113: Pseudo dice [0.9214, 0.9286, 0.9372]\n",
      "2024-01-10 14:09:05.823116: Epoch time: 39.9 s\n",
      "2024-01-10 14:09:07.023403: \n",
      "2024-01-10 14:09:07.030810: Epoch 873\n",
      "2024-01-10 14:09:07.034877: Current learning rate: 0.00156\n",
      "2024-01-10 14:09:46.889270: train_loss -0.9552\n",
      "2024-01-10 14:09:46.917270: val_loss -0.7954\n",
      "2024-01-10 14:09:46.926270: Pseudo dice [0.9194, 0.9299, 0.9396]\n",
      "2024-01-10 14:09:46.955275: Epoch time: 39.87 s\n",
      "2024-01-10 14:09:48.040200: \n",
      "2024-01-10 14:09:48.046843: Epoch 874\n",
      "2024-01-10 14:09:48.051968: Current learning rate: 0.00155\n",
      "2024-01-10 14:10:27.827814: train_loss -0.955\n",
      "2024-01-10 14:10:27.834814: val_loss -0.7954\n",
      "2024-01-10 14:10:27.840815: Pseudo dice [0.9222, 0.9287, 0.9365]\n",
      "2024-01-10 14:10:27.871376: Epoch time: 39.79 s\n",
      "2024-01-10 14:10:29.069989: \n",
      "2024-01-10 14:10:29.077514: Epoch 875\n",
      "2024-01-10 14:10:29.081515: Current learning rate: 0.00154\n",
      "2024-01-10 14:11:08.905299: train_loss -0.9548\n",
      "2024-01-10 14:11:08.915297: val_loss -0.7991\n",
      "2024-01-10 14:11:08.945304: Pseudo dice [0.9236, 0.9304, 0.9387]\n",
      "2024-01-10 14:11:08.954301: Epoch time: 39.84 s\n",
      "2024-01-10 14:11:10.088603: \n",
      "2024-01-10 14:11:10.098023: Epoch 876\n",
      "2024-01-10 14:11:10.103091: Current learning rate: 0.00153\n",
      "2024-01-10 14:11:49.834336: train_loss -0.9543\n",
      "2024-01-10 14:11:49.844335: val_loss -0.7945\n",
      "2024-01-10 14:11:49.874523: Pseudo dice [0.9205, 0.9283, 0.9372]\n",
      "2024-01-10 14:11:49.883524: Epoch time: 39.75 s\n",
      "2024-01-10 14:11:51.039248: \n",
      "2024-01-10 14:11:51.046248: Epoch 877\n",
      "2024-01-10 14:11:51.051248: Current learning rate: 0.00152\n",
      "2024-01-10 14:12:30.966493: train_loss -0.9552\n",
      "2024-01-10 14:12:30.978489: val_loss -0.7993\n",
      "2024-01-10 14:12:31.009496: Pseudo dice [0.9261, 0.9289, 0.9374]\n",
      "2024-01-10 14:12:31.019496: Epoch time: 39.93 s\n",
      "2024-01-10 14:12:32.146982: \n",
      "2024-01-10 14:12:32.155558: Epoch 878\n",
      "2024-01-10 14:12:32.160559: Current learning rate: 0.00151\n",
      "2024-01-10 14:13:12.029407: train_loss -0.9545\n",
      "2024-01-10 14:13:12.039914: val_loss -0.8005\n",
      "2024-01-10 14:13:12.047913: Pseudo dice [0.9219, 0.9293, 0.9377]\n",
      "2024-01-10 14:13:12.053912: Epoch time: 39.88 s\n",
      "2024-01-10 14:13:13.173883: \n",
      "2024-01-10 14:13:13.182957: Epoch 879\n",
      "2024-01-10 14:13:13.187947: Current learning rate: 0.00149\n",
      "2024-01-10 14:13:53.018471: train_loss -0.9552\n",
      "2024-01-10 14:13:53.025474: val_loss -0.7914\n",
      "2024-01-10 14:13:53.034483: Pseudo dice [0.9206, 0.9291, 0.9376]\n",
      "2024-01-10 14:13:53.063986: Epoch time: 39.85 s\n",
      "2024-01-10 14:13:54.215943: \n",
      "2024-01-10 14:13:54.222019: Epoch 880\n",
      "2024-01-10 14:13:54.228008: Current learning rate: 0.00148\n",
      "2024-01-10 14:14:33.909994: train_loss -0.9547\n",
      "2024-01-10 14:14:33.919996: val_loss -0.7967\n",
      "2024-01-10 14:14:33.950000: Pseudo dice [0.9233, 0.9291, 0.937]\n",
      "2024-01-10 14:14:33.955998: Epoch time: 39.7 s\n",
      "2024-01-10 14:14:35.095380: \n",
      "2024-01-10 14:14:35.102380: Epoch 881\n",
      "2024-01-10 14:14:35.107380: Current learning rate: 0.00147\n",
      "2024-01-10 14:15:14.876747: train_loss -0.9557\n",
      "2024-01-10 14:15:14.888751: val_loss -0.7936\n",
      "2024-01-10 14:15:14.922749: Pseudo dice [0.9214, 0.9279, 0.9369]\n",
      "2024-01-10 14:15:14.931747: Epoch time: 39.78 s\n",
      "2024-01-10 14:15:16.092943: \n",
      "2024-01-10 14:15:16.105352: Epoch 882\n",
      "2024-01-10 14:15:16.109338: Current learning rate: 0.00146\n",
      "2024-01-10 14:15:55.996944: train_loss -0.9551\n",
      "2024-01-10 14:15:56.006945: val_loss -0.787\n",
      "2024-01-10 14:15:56.035943: Pseudo dice [0.921, 0.9271, 0.9363]\n",
      "2024-01-10 14:15:56.044943: Epoch time: 39.91 s\n",
      "2024-01-10 14:15:57.199564: \n",
      "2024-01-10 14:15:57.206612: Epoch 883\n",
      "2024-01-10 14:15:57.210631: Current learning rate: 0.00145\n",
      "2024-01-10 14:16:36.939668: train_loss -0.9545\n",
      "2024-01-10 14:16:36.948666: val_loss -0.7932\n",
      "2024-01-10 14:16:36.955666: Pseudo dice [0.9214, 0.928, 0.9365]\n",
      "2024-01-10 14:16:36.978665: Epoch time: 39.74 s\n",
      "2024-01-10 14:16:38.132629: \n",
      "2024-01-10 14:16:38.141214: Epoch 884\n",
      "2024-01-10 14:16:38.145217: Current learning rate: 0.00144\n",
      "2024-01-10 14:17:17.915361: train_loss -0.9545\n",
      "2024-01-10 14:17:17.923879: val_loss -0.7945\n",
      "2024-01-10 14:17:17.931879: Pseudo dice [0.9208, 0.9286, 0.9371]\n",
      "2024-01-10 14:17:17.940880: Epoch time: 39.78 s\n",
      "2024-01-10 14:17:19.132091: \n",
      "2024-01-10 14:17:19.137088: Epoch 885\n",
      "2024-01-10 14:17:19.142163: Current learning rate: 0.00143\n",
      "2024-01-10 14:17:58.851317: train_loss -0.9555\n",
      "2024-01-10 14:17:58.858319: val_loss -0.7928\n",
      "2024-01-10 14:17:58.866318: Pseudo dice [0.9195, 0.9282, 0.9366]\n",
      "2024-01-10 14:17:58.874321: Epoch time: 39.72 s\n",
      "2024-01-10 14:18:00.069411: \n",
      "2024-01-10 14:18:00.079776: Epoch 886\n",
      "2024-01-10 14:18:00.083841: Current learning rate: 0.00142\n",
      "2024-01-10 14:18:39.849404: train_loss -0.9555\n",
      "2024-01-10 14:18:39.858395: val_loss -0.7989\n",
      "2024-01-10 14:18:39.867396: Pseudo dice [0.9242, 0.9295, 0.938]\n",
      "2024-01-10 14:18:39.875395: Epoch time: 39.78 s\n",
      "2024-01-10 14:18:41.031147: \n",
      "2024-01-10 14:18:41.040129: Epoch 887\n",
      "2024-01-10 14:18:41.044464: Current learning rate: 0.00141\n",
      "2024-01-10 14:19:20.959355: train_loss -0.9551\n",
      "2024-01-10 14:19:20.986866: val_loss -0.7942\n",
      "2024-01-10 14:19:20.995866: Pseudo dice [0.9214, 0.9281, 0.937]\n",
      "2024-01-10 14:19:21.003868: Epoch time: 39.93 s\n",
      "2024-01-10 14:19:22.204008: \n",
      "2024-01-10 14:19:22.209513: Epoch 888\n",
      "2024-01-10 14:19:22.214032: Current learning rate: 0.00139\n",
      "2024-01-10 14:20:01.944163: train_loss -0.9554\n",
      "2024-01-10 14:20:01.970169: val_loss -0.7959\n",
      "2024-01-10 14:20:01.978683: Pseudo dice [0.9185, 0.9288, 0.9367]\n",
      "2024-01-10 14:20:01.988685: Epoch time: 39.74 s\n",
      "2024-01-10 14:20:03.168396: \n",
      "2024-01-10 14:20:03.176140: Epoch 889\n",
      "2024-01-10 14:20:03.180160: Current learning rate: 0.00138\n",
      "2024-01-10 14:20:42.967575: train_loss -0.9555\n",
      "2024-01-10 14:20:42.978581: val_loss -0.7931\n",
      "2024-01-10 14:20:42.984586: Pseudo dice [0.9213, 0.9281, 0.9366]\n",
      "2024-01-10 14:20:42.991108: Epoch time: 39.8 s\n",
      "2024-01-10 14:20:44.121473: \n",
      "2024-01-10 14:20:44.129327: Epoch 890\n",
      "2024-01-10 14:20:44.134295: Current learning rate: 0.00137\n",
      "2024-01-10 14:21:23.807719: train_loss -0.9551\n",
      "2024-01-10 14:21:23.816719: val_loss -0.7942\n",
      "2024-01-10 14:21:23.825719: Pseudo dice [0.9198, 0.9287, 0.9375]\n",
      "2024-01-10 14:21:23.833721: Epoch time: 39.69 s\n",
      "2024-01-10 14:21:25.014464: \n",
      "2024-01-10 14:21:25.022832: Epoch 891\n",
      "2024-01-10 14:21:25.026900: Current learning rate: 0.00136\n",
      "2024-01-10 14:22:04.883387: train_loss -0.9549\n",
      "2024-01-10 14:22:04.895384: val_loss -0.7891\n",
      "2024-01-10 14:22:04.931603: Pseudo dice [0.9226, 0.9268, 0.9352]\n",
      "2024-01-10 14:22:04.941599: Epoch time: 39.87 s\n",
      "2024-01-10 14:22:06.136960: \n",
      "2024-01-10 14:22:06.145215: Epoch 892\n",
      "2024-01-10 14:22:06.148795: Current learning rate: 0.00135\n",
      "2024-01-10 14:22:46.070915: train_loss -0.9557\n",
      "2024-01-10 14:22:46.079914: val_loss -0.7912\n",
      "2024-01-10 14:22:46.087914: Pseudo dice [0.9208, 0.9281, 0.9366]\n",
      "2024-01-10 14:22:46.095914: Epoch time: 39.93 s\n",
      "2024-01-10 14:22:47.288364: \n",
      "2024-01-10 14:22:47.296820: Epoch 893\n",
      "2024-01-10 14:22:47.300400: Current learning rate: 0.00134\n",
      "2024-01-10 14:23:27.033892: train_loss -0.9558\n",
      "2024-01-10 14:23:27.047897: val_loss -0.8001\n",
      "2024-01-10 14:23:27.056406: Pseudo dice [0.9255, 0.9289, 0.9379]\n",
      "2024-01-10 14:23:27.064404: Epoch time: 39.75 s\n",
      "2024-01-10 14:23:28.240103: \n",
      "2024-01-10 14:23:28.247106: Epoch 894\n",
      "2024-01-10 14:23:28.255119: Current learning rate: 0.00133\n",
      "2024-01-10 14:24:07.996238: train_loss -0.9552\n",
      "2024-01-10 14:24:08.004238: val_loss -0.7962\n",
      "2024-01-10 14:24:08.011238: Pseudo dice [0.9206, 0.9289, 0.9385]\n",
      "2024-01-10 14:24:08.017239: Epoch time: 39.76 s\n",
      "2024-01-10 14:24:09.222263: \n",
      "2024-01-10 14:24:09.231266: Epoch 895\n",
      "2024-01-10 14:24:09.235328: Current learning rate: 0.00132\n",
      "2024-01-10 14:24:48.921425: train_loss -0.9554\n",
      "2024-01-10 14:24:48.928426: val_loss -0.7898\n",
      "2024-01-10 14:24:48.936428: Pseudo dice [0.921, 0.9273, 0.9359]\n",
      "2024-01-10 14:24:48.944427: Epoch time: 39.7 s\n",
      "2024-01-10 14:24:50.131099: \n",
      "2024-01-10 14:24:50.138512: Epoch 896\n",
      "2024-01-10 14:24:50.142517: Current learning rate: 0.0013\n",
      "2024-01-10 14:25:29.884942: train_loss -0.9558\n",
      "2024-01-10 14:25:29.895947: val_loss -0.793\n",
      "2024-01-10 14:25:29.904462: Pseudo dice [0.9217, 0.9271, 0.9354]\n",
      "2024-01-10 14:25:29.912461: Epoch time: 39.75 s\n",
      "2024-01-10 14:25:31.099735: \n",
      "2024-01-10 14:25:31.105459: Epoch 897\n",
      "2024-01-10 14:25:31.109533: Current learning rate: 0.00129\n",
      "2024-01-10 14:26:10.865102: train_loss -0.955\n",
      "2024-01-10 14:26:10.873616: val_loss -0.7956\n",
      "2024-01-10 14:26:10.881616: Pseudo dice [0.9236, 0.9284, 0.9371]\n",
      "2024-01-10 14:26:10.889618: Epoch time: 39.77 s\n",
      "2024-01-10 14:26:12.103938: \n",
      "2024-01-10 14:26:12.109137: Epoch 898\n",
      "2024-01-10 14:26:12.113138: Current learning rate: 0.00128\n",
      "2024-01-10 14:26:51.967114: train_loss -0.9553\n",
      "2024-01-10 14:26:51.995111: val_loss -0.7958\n",
      "2024-01-10 14:26:52.007111: Pseudo dice [0.9252, 0.9291, 0.9364]\n",
      "2024-01-10 14:26:52.017113: Epoch time: 39.86 s\n",
      "2024-01-10 14:26:53.168433: \n",
      "2024-01-10 14:26:53.177425: Epoch 899\n",
      "2024-01-10 14:26:53.181434: Current learning rate: 0.00127\n",
      "2024-01-10 14:27:32.978386: train_loss -0.956\n",
      "2024-01-10 14:27:32.987386: val_loss -0.7922\n",
      "2024-01-10 14:27:32.996385: Pseudo dice [0.9212, 0.9273, 0.9358]\n",
      "2024-01-10 14:27:33.030387: Epoch time: 39.81 s\n",
      "2024-01-10 14:27:34.410153: \n",
      "2024-01-10 14:27:34.416147: Epoch 900\n",
      "2024-01-10 14:27:34.420146: Current learning rate: 0.00126\n",
      "2024-01-10 14:28:14.283074: train_loss -0.9562\n",
      "2024-01-10 14:28:14.292081: val_loss -0.7923\n",
      "2024-01-10 14:28:14.300080: Pseudo dice [0.9198, 0.9283, 0.9364]\n",
      "2024-01-10 14:28:14.308083: Epoch time: 39.87 s\n",
      "2024-01-10 14:28:15.471558: \n",
      "2024-01-10 14:28:15.476623: Epoch 901\n",
      "2024-01-10 14:28:15.481626: Current learning rate: 0.00125\n",
      "2024-01-10 14:28:55.359496: train_loss -0.9557\n",
      "2024-01-10 14:28:55.369493: val_loss -0.7956\n",
      "2024-01-10 14:28:55.376493: Pseudo dice [0.9199, 0.9291, 0.9377]\n",
      "2024-01-10 14:28:55.409035: Epoch time: 39.89 s\n",
      "2024-01-10 14:28:56.725546: \n",
      "2024-01-10 14:28:56.733866: Epoch 902\n",
      "2024-01-10 14:28:56.737814: Current learning rate: 0.00124\n",
      "2024-01-10 14:29:36.507551: train_loss -0.956\n",
      "2024-01-10 14:29:36.548070: val_loss -0.7902\n",
      "2024-01-10 14:29:36.557069: Pseudo dice [0.9196, 0.9276, 0.9365]\n",
      "2024-01-10 14:29:36.568069: Epoch time: 39.78 s\n",
      "2024-01-10 14:29:37.762526: \n",
      "2024-01-10 14:29:37.767748: Epoch 903\n",
      "2024-01-10 14:29:37.771750: Current learning rate: 0.00122\n",
      "2024-01-10 14:30:17.553638: train_loss -0.9559\n",
      "2024-01-10 14:30:17.564102: val_loss -0.7939\n",
      "2024-01-10 14:30:17.572394: Pseudo dice [0.9205, 0.9292, 0.9374]\n",
      "2024-01-10 14:30:17.580394: Epoch time: 39.79 s\n",
      "2024-01-10 14:30:18.758866: \n",
      "2024-01-10 14:30:18.766292: Epoch 904\n",
      "2024-01-10 14:30:18.776292: Current learning rate: 0.00121\n",
      "2024-01-10 14:30:58.544036: train_loss -0.9564\n",
      "2024-01-10 14:30:58.555044: val_loss -0.7957\n",
      "2024-01-10 14:30:58.565044: Pseudo dice [0.92, 0.9285, 0.9378]\n",
      "2024-01-10 14:30:58.597054: Epoch time: 39.79 s\n",
      "2024-01-10 14:30:59.740725: \n",
      "2024-01-10 14:30:59.746536: Epoch 905\n",
      "2024-01-10 14:30:59.750626: Current learning rate: 0.0012\n",
      "2024-01-10 14:31:39.592142: train_loss -0.956\n",
      "2024-01-10 14:31:39.600142: val_loss -0.7941\n",
      "2024-01-10 14:31:39.622145: Pseudo dice [0.9213, 0.9279, 0.9365]\n",
      "2024-01-10 14:31:39.632141: Epoch time: 39.85 s\n",
      "2024-01-10 14:31:40.772372: \n",
      "2024-01-10 14:31:40.780977: Epoch 906\n",
      "2024-01-10 14:31:40.790980: Current learning rate: 0.00119\n",
      "2024-01-10 14:32:20.689604: train_loss -0.9561\n",
      "2024-01-10 14:32:20.699608: val_loss -0.7972\n",
      "2024-01-10 14:32:20.707607: Pseudo dice [0.9243, 0.9287, 0.9373]\n",
      "2024-01-10 14:32:20.737605: Epoch time: 39.92 s\n",
      "2024-01-10 14:32:21.901723: \n",
      "2024-01-10 14:32:21.908723: Epoch 907\n",
      "2024-01-10 14:32:21.917723: Current learning rate: 0.00118\n",
      "2024-01-10 14:33:01.824389: train_loss -0.9559\n",
      "2024-01-10 14:33:01.838391: val_loss -0.7996\n",
      "2024-01-10 14:33:01.847395: Pseudo dice [0.9226, 0.929, 0.9381]\n",
      "2024-01-10 14:33:01.889911: Epoch time: 39.92 s\n",
      "2024-01-10 14:33:03.048262: \n",
      "2024-01-10 14:33:03.057669: Epoch 908\n",
      "2024-01-10 14:33:03.063698: Current learning rate: 0.00117\n",
      "2024-01-10 14:33:42.875024: train_loss -0.9559\n",
      "2024-01-10 14:33:42.884018: val_loss -0.7943\n",
      "2024-01-10 14:33:42.893022: Pseudo dice [0.919, 0.9284, 0.9367]\n",
      "2024-01-10 14:33:42.900026: Epoch time: 39.83 s\n",
      "2024-01-10 14:33:44.050855: \n",
      "2024-01-10 14:33:44.055807: Epoch 909\n",
      "2024-01-10 14:33:44.060709: Current learning rate: 0.00116\n",
      "2024-01-10 14:34:23.876834: train_loss -0.9562\n",
      "2024-01-10 14:34:23.887834: val_loss -0.799\n",
      "2024-01-10 14:34:23.895835: Pseudo dice [0.9224, 0.9295, 0.9375]\n",
      "2024-01-10 14:34:23.906840: Epoch time: 39.83 s\n",
      "2024-01-10 14:34:25.100594: \n",
      "2024-01-10 14:34:25.108614: Epoch 910\n",
      "2024-01-10 14:34:25.148529: Current learning rate: 0.00115\n",
      "2024-01-10 14:35:05.021678: train_loss -0.9558\n",
      "2024-01-10 14:35:05.031677: val_loss -0.7964\n",
      "2024-01-10 14:35:05.039678: Pseudo dice [0.9227, 0.9285, 0.9378]\n",
      "2024-01-10 14:35:05.048694: Epoch time: 39.92 s\n",
      "2024-01-10 14:35:06.236457: \n",
      "2024-01-10 14:35:06.246111: Epoch 911\n",
      "2024-01-10 14:35:06.249699: Current learning rate: 0.00113\n",
      "2024-01-10 14:35:46.143377: train_loss -0.9564\n",
      "2024-01-10 14:35:46.152377: val_loss -0.7961\n",
      "2024-01-10 14:35:46.181893: Pseudo dice [0.9207, 0.9292, 0.9378]\n",
      "2024-01-10 14:35:46.188892: Epoch time: 39.91 s\n",
      "2024-01-10 14:35:47.318991: \n",
      "2024-01-10 14:35:47.325989: Epoch 912\n",
      "2024-01-10 14:35:47.329989: Current learning rate: 0.00112\n",
      "2024-01-10 14:36:27.255327: train_loss -0.956\n",
      "2024-01-10 14:36:27.266330: val_loss -0.8024\n",
      "2024-01-10 14:36:27.274329: Pseudo dice [0.9229, 0.9293, 0.9381]\n",
      "2024-01-10 14:36:27.282340: Epoch time: 39.94 s\n",
      "2024-01-10 14:36:28.457822: \n",
      "2024-01-10 14:36:28.462822: Epoch 913\n",
      "2024-01-10 14:36:28.471043: Current learning rate: 0.00111\n",
      "2024-01-10 14:37:08.331447: train_loss -0.9566\n",
      "2024-01-10 14:37:08.340454: val_loss -0.7963\n",
      "2024-01-10 14:37:08.348972: Pseudo dice [0.9221, 0.9287, 0.9373]\n",
      "2024-01-10 14:37:08.376961: Epoch time: 39.87 s\n",
      "2024-01-10 14:37:09.478944: \n",
      "2024-01-10 14:37:09.486660: Epoch 914\n",
      "2024-01-10 14:37:09.494827: Current learning rate: 0.0011\n",
      "2024-01-10 14:37:49.286223: train_loss -0.9563\n",
      "2024-01-10 14:37:49.296223: val_loss -0.7957\n",
      "2024-01-10 14:37:49.322753: Pseudo dice [0.9191, 0.9299, 0.9376]\n",
      "2024-01-10 14:37:49.329748: Epoch time: 39.81 s\n",
      "2024-01-10 14:37:50.475238: \n",
      "2024-01-10 14:37:50.482538: Epoch 915\n",
      "2024-01-10 14:37:50.488059: Current learning rate: 0.00109\n",
      "2024-01-10 14:38:30.157705: train_loss -0.9561\n",
      "2024-01-10 14:38:30.166706: val_loss -0.794\n",
      "2024-01-10 14:38:30.176706: Pseudo dice [0.9215, 0.929, 0.9372]\n",
      "2024-01-10 14:38:30.206706: Epoch time: 39.68 s\n",
      "2024-01-10 14:38:31.359790: \n",
      "2024-01-10 14:38:31.364207: Epoch 916\n",
      "2024-01-10 14:38:31.369208: Current learning rate: 0.00108\n",
      "2024-01-10 14:39:11.123186: train_loss -0.9567\n",
      "2024-01-10 14:39:11.131187: val_loss -0.7962\n",
      "2024-01-10 14:39:11.141179: Pseudo dice [0.9198, 0.9289, 0.9367]\n",
      "2024-01-10 14:39:11.176713: Epoch time: 39.76 s\n",
      "2024-01-10 14:39:12.328408: \n",
      "2024-01-10 14:39:12.339820: Epoch 917\n",
      "2024-01-10 14:39:12.343814: Current learning rate: 0.00106\n",
      "2024-01-10 14:39:52.316201: train_loss -0.956\n",
      "2024-01-10 14:39:52.325202: val_loss -0.7943\n",
      "2024-01-10 14:39:52.334201: Pseudo dice [0.921, 0.9296, 0.9385]\n",
      "2024-01-10 14:39:52.343200: Epoch time: 39.99 s\n",
      "2024-01-10 14:39:53.503149: \n",
      "2024-01-10 14:39:53.511117: Epoch 918\n",
      "2024-01-10 14:39:53.517072: Current learning rate: 0.00105\n",
      "2024-01-10 14:40:33.265146: train_loss -0.956\n",
      "2024-01-10 14:40:33.275151: val_loss -0.7961\n",
      "2024-01-10 14:40:33.281153: Pseudo dice [0.9182, 0.9291, 0.9364]\n",
      "2024-01-10 14:40:33.289153: Epoch time: 39.76 s\n",
      "2024-01-10 14:40:34.479741: \n",
      "2024-01-10 14:40:34.484352: Epoch 919\n",
      "2024-01-10 14:40:34.489353: Current learning rate: 0.00104\n",
      "2024-01-10 14:41:14.719945: train_loss -0.9555\n",
      "2024-01-10 14:41:14.727945: val_loss -0.8009\n",
      "2024-01-10 14:41:14.735946: Pseudo dice [0.9206, 0.9303, 0.9389]\n",
      "2024-01-10 14:41:14.770946: Epoch time: 40.24 s\n",
      "2024-01-10 14:41:15.975831: \n",
      "2024-01-10 14:41:15.980823: Epoch 920\n",
      "2024-01-10 14:41:15.985716: Current learning rate: 0.00103\n",
      "2024-01-10 14:41:56.134197: train_loss -0.9556\n",
      "2024-01-10 14:41:56.165196: val_loss -0.7949\n",
      "2024-01-10 14:41:56.173196: Pseudo dice [0.9204, 0.9285, 0.9378]\n",
      "2024-01-10 14:41:56.181198: Epoch time: 40.16 s\n",
      "2024-01-10 14:41:57.341259: \n",
      "2024-01-10 14:41:57.346794: Epoch 921\n",
      "2024-01-10 14:41:57.351813: Current learning rate: 0.00102\n",
      "2024-01-10 14:42:37.704804: train_loss -0.9565\n",
      "2024-01-10 14:42:37.714210: val_loss -0.7944\n",
      "2024-01-10 14:42:37.748346: Pseudo dice [0.9224, 0.9289, 0.9374]\n",
      "2024-01-10 14:42:37.758451: Epoch time: 40.36 s\n",
      "2024-01-10 14:42:39.062139: \n",
      "2024-01-10 14:42:39.068142: Epoch 922\n",
      "2024-01-10 14:42:39.073141: Current learning rate: 0.00101\n",
      "2024-01-10 14:43:19.577040: train_loss -0.9559\n",
      "2024-01-10 14:43:19.590048: val_loss -0.793\n",
      "2024-01-10 14:43:19.610575: Pseudo dice [0.9216, 0.9277, 0.9371]\n",
      "2024-01-10 14:43:19.620580: Epoch time: 40.52 s\n",
      "2024-01-10 14:43:20.805715: \n",
      "2024-01-10 14:43:20.810707: Epoch 923\n",
      "2024-01-10 14:43:20.815339: Current learning rate: 0.001\n",
      "2024-01-10 14:44:01.108182: train_loss -0.9559\n",
      "2024-01-10 14:44:01.118185: val_loss -0.7947\n",
      "2024-01-10 14:44:01.124182: Pseudo dice [0.9213, 0.9279, 0.9367]\n",
      "2024-01-10 14:44:01.131183: Epoch time: 40.3 s\n",
      "2024-01-10 14:44:02.284164: \n",
      "2024-01-10 14:44:02.290783: Epoch 924\n",
      "2024-01-10 14:44:02.294724: Current learning rate: 0.00098\n",
      "2024-01-10 14:44:42.437232: train_loss -0.9569\n",
      "2024-01-10 14:44:42.446231: val_loss -0.8022\n",
      "2024-01-10 14:44:42.454231: Pseudo dice [0.9211, 0.9303, 0.9387]\n",
      "2024-01-10 14:44:42.463233: Epoch time: 40.15 s\n",
      "2024-01-10 14:44:43.645424: \n",
      "2024-01-10 14:44:43.654431: Epoch 925\n",
      "2024-01-10 14:44:43.659428: Current learning rate: 0.00097\n",
      "2024-01-10 14:45:23.898877: train_loss -0.9562\n",
      "2024-01-10 14:45:23.908877: val_loss -0.7939\n",
      "2024-01-10 14:45:23.940392: Pseudo dice [0.9218, 0.9283, 0.9367]\n",
      "2024-01-10 14:45:23.949395: Epoch time: 40.25 s\n",
      "2024-01-10 14:45:25.091272: \n",
      "2024-01-10 14:45:25.099541: Epoch 926\n",
      "2024-01-10 14:45:25.103623: Current learning rate: 0.00096\n",
      "2024-01-10 14:46:06.682096: train_loss -0.9559\n",
      "2024-01-10 14:46:06.690094: val_loss -0.7899\n",
      "2024-01-10 14:46:06.698143: Pseudo dice [0.9205, 0.9267, 0.9355]\n",
      "2024-01-10 14:46:06.738391: Epoch time: 41.59 s\n",
      "2024-01-10 14:46:08.165161: \n",
      "2024-01-10 14:46:08.171175: Epoch 927\n",
      "2024-01-10 14:46:08.176162: Current learning rate: 0.00095\n",
      "2024-01-10 14:46:50.085037: train_loss -0.9563\n",
      "2024-01-10 14:46:50.122547: val_loss -0.7952\n",
      "2024-01-10 14:46:50.130548: Pseudo dice [0.9194, 0.9287, 0.937]\n",
      "2024-01-10 14:46:50.138547: Epoch time: 41.92 s\n",
      "2024-01-10 14:46:51.521021: \n",
      "2024-01-10 14:46:51.530101: Epoch 928\n",
      "2024-01-10 14:46:51.535145: Current learning rate: 0.00094\n",
      "2024-01-10 14:47:33.405499: train_loss -0.9564\n",
      "2024-01-10 14:47:33.417338: val_loss -0.7958\n",
      "2024-01-10 14:47:33.427339: Pseudo dice [0.9229, 0.9292, 0.9372]\n",
      "2024-01-10 14:47:33.440339: Epoch time: 41.89 s\n",
      "2024-01-10 14:47:34.911479: \n",
      "2024-01-10 14:47:34.917498: Epoch 929\n",
      "2024-01-10 14:47:34.921473: Current learning rate: 0.00092\n",
      "2024-01-10 14:48:16.433829: train_loss -0.957\n",
      "2024-01-10 14:48:16.440828: val_loss -0.7951\n",
      "2024-01-10 14:48:16.446828: Pseudo dice [0.9207, 0.9287, 0.9374]\n",
      "2024-01-10 14:48:16.453829: Epoch time: 41.52 s\n",
      "2024-01-10 14:48:17.926003: \n",
      "2024-01-10 14:48:17.932003: Epoch 930\n",
      "2024-01-10 14:48:17.938009: Current learning rate: 0.00091\n",
      "2024-01-10 14:49:00.372803: train_loss -0.9565\n",
      "2024-01-10 14:49:00.384800: val_loss -0.7952\n",
      "2024-01-10 14:49:00.422801: Pseudo dice [0.9241, 0.9296, 0.9375]\n",
      "2024-01-10 14:49:00.432809: Epoch time: 42.45 s\n",
      "2024-01-10 14:49:02.224787: \n",
      "2024-01-10 14:49:02.232787: Epoch 931\n",
      "2024-01-10 14:49:02.240792: Current learning rate: 0.0009\n",
      "2024-01-10 14:49:45.926211: train_loss -0.9574\n",
      "2024-01-10 14:49:45.934210: val_loss -0.7953\n",
      "2024-01-10 14:49:45.943354: Pseudo dice [0.9223, 0.9287, 0.9362]\n",
      "2024-01-10 14:49:45.950357: Epoch time: 43.7 s\n",
      "2024-01-10 14:49:47.399190: \n",
      "2024-01-10 14:49:47.407748: Epoch 932\n",
      "2024-01-10 14:49:47.412755: Current learning rate: 0.00089\n",
      "2024-01-10 14:50:29.817673: train_loss -0.9565\n",
      "2024-01-10 14:50:29.846668: val_loss -0.795\n",
      "2024-01-10 14:50:29.855669: Pseudo dice [0.9224, 0.9282, 0.9367]\n",
      "2024-01-10 14:50:29.863670: Epoch time: 42.42 s\n",
      "2024-01-10 14:50:31.277764: \n",
      "2024-01-10 14:50:31.283926: Epoch 933\n",
      "2024-01-10 14:50:31.287982: Current learning rate: 0.00088\n",
      "2024-01-10 14:51:12.160713: train_loss -0.9574\n",
      "2024-01-10 14:51:12.168715: val_loss -0.7935\n",
      "2024-01-10 14:51:12.176713: Pseudo dice [0.9222, 0.9275, 0.9363]\n",
      "2024-01-10 14:51:12.183714: Epoch time: 40.88 s\n",
      "2024-01-10 14:51:13.668478: \n",
      "2024-01-10 14:51:13.676481: Epoch 934\n",
      "2024-01-10 14:51:13.681424: Current learning rate: 0.00087\n",
      "2024-01-10 14:51:54.888016: train_loss -0.9572\n",
      "2024-01-10 14:51:54.900015: val_loss -0.7903\n",
      "2024-01-10 14:51:54.909017: Pseudo dice [0.9217, 0.9293, 0.937]\n",
      "2024-01-10 14:51:54.918018: Epoch time: 41.22 s\n",
      "2024-01-10 14:51:56.463948: \n",
      "2024-01-10 14:51:56.475894: Epoch 935\n",
      "2024-01-10 14:51:56.482890: Current learning rate: 0.00085\n",
      "2024-01-10 14:52:37.635373: train_loss -0.9566\n",
      "2024-01-10 14:52:37.647372: val_loss -0.7913\n",
      "2024-01-10 14:52:37.656371: Pseudo dice [0.9197, 0.9276, 0.9355]\n",
      "2024-01-10 14:52:37.697889: Epoch time: 41.17 s\n",
      "2024-01-10 14:52:39.035030: \n",
      "2024-01-10 14:52:39.043966: Epoch 936\n",
      "2024-01-10 14:52:39.048041: Current learning rate: 0.00084\n",
      "2024-01-10 14:53:20.053055: train_loss -0.9574\n",
      "2024-01-10 14:53:20.064055: val_loss -0.791\n",
      "2024-01-10 14:53:20.074058: Pseudo dice [0.9202, 0.9273, 0.9365]\n",
      "2024-01-10 14:53:20.082057: Epoch time: 41.02 s\n",
      "2024-01-10 14:53:21.549073: \n",
      "2024-01-10 14:53:21.554073: Epoch 937\n",
      "2024-01-10 14:53:21.559690: Current learning rate: 0.00083\n",
      "2024-01-10 14:54:06.271805: train_loss -0.9567\n",
      "2024-01-10 14:54:06.281806: val_loss -0.796\n",
      "2024-01-10 14:54:06.288807: Pseudo dice [0.9197, 0.9283, 0.9366]\n",
      "2024-01-10 14:54:06.295806: Epoch time: 44.72 s\n",
      "2024-01-10 14:54:07.907799: \n",
      "2024-01-10 14:54:07.913945: Epoch 938\n",
      "2024-01-10 14:54:07.922089: Current learning rate: 0.00082\n",
      "2024-01-10 14:54:49.199872: train_loss -0.9563\n",
      "2024-01-10 14:54:49.210863: val_loss -0.7897\n",
      "2024-01-10 14:54:49.238518: Pseudo dice [0.9173, 0.9288, 0.9373]\n",
      "2024-01-10 14:54:49.246521: Epoch time: 41.29 s\n",
      "2024-01-10 14:54:50.662009: \n",
      "2024-01-10 14:54:50.669004: Epoch 939\n",
      "2024-01-10 14:54:50.674992: Current learning rate: 0.00081\n",
      "2024-01-10 14:55:31.960679: train_loss -0.9571\n",
      "2024-01-10 14:55:32.000176: val_loss -0.7943\n",
      "2024-01-10 14:55:32.011695: Pseudo dice [0.9233, 0.9285, 0.9363]\n",
      "2024-01-10 14:55:32.020216: Epoch time: 41.3 s\n",
      "2024-01-10 14:55:33.509533: \n",
      "2024-01-10 14:55:33.514993: Epoch 940\n",
      "2024-01-10 14:55:33.521924: Current learning rate: 0.00079\n",
      "2024-01-10 14:56:14.643438: train_loss -0.9562\n",
      "2024-01-10 14:56:14.651436: val_loss -0.7986\n",
      "2024-01-10 14:56:14.657436: Pseudo dice [0.9222, 0.9291, 0.9379]\n",
      "2024-01-10 14:56:14.663435: Epoch time: 41.13 s\n",
      "2024-01-10 14:56:16.038732: \n",
      "2024-01-10 14:56:16.047948: Epoch 941\n",
      "2024-01-10 14:56:16.051908: Current learning rate: 0.00078\n",
      "2024-01-10 14:56:57.030052: train_loss -0.9579\n",
      "2024-01-10 14:56:57.062581: val_loss -0.7949\n",
      "2024-01-10 14:56:57.072581: Pseudo dice [0.921, 0.9286, 0.9363]\n",
      "2024-01-10 14:56:57.080280: Epoch time: 40.99 s\n",
      "2024-01-10 14:56:58.442601: \n",
      "2024-01-10 14:56:58.450689: Epoch 942\n",
      "2024-01-10 14:56:58.454669: Current learning rate: 0.00077\n",
      "2024-01-10 14:57:40.095703: train_loss -0.9573\n",
      "2024-01-10 14:57:40.104703: val_loss -0.7993\n",
      "2024-01-10 14:57:40.111703: Pseudo dice [0.9234, 0.9289, 0.9378]\n",
      "2024-01-10 14:57:40.150232: Epoch time: 41.66 s\n",
      "2024-01-10 14:57:41.553288: \n",
      "2024-01-10 14:57:41.560325: Epoch 943\n",
      "2024-01-10 14:57:41.565329: Current learning rate: 0.00076\n",
      "2024-01-10 14:58:24.698116: train_loss -0.9568\n",
      "2024-01-10 14:58:24.709121: val_loss -0.7912\n",
      "2024-01-10 14:58:24.716119: Pseudo dice [0.921, 0.9279, 0.936]\n",
      "2024-01-10 14:58:24.722117: Epoch time: 43.15 s\n",
      "2024-01-10 14:58:26.235107: \n",
      "2024-01-10 14:58:26.240107: Epoch 944\n",
      "2024-01-10 14:58:26.245099: Current learning rate: 0.00075\n",
      "2024-01-10 14:59:08.082787: train_loss -0.9561\n",
      "2024-01-10 14:59:08.090788: val_loss -0.7922\n",
      "2024-01-10 14:59:08.098787: Pseudo dice [0.921, 0.928, 0.9362]\n",
      "2024-01-10 14:59:08.102786: Epoch time: 41.85 s\n",
      "2024-01-10 14:59:09.521756: \n",
      "2024-01-10 14:59:09.531031: Epoch 945\n",
      "2024-01-10 14:59:09.538083: Current learning rate: 0.00074\n",
      "2024-01-10 14:59:50.687424: train_loss -0.9566\n",
      "2024-01-10 14:59:50.697426: val_loss -0.7887\n",
      "2024-01-10 14:59:50.704426: Pseudo dice [0.919, 0.9271, 0.9356]\n",
      "2024-01-10 14:59:50.711425: Epoch time: 41.17 s\n",
      "2024-01-10 14:59:52.110020: \n",
      "2024-01-10 14:59:52.115278: Epoch 946\n",
      "2024-01-10 14:59:52.122344: Current learning rate: 0.00072\n",
      "2024-01-10 15:00:35.729502: train_loss -0.9578\n",
      "2024-01-10 15:00:35.740503: val_loss -0.7927\n",
      "2024-01-10 15:00:35.770062: Pseudo dice [0.9212, 0.9275, 0.9366]\n",
      "2024-01-10 15:00:35.779063: Epoch time: 43.62 s\n",
      "2024-01-10 15:00:37.313312: \n",
      "2024-01-10 15:00:37.318387: Epoch 947\n",
      "2024-01-10 15:00:37.322943: Current learning rate: 0.00071\n",
      "2024-01-10 15:01:19.203263: train_loss -0.957\n",
      "2024-01-10 15:01:19.210265: val_loss -0.7988\n",
      "2024-01-10 15:01:19.218270: Pseudo dice [0.9237, 0.9288, 0.9373]\n",
      "2024-01-10 15:01:19.226269: Epoch time: 41.89 s\n",
      "2024-01-10 15:01:20.714324: \n",
      "2024-01-10 15:01:20.724333: Epoch 948\n",
      "2024-01-10 15:01:20.730356: Current learning rate: 0.0007\n",
      "2024-01-10 15:02:01.742479: train_loss -0.9573\n",
      "2024-01-10 15:02:01.771489: val_loss -0.787\n",
      "2024-01-10 15:02:01.781002: Pseudo dice [0.9198, 0.9274, 0.9349]\n",
      "2024-01-10 15:02:01.788002: Epoch time: 41.03 s\n",
      "2024-01-10 15:02:03.258623: \n",
      "2024-01-10 15:02:03.266630: Epoch 949\n",
      "2024-01-10 15:02:03.272521: Current learning rate: 0.00069\n",
      "2024-01-10 15:02:44.274357: train_loss -0.9575\n",
      "2024-01-10 15:02:44.284699: val_loss -0.7942\n",
      "2024-01-10 15:02:44.290708: Pseudo dice [0.9197, 0.9281, 0.9359]\n",
      "2024-01-10 15:02:44.296708: Epoch time: 41.02 s\n",
      "2024-01-10 15:02:45.978879: \n",
      "2024-01-10 15:02:45.986351: Epoch 950\n",
      "2024-01-10 15:02:45.990349: Current learning rate: 0.00067\n",
      "2024-01-10 15:03:27.282283: train_loss -0.9575\n",
      "2024-01-10 15:03:27.291282: val_loss -0.7931\n",
      "2024-01-10 15:03:27.301284: Pseudo dice [0.9208, 0.9276, 0.9362]\n",
      "2024-01-10 15:03:27.309286: Epoch time: 41.3 s\n",
      "2024-01-10 15:03:28.842967: \n",
      "2024-01-10 15:03:28.851636: Epoch 951\n",
      "2024-01-10 15:03:28.858631: Current learning rate: 0.00066\n",
      "2024-01-10 15:04:09.885663: train_loss -0.9572\n",
      "2024-01-10 15:04:09.894665: val_loss -0.7884\n",
      "2024-01-10 15:04:09.901664: Pseudo dice [0.9187, 0.9283, 0.937]\n",
      "2024-01-10 15:04:09.907666: Epoch time: 41.05 s\n",
      "2024-01-10 15:04:11.314123: \n",
      "2024-01-10 15:04:11.322391: Epoch 952\n",
      "2024-01-10 15:04:11.329349: Current learning rate: 0.00065\n",
      "2024-01-10 15:04:53.320068: train_loss -0.9579\n",
      "2024-01-10 15:04:53.330067: val_loss -0.7922\n",
      "2024-01-10 15:04:53.343111: Pseudo dice [0.9187, 0.9276, 0.936]\n",
      "2024-01-10 15:04:53.351681: Epoch time: 42.01 s\n",
      "2024-01-10 15:04:54.696763: \n",
      "2024-01-10 15:04:54.703753: Epoch 953\n",
      "2024-01-10 15:04:54.710761: Current learning rate: 0.00064\n",
      "2024-01-10 15:05:37.085633: train_loss -0.9572\n",
      "2024-01-10 15:05:37.093630: val_loss -0.7917\n",
      "2024-01-10 15:05:37.130182: Pseudo dice [0.92, 0.9278, 0.9365]\n",
      "2024-01-10 15:05:37.136730: Epoch time: 42.39 s\n",
      "2024-01-10 15:05:38.603358: \n",
      "2024-01-10 15:05:38.608830: Epoch 954\n",
      "2024-01-10 15:05:38.616858: Current learning rate: 0.00063\n",
      "2024-01-10 15:06:20.700400: train_loss -0.9574\n",
      "2024-01-10 15:06:20.708404: val_loss -0.7932\n",
      "2024-01-10 15:06:20.718003: Pseudo dice [0.9204, 0.9273, 0.9359]\n",
      "2024-01-10 15:06:20.756576: Epoch time: 42.1 s\n",
      "2024-01-10 15:06:22.196056: \n",
      "2024-01-10 15:06:22.203574: Epoch 955\n",
      "2024-01-10 15:06:22.208577: Current learning rate: 0.00061\n",
      "2024-01-10 15:07:04.188756: train_loss -0.9576\n",
      "2024-01-10 15:07:04.196786: val_loss -0.7909\n",
      "2024-01-10 15:07:04.205306: Pseudo dice [0.9198, 0.9282, 0.9371]\n",
      "2024-01-10 15:07:04.243381: Epoch time: 41.99 s\n",
      "2024-01-10 15:07:05.672990: \n",
      "2024-01-10 15:07:05.678515: Epoch 956\n",
      "2024-01-10 15:07:05.683527: Current learning rate: 0.0006\n",
      "2024-01-10 15:07:49.038107: train_loss -0.9573\n",
      "2024-01-10 15:07:49.049109: val_loss -0.7935\n",
      "2024-01-10 15:07:49.057107: Pseudo dice [0.9182, 0.9294, 0.938]\n",
      "2024-01-10 15:07:49.065107: Epoch time: 43.37 s\n",
      "2024-01-10 15:07:50.520795: \n",
      "2024-01-10 15:07:50.526121: Epoch 957\n",
      "2024-01-10 15:07:50.531133: Current learning rate: 0.00059\n",
      "2024-01-10 15:08:33.272689: train_loss -0.9574\n",
      "2024-01-10 15:08:33.280692: val_loss -0.7893\n",
      "2024-01-10 15:08:33.286691: Pseudo dice [0.9183, 0.9275, 0.9365]\n",
      "2024-01-10 15:08:33.294688: Epoch time: 42.75 s\n",
      "2024-01-10 15:08:34.952920: \n",
      "2024-01-10 15:08:34.959312: Epoch 958\n",
      "2024-01-10 15:08:34.963742: Current learning rate: 0.00058\n",
      "2024-01-10 15:09:17.274640: train_loss -0.957\n",
      "2024-01-10 15:09:17.288818: val_loss -0.7951\n",
      "2024-01-10 15:09:17.304461: Pseudo dice [0.9197, 0.9281, 0.9361]\n",
      "2024-01-10 15:09:17.320657: Epoch time: 42.32 s\n",
      "2024-01-10 15:09:19.303513: \n",
      "2024-01-10 15:09:19.308542: Epoch 959\n",
      "2024-01-10 15:09:19.312605: Current learning rate: 0.00056\n",
      "2024-01-10 15:10:02.027322: train_loss -0.9571\n",
      "2024-01-10 15:10:02.035444: val_loss -0.7969\n",
      "2024-01-10 15:10:02.044591: Pseudo dice [0.9214, 0.9289, 0.9368]\n",
      "2024-01-10 15:10:02.051642: Epoch time: 42.72 s\n",
      "2024-01-10 15:10:03.526453: \n",
      "2024-01-10 15:10:03.531959: Epoch 960\n",
      "2024-01-10 15:10:03.537060: Current learning rate: 0.00055\n",
      "2024-01-10 15:10:45.298917: train_loss -0.9577\n",
      "2024-01-10 15:10:45.309909: val_loss -0.796\n",
      "2024-01-10 15:10:45.357650: Pseudo dice [0.9216, 0.9283, 0.9363]\n",
      "2024-01-10 15:10:45.369252: Epoch time: 41.77 s\n",
      "2024-01-10 15:10:46.819153: \n",
      "2024-01-10 15:10:46.824146: Epoch 961\n",
      "2024-01-10 15:10:46.829154: Current learning rate: 0.00054\n",
      "2024-01-10 15:11:29.461979: train_loss -0.9576\n",
      "2024-01-10 15:11:29.472978: val_loss -0.7938\n",
      "2024-01-10 15:11:29.478981: Pseudo dice [0.9233, 0.9281, 0.9362]\n",
      "2024-01-10 15:11:29.485978: Epoch time: 42.64 s\n",
      "2024-01-10 15:11:31.084517: \n",
      "2024-01-10 15:11:31.089682: Epoch 962\n",
      "2024-01-10 15:11:31.095688: Current learning rate: 0.00053\n",
      "2024-01-10 15:12:12.736274: train_loss -0.9576\n",
      "2024-01-10 15:12:12.744131: val_loss -0.7892\n",
      "2024-01-10 15:12:12.752137: Pseudo dice [0.9172, 0.9272, 0.9363]\n",
      "2024-01-10 15:12:12.761263: Epoch time: 41.65 s\n",
      "2024-01-10 15:12:14.148619: \n",
      "2024-01-10 15:12:14.153299: Epoch 963\n",
      "2024-01-10 15:12:14.161247: Current learning rate: 0.00051\n",
      "2024-01-10 15:12:55.762135: train_loss -0.9576\n",
      "2024-01-10 15:12:55.776562: val_loss -0.792\n",
      "2024-01-10 15:12:55.787563: Pseudo dice [0.9195, 0.9277, 0.9363]\n",
      "2024-01-10 15:12:55.813560: Epoch time: 41.61 s\n",
      "2024-01-10 15:12:57.375057: \n",
      "2024-01-10 15:12:57.379739: Epoch 964\n",
      "2024-01-10 15:12:57.384871: Current learning rate: 0.0005\n",
      "2024-01-10 15:13:38.929274: train_loss -0.957\n",
      "2024-01-10 15:13:38.965487: val_loss -0.7934\n",
      "2024-01-10 15:13:38.974484: Pseudo dice [0.9248, 0.9282, 0.9366]\n",
      "2024-01-10 15:13:38.982485: Epoch time: 41.56 s\n",
      "2024-01-10 15:13:40.402084: \n",
      "2024-01-10 15:13:40.407985: Epoch 965\n",
      "2024-01-10 15:13:40.416991: Current learning rate: 0.00049\n",
      "2024-01-10 15:14:21.356345: train_loss -0.9573\n",
      "2024-01-10 15:14:21.364357: val_loss -0.7947\n",
      "2024-01-10 15:14:21.374351: Pseudo dice [0.9191, 0.9289, 0.9376]\n",
      "2024-01-10 15:14:21.383713: Epoch time: 40.96 s\n",
      "2024-01-10 15:14:22.969244: \n",
      "2024-01-10 15:14:22.975171: Epoch 966\n",
      "2024-01-10 15:14:22.980170: Current learning rate: 0.00048\n",
      "2024-01-10 15:15:05.337042: train_loss -0.9576\n",
      "2024-01-10 15:15:05.374486: val_loss -0.7898\n",
      "2024-01-10 15:15:05.384126: Pseudo dice [0.9187, 0.928, 0.936]\n",
      "2024-01-10 15:15:05.390587: Epoch time: 42.37 s\n",
      "2024-01-10 15:15:06.966773: \n",
      "2024-01-10 15:15:06.973819: Epoch 967\n",
      "2024-01-10 15:15:06.981897: Current learning rate: 0.00046\n",
      "2024-01-10 15:15:49.108634: train_loss -0.958\n",
      "2024-01-10 15:15:49.134164: val_loss -0.7959\n",
      "2024-01-10 15:15:49.141197: Pseudo dice [0.9205, 0.9292, 0.9386]\n",
      "2024-01-10 15:15:49.150203: Epoch time: 42.14 s\n",
      "2024-01-10 15:15:50.549393: \n",
      "2024-01-10 15:15:50.555399: Epoch 968\n",
      "2024-01-10 15:15:50.560404: Current learning rate: 0.00045\n",
      "2024-01-10 15:16:31.703804: train_loss -0.9578\n",
      "2024-01-10 15:16:31.709787: val_loss -0.7934\n",
      "2024-01-10 15:16:31.720057: Pseudo dice [0.9208, 0.9285, 0.9372]\n",
      "2024-01-10 15:16:31.747703: Epoch time: 41.16 s\n",
      "2024-01-10 15:16:33.082597: \n",
      "2024-01-10 15:16:33.088971: Epoch 969\n",
      "2024-01-10 15:16:33.093045: Current learning rate: 0.00044\n",
      "2024-01-10 15:17:14.083110: train_loss -0.9578\n",
      "2024-01-10 15:17:14.093511: val_loss -0.7956\n",
      "2024-01-10 15:17:14.103021: Pseudo dice [0.9236, 0.9292, 0.9373]\n",
      "2024-01-10 15:17:14.108024: Epoch time: 41.0 s\n",
      "2024-01-10 15:17:15.658005: \n",
      "2024-01-10 15:17:15.663136: Epoch 970\n",
      "2024-01-10 15:17:15.668148: Current learning rate: 0.00043\n",
      "2024-01-10 15:17:56.640033: train_loss -0.9578\n",
      "2024-01-10 15:17:56.648032: val_loss -0.7928\n",
      "2024-01-10 15:17:56.656033: Pseudo dice [0.921, 0.9272, 0.9363]\n",
      "2024-01-10 15:17:56.663031: Epoch time: 40.98 s\n",
      "2024-01-10 15:17:58.145367: \n",
      "2024-01-10 15:17:58.154888: Epoch 971\n",
      "2024-01-10 15:17:58.159888: Current learning rate: 0.00041\n",
      "2024-01-10 15:18:39.190114: train_loss -0.9577\n",
      "2024-01-10 15:18:39.199115: val_loss -0.7941\n",
      "2024-01-10 15:18:39.206121: Pseudo dice [0.9211, 0.9287, 0.9371]\n",
      "2024-01-10 15:18:39.215120: Epoch time: 41.05 s\n",
      "2024-01-10 15:18:40.777940: \n",
      "2024-01-10 15:18:40.782936: Epoch 972\n",
      "2024-01-10 15:18:40.787252: Current learning rate: 0.0004\n",
      "2024-01-10 15:19:22.425676: train_loss -0.9581\n",
      "2024-01-10 15:19:22.437680: val_loss -0.7926\n",
      "2024-01-10 15:19:22.447716: Pseudo dice [0.9218, 0.9278, 0.9361]\n",
      "2024-01-10 15:19:22.452719: Epoch time: 41.65 s\n",
      "2024-01-10 15:19:23.901796: \n",
      "2024-01-10 15:19:23.907950: Epoch 973\n",
      "2024-01-10 15:19:23.912951: Current learning rate: 0.00039\n",
      "2024-01-10 15:20:04.987563: train_loss -0.9584\n",
      "2024-01-10 15:20:04.993564: val_loss -0.7967\n",
      "2024-01-10 15:20:05.020979: Pseudo dice [0.9201, 0.9291, 0.9381]\n",
      "2024-01-10 15:20:05.027980: Epoch time: 41.09 s\n",
      "2024-01-10 15:20:06.469180: \n",
      "2024-01-10 15:20:06.473688: Epoch 974\n",
      "2024-01-10 15:20:06.477711: Current learning rate: 0.00037\n",
      "2024-01-10 15:20:47.723365: train_loss -0.9578\n",
      "2024-01-10 15:20:47.780947: val_loss -0.7921\n",
      "2024-01-10 15:20:47.797495: Pseudo dice [0.9239, 0.9277, 0.9366]\n",
      "2024-01-10 15:20:47.843903: Epoch time: 41.25 s\n",
      "2024-01-10 15:20:49.371505: \n",
      "2024-01-10 15:20:49.377316: Epoch 975\n",
      "2024-01-10 15:20:49.381328: Current learning rate: 0.00036\n",
      "2024-01-10 15:21:30.507250: train_loss -0.9586\n",
      "2024-01-10 15:21:30.520288: val_loss -0.794\n",
      "2024-01-10 15:21:30.556814: Pseudo dice [0.9219, 0.9283, 0.9375]\n",
      "2024-01-10 15:21:30.564810: Epoch time: 41.14 s\n",
      "2024-01-10 15:21:32.032577: \n",
      "2024-01-10 15:21:32.038645: Epoch 976\n",
      "2024-01-10 15:21:32.043577: Current learning rate: 0.00035\n",
      "2024-01-10 15:22:13.526383: train_loss -0.9576\n",
      "2024-01-10 15:22:13.536382: val_loss -0.7952\n",
      "2024-01-10 15:22:13.546895: Pseudo dice [0.9199, 0.9288, 0.9375]\n",
      "2024-01-10 15:22:13.557416: Epoch time: 41.49 s\n",
      "2024-01-10 15:22:15.087550: \n",
      "2024-01-10 15:22:15.093061: Epoch 977\n",
      "2024-01-10 15:22:15.098053: Current learning rate: 0.00034\n",
      "2024-01-10 15:22:56.473109: train_loss -0.9583\n",
      "2024-01-10 15:22:56.506330: val_loss -0.7954\n",
      "2024-01-10 15:22:56.516850: Pseudo dice [0.9197, 0.9287, 0.9371]\n",
      "2024-01-10 15:22:56.522853: Epoch time: 41.39 s\n",
      "2024-01-10 15:22:57.907377: \n",
      "2024-01-10 15:22:57.914825: Epoch 978\n",
      "2024-01-10 15:22:57.919809: Current learning rate: 0.00032\n",
      "2024-01-10 15:23:38.934231: train_loss -0.9585\n",
      "2024-01-10 15:23:38.941230: val_loss -0.7946\n",
      "2024-01-10 15:23:38.947741: Pseudo dice [0.9211, 0.9289, 0.9373]\n",
      "2024-01-10 15:23:38.953743: Epoch time: 41.03 s\n",
      "2024-01-10 15:23:40.473693: \n",
      "2024-01-10 15:23:40.479660: Epoch 979\n",
      "2024-01-10 15:23:40.489745: Current learning rate: 0.00031\n",
      "2024-01-10 15:24:21.932379: train_loss -0.9592\n",
      "2024-01-10 15:24:21.946467: val_loss -0.7959\n",
      "2024-01-10 15:24:21.957396: Pseudo dice [0.9197, 0.9291, 0.9369]\n",
      "2024-01-10 15:24:22.011292: Epoch time: 41.46 s\n",
      "2024-01-10 15:24:23.628511: \n",
      "2024-01-10 15:24:23.636940: Epoch 980\n",
      "2024-01-10 15:24:23.642014: Current learning rate: 0.0003\n",
      "2024-01-10 15:25:04.915723: train_loss -0.9582\n",
      "2024-01-10 15:25:04.924720: val_loss -0.7967\n",
      "2024-01-10 15:25:04.930720: Pseudo dice [0.9199, 0.9288, 0.9371]\n",
      "2024-01-10 15:25:04.935720: Epoch time: 41.29 s\n",
      "2024-01-10 15:25:06.330300: \n",
      "2024-01-10 15:25:06.337289: Epoch 981\n",
      "2024-01-10 15:25:06.342298: Current learning rate: 0.00028\n",
      "2024-01-10 15:25:47.340922: train_loss -0.9591\n",
      "2024-01-10 15:25:47.348925: val_loss -0.7907\n",
      "2024-01-10 15:25:47.356927: Pseudo dice [0.918, 0.9273, 0.936]\n",
      "2024-01-10 15:25:47.363924: Epoch time: 41.01 s\n",
      "2024-01-10 15:25:48.790555: \n",
      "2024-01-10 15:25:48.797068: Epoch 982\n",
      "2024-01-10 15:25:48.805160: Current learning rate: 0.00027\n",
      "2024-01-10 15:26:30.478182: train_loss -0.9587\n",
      "2024-01-10 15:26:30.485182: val_loss -0.797\n",
      "2024-01-10 15:26:30.493181: Pseudo dice [0.9216, 0.9289, 0.9379]\n",
      "2024-01-10 15:26:30.520180: Epoch time: 41.69 s\n",
      "2024-01-10 15:26:31.894292: \n",
      "2024-01-10 15:26:31.900297: Epoch 983\n",
      "2024-01-10 15:26:31.906293: Current learning rate: 0.00026\n",
      "2024-01-10 15:27:13.508789: train_loss -0.9584\n",
      "2024-01-10 15:27:13.520468: val_loss -0.7918\n",
      "2024-01-10 15:27:13.532317: Pseudo dice [0.918, 0.9283, 0.9371]\n",
      "2024-01-10 15:27:13.541676: Epoch time: 41.62 s\n",
      "2024-01-10 15:27:15.127104: \n",
      "2024-01-10 15:27:15.132156: Epoch 984\n",
      "2024-01-10 15:27:15.136216: Current learning rate: 0.00024\n",
      "2024-01-10 15:27:56.791996: train_loss -0.9581\n",
      "2024-01-10 15:27:56.802512: val_loss -0.7926\n",
      "2024-01-10 15:27:56.812770: Pseudo dice [0.9215, 0.9281, 0.9366]\n",
      "2024-01-10 15:27:56.818770: Epoch time: 41.67 s\n",
      "2024-01-10 15:27:58.291670: \n",
      "2024-01-10 15:27:58.300187: Epoch 985\n",
      "2024-01-10 15:27:58.306180: Current learning rate: 0.00023\n",
      "2024-01-10 15:28:40.279035: train_loss -0.9579\n",
      "2024-01-10 15:28:40.289407: val_loss -0.7927\n",
      "2024-01-10 15:28:40.299847: Pseudo dice [0.9181, 0.9282, 0.9375]\n",
      "2024-01-10 15:28:40.311141: Epoch time: 41.99 s\n",
      "2024-01-10 15:28:41.830284: \n",
      "2024-01-10 15:28:41.837836: Epoch 986\n",
      "2024-01-10 15:28:41.844835: Current learning rate: 0.00021\n",
      "2024-01-10 15:29:24.613988: train_loss -0.9576\n",
      "2024-01-10 15:29:24.623990: val_loss -0.7933\n",
      "2024-01-10 15:29:24.632989: Pseudo dice [0.9209, 0.9283, 0.9363]\n",
      "2024-01-10 15:29:24.640992: Epoch time: 42.78 s\n",
      "2024-01-10 15:29:25.975592: \n",
      "2024-01-10 15:29:25.983593: Epoch 987\n",
      "2024-01-10 15:29:25.987738: Current learning rate: 0.0002\n",
      "2024-01-10 15:30:07.366051: train_loss -0.9584\n",
      "2024-01-10 15:30:07.375062: val_loss -0.7934\n",
      "2024-01-10 15:30:07.382069: Pseudo dice [0.9192, 0.9288, 0.9375]\n",
      "2024-01-10 15:30:07.388068: Epoch time: 41.39 s\n",
      "2024-01-10 15:30:08.847440: \n",
      "2024-01-10 15:30:08.852563: Epoch 988\n",
      "2024-01-10 15:30:08.856903: Current learning rate: 0.00019\n",
      "2024-01-10 15:30:50.803427: train_loss -0.9586\n",
      "2024-01-10 15:30:50.812935: val_loss -0.7953\n",
      "2024-01-10 15:30:50.820935: Pseudo dice [0.9208, 0.9285, 0.9371]\n",
      "2024-01-10 15:30:50.827937: Epoch time: 41.96 s\n",
      "2024-01-10 15:30:52.210587: \n",
      "2024-01-10 15:30:52.217645: Epoch 989\n",
      "2024-01-10 15:30:52.223620: Current learning rate: 0.00017\n",
      "2024-01-10 15:31:33.076285: train_loss -0.9582\n",
      "2024-01-10 15:31:33.082290: val_loss -0.7946\n",
      "2024-01-10 15:31:33.089286: Pseudo dice [0.9191, 0.9282, 0.9368]\n",
      "2024-01-10 15:31:33.095287: Epoch time: 40.87 s\n",
      "2024-01-10 15:31:34.497398: \n",
      "2024-01-10 15:31:34.506541: Epoch 990\n",
      "2024-01-10 15:31:34.515574: Current learning rate: 0.00016\n",
      "2024-01-10 15:32:15.968087: train_loss -0.958\n",
      "2024-01-10 15:32:15.977089: val_loss -0.7954\n",
      "2024-01-10 15:32:16.011695: Pseudo dice [0.9182, 0.929, 0.9366]\n",
      "2024-01-10 15:32:16.019694: Epoch time: 41.47 s\n",
      "2024-01-10 15:32:17.323386: \n",
      "2024-01-10 15:32:17.328386: Epoch 991\n",
      "2024-01-10 15:32:17.333186: Current learning rate: 0.00014\n",
      "2024-01-10 15:32:58.298566: train_loss -0.9585\n",
      "2024-01-10 15:32:58.312564: val_loss -0.7899\n",
      "2024-01-10 15:32:58.324565: Pseudo dice [0.9217, 0.928, 0.9367]\n",
      "2024-01-10 15:32:58.354669: Epoch time: 40.98 s\n",
      "2024-01-10 15:32:59.917907: \n",
      "2024-01-10 15:32:59.923389: Epoch 992\n",
      "2024-01-10 15:32:59.934474: Current learning rate: 0.00013\n",
      "2024-01-10 15:33:41.057305: train_loss -0.959\n",
      "2024-01-10 15:33:41.066306: val_loss -0.7943\n",
      "2024-01-10 15:33:41.073306: Pseudo dice [0.9168, 0.929, 0.9381]\n",
      "2024-01-10 15:33:41.079307: Epoch time: 41.14 s\n",
      "2024-01-10 15:33:42.605209: \n",
      "2024-01-10 15:33:42.611269: Epoch 993\n",
      "2024-01-10 15:33:42.619343: Current learning rate: 0.00011\n",
      "2024-01-10 15:34:24.765411: train_loss -0.9583\n",
      "2024-01-10 15:34:24.773410: val_loss -0.7896\n",
      "2024-01-10 15:34:24.779408: Pseudo dice [0.9214, 0.928, 0.9371]\n",
      "2024-01-10 15:34:24.784416: Epoch time: 42.16 s\n",
      "2024-01-10 15:34:26.284822: \n",
      "2024-01-10 15:34:26.289658: Epoch 994\n",
      "2024-01-10 15:34:26.295539: Current learning rate: 0.0001\n",
      "2024-01-10 15:35:07.847032: train_loss -0.9592\n",
      "2024-01-10 15:35:07.860037: val_loss -0.7916\n",
      "2024-01-10 15:35:07.869036: Pseudo dice [0.9207, 0.9277, 0.9358]\n",
      "2024-01-10 15:35:07.878544: Epoch time: 41.56 s\n",
      "2024-01-10 15:35:09.370187: \n",
      "2024-01-10 15:35:09.374718: Epoch 995\n",
      "2024-01-10 15:35:09.379718: Current learning rate: 8e-05\n",
      "2024-01-10 15:35:50.997268: train_loss -0.9589\n",
      "2024-01-10 15:35:51.004779: val_loss -0.7884\n",
      "2024-01-10 15:35:51.011781: Pseudo dice [0.9213, 0.928, 0.9361]\n",
      "2024-01-10 15:35:51.018781: Epoch time: 41.63 s\n",
      "2024-01-10 15:35:52.577413: \n",
      "2024-01-10 15:35:52.582409: Epoch 996\n",
      "2024-01-10 15:35:52.587489: Current learning rate: 7e-05\n",
      "2024-01-10 15:36:34.309969: train_loss -0.9588\n",
      "2024-01-10 15:36:34.318964: val_loss -0.7918\n",
      "2024-01-10 15:36:34.366143: Pseudo dice [0.919, 0.9277, 0.9361]\n",
      "2024-01-10 15:36:34.376478: Epoch time: 41.73 s\n",
      "2024-01-10 15:36:36.033429: \n",
      "2024-01-10 15:36:36.039983: Epoch 997\n",
      "2024-01-10 15:36:36.048983: Current learning rate: 5e-05\n",
      "2024-01-10 15:37:17.461935: train_loss -0.9594\n",
      "2024-01-10 15:37:17.469447: val_loss -0.7917\n",
      "2024-01-10 15:37:17.501688: Pseudo dice [0.9188, 0.928, 0.9367]\n",
      "2024-01-10 15:37:17.510682: Epoch time: 41.43 s\n",
      "2024-01-10 15:37:18.954834: \n",
      "2024-01-10 15:37:18.965802: Epoch 998\n",
      "2024-01-10 15:37:18.970363: Current learning rate: 4e-05\n",
      "2024-01-10 15:38:00.146479: train_loss -0.9593\n",
      "2024-01-10 15:38:00.154478: val_loss -0.7959\n",
      "2024-01-10 15:38:00.160481: Pseudo dice [0.9199, 0.9286, 0.9368]\n",
      "2024-01-10 15:38:00.193513: Epoch time: 41.19 s\n",
      "2024-01-10 15:38:01.680138: \n",
      "2024-01-10 15:38:01.685701: Epoch 999\n",
      "2024-01-10 15:38:01.693777: Current learning rate: 2e-05\n",
      "2024-01-10 15:38:44.152801: train_loss -0.9584\n",
      "2024-01-10 15:38:44.162318: val_loss -0.7937\n",
      "2024-01-10 15:38:44.174112: Pseudo dice [0.9209, 0.9283, 0.9368]\n",
      "2024-01-10 15:38:44.224823: Epoch time: 42.47 s\n",
      "2024-01-10 15:38:46.404896: Training done.\n",
      "2024-01-10 15:38:46.440425: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 15:38:46.449412: The split file contains 5 splits.\n",
      "2024-01-10 15:38:46.455412: Desired fold for training: 2\n",
      "2024-01-10 15:38:46.460412: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 15:38:46.466412: predicting case_4\n",
      "2024-01-10 15:38:49.498147: predicting case_6\n",
      "2024-01-10 15:39:00.635649: Validation complete\n",
      "2024-01-10 15:39:00.647169: Mean Validation Dice:  0.9328017735992838\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': [160, 160], 'median_image_size_in_voxels': [139.0, 144.5], 'spacing': [0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-10 15:39:01.217597: unpacking dataset...\n",
      "2024-01-10 15:39:01.486353: unpacking done...\n",
      "2024-01-10 15:39:01.491361: do_dummy_2d_data_aug: False\n",
      "2024-01-10 15:39:01.495361: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-10 15:39:01.499361: The split file contains 5 splits.\n",
      "2024-01-10 15:39:01.503361: Desired fold for training: 3\n",
      "2024-01-10 15:39:01.506360: This split has 8 training and 2 validation cases.\n",
      "2024-01-10 15:39:01.536678: Unable to plot network architecture:\n",
      "2024-01-10 15:39:01.540678: No module named 'hiddenlayer'\n",
      "2024-01-10 15:39:01.562945: \n",
      "2024-01-10 15:39:01.568456: Epoch 0\n",
      "2024-01-10 15:39:01.572467: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-10 15:39:51.491812: train_loss -0.367\n",
      "2024-01-10 15:39:51.500813: val_loss -0.6279\n",
      "2024-01-10 15:39:51.506812: Pseudo dice [0.7133, 0.9011, 0.8634]\n",
      "2024-01-10 15:39:51.513816: Epoch time: 49.93 s\n",
      "2024-01-10 15:39:51.544812: Yayy! New best EMA pseudo Dice: 0.826\n",
      "2024-01-10 15:39:53.022242: \n",
      "2024-01-10 15:39:53.027294: Epoch 1\n",
      "2024-01-10 15:39:53.031360: Current learning rate: 0.00999\n",
      "2024-01-10 15:40:35.637532: train_loss -0.7106\n",
      "2024-01-10 15:40:35.648532: val_loss -0.7236\n",
      "2024-01-10 15:40:35.658532: Pseudo dice [0.7946, 0.9221, 0.8873]\n",
      "2024-01-10 15:40:35.665532: Epoch time: 42.62 s\n",
      "2024-01-10 15:40:35.672537: Yayy! New best EMA pseudo Dice: 0.8302\n",
      "2024-01-10 15:40:37.762347: \n",
      "2024-01-10 15:40:37.766853: Epoch 2\n",
      "2024-01-10 15:40:37.770833: Current learning rate: 0.00998\n",
      "2024-01-10 15:41:19.706028: train_loss -0.7694\n",
      "2024-01-10 15:41:19.715027: val_loss -0.7569\n",
      "2024-01-10 15:41:19.723032: Pseudo dice [0.8117, 0.9301, 0.8989]\n",
      "2024-01-10 15:41:19.730040: Epoch time: 41.94 s\n",
      "2024-01-10 15:41:19.735548: Yayy! New best EMA pseudo Dice: 0.8352\n",
      "2024-01-10 15:41:21.541550: \n",
      "2024-01-10 15:41:21.548741: Epoch 3\n",
      "2024-01-10 15:41:21.552716: Current learning rate: 0.00997\n",
      "2024-01-10 15:42:04.521956: train_loss -0.7958\n",
      "2024-01-10 15:42:04.533956: val_loss -0.7784\n",
      "2024-01-10 15:42:04.542965: Pseudo dice [0.823, 0.9363, 0.9089]\n",
      "2024-01-10 15:42:04.554479: Epoch time: 42.98 s\n",
      "2024-01-10 15:42:04.593488: Yayy! New best EMA pseudo Dice: 0.8406\n",
      "2024-01-10 15:42:06.587299: \n",
      "2024-01-10 15:42:06.593299: Epoch 4\n",
      "2024-01-10 15:42:06.597299: Current learning rate: 0.00996\n",
      "2024-01-10 15:42:49.495363: train_loss -0.8108\n",
      "2024-01-10 15:42:49.506362: val_loss -0.7985\n",
      "2024-01-10 15:42:49.514363: Pseudo dice [0.8382, 0.9418, 0.9185]\n",
      "2024-01-10 15:42:49.550361: Epoch time: 42.91 s\n",
      "2024-01-10 15:42:49.558363: Yayy! New best EMA pseudo Dice: 0.8465\n",
      "2024-01-10 15:42:51.387723: \n",
      "2024-01-10 15:42:51.397741: Epoch 5\n",
      "2024-01-10 15:42:51.402724: Current learning rate: 0.00995\n",
      "2024-01-10 15:43:33.976837: train_loss -0.8222\n",
      "2024-01-10 15:43:34.012849: val_loss -0.8044\n",
      "2024-01-10 15:43:34.022853: Pseudo dice [0.8427, 0.9439, 0.9204]\n",
      "2024-01-10 15:43:34.029851: Epoch time: 42.59 s\n",
      "2024-01-10 15:43:34.051888: Yayy! New best EMA pseudo Dice: 0.8521\n",
      "2024-01-10 15:43:35.666512: \n",
      "2024-01-10 15:43:35.671724: Epoch 6\n",
      "2024-01-10 15:43:35.675811: Current learning rate: 0.00995\n",
      "2024-01-10 15:44:17.707068: train_loss -0.8287\n",
      "2024-01-10 15:44:17.715070: val_loss -0.8055\n",
      "2024-01-10 15:44:17.723072: Pseudo dice [0.8389, 0.944, 0.9199]\n",
      "2024-01-10 15:44:17.756104: Epoch time: 42.04 s\n",
      "2024-01-10 15:44:17.764619: Yayy! New best EMA pseudo Dice: 0.857\n",
      "2024-01-10 15:44:19.605355: \n",
      "2024-01-10 15:44:19.610351: Epoch 7\n",
      "2024-01-10 15:44:19.615351: Current learning rate: 0.00994\n",
      "2024-01-10 15:45:01.231896: train_loss -0.8337\n",
      "2024-01-10 15:45:01.241413: val_loss -0.8083\n",
      "2024-01-10 15:45:01.250409: Pseudo dice [0.8483, 0.944, 0.9202]\n",
      "2024-01-10 15:45:01.257410: Epoch time: 41.63 s\n",
      "2024-01-10 15:45:01.263411: Yayy! New best EMA pseudo Dice: 0.8617\n",
      "2024-01-10 15:45:02.981746: \n",
      "2024-01-10 15:45:02.986810: Epoch 8\n",
      "2024-01-10 15:45:02.991749: Current learning rate: 0.00993\n",
      "2024-01-10 15:45:44.589385: train_loss -0.8391\n",
      "2024-01-10 15:45:44.597387: val_loss -0.8175\n",
      "2024-01-10 15:45:44.606383: Pseudo dice [0.8475, 0.9477, 0.9251]\n",
      "2024-01-10 15:45:44.613385: Epoch time: 41.61 s\n",
      "2024-01-10 15:45:44.619385: Yayy! New best EMA pseudo Dice: 0.8662\n",
      "2024-01-10 15:45:46.424471: \n",
      "2024-01-10 15:45:46.430490: Epoch 9\n",
      "2024-01-10 15:45:46.434487: Current learning rate: 0.00992\n",
      "2024-01-10 15:46:28.292188: train_loss -0.8444\n",
      "2024-01-10 15:46:28.301187: val_loss -0.8166\n",
      "2024-01-10 15:46:28.308186: Pseudo dice [0.8428, 0.948, 0.9263]\n",
      "2024-01-10 15:46:28.315196: Epoch time: 41.87 s\n",
      "2024-01-10 15:46:28.323193: Yayy! New best EMA pseudo Dice: 0.8701\n",
      "2024-01-10 15:46:30.069623: \n",
      "2024-01-10 15:46:30.075348: Epoch 10\n",
      "2024-01-10 15:46:30.079392: Current learning rate: 0.00991\n",
      "2024-01-10 15:47:11.784093: train_loss -0.8481\n",
      "2024-01-10 15:47:11.794101: val_loss -0.8188\n",
      "2024-01-10 15:47:11.800302: Pseudo dice [0.8486, 0.949, 0.9273]\n",
      "2024-01-10 15:47:11.805302: Epoch time: 41.72 s\n",
      "2024-01-10 15:47:11.810303: Yayy! New best EMA pseudo Dice: 0.874\n",
      "2024-01-10 15:47:13.490039: \n",
      "2024-01-10 15:47:13.495777: Epoch 11\n",
      "2024-01-10 15:47:13.499329: Current learning rate: 0.0099\n",
      "2024-01-10 15:47:56.263469: train_loss -0.8515\n",
      "2024-01-10 15:47:56.274470: val_loss -0.8194\n",
      "2024-01-10 15:47:56.304930: Pseudo dice [0.8508, 0.9474, 0.9259]\n",
      "2024-01-10 15:47:56.315067: Epoch time: 42.77 s\n",
      "2024-01-10 15:47:56.323586: Yayy! New best EMA pseudo Dice: 0.8774\n",
      "2024-01-10 15:47:58.048426: \n",
      "2024-01-10 15:47:58.053173: Epoch 12\n",
      "2024-01-10 15:47:58.058124: Current learning rate: 0.00989\n",
      "2024-01-10 15:48:40.517682: train_loss -0.8538\n",
      "2024-01-10 15:48:40.529690: val_loss -0.8216\n",
      "2024-01-10 15:48:40.559113: Pseudo dice [0.8504, 0.9496, 0.9273]\n",
      "2024-01-10 15:48:40.568656: Epoch time: 42.47 s\n",
      "2024-01-10 15:48:40.576672: Yayy! New best EMA pseudo Dice: 0.8805\n",
      "2024-01-10 15:48:42.444978: \n",
      "2024-01-10 15:48:42.450970: Epoch 13\n",
      "2024-01-10 15:48:42.455979: Current learning rate: 0.00988\n",
      "2024-01-10 15:49:24.902662: train_loss -0.8556\n",
      "2024-01-10 15:49:24.914662: val_loss -0.8247\n",
      "2024-01-10 15:49:24.923664: Pseudo dice [0.8481, 0.9501, 0.9291]\n",
      "2024-01-10 15:49:24.934342: Epoch time: 42.46 s\n",
      "2024-01-10 15:49:24.946862: Yayy! New best EMA pseudo Dice: 0.8834\n",
      "2024-01-10 15:49:26.902043: \n",
      "2024-01-10 15:49:26.907044: Epoch 14\n",
      "2024-01-10 15:49:26.911049: Current learning rate: 0.00987\n",
      "2024-01-10 15:50:08.818522: train_loss -0.8589\n",
      "2024-01-10 15:50:08.826522: val_loss -0.8226\n",
      "2024-01-10 15:50:08.834522: Pseudo dice [0.8509, 0.9497, 0.9288]\n",
      "2024-01-10 15:50:08.841522: Epoch time: 41.92 s\n",
      "2024-01-10 15:50:08.847522: Yayy! New best EMA pseudo Dice: 0.886\n",
      "2024-01-10 15:50:10.581018: \n",
      "2024-01-10 15:50:10.587228: Epoch 15\n",
      "2024-01-10 15:50:10.597235: Current learning rate: 0.00986\n",
      "2024-01-10 15:50:53.929559: train_loss -0.8611\n",
      "2024-01-10 15:50:53.936553: val_loss -0.8277\n",
      "2024-01-10 15:50:53.966587: Pseudo dice [0.8557, 0.9514, 0.931]\n",
      "2024-01-10 15:50:53.976149: Epoch time: 43.35 s\n",
      "2024-01-10 15:50:53.983206: Yayy! New best EMA pseudo Dice: 0.8887\n",
      "2024-01-10 15:50:55.902927: \n",
      "2024-01-10 15:50:55.908855: Epoch 16\n",
      "2024-01-10 15:50:55.914853: Current learning rate: 0.00986\n",
      "2024-01-10 15:51:38.331855: train_loss -0.8627\n",
      "2024-01-10 15:51:38.342855: val_loss -0.8272\n",
      "2024-01-10 15:51:38.352855: Pseudo dice [0.8499, 0.9516, 0.9308]\n",
      "2024-01-10 15:51:38.358854: Epoch time: 42.43 s\n",
      "2024-01-10 15:51:38.366854: Yayy! New best EMA pseudo Dice: 0.8909\n",
      "2024-01-10 15:51:40.152818: \n",
      "2024-01-10 15:51:40.159753: Epoch 17\n",
      "2024-01-10 15:51:40.165750: Current learning rate: 0.00985\n",
      "2024-01-10 15:52:22.777602: train_loss -0.8652\n",
      "2024-01-10 15:52:22.787605: val_loss -0.8287\n",
      "2024-01-10 15:52:22.796603: Pseudo dice [0.8598, 0.9518, 0.9312]\n",
      "2024-01-10 15:52:22.804603: Epoch time: 42.62 s\n",
      "2024-01-10 15:52:22.811602: Yayy! New best EMA pseudo Dice: 0.8932\n",
      "2024-01-10 15:52:24.485734: \n",
      "2024-01-10 15:52:24.491748: Epoch 18\n",
      "2024-01-10 15:52:24.496735: Current learning rate: 0.00984\n",
      "2024-01-10 15:53:07.474048: train_loss -0.8665\n",
      "2024-01-10 15:53:07.484358: val_loss -0.8266\n",
      "2024-01-10 15:53:07.495382: Pseudo dice [0.8539, 0.9516, 0.9314]\n",
      "2024-01-10 15:53:07.505380: Epoch time: 42.99 s\n",
      "2024-01-10 15:53:07.513913: Yayy! New best EMA pseudo Dice: 0.8951\n",
      "2024-01-10 15:53:09.511621: \n",
      "2024-01-10 15:53:09.521538: Epoch 19\n",
      "2024-01-10 15:53:09.526540: Current learning rate: 0.00983\n",
      "2024-01-10 15:53:51.579361: train_loss -0.8678\n",
      "2024-01-10 15:53:51.586361: val_loss -0.8273\n",
      "2024-01-10 15:53:51.593369: Pseudo dice [0.8544, 0.9518, 0.9317]\n",
      "2024-01-10 15:53:51.599369: Epoch time: 42.07 s\n",
      "2024-01-10 15:53:51.604370: Yayy! New best EMA pseudo Dice: 0.8969\n",
      "2024-01-10 15:53:53.542991: \n",
      "2024-01-10 15:53:53.553993: Epoch 20\n",
      "2024-01-10 15:53:53.557997: Current learning rate: 0.00982\n",
      "2024-01-10 15:54:36.147976: train_loss -0.8708\n",
      "2024-01-10 15:54:36.155975: val_loss -0.833\n",
      "2024-01-10 15:54:36.161983: Pseudo dice [0.8598, 0.9519, 0.9319]\n",
      "2024-01-10 15:54:36.169933: Epoch time: 42.61 s\n",
      "2024-01-10 15:54:36.175964: Yayy! New best EMA pseudo Dice: 0.8987\n",
      "2024-01-10 15:54:37.990883: \n",
      "2024-01-10 15:54:37.999062: Epoch 21\n",
      "2024-01-10 15:54:38.004050: Current learning rate: 0.00981\n",
      "2024-01-10 15:55:20.252601: train_loss -0.8709\n",
      "2024-01-10 15:55:20.262619: val_loss -0.8321\n",
      "2024-01-10 15:55:20.271133: Pseudo dice [0.8592, 0.9527, 0.9328]\n",
      "2024-01-10 15:55:20.279133: Epoch time: 42.26 s\n",
      "2024-01-10 15:55:20.286132: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2024-01-10 15:55:22.082824: \n",
      "2024-01-10 15:55:22.090837: Epoch 22\n",
      "2024-01-10 15:55:22.095846: Current learning rate: 0.0098\n",
      "2024-01-10 15:56:04.943346: train_loss -0.8722\n",
      "2024-01-10 15:56:04.951345: val_loss -0.8293\n",
      "2024-01-10 15:56:04.958346: Pseudo dice [0.8557, 0.9511, 0.9312]\n",
      "2024-01-10 15:56:04.963346: Epoch time: 42.86 s\n",
      "2024-01-10 15:56:04.969346: Yayy! New best EMA pseudo Dice: 0.9015\n",
      "2024-01-10 15:56:06.539635: \n",
      "2024-01-10 15:56:06.548651: Epoch 23\n",
      "2024-01-10 15:56:06.556690: Current learning rate: 0.00979\n",
      "2024-01-10 15:56:48.389678: train_loss -0.8745\n",
      "2024-01-10 15:56:48.399724: val_loss -0.8272\n",
      "2024-01-10 15:56:48.405727: Pseudo dice [0.8518, 0.9515, 0.932]\n",
      "2024-01-10 15:56:48.410732: Epoch time: 41.85 s\n",
      "2024-01-10 15:56:48.415732: Yayy! New best EMA pseudo Dice: 0.9026\n",
      "2024-01-10 15:56:50.104046: \n",
      "2024-01-10 15:56:50.111049: Epoch 24\n",
      "2024-01-10 15:56:50.118048: Current learning rate: 0.00978\n",
      "2024-01-10 15:57:31.278023: train_loss -0.8766\n",
      "2024-01-10 15:57:31.305024: val_loss -0.8236\n",
      "2024-01-10 15:57:31.315024: Pseudo dice [0.851, 0.9503, 0.9294]\n",
      "2024-01-10 15:57:31.322023: Epoch time: 41.17 s\n",
      "2024-01-10 15:57:31.330560: Yayy! New best EMA pseudo Dice: 0.9033\n",
      "2024-01-10 15:57:33.119262: \n",
      "2024-01-10 15:57:33.125365: Epoch 25\n",
      "2024-01-10 15:57:33.131275: Current learning rate: 0.00977\n",
      "2024-01-10 15:58:14.950800: train_loss -0.8771\n",
      "2024-01-10 15:58:14.961022: val_loss -0.8289\n",
      "2024-01-10 15:58:14.997469: Pseudo dice [0.8469, 0.9541, 0.9353]\n",
      "2024-01-10 15:58:15.007828: Epoch time: 41.83 s\n",
      "2024-01-10 15:58:15.014834: Yayy! New best EMA pseudo Dice: 0.9042\n",
      "2024-01-10 15:58:16.919463: \n",
      "2024-01-10 15:58:16.925345: Epoch 26\n",
      "2024-01-10 15:58:16.930259: Current learning rate: 0.00977\n",
      "2024-01-10 15:58:59.394587: train_loss -0.8791\n",
      "2024-01-10 15:58:59.405588: val_loss -0.8331\n",
      "2024-01-10 15:58:59.413098: Pseudo dice [0.8518, 0.9549, 0.9365]\n",
      "2024-01-10 15:58:59.449096: Epoch time: 42.48 s\n",
      "2024-01-10 15:58:59.457102: Yayy! New best EMA pseudo Dice: 0.9052\n",
      "2024-01-10 15:59:01.096785: \n",
      "2024-01-10 15:59:01.102548: Epoch 27\n",
      "2024-01-10 15:59:01.108254: Current learning rate: 0.00976\n",
      "2024-01-10 15:59:42.732043: train_loss -0.8808\n",
      "2024-01-10 15:59:42.743043: val_loss -0.8297\n",
      "2024-01-10 15:59:42.750047: Pseudo dice [0.8497, 0.9536, 0.9342]\n",
      "2024-01-10 15:59:42.755046: Epoch time: 41.64 s\n",
      "2024-01-10 15:59:42.760045: Yayy! New best EMA pseudo Dice: 0.9059\n",
      "2024-01-10 15:59:44.455297: \n",
      "2024-01-10 15:59:44.460369: Epoch 28\n",
      "2024-01-10 15:59:44.465367: Current learning rate: 0.00975\n",
      "2024-01-10 16:00:27.263258: train_loss -0.8824\n",
      "2024-01-10 16:00:27.290876: val_loss -0.8375\n",
      "2024-01-10 16:00:27.299524: Pseudo dice [0.8586, 0.9553, 0.937]\n",
      "2024-01-10 16:00:27.307026: Epoch time: 42.81 s\n",
      "2024-01-10 16:00:27.348379: Yayy! New best EMA pseudo Dice: 0.907\n",
      "2024-01-10 16:00:29.078067: \n",
      "2024-01-10 16:00:29.083325: Epoch 29\n",
      "2024-01-10 16:00:29.087326: Current learning rate: 0.00974\n",
      "2024-01-10 16:01:12.612584: train_loss -0.8834\n",
      "2024-01-10 16:01:12.622585: val_loss -0.8337\n",
      "2024-01-10 16:01:12.631799: Pseudo dice [0.8542, 0.9545, 0.936]\n",
      "2024-01-10 16:01:12.638807: Epoch time: 43.54 s\n",
      "2024-01-10 16:01:12.646815: Yayy! New best EMA pseudo Dice: 0.9078\n",
      "2024-01-10 16:01:14.545693: \n",
      "2024-01-10 16:01:14.550682: Epoch 30\n",
      "2024-01-10 16:01:14.554854: Current learning rate: 0.00973\n",
      "2024-01-10 16:01:57.074407: train_loss -0.8841\n",
      "2024-01-10 16:01:57.082407: val_loss -0.8374\n",
      "2024-01-10 16:01:57.089409: Pseudo dice [0.86, 0.955, 0.9365]\n",
      "2024-01-10 16:01:57.094415: Epoch time: 42.53 s\n",
      "2024-01-10 16:01:57.100415: Yayy! New best EMA pseudo Dice: 0.9088\n",
      "2024-01-10 16:01:59.049751: \n",
      "2024-01-10 16:01:59.055288: Epoch 31\n",
      "2024-01-10 16:01:59.059362: Current learning rate: 0.00972\n",
      "2024-01-10 16:02:41.076068: train_loss -0.8843\n",
      "2024-01-10 16:02:41.092069: val_loss -0.838\n",
      "2024-01-10 16:02:41.101533: Pseudo dice [0.8598, 0.9559, 0.9379]\n",
      "2024-01-10 16:02:41.113534: Epoch time: 42.03 s\n",
      "2024-01-10 16:02:41.121531: Yayy! New best EMA pseudo Dice: 0.9097\n",
      "2024-01-10 16:02:42.912571: \n",
      "2024-01-10 16:02:42.917568: Epoch 32\n",
      "2024-01-10 16:02:42.923569: Current learning rate: 0.00971\n",
      "2024-01-10 16:03:25.121190: train_loss -0.8862\n",
      "2024-01-10 16:03:25.128180: val_loss -0.8355\n",
      "2024-01-10 16:03:25.137181: Pseudo dice [0.8574, 0.9555, 0.9368]\n",
      "2024-01-10 16:03:25.143179: Epoch time: 42.21 s\n",
      "2024-01-10 16:03:25.149181: Yayy! New best EMA pseudo Dice: 0.9104\n",
      "2024-01-10 16:03:26.881248: \n",
      "2024-01-10 16:03:26.886257: Epoch 33\n",
      "2024-01-10 16:03:26.891276: Current learning rate: 0.0097\n",
      "2024-01-10 16:04:09.045890: train_loss -0.8872\n",
      "2024-01-10 16:04:09.078888: val_loss -0.836\n",
      "2024-01-10 16:04:09.087889: Pseudo dice [0.858, 0.9555, 0.9382]\n",
      "2024-01-10 16:04:09.095889: Epoch time: 42.17 s\n",
      "2024-01-10 16:04:09.104244: Yayy! New best EMA pseudo Dice: 0.9111\n",
      "2024-01-10 16:04:10.866942: \n",
      "2024-01-10 16:04:10.871948: Epoch 34\n",
      "2024-01-10 16:04:10.877940: Current learning rate: 0.00969\n",
      "2024-01-10 16:04:52.928598: train_loss -0.8876\n",
      "2024-01-10 16:04:52.938597: val_loss -0.8374\n",
      "2024-01-10 16:04:52.946600: Pseudo dice [0.8547, 0.9567, 0.9399]\n",
      "2024-01-10 16:04:52.952607: Epoch time: 42.06 s\n",
      "2024-01-10 16:04:52.957606: Yayy! New best EMA pseudo Dice: 0.9117\n",
      "2024-01-10 16:04:54.849216: \n",
      "2024-01-10 16:04:54.854449: Epoch 35\n",
      "2024-01-10 16:04:54.858457: Current learning rate: 0.00968\n",
      "2024-01-10 16:05:36.707546: train_loss -0.889\n",
      "2024-01-10 16:05:36.719548: val_loss -0.8361\n",
      "2024-01-10 16:05:36.730544: Pseudo dice [0.8538, 0.956, 0.9381]\n",
      "2024-01-10 16:05:36.738547: Epoch time: 41.86 s\n",
      "2024-01-10 16:05:36.745545: Yayy! New best EMA pseudo Dice: 0.9121\n",
      "2024-01-10 16:05:38.775395: \n",
      "2024-01-10 16:05:38.782976: Epoch 36\n",
      "2024-01-10 16:05:38.790214: Current learning rate: 0.00968\n",
      "2024-01-10 16:06:21.231147: train_loss -0.8896\n",
      "2024-01-10 16:06:21.239147: val_loss -0.8393\n",
      "2024-01-10 16:06:21.245147: Pseudo dice [0.8604, 0.9565, 0.9389]\n",
      "2024-01-10 16:06:21.253147: Epoch time: 42.46 s\n",
      "2024-01-10 16:06:21.259150: Yayy! New best EMA pseudo Dice: 0.9127\n",
      "2024-01-10 16:06:23.210451: \n",
      "2024-01-10 16:06:23.217456: Epoch 37\n",
      "2024-01-10 16:06:23.224457: Current learning rate: 0.00967\n",
      "2024-01-10 16:07:05.702452: train_loss -0.8903\n",
      "2024-01-10 16:07:05.710562: val_loss -0.8414\n",
      "2024-01-10 16:07:05.720860: Pseudo dice [0.8665, 0.9564, 0.9398]\n",
      "2024-01-10 16:07:05.731756: Epoch time: 42.49 s\n",
      "2024-01-10 16:07:05.739830: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2024-01-10 16:07:07.646441: \n",
      "2024-01-10 16:07:07.652459: Epoch 38\n",
      "2024-01-10 16:07:07.659460: Current learning rate: 0.00966\n",
      "2024-01-10 16:07:49.407762: train_loss -0.8906\n",
      "2024-01-10 16:07:49.441761: val_loss -0.8393\n",
      "2024-01-10 16:07:49.450761: Pseudo dice [0.8619, 0.956, 0.9392]\n",
      "2024-01-10 16:07:49.456764: Epoch time: 41.76 s\n",
      "2024-01-10 16:07:49.465523: Yayy! New best EMA pseudo Dice: 0.9141\n",
      "2024-01-10 16:07:51.267638: \n",
      "2024-01-10 16:07:51.273635: Epoch 39\n",
      "2024-01-10 16:07:51.278643: Current learning rate: 0.00965\n",
      "2024-01-10 16:08:33.034985: train_loss -0.8913\n",
      "2024-01-10 16:08:33.045984: val_loss -0.838\n",
      "2024-01-10 16:08:33.056985: Pseudo dice [0.8592, 0.9561, 0.9391]\n",
      "2024-01-10 16:08:33.065985: Epoch time: 41.77 s\n",
      "2024-01-10 16:08:33.076984: Yayy! New best EMA pseudo Dice: 0.9145\n",
      "2024-01-10 16:08:34.857868: \n",
      "2024-01-10 16:08:34.863870: Epoch 40\n",
      "2024-01-10 16:08:34.869861: Current learning rate: 0.00964\n",
      "2024-01-10 16:09:18.543810: train_loss -0.8912\n",
      "2024-01-10 16:09:18.555812: val_loss -0.8339\n",
      "2024-01-10 16:09:18.568434: Pseudo dice [0.8584, 0.9552, 0.9373]\n",
      "2024-01-10 16:09:18.577433: Epoch time: 43.69 s\n",
      "2024-01-10 16:09:18.587237: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2024-01-10 16:09:21.365070: \n",
      "2024-01-10 16:09:21.372070: Epoch 41\n",
      "2024-01-10 16:09:21.379058: Current learning rate: 0.00963\n",
      "2024-01-10 16:10:03.871492: train_loss -0.8933\n",
      "2024-01-10 16:10:03.904726: val_loss -0.8412\n",
      "2024-01-10 16:10:03.914724: Pseudo dice [0.8655, 0.956, 0.9391]\n",
      "2024-01-10 16:10:03.923725: Epoch time: 42.51 s\n",
      "2024-01-10 16:10:03.934567: Yayy! New best EMA pseudo Dice: 0.9153\n",
      "2024-01-10 16:10:05.966797: \n",
      "2024-01-10 16:10:05.972792: Epoch 42\n",
      "2024-01-10 16:10:05.978311: Current learning rate: 0.00962\n",
      "2024-01-10 16:10:47.480729: train_loss -0.8929\n",
      "2024-01-10 16:10:47.495255: val_loss -0.8368\n",
      "2024-01-10 16:10:47.508244: Pseudo dice [0.8481, 0.9573, 0.9399]\n",
      "2024-01-10 16:10:47.540219: Epoch time: 41.51 s\n",
      "2024-01-10 16:10:49.059850: \n",
      "2024-01-10 16:10:49.066045: Epoch 43\n",
      "2024-01-10 16:10:49.070042: Current learning rate: 0.00961\n",
      "2024-01-10 16:11:30.926764: train_loss -0.8945\n",
      "2024-01-10 16:11:30.961277: val_loss -0.8401\n",
      "2024-01-10 16:11:30.971277: Pseudo dice [0.8652, 0.9563, 0.9393]\n",
      "2024-01-10 16:11:31.004277: Epoch time: 41.87 s\n",
      "2024-01-10 16:11:31.016283: Yayy! New best EMA pseudo Dice: 0.9158\n",
      "2024-01-10 16:11:32.644048: \n",
      "2024-01-10 16:11:32.649108: Epoch 44\n",
      "2024-01-10 16:11:32.654050: Current learning rate: 0.0096\n",
      "2024-01-10 16:12:18.328015: train_loss -0.8945\n",
      "2024-01-10 16:12:18.337401: val_loss -0.8383\n",
      "2024-01-10 16:12:18.348401: Pseudo dice [0.8608, 0.9561, 0.939]\n",
      "2024-01-10 16:12:18.356402: Epoch time: 45.68 s\n",
      "2024-01-10 16:12:18.363402: Yayy! New best EMA pseudo Dice: 0.9161\n",
      "2024-01-10 16:12:20.110249: \n",
      "2024-01-10 16:12:20.119242: Epoch 45\n",
      "2024-01-10 16:12:20.123252: Current learning rate: 0.00959\n",
      "2024-01-10 16:13:02.417949: train_loss -0.8948\n",
      "2024-01-10 16:13:02.428948: val_loss -0.8408\n",
      "2024-01-10 16:13:02.436947: Pseudo dice [0.8661, 0.9568, 0.9397]\n",
      "2024-01-10 16:13:02.442948: Epoch time: 42.31 s\n",
      "2024-01-10 16:13:02.451949: Yayy! New best EMA pseudo Dice: 0.9165\n",
      "2024-01-10 16:13:04.404813: \n",
      "2024-01-10 16:13:04.409850: Epoch 46\n",
      "2024-01-10 16:13:04.419805: Current learning rate: 0.00959\n",
      "2024-01-10 16:13:46.782056: train_loss -0.8962\n",
      "2024-01-10 16:13:46.827063: val_loss -0.8484\n",
      "2024-01-10 16:13:46.837083: Pseudo dice [0.8677, 0.9587, 0.9419]\n",
      "2024-01-10 16:13:46.875082: Epoch time: 42.38 s\n",
      "2024-01-10 16:13:46.882081: Yayy! New best EMA pseudo Dice: 0.9172\n",
      "2024-01-10 16:13:48.470731: \n",
      "2024-01-10 16:13:48.476627: Epoch 47\n",
      "2024-01-10 16:13:48.480866: Current learning rate: 0.00958\n",
      "2024-01-10 16:14:29.830660: train_loss -0.8969\n",
      "2024-01-10 16:14:29.838659: val_loss -0.8399\n",
      "2024-01-10 16:14:29.869273: Pseudo dice [0.8662, 0.956, 0.9381]\n",
      "2024-01-10 16:14:29.876272: Epoch time: 41.36 s\n",
      "2024-01-10 16:14:29.881275: Yayy! New best EMA pseudo Dice: 0.9175\n",
      "2024-01-10 16:14:31.550785: \n",
      "2024-01-10 16:14:31.559790: Epoch 48\n",
      "2024-01-10 16:14:31.568329: Current learning rate: 0.00957\n",
      "2024-01-10 16:15:15.113138: train_loss -0.8974\n",
      "2024-01-10 16:15:15.148135: val_loss -0.8409\n",
      "2024-01-10 16:15:15.157135: Pseudo dice [0.8656, 0.9564, 0.9387]\n",
      "2024-01-10 16:15:15.196306: Epoch time: 43.56 s\n",
      "2024-01-10 16:15:15.206307: Yayy! New best EMA pseudo Dice: 0.9177\n",
      "2024-01-10 16:15:16.892649: \n",
      "2024-01-10 16:15:16.898181: Epoch 49\n",
      "2024-01-10 16:15:16.902925: Current learning rate: 0.00956\n",
      "2024-01-10 16:15:58.871152: train_loss -0.8976\n",
      "2024-01-10 16:15:58.880152: val_loss -0.8352\n",
      "2024-01-10 16:15:58.888157: Pseudo dice [0.8617, 0.9548, 0.9363]\n",
      "2024-01-10 16:15:58.893835: Epoch time: 41.98 s\n",
      "2024-01-10 16:16:00.561932: \n",
      "2024-01-10 16:16:00.567654: Epoch 50\n",
      "2024-01-10 16:16:00.572668: Current learning rate: 0.00955\n",
      "2024-01-10 16:16:42.335263: train_loss -0.8983\n",
      "2024-01-10 16:16:42.344265: val_loss -0.839\n",
      "2024-01-10 16:16:42.355781: Pseudo dice [0.8616, 0.9564, 0.9379]\n",
      "2024-01-10 16:16:42.361779: Epoch time: 41.77 s\n",
      "2024-01-10 16:16:42.367779: Yayy! New best EMA pseudo Dice: 0.9178\n",
      "2024-01-10 16:16:44.208783: \n",
      "2024-01-10 16:16:44.218830: Epoch 51\n",
      "2024-01-10 16:16:44.224788: Current learning rate: 0.00954\n",
      "2024-01-10 16:17:27.013736: train_loss -0.8976\n",
      "2024-01-10 16:17:27.023731: val_loss -0.8391\n",
      "2024-01-10 16:17:27.049731: Pseudo dice [0.8591, 0.9572, 0.9397]\n",
      "2024-01-10 16:17:27.057735: Epoch time: 42.81 s\n",
      "2024-01-10 16:17:27.065200: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2024-01-10 16:17:28.697772: \n",
      "2024-01-10 16:17:28.706791: Epoch 52\n",
      "2024-01-10 16:17:28.711700: Current learning rate: 0.00953\n",
      "2024-01-10 16:18:12.035156: train_loss -0.8984\n",
      "2024-01-10 16:18:12.061157: val_loss -0.8389\n",
      "2024-01-10 16:18:12.068156: Pseudo dice [0.8598, 0.9567, 0.939]\n",
      "2024-01-10 16:18:12.074157: Epoch time: 43.34 s\n",
      "2024-01-10 16:18:12.079156: Yayy! New best EMA pseudo Dice: 0.918\n",
      "2024-01-10 16:18:13.760544: \n",
      "2024-01-10 16:18:13.766109: Epoch 53\n",
      "2024-01-10 16:18:13.775111: Current learning rate: 0.00952\n",
      "2024-01-10 16:18:57.174932: train_loss -0.8999\n",
      "2024-01-10 16:18:57.183931: val_loss -0.8428\n",
      "2024-01-10 16:18:57.189931: Pseudo dice [0.8601, 0.9587, 0.9423]\n",
      "2024-01-10 16:18:57.195932: Epoch time: 43.42 s\n",
      "2024-01-10 16:18:57.201931: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2024-01-10 16:18:59.014055: \n",
      "2024-01-10 16:18:59.019177: Epoch 54\n",
      "2024-01-10 16:18:59.023269: Current learning rate: 0.00951\n",
      "2024-01-10 16:19:40.802368: train_loss -0.9005\n",
      "2024-01-10 16:19:40.811371: val_loss -0.8372\n",
      "2024-01-10 16:19:40.819371: Pseudo dice [0.8602, 0.9567, 0.9395]\n",
      "2024-01-10 16:19:40.827369: Epoch time: 41.79 s\n",
      "2024-01-10 16:19:40.836371: Yayy! New best EMA pseudo Dice: 0.9183\n",
      "2024-01-10 16:19:42.781206: \n",
      "2024-01-10 16:19:42.787354: Epoch 55\n",
      "2024-01-10 16:19:42.791370: Current learning rate: 0.0095\n",
      "2024-01-10 16:20:24.378020: train_loss -0.8998\n",
      "2024-01-10 16:20:24.388018: val_loss -0.8398\n",
      "2024-01-10 16:20:24.396544: Pseudo dice [0.8621, 0.9573, 0.9407]\n",
      "2024-01-10 16:20:24.429071: Epoch time: 41.6 s\n",
      "2024-01-10 16:20:24.443254: Yayy! New best EMA pseudo Dice: 0.9184\n",
      "2024-01-10 16:20:26.110847: \n",
      "2024-01-10 16:20:26.116905: Epoch 56\n",
      "2024-01-10 16:20:26.122845: Current learning rate: 0.00949\n",
      "2024-01-10 16:21:07.248313: train_loss -0.901\n",
      "2024-01-10 16:21:07.291816: val_loss -0.8423\n",
      "2024-01-10 16:21:07.302817: Pseudo dice [0.8615, 0.958, 0.9408]\n",
      "2024-01-10 16:21:07.313334: Epoch time: 41.14 s\n",
      "2024-01-10 16:21:07.321848: Yayy! New best EMA pseudo Dice: 0.9186\n",
      "2024-01-10 16:21:08.987063: \n",
      "2024-01-10 16:21:08.993095: Epoch 57\n",
      "2024-01-10 16:21:08.998157: Current learning rate: 0.00949\n",
      "2024-01-10 16:21:51.252208: train_loss -0.9018\n",
      "2024-01-10 16:21:51.260717: val_loss -0.8388\n",
      "2024-01-10 16:21:51.268721: Pseudo dice [0.8629, 0.9564, 0.9393]\n",
      "2024-01-10 16:21:51.276716: Epoch time: 42.27 s\n",
      "2024-01-10 16:21:51.282718: Yayy! New best EMA pseudo Dice: 0.9187\n",
      "2024-01-10 16:21:52.908300: \n",
      "2024-01-10 16:21:52.914483: Epoch 58\n",
      "2024-01-10 16:21:52.919397: Current learning rate: 0.00948\n",
      "2024-01-10 16:22:34.756238: train_loss -0.9023\n",
      "2024-01-10 16:22:34.785244: val_loss -0.8381\n",
      "2024-01-10 16:22:34.795762: Pseudo dice [0.8597, 0.9572, 0.94]\n",
      "2024-01-10 16:22:34.804755: Epoch time: 41.85 s\n",
      "2024-01-10 16:22:34.813999: Yayy! New best EMA pseudo Dice: 0.9187\n",
      "2024-01-10 16:22:36.214532: \n",
      "2024-01-10 16:22:36.220533: Epoch 59\n",
      "2024-01-10 16:22:36.227533: Current learning rate: 0.00947\n",
      "2024-01-10 16:23:16.789124: train_loss -0.9032\n",
      "2024-01-10 16:23:16.800124: val_loss -0.8361\n",
      "2024-01-10 16:23:16.841642: Pseudo dice [0.8609, 0.9556, 0.9372]\n",
      "2024-01-10 16:23:16.855643: Epoch time: 40.58 s\n",
      "2024-01-10 16:23:18.407580: \n",
      "2024-01-10 16:23:18.413563: Epoch 60\n",
      "2024-01-10 16:23:18.420097: Current learning rate: 0.00946\n",
      "2024-01-10 16:23:59.654728: train_loss -0.9022\n",
      "2024-01-10 16:23:59.666718: val_loss -0.8396\n",
      "2024-01-10 16:23:59.676721: Pseudo dice [0.8653, 0.957, 0.94]\n",
      "2024-01-10 16:23:59.686716: Epoch time: 41.25 s\n",
      "2024-01-10 16:23:59.716716: Yayy! New best EMA pseudo Dice: 0.9189\n",
      "2024-01-10 16:24:01.122399: \n",
      "2024-01-10 16:24:01.129398: Epoch 61\n",
      "2024-01-10 16:24:01.135304: Current learning rate: 0.00945\n",
      "2024-01-10 16:24:41.691656: train_loss -0.9033\n",
      "2024-01-10 16:24:41.698657: val_loss -0.8399\n",
      "2024-01-10 16:24:41.707660: Pseudo dice [0.8649, 0.9573, 0.94]\n",
      "2024-01-10 16:24:41.717661: Epoch time: 40.57 s\n",
      "2024-01-10 16:24:41.727656: Yayy! New best EMA pseudo Dice: 0.919\n",
      "2024-01-10 16:24:43.165838: \n",
      "2024-01-10 16:24:43.170838: Epoch 62\n",
      "2024-01-10 16:24:43.175838: Current learning rate: 0.00944\n",
      "2024-01-10 16:25:23.868778: train_loss -0.9031\n",
      "2024-01-10 16:25:23.901818: val_loss -0.8381\n",
      "2024-01-10 16:25:23.910328: Pseudo dice [0.8569, 0.9573, 0.9401]\n",
      "2024-01-10 16:25:23.918457: Epoch time: 40.7 s\n",
      "2024-01-10 16:25:25.277656: \n",
      "2024-01-10 16:25:25.282671: Epoch 63\n",
      "2024-01-10 16:25:25.287666: Current learning rate: 0.00943\n",
      "2024-01-10 16:26:06.355029: train_loss -0.9053\n",
      "2024-01-10 16:26:06.367031: val_loss -0.8398\n",
      "2024-01-10 16:26:06.378030: Pseudo dice [0.8573, 0.9576, 0.9411]\n",
      "2024-01-10 16:26:06.386041: Epoch time: 41.08 s\n",
      "2024-01-10 16:26:07.696598: \n",
      "2024-01-10 16:26:07.701137: Epoch 64\n",
      "2024-01-10 16:26:07.706212: Current learning rate: 0.00942\n",
      "2024-01-10 16:26:50.361067: train_loss -0.9035\n",
      "2024-01-10 16:26:50.371066: val_loss -0.8358\n",
      "2024-01-10 16:26:50.378069: Pseudo dice [0.8605, 0.9563, 0.9394]\n",
      "2024-01-10 16:26:50.385066: Epoch time: 42.67 s\n",
      "2024-01-10 16:26:52.224485: \n",
      "2024-01-10 16:26:52.234005: Epoch 65\n",
      "2024-01-10 16:26:52.243038: Current learning rate: 0.00941\n",
      "2024-01-10 16:27:34.314713: train_loss -0.9049\n",
      "2024-01-10 16:27:34.323713: val_loss -0.8356\n",
      "2024-01-10 16:27:34.331711: Pseudo dice [0.8616, 0.9564, 0.9383]\n",
      "2024-01-10 16:27:34.339591: Epoch time: 42.09 s\n",
      "2024-01-10 16:27:35.845188: \n",
      "2024-01-10 16:27:35.852462: Epoch 66\n",
      "2024-01-10 16:27:35.857516: Current learning rate: 0.0094\n",
      "2024-01-10 16:28:16.610945: train_loss -0.9049\n",
      "2024-01-10 16:28:16.621946: val_loss -0.8375\n",
      "2024-01-10 16:28:16.629943: Pseudo dice [0.859, 0.9579, 0.9406]\n",
      "2024-01-10 16:28:16.638946: Epoch time: 40.77 s\n",
      "2024-01-10 16:28:18.002851: \n",
      "2024-01-10 16:28:18.008851: Epoch 67\n",
      "2024-01-10 16:28:18.012926: Current learning rate: 0.00939\n",
      "2024-01-10 16:28:58.716435: train_loss -0.9051\n",
      "2024-01-10 16:28:58.730432: val_loss -0.8425\n",
      "2024-01-10 16:28:58.738432: Pseudo dice [0.8625, 0.9583, 0.9411]\n",
      "2024-01-10 16:28:58.748432: Epoch time: 40.72 s\n",
      "2024-01-10 16:28:58.792438: Yayy! New best EMA pseudo Dice: 0.9191\n",
      "2024-01-10 16:29:00.403337: \n",
      "2024-01-10 16:29:00.415663: Epoch 68\n",
      "2024-01-10 16:29:00.422659: Current learning rate: 0.00939\n",
      "2024-01-10 16:29:41.142068: train_loss -0.9055\n",
      "2024-01-10 16:29:41.149069: val_loss -0.8429\n",
      "2024-01-10 16:29:41.155069: Pseudo dice [0.8589, 0.9587, 0.9427]\n",
      "2024-01-10 16:29:41.163067: Epoch time: 40.74 s\n",
      "2024-01-10 16:29:41.188068: Yayy! New best EMA pseudo Dice: 0.9192\n",
      "2024-01-10 16:29:42.752887: \n",
      "2024-01-10 16:29:42.761040: Epoch 69\n",
      "2024-01-10 16:29:42.767981: Current learning rate: 0.00938\n",
      "2024-01-10 16:30:23.479396: train_loss -0.9065\n",
      "2024-01-10 16:30:23.492395: val_loss -0.8397\n",
      "2024-01-10 16:30:23.500396: Pseudo dice [0.8614, 0.9576, 0.9401]\n",
      "2024-01-10 16:30:23.507396: Epoch time: 40.73 s\n",
      "2024-01-10 16:30:23.515396: Yayy! New best EMA pseudo Dice: 0.9192\n",
      "2024-01-10 16:30:25.262278: \n",
      "2024-01-10 16:30:25.268277: Epoch 70\n",
      "2024-01-10 16:30:25.273708: Current learning rate: 0.00937\n",
      "2024-01-10 16:31:06.242813: train_loss -0.9065\n",
      "2024-01-10 16:31:06.255812: val_loss -0.8426\n",
      "2024-01-10 16:31:06.266818: Pseudo dice [0.8678, 0.9581, 0.9417]\n",
      "2024-01-10 16:31:06.275355: Epoch time: 40.98 s\n",
      "2024-01-10 16:31:06.284363: Yayy! New best EMA pseudo Dice: 0.9196\n",
      "2024-01-10 16:31:07.679167: \n",
      "2024-01-10 16:31:07.689954: Epoch 71\n",
      "2024-01-10 16:31:07.693927: Current learning rate: 0.00936\n",
      "2024-01-10 16:31:48.205370: train_loss -0.907\n",
      "2024-01-10 16:31:48.215365: val_loss -0.8386\n",
      "2024-01-10 16:31:48.224365: Pseudo dice [0.8587, 0.9578, 0.9411]\n",
      "2024-01-10 16:31:48.232366: Epoch time: 40.53 s\n",
      "2024-01-10 16:31:49.403118: \n",
      "2024-01-10 16:31:49.407594: Epoch 72\n",
      "2024-01-10 16:31:49.411544: Current learning rate: 0.00935\n",
      "2024-01-10 16:32:29.814193: train_loss -0.9065\n",
      "2024-01-10 16:32:29.824193: val_loss -0.8404\n",
      "2024-01-10 16:32:29.832194: Pseudo dice [0.8652, 0.9581, 0.9416]\n",
      "2024-01-10 16:32:29.865193: Epoch time: 40.41 s\n",
      "2024-01-10 16:32:29.874193: Yayy! New best EMA pseudo Dice: 0.9197\n",
      "2024-01-10 16:32:31.245344: \n",
      "2024-01-10 16:32:31.250769: Epoch 73\n",
      "2024-01-10 16:32:31.254864: Current learning rate: 0.00934\n",
      "2024-01-10 16:33:11.913154: train_loss -0.9076\n",
      "2024-01-10 16:33:11.927164: val_loss -0.836\n",
      "2024-01-10 16:33:11.974673: Pseudo dice [0.8548, 0.9569, 0.9402]\n",
      "2024-01-10 16:33:11.985919: Epoch time: 40.67 s\n",
      "2024-01-10 16:33:13.282213: \n",
      "2024-01-10 16:33:13.287272: Epoch 74\n",
      "2024-01-10 16:33:13.292283: Current learning rate: 0.00933\n",
      "2024-01-10 16:33:54.033937: train_loss -0.9082\n",
      "2024-01-10 16:33:54.047943: val_loss -0.8413\n",
      "2024-01-10 16:33:54.074715: Pseudo dice [0.8655, 0.9578, 0.9408]\n",
      "2024-01-10 16:33:54.085809: Epoch time: 40.75 s\n",
      "2024-01-10 16:33:55.505428: \n",
      "2024-01-10 16:33:55.516575: Epoch 75\n",
      "2024-01-10 16:33:55.520595: Current learning rate: 0.00932\n",
      "2024-01-10 16:34:36.716682: train_loss -0.9085\n",
      "2024-01-10 16:34:36.729681: val_loss -0.8413\n",
      "2024-01-10 16:34:36.772923: Pseudo dice [0.8628, 0.9589, 0.9422]\n",
      "2024-01-10 16:34:36.785429: Epoch time: 41.21 s\n",
      "2024-01-10 16:34:36.798430: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2024-01-10 16:34:38.263978: \n",
      "2024-01-10 16:34:38.268710: Epoch 76\n",
      "2024-01-10 16:34:38.273640: Current learning rate: 0.00931\n",
      "2024-01-10 16:35:19.555784: train_loss -0.9084\n",
      "2024-01-10 16:35:19.574785: val_loss -0.8383\n",
      "2024-01-10 16:35:19.586786: Pseudo dice [0.8573, 0.9585, 0.9414]\n",
      "2024-01-10 16:35:19.624305: Epoch time: 41.29 s\n",
      "2024-01-10 16:35:20.869419: \n",
      "2024-01-10 16:35:20.875076: Epoch 77\n",
      "2024-01-10 16:35:20.879162: Current learning rate: 0.0093\n",
      "2024-01-10 16:36:01.976498: train_loss -0.9083\n",
      "2024-01-10 16:36:01.987498: val_loss -0.8426\n",
      "2024-01-10 16:36:01.993497: Pseudo dice [0.861, 0.9591, 0.9435]\n",
      "2024-01-10 16:36:01.999497: Epoch time: 41.11 s\n",
      "2024-01-10 16:36:02.009497: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2024-01-10 16:36:03.568315: \n",
      "2024-01-10 16:36:03.572316: Epoch 78\n",
      "2024-01-10 16:36:03.576321: Current learning rate: 0.0093\n",
      "2024-01-10 16:36:45.119131: train_loss -0.9093\n",
      "2024-01-10 16:36:45.131132: val_loss -0.8414\n",
      "2024-01-10 16:36:45.142129: Pseudo dice [0.8598, 0.9586, 0.9417]\n",
      "2024-01-10 16:36:45.152906: Epoch time: 41.55 s\n",
      "2024-01-10 16:36:45.163420: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2024-01-10 16:36:46.698734: \n",
      "2024-01-10 16:36:46.706472: Epoch 79\n",
      "2024-01-10 16:36:46.712379: Current learning rate: 0.00929\n",
      "2024-01-10 16:37:27.675189: train_loss -0.9094\n",
      "2024-01-10 16:37:27.709705: val_loss -0.8445\n",
      "2024-01-10 16:37:27.720067: Pseudo dice [0.8675, 0.9595, 0.9438]\n",
      "2024-01-10 16:37:27.731201: Epoch time: 40.98 s\n",
      "2024-01-10 16:37:27.742203: Yayy! New best EMA pseudo Dice: 0.9203\n",
      "2024-01-10 16:37:29.438434: \n",
      "2024-01-10 16:37:29.443154: Epoch 80\n",
      "2024-01-10 16:37:29.447172: Current learning rate: 0.00928\n",
      "2024-01-10 16:38:10.749460: train_loss -0.91\n",
      "2024-01-10 16:38:10.762664: val_loss -0.8425\n",
      "2024-01-10 16:38:10.773844: Pseudo dice [0.8713, 0.9581, 0.9418]\n",
      "2024-01-10 16:38:10.783840: Epoch time: 41.31 s\n",
      "2024-01-10 16:38:10.810512: Yayy! New best EMA pseudo Dice: 0.9206\n",
      "2024-01-10 16:38:12.349565: \n",
      "2024-01-10 16:38:12.360626: Epoch 81\n",
      "2024-01-10 16:38:12.365672: Current learning rate: 0.00927\n",
      "2024-01-10 16:38:53.778395: train_loss -0.9112\n",
      "2024-01-10 16:38:53.789398: val_loss -0.84\n",
      "2024-01-10 16:38:53.830911: Pseudo dice [0.8663, 0.9572, 0.9392]\n",
      "2024-01-10 16:38:53.841920: Epoch time: 41.43 s\n",
      "2024-01-10 16:38:53.851918: Yayy! New best EMA pseudo Dice: 0.9207\n",
      "2024-01-10 16:38:55.429517: \n",
      "2024-01-10 16:38:55.435914: Epoch 82\n",
      "2024-01-10 16:38:55.441914: Current learning rate: 0.00926\n",
      "2024-01-10 16:39:36.844892: train_loss -0.9106\n",
      "2024-01-10 16:39:36.858404: val_loss -0.8415\n",
      "2024-01-10 16:39:36.868405: Pseudo dice [0.8658, 0.9579, 0.9411]\n",
      "2024-01-10 16:39:36.878405: Epoch time: 41.42 s\n",
      "2024-01-10 16:39:36.888404: Yayy! New best EMA pseudo Dice: 0.9208\n",
      "2024-01-10 16:39:38.311842: \n",
      "2024-01-10 16:39:38.324832: Epoch 83\n",
      "2024-01-10 16:39:38.329833: Current learning rate: 0.00925\n",
      "2024-01-10 16:40:19.642480: train_loss -0.9105\n",
      "2024-01-10 16:40:19.654481: val_loss -0.8359\n",
      "2024-01-10 16:40:19.667485: Pseudo dice [0.8578, 0.958, 0.942]\n",
      "2024-01-10 16:40:19.681005: Epoch time: 41.33 s\n",
      "2024-01-10 16:40:21.007388: \n",
      "2024-01-10 16:40:21.014387: Epoch 84\n",
      "2024-01-10 16:40:21.021386: Current learning rate: 0.00924\n",
      "2024-01-10 16:41:02.116113: train_loss -0.9101\n",
      "2024-01-10 16:41:02.125114: val_loss -0.8425\n",
      "2024-01-10 16:41:02.156115: Pseudo dice [0.8632, 0.9587, 0.9426]\n",
      "2024-01-10 16:41:02.166114: Epoch time: 41.11 s\n",
      "2024-01-10 16:41:03.349245: \n",
      "2024-01-10 16:41:03.355257: Epoch 85\n",
      "2024-01-10 16:41:03.361258: Current learning rate: 0.00923\n",
      "2024-01-10 16:41:44.392867: train_loss -0.9113\n",
      "2024-01-10 16:41:44.405866: val_loss -0.8426\n",
      "2024-01-10 16:41:44.414553: Pseudo dice [0.8651, 0.9588, 0.9426]\n",
      "2024-01-10 16:41:44.445555: Epoch time: 41.04 s\n",
      "2024-01-10 16:41:44.457554: Yayy! New best EMA pseudo Dice: 0.9208\n",
      "2024-01-10 16:41:45.904804: \n",
      "2024-01-10 16:41:45.909940: Epoch 86\n",
      "2024-01-10 16:41:45.915933: Current learning rate: 0.00922\n",
      "2024-01-10 16:42:27.073851: train_loss -0.9112\n",
      "2024-01-10 16:42:27.111851: val_loss -0.8373\n",
      "2024-01-10 16:42:27.126859: Pseudo dice [0.8597, 0.9578, 0.9415]\n",
      "2024-01-10 16:42:27.136376: Epoch time: 41.17 s\n",
      "2024-01-10 16:42:28.420648: \n",
      "2024-01-10 16:42:28.431011: Epoch 87\n",
      "2024-01-10 16:42:28.435014: Current learning rate: 0.00921\n",
      "2024-01-10 16:43:09.376806: train_loss -0.9119\n",
      "2024-01-10 16:43:09.387805: val_loss -0.8411\n",
      "2024-01-10 16:43:09.429964: Pseudo dice [0.8648, 0.9578, 0.9412]\n",
      "2024-01-10 16:43:09.443971: Epoch time: 40.96 s\n",
      "2024-01-10 16:43:10.609838: \n",
      "2024-01-10 16:43:10.615171: Epoch 88\n",
      "2024-01-10 16:43:10.619241: Current learning rate: 0.0092\n",
      "2024-01-10 16:43:52.002177: train_loss -0.9113\n",
      "2024-01-10 16:43:52.012178: val_loss -0.8397\n",
      "2024-01-10 16:43:52.023178: Pseudo dice [0.8637, 0.9579, 0.9407]\n",
      "2024-01-10 16:43:52.065185: Epoch time: 41.39 s\n",
      "2024-01-10 16:43:53.558226: \n",
      "2024-01-10 16:43:53.564241: Epoch 89\n",
      "2024-01-10 16:43:53.569241: Current learning rate: 0.0092\n",
      "2024-01-10 16:44:34.555822: train_loss -0.9122\n",
      "2024-01-10 16:44:34.564824: val_loss -0.8468\n",
      "2024-01-10 16:44:34.573822: Pseudo dice [0.8673, 0.9603, 0.945]\n",
      "2024-01-10 16:44:34.583076: Epoch time: 41.0 s\n",
      "2024-01-10 16:44:34.592075: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-01-10 16:44:36.064907: \n",
      "2024-01-10 16:44:36.073474: Epoch 90\n",
      "2024-01-10 16:44:36.077400: Current learning rate: 0.00919\n",
      "2024-01-10 16:45:17.559030: train_loss -0.9129\n",
      "2024-01-10 16:45:17.569036: val_loss -0.8393\n",
      "2024-01-10 16:45:17.578037: Pseudo dice [0.8647, 0.9576, 0.9407]\n",
      "2024-01-10 16:45:17.588038: Epoch time: 41.5 s\n",
      "2024-01-10 16:45:18.956983: \n",
      "2024-01-10 16:45:18.964968: Epoch 91\n",
      "2024-01-10 16:45:18.968976: Current learning rate: 0.00918\n",
      "2024-01-10 16:46:00.507715: train_loss -0.913\n",
      "2024-01-10 16:46:00.518718: val_loss -0.8432\n",
      "2024-01-10 16:46:00.530196: Pseudo dice [0.8595, 0.9593, 0.943]\n",
      "2024-01-10 16:46:00.539195: Epoch time: 41.55 s\n",
      "2024-01-10 16:46:01.883757: \n",
      "2024-01-10 16:46:01.889622: Epoch 92\n",
      "2024-01-10 16:46:01.894208: Current learning rate: 0.00917\n",
      "2024-01-10 16:46:42.914518: train_loss -0.9134\n",
      "2024-01-10 16:46:42.924520: val_loss -0.8377\n",
      "2024-01-10 16:46:42.934519: Pseudo dice [0.8582, 0.9582, 0.9418]\n",
      "2024-01-10 16:46:42.946523: Epoch time: 41.03 s\n",
      "2024-01-10 16:46:44.246347: \n",
      "2024-01-10 16:46:44.252343: Epoch 93\n",
      "2024-01-10 16:46:44.257358: Current learning rate: 0.00916\n",
      "2024-01-10 16:47:24.990353: train_loss -0.9128\n",
      "2024-01-10 16:47:25.000348: val_loss -0.8402\n",
      "2024-01-10 16:47:25.009352: Pseudo dice [0.8635, 0.9583, 0.9427]\n",
      "2024-01-10 16:47:25.016353: Epoch time: 40.75 s\n",
      "2024-01-10 16:47:26.489633: \n",
      "2024-01-10 16:47:26.497428: Epoch 94\n",
      "2024-01-10 16:47:26.503510: Current learning rate: 0.00915\n",
      "2024-01-10 16:48:07.664711: train_loss -0.9123\n",
      "2024-01-10 16:48:07.677710: val_loss -0.8448\n",
      "2024-01-10 16:48:07.712710: Pseudo dice [0.8665, 0.9602, 0.9445]\n",
      "2024-01-10 16:48:07.722710: Epoch time: 41.18 s\n",
      "2024-01-10 16:48:07.731717: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2024-01-10 16:48:09.184018: \n",
      "2024-01-10 16:48:09.190017: Epoch 95\n",
      "2024-01-10 16:48:09.195013: Current learning rate: 0.00914\n",
      "2024-01-10 16:48:50.163700: train_loss -0.914\n",
      "2024-01-10 16:48:50.171699: val_loss -0.8429\n",
      "2024-01-10 16:48:50.181219: Pseudo dice [0.8673, 0.9591, 0.9426]\n",
      "2024-01-10 16:48:50.189219: Epoch time: 40.98 s\n",
      "2024-01-10 16:48:50.197222: Yayy! New best EMA pseudo Dice: 0.9214\n",
      "2024-01-10 16:48:51.612654: \n",
      "2024-01-10 16:48:51.622278: Epoch 96\n",
      "2024-01-10 16:48:51.627337: Current learning rate: 0.00913\n",
      "2024-01-10 16:49:32.565601: train_loss -0.9135\n",
      "2024-01-10 16:49:32.574599: val_loss -0.8438\n",
      "2024-01-10 16:49:32.604117: Pseudo dice [0.8669, 0.9587, 0.9425]\n",
      "2024-01-10 16:49:32.610118: Epoch time: 40.95 s\n",
      "2024-01-10 16:49:32.616118: Yayy! New best EMA pseudo Dice: 0.9215\n",
      "2024-01-10 16:49:34.138995: \n",
      "2024-01-10 16:49:34.144074: Epoch 97\n",
      "2024-01-10 16:49:34.148069: Current learning rate: 0.00912\n",
      "2024-01-10 16:50:16.071961: train_loss -0.915\n",
      "2024-01-10 16:50:16.082960: val_loss -0.8435\n",
      "2024-01-10 16:50:16.118970: Pseudo dice [0.8696, 0.9587, 0.942]\n",
      "2024-01-10 16:50:16.129485: Epoch time: 41.93 s\n",
      "2024-01-10 16:50:16.137484: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2024-01-10 16:50:17.767498: \n",
      "2024-01-10 16:50:17.772833: Epoch 98\n",
      "2024-01-10 16:50:17.778891: Current learning rate: 0.00911\n",
      "2024-01-10 16:50:58.836739: train_loss -0.9149\n",
      "2024-01-10 16:50:58.864252: val_loss -0.8395\n",
      "2024-01-10 16:50:58.872254: Pseudo dice [0.8613, 0.9584, 0.9412]\n",
      "2024-01-10 16:50:58.881256: Epoch time: 41.07 s\n",
      "2024-01-10 16:51:00.444189: \n",
      "2024-01-10 16:51:00.454998: Epoch 99\n",
      "2024-01-10 16:51:00.464983: Current learning rate: 0.0091\n",
      "2024-01-10 16:51:41.711506: train_loss -0.9143\n",
      "2024-01-10 16:51:41.722507: val_loss -0.8401\n",
      "2024-01-10 16:51:41.735510: Pseudo dice [0.865, 0.9579, 0.942]\n",
      "2024-01-10 16:51:41.745508: Epoch time: 41.27 s\n",
      "2024-01-10 16:51:43.384007: \n",
      "2024-01-10 16:51:43.389008: Epoch 100\n",
      "2024-01-10 16:51:43.394002: Current learning rate: 0.0091\n",
      "2024-01-10 16:52:24.373631: train_loss -0.9148\n",
      "2024-01-10 16:52:24.384631: val_loss -0.846\n",
      "2024-01-10 16:52:24.392632: Pseudo dice [0.8683, 0.9597, 0.9437]\n",
      "2024-01-10 16:52:24.400725: Epoch time: 40.99 s\n",
      "2024-01-10 16:52:24.432727: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2024-01-10 16:52:25.808949: \n",
      "2024-01-10 16:52:25.817471: Epoch 101\n",
      "2024-01-10 16:52:25.821549: Current learning rate: 0.00909\n",
      "2024-01-10 16:53:07.245610: train_loss -0.9152\n",
      "2024-01-10 16:53:07.257011: val_loss -0.8458\n",
      "2024-01-10 16:53:07.269020: Pseudo dice [0.8666, 0.9599, 0.9447]\n",
      "2024-01-10 16:53:07.298026: Epoch time: 41.44 s\n",
      "2024-01-10 16:53:07.307028: Yayy! New best EMA pseudo Dice: 0.922\n",
      "2024-01-10 16:53:08.871336: \n",
      "2024-01-10 16:53:08.879579: Epoch 102\n",
      "2024-01-10 16:53:08.883967: Current learning rate: 0.00908\n",
      "2024-01-10 16:53:51.386890: train_loss -0.9141\n",
      "2024-01-10 16:53:51.399890: val_loss -0.834\n",
      "2024-01-10 16:53:51.410242: Pseudo dice [0.8571, 0.9563, 0.9396]\n",
      "2024-01-10 16:53:51.446766: Epoch time: 42.52 s\n",
      "2024-01-10 16:53:52.862599: \n",
      "2024-01-10 16:53:52.869588: Epoch 103\n",
      "2024-01-10 16:53:52.876591: Current learning rate: 0.00907\n",
      "2024-01-10 16:54:34.773803: train_loss -0.916\n",
      "2024-01-10 16:54:34.786805: val_loss -0.8397\n",
      "2024-01-10 16:54:34.796804: Pseudo dice [0.8668, 0.9577, 0.9411]\n",
      "2024-01-10 16:54:34.806803: Epoch time: 41.91 s\n",
      "2024-01-10 16:54:36.200931: \n",
      "2024-01-10 16:54:36.206908: Epoch 104\n",
      "2024-01-10 16:54:36.214907: Current learning rate: 0.00906\n",
      "2024-01-10 16:55:17.238639: train_loss -0.9157\n",
      "2024-01-10 16:55:17.288527: val_loss -0.8401\n",
      "2024-01-10 16:55:17.302529: Pseudo dice [0.8591, 0.9586, 0.9426]\n",
      "2024-01-10 16:55:17.341525: Epoch time: 41.04 s\n",
      "2024-01-10 16:55:18.604742: \n",
      "2024-01-10 16:55:18.611718: Epoch 105\n",
      "2024-01-10 16:55:18.617717: Current learning rate: 0.00905\n",
      "2024-01-10 16:56:00.354902: train_loss -0.9164\n",
      "2024-01-10 16:56:00.368019: val_loss -0.8416\n",
      "2024-01-10 16:56:00.377424: Pseudo dice [0.8639, 0.959, 0.9431]\n",
      "2024-01-10 16:56:00.386424: Epoch time: 41.75 s\n",
      "2024-01-10 16:56:01.640381: \n",
      "2024-01-10 16:56:01.645326: Epoch 106\n",
      "2024-01-10 16:56:01.650331: Current learning rate: 0.00904\n",
      "2024-01-10 16:56:43.471684: train_loss -0.9162\n",
      "2024-01-10 16:56:43.481204: val_loss -0.8407\n",
      "2024-01-10 16:56:43.510755: Pseudo dice [0.8613, 0.959, 0.9429]\n",
      "2024-01-10 16:56:43.518271: Epoch time: 41.83 s\n",
      "2024-01-10 16:56:45.153814: \n",
      "2024-01-10 16:56:45.159388: Epoch 107\n",
      "2024-01-10 16:56:45.164457: Current learning rate: 0.00903\n",
      "2024-01-10 16:57:26.977882: train_loss -0.9159\n",
      "2024-01-10 16:57:26.985876: val_loss -0.8392\n",
      "2024-01-10 16:57:27.014877: Pseudo dice [0.8628, 0.9583, 0.9418]\n",
      "2024-01-10 16:57:27.022877: Epoch time: 41.83 s\n",
      "2024-01-10 16:57:28.391113: \n",
      "2024-01-10 16:57:28.397570: Epoch 108\n",
      "2024-01-10 16:57:28.402617: Current learning rate: 0.00902\n",
      "2024-01-10 16:58:09.453614: train_loss -0.917\n",
      "2024-01-10 16:58:09.461620: val_loss -0.8418\n",
      "2024-01-10 16:58:09.473127: Pseudo dice [0.8614, 0.9594, 0.9433]\n",
      "2024-01-10 16:58:09.479127: Epoch time: 41.06 s\n",
      "2024-01-10 16:58:10.969249: \n",
      "2024-01-10 16:58:10.974506: Epoch 109\n",
      "2024-01-10 16:58:10.982505: Current learning rate: 0.00901\n",
      "2024-01-10 16:58:52.377763: train_loss -0.917\n",
      "2024-01-10 16:58:52.388143: val_loss -0.8389\n",
      "2024-01-10 16:58:52.397149: Pseudo dice [0.8665, 0.9578, 0.9404]\n",
      "2024-01-10 16:58:52.406150: Epoch time: 41.41 s\n",
      "2024-01-10 16:58:53.889856: \n",
      "2024-01-10 16:58:53.894863: Epoch 110\n",
      "2024-01-10 16:58:53.899869: Current learning rate: 0.009\n",
      "2024-01-10 16:59:35.885273: train_loss -0.9182\n",
      "2024-01-10 16:59:35.899268: val_loss -0.8431\n",
      "2024-01-10 16:59:35.941787: Pseudo dice [0.8607, 0.9597, 0.9438]\n",
      "2024-01-10 16:59:35.955268: Epoch time: 42.0 s\n",
      "2024-01-10 16:59:37.417135: \n",
      "2024-01-10 16:59:37.422135: Epoch 111\n",
      "2024-01-10 16:59:37.427691: Current learning rate: 0.009\n",
      "2024-01-10 17:00:18.669437: train_loss -0.9177\n",
      "2024-01-10 17:00:18.680437: val_loss -0.8375\n",
      "2024-01-10 17:00:18.691327: Pseudo dice [0.857, 0.9581, 0.9417]\n",
      "2024-01-10 17:00:18.700327: Epoch time: 41.25 s\n",
      "2024-01-10 17:00:20.049255: \n",
      "2024-01-10 17:00:20.054908: Epoch 112\n",
      "2024-01-10 17:00:20.058990: Current learning rate: 0.00899\n",
      "2024-01-10 17:01:01.382251: train_loss -0.9178\n",
      "2024-01-10 17:01:01.392253: val_loss -0.842\n",
      "2024-01-10 17:01:01.419425: Pseudo dice [0.8627, 0.9593, 0.9433]\n",
      "2024-01-10 17:01:01.431425: Epoch time: 41.33 s\n",
      "2024-01-10 17:01:02.601818: \n",
      "2024-01-10 17:01:02.610818: Epoch 113\n",
      "2024-01-10 17:01:02.615818: Current learning rate: 0.00898\n",
      "2024-01-10 17:01:43.245093: train_loss -0.9182\n",
      "2024-01-10 17:01:43.256090: val_loss -0.8443\n",
      "2024-01-10 17:01:43.269093: Pseudo dice [0.8676, 0.9597, 0.9447]\n",
      "2024-01-10 17:01:43.300637: Epoch time: 40.64 s\n",
      "2024-01-10 17:01:44.693210: \n",
      "2024-01-10 17:01:44.700208: Epoch 114\n",
      "2024-01-10 17:01:44.705210: Current learning rate: 0.00897\n",
      "2024-01-10 17:02:26.652524: train_loss -0.9189\n",
      "2024-01-10 17:02:26.662521: val_loss -0.8425\n",
      "2024-01-10 17:02:26.672527: Pseudo dice [0.8606, 0.9596, 0.9439]\n",
      "2024-01-10 17:02:26.681521: Epoch time: 41.96 s\n",
      "2024-01-10 17:02:27.984611: \n",
      "2024-01-10 17:02:27.994915: Epoch 115\n",
      "2024-01-10 17:02:27.998975: Current learning rate: 0.00896\n",
      "2024-01-10 17:03:08.579552: train_loss -0.9186\n",
      "2024-01-10 17:03:08.590550: val_loss -0.8423\n",
      "2024-01-10 17:03:08.641771: Pseudo dice [0.8608, 0.9598, 0.9437]\n",
      "2024-01-10 17:03:08.651773: Epoch time: 40.6 s\n",
      "2024-01-10 17:03:09.877284: \n",
      "2024-01-10 17:03:09.886258: Epoch 116\n",
      "2024-01-10 17:03:09.890350: Current learning rate: 0.00895\n",
      "2024-01-10 17:03:50.847774: train_loss -0.9179\n",
      "2024-01-10 17:03:50.891773: val_loss -0.8399\n",
      "2024-01-10 17:03:50.901774: Pseudo dice [0.8629, 0.9585, 0.9432]\n",
      "2024-01-10 17:03:50.912772: Epoch time: 40.97 s\n",
      "2024-01-10 17:03:52.186083: \n",
      "2024-01-10 17:03:52.192074: Epoch 117\n",
      "2024-01-10 17:03:52.196079: Current learning rate: 0.00894\n",
      "2024-01-10 17:04:33.209904: train_loss -0.9188\n",
      "2024-01-10 17:04:33.223902: val_loss -0.8395\n",
      "2024-01-10 17:04:33.233948: Pseudo dice [0.861, 0.9583, 0.9407]\n",
      "2024-01-10 17:04:33.246236: Epoch time: 41.02 s\n",
      "2024-01-10 17:04:34.597770: \n",
      "2024-01-10 17:04:34.609182: Epoch 118\n",
      "2024-01-10 17:04:34.618284: Current learning rate: 0.00893\n",
      "2024-01-10 17:05:15.456205: train_loss -0.9188\n",
      "2024-01-10 17:05:15.465206: val_loss -0.8411\n",
      "2024-01-10 17:05:15.498234: Pseudo dice [0.8657, 0.9579, 0.941]\n",
      "2024-01-10 17:05:15.509234: Epoch time: 40.86 s\n",
      "2024-01-10 17:05:16.832622: \n",
      "2024-01-10 17:05:16.841187: Epoch 119\n",
      "2024-01-10 17:05:16.846192: Current learning rate: 0.00892\n",
      "2024-01-10 17:05:58.541562: train_loss -0.9187\n",
      "2024-01-10 17:05:58.551563: val_loss -0.8415\n",
      "2024-01-10 17:05:58.560564: Pseudo dice [0.8631, 0.9587, 0.9426]\n",
      "2024-01-10 17:05:58.572561: Epoch time: 41.71 s\n",
      "2024-01-10 17:06:00.294633: \n",
      "2024-01-10 17:06:00.300633: Epoch 120\n",
      "2024-01-10 17:06:00.305640: Current learning rate: 0.00891\n",
      "2024-01-10 17:06:43.776294: train_loss -0.919\n",
      "2024-01-10 17:06:43.787295: val_loss -0.8438\n",
      "2024-01-10 17:06:43.822301: Pseudo dice [0.8631, 0.9594, 0.9438]\n",
      "2024-01-10 17:06:43.831817: Epoch time: 43.48 s\n",
      "2024-01-10 17:06:45.541133: \n",
      "2024-01-10 17:06:45.549132: Epoch 121\n",
      "2024-01-10 17:06:45.554196: Current learning rate: 0.0089\n",
      "2024-01-10 17:07:29.114454: train_loss -0.9194\n",
      "2024-01-10 17:07:29.128455: val_loss -0.8447\n",
      "2024-01-10 17:07:29.140969: Pseudo dice [0.8684, 0.9588, 0.9427]\n",
      "2024-01-10 17:07:29.191329: Epoch time: 43.57 s\n",
      "2024-01-10 17:07:30.869032: \n",
      "2024-01-10 17:07:30.876044: Epoch 122\n",
      "2024-01-10 17:07:30.880031: Current learning rate: 0.00889\n",
      "2024-01-10 17:08:15.197910: train_loss -0.9197\n",
      "2024-01-10 17:08:15.208424: val_loss -0.8465\n",
      "2024-01-10 17:08:15.217421: Pseudo dice [0.8687, 0.9593, 0.944]\n",
      "2024-01-10 17:08:15.226423: Epoch time: 44.33 s\n",
      "2024-01-10 17:08:16.928732: \n",
      "2024-01-10 17:08:16.935728: Epoch 123\n",
      "2024-01-10 17:08:16.941722: Current learning rate: 0.00889\n",
      "2024-01-10 17:09:00.539868: train_loss -0.9193\n",
      "2024-01-10 17:09:00.549392: val_loss -0.8435\n",
      "2024-01-10 17:09:00.572119: Pseudo dice [0.8662, 0.9587, 0.9429]\n",
      "2024-01-10 17:09:00.584829: Epoch time: 43.61 s\n",
      "2024-01-10 17:09:02.051863: \n",
      "2024-01-10 17:09:02.057857: Epoch 124\n",
      "2024-01-10 17:09:02.061865: Current learning rate: 0.00888\n",
      "2024-01-10 17:09:44.233551: train_loss -0.9201\n",
      "2024-01-10 17:09:44.243549: val_loss -0.8389\n",
      "2024-01-10 17:09:44.251549: Pseudo dice [0.8618, 0.9581, 0.9415]\n",
      "2024-01-10 17:09:44.260551: Epoch time: 42.18 s\n",
      "2024-01-10 17:09:45.698530: \n",
      "2024-01-10 17:09:45.703531: Epoch 125\n",
      "2024-01-10 17:09:45.708611: Current learning rate: 0.00887\n",
      "2024-01-10 17:10:27.088789: train_loss -0.919\n",
      "2024-01-10 17:10:27.118815: val_loss -0.8448\n",
      "2024-01-10 17:10:27.126816: Pseudo dice [0.8627, 0.9593, 0.943]\n",
      "2024-01-10 17:10:27.132816: Epoch time: 41.39 s\n",
      "2024-01-10 17:10:28.701128: \n",
      "2024-01-10 17:10:28.708906: Epoch 126\n",
      "2024-01-10 17:10:28.713845: Current learning rate: 0.00886\n",
      "2024-01-10 17:11:09.747735: train_loss -0.9211\n",
      "2024-01-10 17:11:09.757733: val_loss -0.8367\n",
      "2024-01-10 17:11:09.767734: Pseudo dice [0.8591, 0.9575, 0.9419]\n",
      "2024-01-10 17:11:09.776733: Epoch time: 41.05 s\n",
      "2024-01-10 17:11:11.227737: \n",
      "2024-01-10 17:11:11.234869: Epoch 127\n",
      "2024-01-10 17:11:11.240876: Current learning rate: 0.00885\n",
      "2024-01-10 17:11:52.344755: train_loss -0.9213\n",
      "2024-01-10 17:11:52.371725: val_loss -0.8458\n",
      "2024-01-10 17:11:52.379731: Pseudo dice [0.8665, 0.9598, 0.9444]\n",
      "2024-01-10 17:11:52.411728: Epoch time: 41.12 s\n",
      "2024-01-10 17:11:53.628549: \n",
      "2024-01-10 17:11:53.636698: Epoch 128\n",
      "2024-01-10 17:11:53.640719: Current learning rate: 0.00884\n",
      "2024-01-10 17:12:35.693266: train_loss -0.9208\n",
      "2024-01-10 17:12:35.708272: val_loss -0.8447\n",
      "2024-01-10 17:12:35.755268: Pseudo dice [0.8685, 0.959, 0.9433]\n",
      "2024-01-10 17:12:35.777787: Epoch time: 42.07 s\n",
      "2024-01-10 17:12:37.620040: \n",
      "2024-01-10 17:12:37.626050: Epoch 129\n",
      "2024-01-10 17:12:37.632036: Current learning rate: 0.00883\n",
      "2024-01-10 17:13:19.429592: train_loss -0.9211\n",
      "2024-01-10 17:13:19.440973: val_loss -0.8468\n",
      "2024-01-10 17:13:19.452471: Pseudo dice [0.8703, 0.9599, 0.9439]\n",
      "2024-01-10 17:13:19.461594: Epoch time: 41.81 s\n",
      "2024-01-10 17:13:19.473482: Yayy! New best EMA pseudo Dice: 0.9222\n",
      "2024-01-10 17:13:21.135748: \n",
      "2024-01-10 17:13:21.145648: Epoch 130\n",
      "2024-01-10 17:13:21.153646: Current learning rate: 0.00882\n",
      "2024-01-10 17:14:02.604659: train_loss -0.9214\n",
      "2024-01-10 17:14:02.615658: val_loss -0.8398\n",
      "2024-01-10 17:14:02.645034: Pseudo dice [0.86, 0.9587, 0.943]\n",
      "2024-01-10 17:14:02.657038: Epoch time: 41.47 s\n",
      "2024-01-10 17:14:03.849883: \n",
      "2024-01-10 17:14:03.856889: Epoch 131\n",
      "2024-01-10 17:14:03.860958: Current learning rate: 0.00881\n",
      "2024-01-10 17:14:45.803181: train_loss -0.9202\n",
      "2024-01-10 17:14:45.839182: val_loss -0.8366\n",
      "2024-01-10 17:14:45.852442: Pseudo dice [0.8578, 0.9583, 0.942]\n",
      "2024-01-10 17:14:45.864954: Epoch time: 41.95 s\n",
      "2024-01-10 17:14:47.394177: \n",
      "2024-01-10 17:14:47.404197: Epoch 132\n",
      "2024-01-10 17:14:47.413192: Current learning rate: 0.0088\n",
      "2024-01-10 17:15:29.233167: train_loss -0.9204\n",
      "2024-01-10 17:15:29.243170: val_loss -0.8432\n",
      "2024-01-10 17:15:29.252171: Pseudo dice [0.8644, 0.9595, 0.9439]\n",
      "2024-01-10 17:15:29.284171: Epoch time: 41.84 s\n",
      "2024-01-10 17:15:30.615870: \n",
      "2024-01-10 17:15:30.623219: Epoch 133\n",
      "2024-01-10 17:15:30.628220: Current learning rate: 0.00879\n",
      "2024-01-10 17:16:10.931462: train_loss -0.9214\n",
      "2024-01-10 17:16:10.961462: val_loss -0.8425\n",
      "2024-01-10 17:16:10.974462: Pseudo dice [0.8585, 0.9602, 0.9455]\n",
      "2024-01-10 17:16:10.985465: Epoch time: 40.32 s\n",
      "2024-01-10 17:16:12.573911: \n",
      "2024-01-10 17:16:12.580914: Epoch 134\n",
      "2024-01-10 17:16:12.587909: Current learning rate: 0.00879\n",
      "2024-01-10 17:16:53.803849: train_loss -0.92\n",
      "2024-01-10 17:16:53.815854: val_loss -0.8406\n",
      "2024-01-10 17:16:53.826374: Pseudo dice [0.8583, 0.9591, 0.9441]\n",
      "2024-01-10 17:16:53.838375: Epoch time: 41.23 s\n",
      "2024-01-10 17:16:55.289797: \n",
      "2024-01-10 17:16:55.296801: Epoch 135\n",
      "2024-01-10 17:16:55.303787: Current learning rate: 0.00878\n",
      "2024-01-10 17:17:37.930531: train_loss -0.9204\n",
      "2024-01-10 17:17:37.940526: val_loss -0.8374\n",
      "2024-01-10 17:17:37.954048: Pseudo dice [0.8619, 0.9575, 0.9426]\n",
      "2024-01-10 17:17:37.966048: Epoch time: 42.64 s\n",
      "2024-01-10 17:17:39.099907: \n",
      "2024-01-10 17:17:39.104985: Epoch 136\n",
      "2024-01-10 17:17:39.109972: Current learning rate: 0.00877\n",
      "2024-01-10 17:18:21.131105: train_loss -0.9213\n",
      "2024-01-10 17:18:21.144482: val_loss -0.8439\n",
      "2024-01-10 17:18:21.162492: Pseudo dice [0.868, 0.9591, 0.9429]\n",
      "2024-01-10 17:18:21.207015: Epoch time: 42.03 s\n",
      "2024-01-10 17:18:22.652516: \n",
      "2024-01-10 17:18:22.658521: Epoch 137\n",
      "2024-01-10 17:18:22.664528: Current learning rate: 0.00876\n",
      "2024-01-10 17:19:03.209656: train_loss -0.9212\n",
      "2024-01-10 17:19:03.220657: val_loss -0.84\n",
      "2024-01-10 17:19:03.230657: Pseudo dice [0.8604, 0.9593, 0.9436]\n",
      "2024-01-10 17:19:03.240660: Epoch time: 40.56 s\n",
      "2024-01-10 17:19:04.490488: \n",
      "2024-01-10 17:19:04.499855: Epoch 138\n",
      "2024-01-10 17:19:04.504502: Current learning rate: 0.00875\n",
      "2024-01-10 17:19:45.361484: train_loss -0.9209\n",
      "2024-01-10 17:19:45.371484: val_loss -0.8472\n",
      "2024-01-10 17:19:45.402490: Pseudo dice [0.8707, 0.9599, 0.9444]\n",
      "2024-01-10 17:19:45.412003: Epoch time: 40.87 s\n",
      "2024-01-10 17:19:46.881910: \n",
      "2024-01-10 17:19:46.887910: Epoch 139\n",
      "2024-01-10 17:19:46.892914: Current learning rate: 0.00874\n",
      "2024-01-10 17:20:28.677682: train_loss -0.9217\n",
      "2024-01-10 17:20:28.691681: val_loss -0.8393\n",
      "2024-01-10 17:20:28.730203: Pseudo dice [0.8621, 0.958, 0.941]\n",
      "2024-01-10 17:20:28.743215: Epoch time: 41.8 s\n",
      "2024-01-10 17:20:30.289738: \n",
      "2024-01-10 17:20:30.299077: Epoch 140\n",
      "2024-01-10 17:20:30.303078: Current learning rate: 0.00873\n",
      "2024-01-10 17:21:11.832643: train_loss -0.9218\n",
      "2024-01-10 17:21:11.843644: val_loss -0.843\n",
      "2024-01-10 17:21:11.854158: Pseudo dice [0.87, 0.959, 0.9424]\n",
      "2024-01-10 17:21:11.865159: Epoch time: 41.54 s\n",
      "2024-01-10 17:21:13.108180: \n",
      "2024-01-10 17:21:13.114276: Epoch 141\n",
      "2024-01-10 17:21:13.118853: Current learning rate: 0.00872\n",
      "2024-01-10 17:21:55.097967: train_loss -0.9215\n",
      "2024-01-10 17:21:55.111968: val_loss -0.8426\n",
      "2024-01-10 17:21:55.123963: Pseudo dice [0.865, 0.9587, 0.9419]\n",
      "2024-01-10 17:21:55.151971: Epoch time: 41.99 s\n",
      "2024-01-10 17:21:56.375343: \n",
      "2024-01-10 17:21:56.380333: Epoch 142\n",
      "2024-01-10 17:21:56.385343: Current learning rate: 0.00871\n",
      "2024-01-10 17:22:36.944994: train_loss -0.922\n",
      "2024-01-10 17:22:36.954995: val_loss -0.845\n",
      "2024-01-10 17:22:36.983504: Pseudo dice [0.8609, 0.9606, 0.9454]\n",
      "2024-01-10 17:22:36.993511: Epoch time: 40.57 s\n",
      "2024-01-10 17:22:38.187717: \n",
      "2024-01-10 17:22:38.192714: Epoch 143\n",
      "2024-01-10 17:22:38.198090: Current learning rate: 0.0087\n",
      "2024-01-10 17:23:18.945647: train_loss -0.9224\n",
      "2024-01-10 17:23:18.958647: val_loss -0.8411\n",
      "2024-01-10 17:23:18.969645: Pseudo dice [0.8658, 0.959, 0.9438]\n",
      "2024-01-10 17:23:18.977647: Epoch time: 40.76 s\n",
      "2024-01-10 17:23:20.502843: \n",
      "2024-01-10 17:23:20.511418: Epoch 144\n",
      "2024-01-10 17:23:20.516420: Current learning rate: 0.00869\n",
      "2024-01-10 17:24:02.348079: train_loss -0.9225\n",
      "2024-01-10 17:24:02.361508: val_loss -0.8432\n",
      "2024-01-10 17:24:02.374242: Pseudo dice [0.8687, 0.9597, 0.9435]\n",
      "2024-01-10 17:24:02.384241: Epoch time: 41.85 s\n",
      "2024-01-10 17:24:02.394758: Yayy! New best EMA pseudo Dice: 0.9223\n",
      "2024-01-10 17:24:04.173915: \n",
      "2024-01-10 17:24:04.179917: Epoch 145\n",
      "2024-01-10 17:24:04.184915: Current learning rate: 0.00868\n",
      "2024-01-10 17:24:45.983953: train_loss -0.9237\n",
      "2024-01-10 17:24:45.995485: val_loss -0.8423\n",
      "2024-01-10 17:24:46.035571: Pseudo dice [0.8659, 0.9583, 0.9424]\n",
      "2024-01-10 17:24:46.047092: Epoch time: 41.81 s\n",
      "2024-01-10 17:24:47.262540: \n",
      "2024-01-10 17:24:47.271791: Epoch 146\n",
      "2024-01-10 17:24:47.278891: Current learning rate: 0.00868\n",
      "2024-01-10 17:25:29.142769: train_loss -0.9233\n",
      "2024-01-10 17:25:29.183860: val_loss -0.8419\n",
      "2024-01-10 17:25:29.194377: Pseudo dice [0.8664, 0.9586, 0.9425]\n",
      "2024-01-10 17:25:29.200385: Epoch time: 41.88 s\n",
      "2024-01-10 17:25:29.208903: Yayy! New best EMA pseudo Dice: 0.9223\n",
      "2024-01-10 17:25:30.995043: \n",
      "2024-01-10 17:25:31.000412: Epoch 147\n",
      "2024-01-10 17:25:31.006979: Current learning rate: 0.00867\n",
      "2024-01-10 17:26:14.414742: train_loss -0.924\n",
      "2024-01-10 17:26:14.424746: val_loss -0.8439\n",
      "2024-01-10 17:26:14.432742: Pseudo dice [0.8674, 0.9597, 0.9447]\n",
      "2024-01-10 17:26:14.441751: Epoch time: 43.42 s\n",
      "2024-01-10 17:26:14.449750: Yayy! New best EMA pseudo Dice: 0.9225\n",
      "2024-01-10 17:26:17.106339: \n",
      "2024-01-10 17:26:17.114327: Epoch 148\n",
      "2024-01-10 17:26:17.122327: Current learning rate: 0.00866\n",
      "2024-01-10 17:26:59.580555: train_loss -0.9238\n",
      "2024-01-10 17:26:59.591562: val_loss -0.8396\n",
      "2024-01-10 17:26:59.601562: Pseudo dice [0.8583, 0.9586, 0.9428]\n",
      "2024-01-10 17:26:59.610564: Epoch time: 42.48 s\n",
      "2024-01-10 17:27:01.036319: \n",
      "2024-01-10 17:27:01.044322: Epoch 149\n",
      "2024-01-10 17:27:01.051323: Current learning rate: 0.00865\n",
      "2024-01-10 17:27:43.552262: train_loss -0.9233\n",
      "2024-01-10 17:27:43.567751: val_loss -0.8386\n",
      "2024-01-10 17:27:43.586751: Pseudo dice [0.8557, 0.9593, 0.9435]\n",
      "2024-01-10 17:27:43.597759: Epoch time: 42.52 s\n",
      "2024-01-10 17:27:45.264856: \n",
      "2024-01-10 17:27:45.269855: Epoch 150\n",
      "2024-01-10 17:27:45.274859: Current learning rate: 0.00864\n",
      "2024-01-10 17:28:26.027534: train_loss -0.924\n",
      "2024-01-10 17:28:26.039899: val_loss -0.8423\n",
      "2024-01-10 17:28:26.051947: Pseudo dice [0.8657, 0.9592, 0.9436]\n",
      "2024-01-10 17:28:26.062952: Epoch time: 40.76 s\n",
      "2024-01-10 17:28:27.318346: \n",
      "2024-01-10 17:28:27.327346: Epoch 151\n",
      "2024-01-10 17:28:27.334026: Current learning rate: 0.00863\n",
      "2024-01-10 17:29:08.662071: train_loss -0.9232\n",
      "2024-01-10 17:29:08.697361: val_loss -0.8449\n",
      "2024-01-10 17:29:08.709727: Pseudo dice [0.8704, 0.9595, 0.9432]\n",
      "2024-01-10 17:29:08.724723: Epoch time: 41.35 s\n",
      "2024-01-10 17:29:10.054652: \n",
      "2024-01-10 17:29:10.060652: Epoch 152\n",
      "2024-01-10 17:29:10.066170: Current learning rate: 0.00862\n",
      "2024-01-10 17:29:52.022000: train_loss -0.9241\n",
      "2024-01-10 17:29:52.062999: val_loss -0.8388\n",
      "2024-01-10 17:29:52.075006: Pseudo dice [0.8596, 0.9589, 0.9428]\n",
      "2024-01-10 17:29:52.084516: Epoch time: 41.97 s\n",
      "2024-01-10 17:29:53.811836: \n",
      "2024-01-10 17:29:53.818837: Epoch 153\n",
      "2024-01-10 17:29:53.824709: Current learning rate: 0.00861\n",
      "2024-01-10 17:30:36.806033: train_loss -0.9243\n",
      "2024-01-10 17:30:36.820548: val_loss -0.8401\n",
      "2024-01-10 17:30:36.834547: Pseudo dice [0.8614, 0.959, 0.9429]\n",
      "2024-01-10 17:30:36.845549: Epoch time: 43.0 s\n",
      "2024-01-10 17:30:38.492121: \n",
      "2024-01-10 17:30:38.501128: Epoch 154\n",
      "2024-01-10 17:30:38.508125: Current learning rate: 0.0086\n",
      "2024-01-10 17:31:21.718151: train_loss -0.9238\n",
      "2024-01-10 17:31:21.729852: val_loss -0.8418\n",
      "2024-01-10 17:31:21.760456: Pseudo dice [0.8671, 0.9593, 0.9426]\n",
      "2024-01-10 17:31:21.770977: Epoch time: 43.23 s\n",
      "2024-01-10 17:31:23.153913: \n",
      "2024-01-10 17:31:23.160827: Epoch 155\n",
      "2024-01-10 17:31:23.167828: Current learning rate: 0.00859\n",
      "2024-01-10 17:32:06.271807: train_loss -0.9251\n",
      "2024-01-10 17:32:06.316211: val_loss -0.8419\n",
      "2024-01-10 17:32:06.327200: Pseudo dice [0.8646, 0.9596, 0.9437]\n",
      "2024-01-10 17:32:06.340203: Epoch time: 43.12 s\n",
      "2024-01-10 17:32:07.731796: \n",
      "2024-01-10 17:32:07.741891: Epoch 156\n",
      "2024-01-10 17:32:07.747788: Current learning rate: 0.00858\n",
      "2024-01-10 17:32:50.929307: train_loss -0.924\n",
      "2024-01-10 17:32:50.982575: val_loss -0.8434\n",
      "2024-01-10 17:32:50.997008: Pseudo dice [0.8642, 0.9597, 0.9433]\n",
      "2024-01-10 17:32:51.009014: Epoch time: 43.2 s\n",
      "2024-01-10 17:32:52.523743: \n",
      "2024-01-10 17:32:52.529742: Epoch 157\n",
      "2024-01-10 17:32:52.536747: Current learning rate: 0.00858\n",
      "2024-01-10 17:33:36.513482: train_loss -0.9239\n",
      "2024-01-10 17:33:36.526435: val_loss -0.8441\n",
      "2024-01-10 17:33:36.538124: Pseudo dice [0.8724, 0.9593, 0.9438]\n",
      "2024-01-10 17:33:36.547132: Epoch time: 43.99 s\n",
      "2024-01-10 17:33:37.925543: \n",
      "2024-01-10 17:33:37.931546: Epoch 158\n",
      "2024-01-10 17:33:37.937549: Current learning rate: 0.00857\n",
      "2024-01-10 17:34:20.197834: train_loss -0.9246\n",
      "2024-01-10 17:34:20.210127: val_loss -0.8449\n",
      "2024-01-10 17:34:20.222456: Pseudo dice [0.864, 0.9609, 0.9448]\n",
      "2024-01-10 17:34:20.242570: Epoch time: 42.27 s\n",
      "2024-01-10 17:34:20.281661: Yayy! New best EMA pseudo Dice: 0.9226\n",
      "2024-01-10 17:34:22.093741: \n",
      "2024-01-10 17:34:22.099744: Epoch 159\n",
      "2024-01-10 17:34:22.105744: Current learning rate: 0.00856\n",
      "2024-01-10 17:35:03.598896: train_loss -0.9261\n",
      "2024-01-10 17:35:03.638419: val_loss -0.844\n",
      "2024-01-10 17:35:03.648417: Pseudo dice [0.8697, 0.9595, 0.9437]\n",
      "2024-01-10 17:35:03.681048: Epoch time: 41.51 s\n",
      "2024-01-10 17:35:03.694050: Yayy! New best EMA pseudo Dice: 0.9227\n",
      "2024-01-10 17:35:05.092122: \n",
      "2024-01-10 17:35:05.098157: Epoch 160\n",
      "2024-01-10 17:35:05.104856: Current learning rate: 0.00855\n",
      "2024-01-10 17:35:46.613873: train_loss -0.9245\n",
      "2024-01-10 17:35:46.625346: val_loss -0.8397\n",
      "2024-01-10 17:35:46.638353: Pseudo dice [0.8635, 0.9586, 0.942]\n",
      "2024-01-10 17:35:46.647811: Epoch time: 41.52 s\n",
      "2024-01-10 17:35:47.924186: \n",
      "2024-01-10 17:35:47.931724: Epoch 161\n",
      "2024-01-10 17:35:47.936845: Current learning rate: 0.00854\n",
      "2024-01-10 17:36:29.273153: train_loss -0.9255\n",
      "2024-01-10 17:36:29.284156: val_loss -0.8428\n",
      "2024-01-10 17:36:29.295155: Pseudo dice [0.8599, 0.9593, 0.9436]\n",
      "2024-01-10 17:36:29.342550: Epoch time: 41.35 s\n",
      "2024-01-10 17:36:30.592705: \n",
      "2024-01-10 17:36:30.600001: Epoch 162\n",
      "2024-01-10 17:36:30.608010: Current learning rate: 0.00853\n",
      "2024-01-10 17:37:12.505391: train_loss -0.9258\n",
      "2024-01-10 17:37:12.513393: val_loss -0.8392\n",
      "2024-01-10 17:37:12.522394: Pseudo dice [0.8608, 0.959, 0.9437]\n",
      "2024-01-10 17:37:12.531392: Epoch time: 41.91 s\n",
      "2024-01-10 17:37:13.769683: \n",
      "2024-01-10 17:37:13.774678: Epoch 163\n",
      "2024-01-10 17:37:13.780094: Current learning rate: 0.00852\n",
      "2024-01-10 17:37:56.775165: train_loss -0.9256\n",
      "2024-01-10 17:37:56.805681: val_loss -0.8409\n",
      "2024-01-10 17:37:56.816191: Pseudo dice [0.8675, 0.959, 0.943]\n",
      "2024-01-10 17:37:56.827193: Epoch time: 43.01 s\n",
      "2024-01-10 17:37:58.105692: \n",
      "2024-01-10 17:37:58.111210: Epoch 164\n",
      "2024-01-10 17:37:58.117202: Current learning rate: 0.00851\n",
      "2024-01-10 17:38:40.234452: train_loss -0.9253\n",
      "2024-01-10 17:38:40.246463: val_loss -0.8423\n",
      "2024-01-10 17:38:40.257464: Pseudo dice [0.863, 0.9597, 0.9438]\n",
      "2024-01-10 17:38:40.295461: Epoch time: 42.13 s\n",
      "2024-01-10 17:38:41.655347: \n",
      "2024-01-10 17:38:41.661329: Epoch 165\n",
      "2024-01-10 17:38:41.666333: Current learning rate: 0.0085\n",
      "2024-01-10 17:39:24.930460: train_loss -0.9258\n",
      "2024-01-10 17:39:24.947464: val_loss -0.8433\n",
      "2024-01-10 17:39:24.965984: Pseudo dice [0.8673, 0.9591, 0.9429]\n",
      "2024-01-10 17:39:24.982981: Epoch time: 43.28 s\n",
      "2024-01-10 17:39:26.441639: \n",
      "2024-01-10 17:39:26.448326: Epoch 166\n",
      "2024-01-10 17:39:26.453260: Current learning rate: 0.00849\n",
      "2024-01-10 17:40:08.949818: train_loss -0.9256\n",
      "2024-01-10 17:40:08.965826: val_loss -0.8419\n",
      "2024-01-10 17:40:08.981353: Pseudo dice [0.8752, 0.9583, 0.9417]\n",
      "2024-01-10 17:40:09.001353: Epoch time: 42.51 s\n",
      "2024-01-10 17:40:10.618684: \n",
      "2024-01-10 17:40:10.624685: Epoch 167\n",
      "2024-01-10 17:40:10.631684: Current learning rate: 0.00848\n",
      "2024-01-10 17:40:52.599178: train_loss -0.9253\n",
      "2024-01-10 17:40:52.610690: val_loss -0.8437\n",
      "2024-01-10 17:40:52.619696: Pseudo dice [0.8662, 0.9597, 0.9435]\n",
      "2024-01-10 17:40:52.628691: Epoch time: 41.98 s\n",
      "2024-01-10 17:40:52.637693: Yayy! New best EMA pseudo Dice: 0.9227\n",
      "2024-01-10 17:40:54.389446: \n",
      "2024-01-10 17:40:54.398446: Epoch 168\n",
      "2024-01-10 17:40:54.403965: Current learning rate: 0.00847\n",
      "2024-01-10 17:41:36.067596: train_loss -0.9264\n",
      "2024-01-10 17:41:36.111072: val_loss -0.8446\n",
      "2024-01-10 17:41:36.123956: Pseudo dice [0.8711, 0.9598, 0.9442]\n",
      "2024-01-10 17:41:36.136741: Epoch time: 41.68 s\n",
      "2024-01-10 17:41:36.151038: Yayy! New best EMA pseudo Dice: 0.923\n",
      "2024-01-10 17:41:37.609045: \n",
      "2024-01-10 17:41:37.615038: Epoch 169\n",
      "2024-01-10 17:41:37.620037: Current learning rate: 0.00847\n",
      "2024-01-10 17:42:18.633863: train_loss -0.927\n",
      "2024-01-10 17:42:18.645862: val_loss -0.845\n",
      "2024-01-10 17:42:18.658382: Pseudo dice [0.8717, 0.9596, 0.9436]\n",
      "2024-01-10 17:42:18.670381: Epoch time: 41.03 s\n",
      "2024-01-10 17:42:18.681896: Yayy! New best EMA pseudo Dice: 0.9232\n",
      "2024-01-10 17:42:20.226389: \n",
      "2024-01-10 17:42:20.232477: Epoch 170\n",
      "2024-01-10 17:42:20.236665: Current learning rate: 0.00846\n",
      "2024-01-10 17:43:02.060884: train_loss -0.9262\n",
      "2024-01-10 17:43:02.074893: val_loss -0.8422\n",
      "2024-01-10 17:43:02.128422: Pseudo dice [0.8695, 0.9593, 0.943]\n",
      "2024-01-10 17:43:02.146421: Epoch time: 41.84 s\n",
      "2024-01-10 17:43:02.163419: Yayy! New best EMA pseudo Dice: 0.9232\n",
      "2024-01-10 17:43:04.097050: \n",
      "2024-01-10 17:43:04.106573: Epoch 171\n",
      "2024-01-10 17:43:04.116577: Current learning rate: 0.00845\n",
      "2024-01-10 17:43:46.589026: train_loss -0.9261\n",
      "2024-01-10 17:43:46.599026: val_loss -0.8376\n",
      "2024-01-10 17:43:46.611037: Pseudo dice [0.8627, 0.958, 0.9413]\n",
      "2024-01-10 17:43:46.620544: Epoch time: 42.49 s\n",
      "2024-01-10 17:43:48.126484: \n",
      "2024-01-10 17:43:48.132489: Epoch 172\n",
      "2024-01-10 17:43:48.138488: Current learning rate: 0.00844\n",
      "2024-01-10 17:44:31.845773: train_loss -0.9265\n",
      "2024-01-10 17:44:31.858777: val_loss -0.8433\n",
      "2024-01-10 17:44:31.865777: Pseudo dice [0.8661, 0.9598, 0.944]\n",
      "2024-01-10 17:44:31.875285: Epoch time: 43.72 s\n",
      "2024-01-10 17:44:33.250241: \n",
      "2024-01-10 17:44:33.257162: Epoch 173\n",
      "2024-01-10 17:44:33.263177: Current learning rate: 0.00843\n",
      "2024-01-10 17:45:18.541006: train_loss -0.9267\n",
      "2024-01-10 17:45:18.556626: val_loss -0.8363\n",
      "2024-01-10 17:45:18.568158: Pseudo dice [0.8516, 0.9593, 0.9433]\n",
      "2024-01-10 17:45:18.579558: Epoch time: 45.29 s\n",
      "2024-01-10 17:45:20.063607: \n",
      "2024-01-10 17:45:20.071605: Epoch 174\n",
      "2024-01-10 17:45:20.076607: Current learning rate: 0.00842\n",
      "2024-01-10 17:46:06.549813: train_loss -0.9274\n",
      "2024-01-10 17:46:06.576277: val_loss -0.842\n",
      "2024-01-10 17:46:06.619135: Pseudo dice [0.8641, 0.9595, 0.9427]\n",
      "2024-01-10 17:46:06.630440: Epoch time: 46.49 s\n",
      "2024-01-10 17:46:08.453020: \n",
      "2024-01-10 17:46:08.459018: Epoch 175\n",
      "2024-01-10 17:46:08.465018: Current learning rate: 0.00841\n",
      "2024-01-10 17:46:51.848545: train_loss -0.9272\n",
      "2024-01-10 17:46:51.858542: val_loss -0.8436\n",
      "2024-01-10 17:46:51.868544: Pseudo dice [0.8708, 0.9592, 0.9431]\n",
      "2024-01-10 17:46:51.900546: Epoch time: 43.4 s\n",
      "2024-01-10 17:46:53.131732: \n",
      "2024-01-10 17:46:53.136732: Epoch 176\n",
      "2024-01-10 17:46:53.141733: Current learning rate: 0.0084\n",
      "2024-01-10 17:47:33.744027: train_loss -0.9278\n",
      "2024-01-10 17:47:33.759028: val_loss -0.8441\n",
      "2024-01-10 17:47:33.768027: Pseudo dice [0.8695, 0.9599, 0.9439]\n",
      "2024-01-10 17:47:33.777029: Epoch time: 40.61 s\n",
      "2024-01-10 17:47:35.193310: \n",
      "2024-01-10 17:47:35.199311: Epoch 177\n",
      "2024-01-10 17:47:35.204312: Current learning rate: 0.00839\n",
      "2024-01-10 17:48:15.689063: train_loss -0.927\n",
      "2024-01-10 17:48:15.701066: val_loss -0.8451\n",
      "2024-01-10 17:48:15.731064: Pseudo dice [0.8671, 0.9602, 0.9443]\n",
      "2024-01-10 17:48:15.740069: Epoch time: 40.5 s\n",
      "2024-01-10 17:48:16.944010: \n",
      "2024-01-10 17:48:16.951954: Epoch 178\n",
      "2024-01-10 17:48:16.956628: Current learning rate: 0.00838\n",
      "2024-01-10 17:48:57.615093: train_loss -0.9268\n",
      "2024-01-10 17:48:57.626219: val_loss -0.845\n",
      "2024-01-10 17:48:57.652018: Pseudo dice [0.8713, 0.9596, 0.9435]\n",
      "2024-01-10 17:48:57.663570: Epoch time: 40.67 s\n",
      "2024-01-10 17:48:58.879852: \n",
      "2024-01-10 17:48:58.888840: Epoch 179\n",
      "2024-01-10 17:48:58.892847: Current learning rate: 0.00837\n",
      "2024-01-10 17:49:39.609353: train_loss -0.9264\n",
      "2024-01-10 17:49:39.619355: val_loss -0.8412\n",
      "2024-01-10 17:49:39.628355: Pseudo dice [0.869, 0.9592, 0.9435]\n",
      "2024-01-10 17:49:39.638355: Epoch time: 40.73 s\n",
      "2024-01-10 17:49:40.887248: \n",
      "2024-01-10 17:49:40.892231: Epoch 180\n",
      "2024-01-10 17:49:40.897162: Current learning rate: 0.00836\n",
      "2024-01-10 17:50:21.331965: train_loss -0.9271\n",
      "2024-01-10 17:50:21.341963: val_loss -0.8412\n",
      "2024-01-10 17:50:21.374965: Pseudo dice [0.8616, 0.9597, 0.9437]\n",
      "2024-01-10 17:50:21.385965: Epoch time: 40.45 s\n",
      "2024-01-10 17:50:22.614502: \n",
      "2024-01-10 17:50:22.622871: Epoch 181\n",
      "2024-01-10 17:50:22.626892: Current learning rate: 0.00836\n",
      "2024-01-10 17:51:02.995988: train_loss -0.9276\n",
      "2024-01-10 17:51:03.003973: val_loss -0.8408\n",
      "2024-01-10 17:51:03.014132: Pseudo dice [0.8637, 0.9584, 0.9421]\n",
      "2024-01-10 17:51:03.023139: Epoch time: 40.38 s\n",
      "2024-01-10 17:51:04.401850: \n",
      "2024-01-10 17:51:04.412860: Epoch 182\n",
      "2024-01-10 17:51:04.417847: Current learning rate: 0.00835\n",
      "2024-01-10 17:51:45.369585: train_loss -0.9286\n",
      "2024-01-10 17:51:45.379585: val_loss -0.8396\n",
      "2024-01-10 17:51:45.411595: Pseudo dice [0.8634, 0.9586, 0.9424]\n",
      "2024-01-10 17:51:45.420585: Epoch time: 40.97 s\n",
      "2024-01-10 17:51:46.657132: \n",
      "2024-01-10 17:51:46.667212: Epoch 183\n",
      "2024-01-10 17:51:46.671229: Current learning rate: 0.00834\n",
      "2024-01-10 17:52:27.247665: train_loss -0.9273\n",
      "2024-01-10 17:52:27.260184: val_loss -0.8404\n",
      "2024-01-10 17:52:27.294229: Pseudo dice [0.8669, 0.9593, 0.9434]\n",
      "2024-01-10 17:52:27.305251: Epoch time: 40.59 s\n",
      "2024-01-10 17:52:28.554310: \n",
      "2024-01-10 17:52:28.559989: Epoch 184\n",
      "2024-01-10 17:52:28.563989: Current learning rate: 0.00833\n",
      "2024-01-10 17:53:10.712402: train_loss -0.928\n",
      "2024-01-10 17:53:10.721920: val_loss -0.8456\n",
      "2024-01-10 17:53:10.728918: Pseudo dice [0.8696, 0.9596, 0.9433]\n",
      "2024-01-10 17:53:10.737918: Epoch time: 42.16 s\n",
      "2024-01-10 17:53:12.239935: \n",
      "2024-01-10 17:53:12.247926: Epoch 185\n",
      "2024-01-10 17:53:12.255934: Current learning rate: 0.00832\n",
      "2024-01-10 17:53:53.717069: train_loss -0.9277\n",
      "2024-01-10 17:53:53.726074: val_loss -0.8378\n",
      "2024-01-10 17:53:53.735589: Pseudo dice [0.8582, 0.9593, 0.9436]\n",
      "2024-01-10 17:53:53.743586: Epoch time: 41.48 s\n",
      "2024-01-10 17:53:54.969978: \n",
      "2024-01-10 17:53:54.977442: Epoch 186\n",
      "2024-01-10 17:53:54.981843: Current learning rate: 0.00831\n",
      "2024-01-10 17:54:35.361033: train_loss -0.9272\n",
      "2024-01-10 17:54:35.373194: val_loss -0.8344\n",
      "2024-01-10 17:54:35.398198: Pseudo dice [0.8555, 0.9582, 0.9417]\n",
      "2024-01-10 17:54:35.408456: Epoch time: 40.39 s\n",
      "2024-01-10 17:54:36.761934: \n",
      "2024-01-10 17:54:36.771317: Epoch 187\n",
      "2024-01-10 17:54:36.779316: Current learning rate: 0.0083\n",
      "2024-01-10 17:55:16.848438: train_loss -0.9274\n",
      "2024-01-10 17:55:16.857434: val_loss -0.8445\n",
      "2024-01-10 17:55:16.867440: Pseudo dice [0.8706, 0.9599, 0.9441]\n",
      "2024-01-10 17:55:16.878445: Epoch time: 40.09 s\n",
      "2024-01-10 17:55:18.139679: \n",
      "2024-01-10 17:55:18.147560: Epoch 188\n",
      "2024-01-10 17:55:18.151874: Current learning rate: 0.00829\n",
      "2024-01-10 17:55:58.452382: train_loss -0.9273\n",
      "2024-01-10 17:55:58.462382: val_loss -0.841\n",
      "2024-01-10 17:55:58.471384: Pseudo dice [0.8644, 0.9591, 0.9434]\n",
      "2024-01-10 17:55:58.479381: Epoch time: 40.31 s\n",
      "2024-01-10 17:55:59.682003: \n",
      "2024-01-10 17:55:59.692861: Epoch 189\n",
      "2024-01-10 17:55:59.701907: Current learning rate: 0.00828\n",
      "2024-01-10 17:56:40.659499: train_loss -0.9286\n",
      "2024-01-10 17:56:40.697503: val_loss -0.846\n",
      "2024-01-10 17:56:40.705503: Pseudo dice [0.8692, 0.9594, 0.9436]\n",
      "2024-01-10 17:56:40.714019: Epoch time: 40.98 s\n",
      "2024-01-10 17:56:42.391679: \n",
      "2024-01-10 17:56:42.396678: Epoch 190\n",
      "2024-01-10 17:56:42.401678: Current learning rate: 0.00827\n",
      "2024-01-10 17:57:23.251693: train_loss -0.9284\n",
      "2024-01-10 17:57:23.263199: val_loss -0.8317\n",
      "2024-01-10 17:57:23.305201: Pseudo dice [0.8502, 0.9584, 0.9419]\n",
      "2024-01-10 17:57:23.316200: Epoch time: 40.86 s\n",
      "2024-01-10 17:57:24.801164: \n",
      "2024-01-10 17:57:24.806166: Epoch 191\n",
      "2024-01-10 17:57:24.811317: Current learning rate: 0.00826\n",
      "2024-01-10 17:58:08.167081: train_loss -0.9295\n",
      "2024-01-10 17:58:08.181798: val_loss -0.8461\n",
      "2024-01-10 17:58:08.190084: Pseudo dice [0.8671, 0.9601, 0.9448]\n",
      "2024-01-10 17:58:08.199090: Epoch time: 43.37 s\n",
      "2024-01-10 17:58:10.508884: \n",
      "2024-01-10 17:58:10.515138: Epoch 192\n",
      "2024-01-10 17:58:10.520602: Current learning rate: 0.00825\n",
      "2024-01-10 17:58:52.785968: train_loss -0.9295\n",
      "2024-01-10 17:58:52.812396: val_loss -0.8428\n",
      "2024-01-10 17:58:52.820917: Pseudo dice [0.8689, 0.9594, 0.9441]\n",
      "2024-01-10 17:58:52.828433: Epoch time: 42.28 s\n",
      "2024-01-10 17:58:54.470937: \n",
      "2024-01-10 17:58:54.483285: Epoch 193\n",
      "2024-01-10 17:58:54.487286: Current learning rate: 0.00824\n",
      "2024-01-10 17:59:36.665264: train_loss -0.9294\n",
      "2024-01-10 17:59:36.676264: val_loss -0.8396\n",
      "2024-01-10 17:59:36.687263: Pseudo dice [0.8634, 0.9598, 0.9439]\n",
      "2024-01-10 17:59:36.717491: Epoch time: 42.2 s\n",
      "2024-01-10 17:59:38.430214: \n",
      "2024-01-10 17:59:38.436225: Epoch 194\n",
      "2024-01-10 17:59:38.440735: Current learning rate: 0.00824\n",
      "2024-01-10 18:00:22.333783: train_loss -0.929\n",
      "2024-01-10 18:00:22.347321: val_loss -0.8402\n",
      "2024-01-10 18:00:22.359074: Pseudo dice [0.8671, 0.9591, 0.9428]\n",
      "2024-01-10 18:00:22.396909: Epoch time: 43.9 s\n",
      "2024-01-10 18:00:24.301416: \n",
      "2024-01-10 18:00:24.308422: Epoch 195\n",
      "2024-01-10 18:00:24.313961: Current learning rate: 0.00823\n",
      "2024-01-10 18:01:08.118701: train_loss -0.929\n",
      "2024-01-10 18:01:08.131638: val_loss -0.841\n",
      "2024-01-10 18:01:08.145722: Pseudo dice [0.8655, 0.9591, 0.9434]\n",
      "2024-01-10 18:01:08.187816: Epoch time: 43.82 s\n",
      "2024-01-10 18:01:09.747564: \n",
      "2024-01-10 18:01:09.753313: Epoch 196\n",
      "2024-01-10 18:01:09.758480: Current learning rate: 0.00822\n",
      "2024-01-10 18:01:52.052125: train_loss -0.9298\n",
      "2024-01-10 18:01:52.087403: val_loss -0.8429\n",
      "2024-01-10 18:01:52.097490: Pseudo dice [0.8679, 0.958, 0.9413]\n",
      "2024-01-10 18:01:52.108634: Epoch time: 42.31 s\n",
      "2024-01-10 18:01:53.848938: \n",
      "2024-01-10 18:01:53.854469: Epoch 197\n",
      "2024-01-10 18:01:53.859469: Current learning rate: 0.00821\n",
      "2024-01-10 18:02:37.102547: train_loss -0.9292\n",
      "2024-01-10 18:02:37.114574: val_loss -0.8442\n",
      "2024-01-10 18:02:37.124095: Pseudo dice [0.8658, 0.9608, 0.946]\n",
      "2024-01-10 18:02:37.135654: Epoch time: 43.25 s\n",
      "2024-01-10 18:02:38.859994: \n",
      "2024-01-10 18:02:38.865812: Epoch 198\n",
      "2024-01-10 18:02:38.870686: Current learning rate: 0.0082\n",
      "2024-01-10 18:03:21.178094: train_loss -0.9298\n",
      "2024-01-10 18:03:21.186133: val_loss -0.8361\n",
      "2024-01-10 18:03:21.195139: Pseudo dice [0.8612, 0.9577, 0.9408]\n",
      "2024-01-10 18:03:21.233752: Epoch time: 42.32 s\n",
      "2024-01-10 18:03:22.853983: \n",
      "2024-01-10 18:03:22.860510: Epoch 199\n",
      "2024-01-10 18:03:22.866016: Current learning rate: 0.00819\n",
      "2024-01-10 18:04:05.016548: train_loss -0.9296\n",
      "2024-01-10 18:04:05.049907: val_loss -0.8399\n",
      "2024-01-10 18:04:05.057418: Pseudo dice [0.8644, 0.9594, 0.9438]\n",
      "2024-01-10 18:04:05.066441: Epoch time: 42.16 s\n",
      "2024-01-10 18:04:06.839641: \n",
      "2024-01-10 18:04:06.845152: Epoch 200\n",
      "2024-01-10 18:04:06.850501: Current learning rate: 0.00818\n",
      "2024-01-10 18:04:49.134149: train_loss -0.9292\n",
      "2024-01-10 18:04:49.163292: val_loss -0.8421\n",
      "2024-01-10 18:04:49.173498: Pseudo dice [0.8659, 0.9597, 0.9443]\n",
      "2024-01-10 18:04:49.182219: Epoch time: 42.3 s\n",
      "2024-01-10 18:04:50.776644: \n",
      "2024-01-10 18:04:50.782020: Epoch 201\n",
      "2024-01-10 18:04:50.787713: Current learning rate: 0.00817\n",
      "2024-01-10 18:05:32.306252: train_loss -0.9297\n",
      "2024-01-10 18:05:32.314252: val_loss -0.8419\n",
      "2024-01-10 18:05:32.321545: Pseudo dice [0.8667, 0.9592, 0.9425]\n",
      "2024-01-10 18:05:32.327548: Epoch time: 41.53 s\n",
      "2024-01-10 18:05:33.784197: \n",
      "2024-01-10 18:05:33.790197: Epoch 202\n",
      "2024-01-10 18:05:33.794192: Current learning rate: 0.00816\n",
      "2024-01-10 18:06:14.691482: train_loss -0.9295\n",
      "2024-01-10 18:06:14.699488: val_loss -0.8446\n",
      "2024-01-10 18:06:14.706487: Pseudo dice [0.8707, 0.9599, 0.9438]\n",
      "2024-01-10 18:06:14.714489: Epoch time: 40.91 s\n",
      "2024-01-10 18:06:16.175652: \n",
      "2024-01-10 18:06:16.181660: Epoch 203\n",
      "2024-01-10 18:06:16.186668: Current learning rate: 0.00815\n",
      "2024-01-10 18:06:56.968416: train_loss -0.9294\n",
      "2024-01-10 18:06:56.977419: val_loss -0.8388\n",
      "2024-01-10 18:06:56.983417: Pseudo dice [0.8616, 0.9584, 0.9418]\n",
      "2024-01-10 18:06:56.992419: Epoch time: 40.79 s\n",
      "2024-01-10 18:06:58.469608: \n",
      "2024-01-10 18:06:58.473851: Epoch 204\n",
      "2024-01-10 18:06:58.478770: Current learning rate: 0.00814\n",
      "2024-01-10 18:07:39.476835: train_loss -0.9302\n",
      "2024-01-10 18:07:39.486123: val_loss -0.8405\n",
      "2024-01-10 18:07:39.493117: Pseudo dice [0.8687, 0.9592, 0.9424]\n",
      "2024-01-10 18:07:39.501117: Epoch time: 41.01 s\n",
      "2024-01-10 18:07:41.066113: \n",
      "2024-01-10 18:07:41.076176: Epoch 205\n",
      "2024-01-10 18:07:41.081149: Current learning rate: 0.00813\n",
      "2024-01-10 18:08:21.896533: train_loss -0.9292\n",
      "2024-01-10 18:08:21.905041: val_loss -0.8459\n",
      "2024-01-10 18:08:21.913041: Pseudo dice [0.867, 0.9602, 0.9446]\n",
      "2024-01-10 18:08:21.935042: Epoch time: 40.83 s\n",
      "2024-01-10 18:08:23.489847: \n",
      "2024-01-10 18:08:23.496170: Epoch 206\n",
      "2024-01-10 18:08:23.500174: Current learning rate: 0.00813\n",
      "2024-01-10 18:09:04.589915: train_loss -0.9297\n",
      "2024-01-10 18:09:04.598915: val_loss -0.8383\n",
      "2024-01-10 18:09:04.626427: Pseudo dice [0.8665, 0.9589, 0.9424]\n",
      "2024-01-10 18:09:04.634426: Epoch time: 41.1 s\n",
      "2024-01-10 18:09:06.044898: \n",
      "2024-01-10 18:09:06.050901: Epoch 207\n",
      "2024-01-10 18:09:06.056948: Current learning rate: 0.00812\n",
      "2024-01-10 18:09:46.814153: train_loss -0.9308\n",
      "2024-01-10 18:09:46.823150: val_loss -0.8421\n",
      "2024-01-10 18:09:46.832151: Pseudo dice [0.868, 0.9599, 0.9444]\n",
      "2024-01-10 18:09:46.840151: Epoch time: 40.77 s\n",
      "2024-01-10 18:09:48.237101: \n",
      "2024-01-10 18:09:48.243121: Epoch 208\n",
      "2024-01-10 18:09:48.247165: Current learning rate: 0.00811\n",
      "2024-01-10 18:10:28.847892: train_loss -0.9301\n",
      "2024-01-10 18:10:28.855893: val_loss -0.8423\n",
      "2024-01-10 18:10:28.862898: Pseudo dice [0.8669, 0.959, 0.9432]\n",
      "2024-01-10 18:10:28.871404: Epoch time: 40.61 s\n",
      "2024-01-10 18:10:30.242758: \n",
      "2024-01-10 18:10:30.248667: Epoch 209\n",
      "2024-01-10 18:10:30.252728: Current learning rate: 0.0081\n",
      "2024-01-10 18:11:10.609285: train_loss -0.9308\n",
      "2024-01-10 18:11:10.620277: val_loss -0.8339\n",
      "2024-01-10 18:11:10.625789: Pseudo dice [0.8561, 0.9583, 0.9411]\n",
      "2024-01-10 18:11:10.630788: Epoch time: 40.37 s\n",
      "2024-01-10 18:11:12.001614: \n",
      "2024-01-10 18:11:12.009682: Epoch 210\n",
      "2024-01-10 18:11:12.013674: Current learning rate: 0.00809\n",
      "2024-01-10 18:11:52.233590: train_loss -0.9295\n",
      "2024-01-10 18:11:52.239591: val_loss -0.8446\n",
      "2024-01-10 18:11:52.247593: Pseudo dice [0.867, 0.9601, 0.9445]\n",
      "2024-01-10 18:11:52.256592: Epoch time: 40.23 s\n",
      "2024-01-10 18:11:53.799835: \n",
      "2024-01-10 18:11:53.808059: Epoch 211\n",
      "2024-01-10 18:11:53.816808: Current learning rate: 0.00808\n",
      "2024-01-10 18:12:34.153982: train_loss -0.9304\n",
      "2024-01-10 18:12:34.162983: val_loss -0.8414\n",
      "2024-01-10 18:12:34.201570: Pseudo dice [0.8641, 0.9587, 0.9429]\n",
      "2024-01-10 18:12:34.210572: Epoch time: 40.36 s\n",
      "2024-01-10 18:12:35.588056: \n",
      "2024-01-10 18:12:35.594031: Epoch 212\n",
      "2024-01-10 18:12:35.599013: Current learning rate: 0.00807\n",
      "2024-01-10 18:13:15.946890: train_loss -0.9295\n",
      "2024-01-10 18:13:15.973890: val_loss -0.8433\n",
      "2024-01-10 18:13:15.979889: Pseudo dice [0.8658, 0.9599, 0.9446]\n",
      "2024-01-10 18:13:15.984891: Epoch time: 40.36 s\n",
      "2024-01-10 18:13:17.326648: \n",
      "2024-01-10 18:13:17.333804: Epoch 213\n",
      "2024-01-10 18:13:17.344829: Current learning rate: 0.00806\n",
      "2024-01-10 18:13:57.610244: train_loss -0.9305\n",
      "2024-01-10 18:13:57.618245: val_loss -0.8426\n",
      "2024-01-10 18:13:57.648470: Pseudo dice [0.8648, 0.9602, 0.9437]\n",
      "2024-01-10 18:13:57.655471: Epoch time: 40.28 s\n",
      "2024-01-10 18:13:59.053181: \n",
      "2024-01-10 18:13:59.065242: Epoch 214\n",
      "2024-01-10 18:13:59.074253: Current learning rate: 0.00805\n",
      "2024-01-10 18:14:39.319747: train_loss -0.9305\n",
      "2024-01-10 18:14:39.327758: val_loss -0.8429\n",
      "2024-01-10 18:14:39.333756: Pseudo dice [0.867, 0.9598, 0.9439]\n",
      "2024-01-10 18:14:39.342747: Epoch time: 40.27 s\n",
      "2024-01-10 18:14:40.757714: \n",
      "2024-01-10 18:14:40.767575: Epoch 215\n",
      "2024-01-10 18:14:40.777644: Current learning rate: 0.00804\n",
      "2024-01-10 18:15:20.961526: train_loss -0.9307\n",
      "2024-01-10 18:15:20.969527: val_loss -0.8422\n",
      "2024-01-10 18:15:20.976526: Pseudo dice [0.8663, 0.9596, 0.9441]\n",
      "2024-01-10 18:15:20.983527: Epoch time: 40.2 s\n",
      "2024-01-10 18:15:22.500177: \n",
      "2024-01-10 18:15:22.505177: Epoch 216\n",
      "2024-01-10 18:15:22.510184: Current learning rate: 0.00803\n",
      "2024-01-10 18:16:03.068940: train_loss -0.9319\n",
      "2024-01-10 18:16:03.078939: val_loss -0.8423\n",
      "2024-01-10 18:16:03.087938: Pseudo dice [0.8712, 0.9588, 0.942]\n",
      "2024-01-10 18:16:03.096939: Epoch time: 40.57 s\n",
      "2024-01-10 18:16:04.459207: \n",
      "2024-01-10 18:16:04.464632: Epoch 217\n",
      "2024-01-10 18:16:04.468653: Current learning rate: 0.00802\n",
      "2024-01-10 18:16:46.774178: train_loss -0.9315\n",
      "2024-01-10 18:16:46.797242: val_loss -0.8349\n",
      "2024-01-10 18:16:46.844844: Pseudo dice [0.864, 0.9579, 0.9419]\n",
      "2024-01-10 18:16:46.855367: Epoch time: 42.32 s\n",
      "2024-01-10 18:16:48.350952: \n",
      "2024-01-10 18:16:48.356761: Epoch 218\n",
      "2024-01-10 18:16:48.361776: Current learning rate: 0.00801\n",
      "2024-01-10 18:17:30.477043: train_loss -0.9307\n",
      "2024-01-10 18:17:30.491560: val_loss -0.8393\n",
      "2024-01-10 18:17:30.504605: Pseudo dice [0.8634, 0.9591, 0.9426]\n",
      "2024-01-10 18:17:30.513133: Epoch time: 42.13 s\n",
      "2024-01-10 18:17:31.973133: \n",
      "2024-01-10 18:17:31.978208: Epoch 219\n",
      "2024-01-10 18:17:31.983159: Current learning rate: 0.00801\n",
      "2024-01-10 18:18:14.098613: train_loss -0.9317\n",
      "2024-01-10 18:18:14.108649: val_loss -0.8455\n",
      "2024-01-10 18:18:14.118732: Pseudo dice [0.8689, 0.961, 0.9463]\n",
      "2024-01-10 18:18:14.165023: Epoch time: 42.13 s\n",
      "2024-01-10 18:18:15.637680: \n",
      "2024-01-10 18:18:15.643351: Epoch 220\n",
      "2024-01-10 18:18:15.648237: Current learning rate: 0.008\n",
      "2024-01-10 18:18:57.666648: train_loss -0.9308\n",
      "2024-01-10 18:18:57.675968: val_loss -0.8386\n",
      "2024-01-10 18:18:57.687297: Pseudo dice [0.8704, 0.959, 0.9425]\n",
      "2024-01-10 18:18:57.698440: Epoch time: 42.03 s\n",
      "2024-01-10 18:18:59.466126: \n",
      "2024-01-10 18:18:59.472117: Epoch 221\n",
      "2024-01-10 18:18:59.476417: Current learning rate: 0.00799\n",
      "2024-01-10 18:19:41.738964: train_loss -0.9318\n",
      "2024-01-10 18:19:41.750004: val_loss -0.8398\n",
      "2024-01-10 18:19:41.759534: Pseudo dice [0.8647, 0.9591, 0.9436]\n",
      "2024-01-10 18:19:41.791527: Epoch time: 42.27 s\n",
      "2024-01-10 18:19:43.185481: \n",
      "2024-01-10 18:19:43.191478: Epoch 222\n",
      "2024-01-10 18:19:43.197016: Current learning rate: 0.00798\n",
      "2024-01-10 18:20:25.453019: train_loss -0.9314\n",
      "2024-01-10 18:20:25.466551: val_loss -0.8389\n",
      "2024-01-10 18:20:25.516200: Pseudo dice [0.8597, 0.9592, 0.9434]\n",
      "2024-01-10 18:20:25.530724: Epoch time: 42.27 s\n",
      "2024-01-10 18:20:27.015936: \n",
      "2024-01-10 18:20:27.020924: Epoch 223\n",
      "2024-01-10 18:20:27.026474: Current learning rate: 0.00797\n",
      "2024-01-10 18:21:09.933413: train_loss -0.9314\n",
      "2024-01-10 18:21:09.940455: val_loss -0.8408\n",
      "2024-01-10 18:21:09.948462: Pseudo dice [0.8702, 0.9594, 0.9431]\n",
      "2024-01-10 18:21:09.957461: Epoch time: 42.92 s\n",
      "2024-01-10 18:21:11.433901: \n",
      "2024-01-10 18:21:11.443901: Epoch 224\n",
      "2024-01-10 18:21:11.449495: Current learning rate: 0.00796\n",
      "2024-01-10 18:21:53.648385: train_loss -0.9319\n",
      "2024-01-10 18:21:53.662384: val_loss -0.8411\n",
      "2024-01-10 18:21:53.673383: Pseudo dice [0.8716, 0.9585, 0.9431]\n",
      "2024-01-10 18:21:53.684386: Epoch time: 42.22 s\n",
      "2024-01-10 18:21:55.054017: \n",
      "2024-01-10 18:21:55.059013: Epoch 225\n",
      "2024-01-10 18:21:55.063012: Current learning rate: 0.00795\n",
      "2024-01-10 18:22:37.252443: train_loss -0.9321\n",
      "2024-01-10 18:22:37.262444: val_loss -0.844\n",
      "2024-01-10 18:22:37.270450: Pseudo dice [0.8669, 0.9599, 0.9442]\n",
      "2024-01-10 18:22:37.282444: Epoch time: 42.2 s\n",
      "2024-01-10 18:22:38.831342: \n",
      "2024-01-10 18:22:38.837360: Epoch 226\n",
      "2024-01-10 18:22:38.842345: Current learning rate: 0.00794\n",
      "2024-01-10 18:23:22.942929: train_loss -0.9321\n",
      "2024-01-10 18:23:22.956438: val_loss -0.8381\n",
      "2024-01-10 18:23:22.969439: Pseudo dice [0.8627, 0.9587, 0.9418]\n",
      "2024-01-10 18:23:23.007437: Epoch time: 44.11 s\n",
      "2024-01-10 18:23:24.510191: \n",
      "2024-01-10 18:23:24.517193: Epoch 227\n",
      "2024-01-10 18:23:24.525199: Current learning rate: 0.00793\n",
      "2024-01-10 18:24:07.803251: train_loss -0.9317\n",
      "2024-01-10 18:24:07.813251: val_loss -0.8453\n",
      "2024-01-10 18:24:07.822254: Pseudo dice [0.87, 0.9604, 0.9457]\n",
      "2024-01-10 18:24:07.832253: Epoch time: 43.3 s\n",
      "2024-01-10 18:24:09.189262: \n",
      "2024-01-10 18:24:09.196261: Epoch 228\n",
      "2024-01-10 18:24:09.202258: Current learning rate: 0.00792\n",
      "2024-01-10 18:24:50.854396: train_loss -0.9312\n",
      "2024-01-10 18:24:50.865474: val_loss -0.8328\n",
      "2024-01-10 18:24:50.874994: Pseudo dice [0.8542, 0.9578, 0.9412]\n",
      "2024-01-10 18:24:50.885512: Epoch time: 41.67 s\n",
      "2024-01-10 18:24:52.170611: \n",
      "2024-01-10 18:24:52.176619: Epoch 229\n",
      "2024-01-10 18:24:52.182154: Current learning rate: 0.00791\n",
      "2024-01-10 18:25:32.811400: train_loss -0.9321\n",
      "2024-01-10 18:25:32.824402: val_loss -0.8455\n",
      "2024-01-10 18:25:32.833914: Pseudo dice [0.8695, 0.96, 0.9448]\n",
      "2024-01-10 18:25:32.842914: Epoch time: 40.64 s\n",
      "2024-01-10 18:25:34.099494: \n",
      "2024-01-10 18:25:34.106501: Epoch 230\n",
      "2024-01-10 18:25:34.115502: Current learning rate: 0.0079\n",
      "2024-01-10 18:26:14.591333: train_loss -0.932\n",
      "2024-01-10 18:26:14.603334: val_loss -0.8407\n",
      "2024-01-10 18:26:14.614334: Pseudo dice [0.864, 0.9589, 0.9426]\n",
      "2024-01-10 18:26:14.626335: Epoch time: 40.49 s\n",
      "2024-01-10 18:26:15.895555: \n",
      "2024-01-10 18:26:15.905558: Epoch 231\n",
      "2024-01-10 18:26:15.913465: Current learning rate: 0.00789\n",
      "2024-01-10 18:26:57.345012: train_loss -0.9317\n",
      "2024-01-10 18:26:57.400530: val_loss -0.8452\n",
      "2024-01-10 18:26:57.411866: Pseudo dice [0.8658, 0.96, 0.9445]\n",
      "2024-01-10 18:26:57.420866: Epoch time: 41.45 s\n",
      "2024-01-10 18:26:58.901962: \n",
      "2024-01-10 18:26:58.907031: Epoch 232\n",
      "2024-01-10 18:26:58.912596: Current learning rate: 0.00789\n",
      "2024-01-10 18:27:41.414210: train_loss -0.933\n",
      "2024-01-10 18:27:41.425212: val_loss -0.8449\n",
      "2024-01-10 18:27:41.436212: Pseudo dice [0.8721, 0.9593, 0.943]\n",
      "2024-01-10 18:27:41.468211: Epoch time: 42.51 s\n",
      "2024-01-10 18:27:42.790013: \n",
      "2024-01-10 18:27:42.796026: Epoch 233\n",
      "2024-01-10 18:27:42.801013: Current learning rate: 0.00788\n",
      "2024-01-10 18:28:26.137631: train_loss -0.933\n",
      "2024-01-10 18:28:26.150635: val_loss -0.841\n",
      "2024-01-10 18:28:26.160631: Pseudo dice [0.8696, 0.9599, 0.9451]\n",
      "2024-01-10 18:28:26.189634: Epoch time: 43.35 s\n",
      "2024-01-10 18:28:27.498836: \n",
      "2024-01-10 18:28:27.506827: Epoch 234\n",
      "2024-01-10 18:28:27.518369: Current learning rate: 0.00787\n",
      "2024-01-10 18:29:09.700348: train_loss -0.9323\n",
      "2024-01-10 18:29:09.710357: val_loss -0.8389\n",
      "2024-01-10 18:29:09.720348: Pseudo dice [0.8662, 0.9586, 0.9427]\n",
      "2024-01-10 18:29:09.729363: Epoch time: 42.2 s\n",
      "2024-01-10 18:29:11.067068: \n",
      "2024-01-10 18:29:11.072130: Epoch 235\n",
      "2024-01-10 18:29:11.077130: Current learning rate: 0.00786\n",
      "2024-01-10 18:29:52.719376: train_loss -0.9323\n",
      "2024-01-10 18:29:52.729378: val_loss -0.8369\n",
      "2024-01-10 18:29:52.740378: Pseudo dice [0.8619, 0.9585, 0.9423]\n",
      "2024-01-10 18:29:52.750378: Epoch time: 41.65 s\n",
      "2024-01-10 18:29:54.042179: \n",
      "2024-01-10 18:29:54.047179: Epoch 236\n",
      "2024-01-10 18:29:54.051190: Current learning rate: 0.00785\n",
      "2024-01-10 18:30:39.241067: train_loss -0.9327\n",
      "2024-01-10 18:30:39.251070: val_loss -0.8349\n",
      "2024-01-10 18:30:39.262068: Pseudo dice [0.8585, 0.9581, 0.9414]\n",
      "2024-01-10 18:30:39.271068: Epoch time: 45.2 s\n",
      "2024-01-10 18:30:40.962712: \n",
      "2024-01-10 18:30:40.968704: Epoch 237\n",
      "2024-01-10 18:30:40.973703: Current learning rate: 0.00784\n",
      "2024-01-10 18:31:22.182509: train_loss -0.9317\n",
      "2024-01-10 18:31:22.191915: val_loss -0.8456\n",
      "2024-01-10 18:31:22.200418: Pseudo dice [0.8661, 0.9602, 0.9444]\n",
      "2024-01-10 18:31:22.208426: Epoch time: 41.22 s\n",
      "2024-01-10 18:31:23.577265: \n",
      "2024-01-10 18:31:23.584338: Epoch 238\n",
      "2024-01-10 18:31:23.590267: Current learning rate: 0.00783\n",
      "2024-01-10 18:32:04.615854: train_loss -0.9323\n",
      "2024-01-10 18:32:04.625854: val_loss -0.8417\n",
      "2024-01-10 18:32:04.671378: Pseudo dice [0.8656, 0.9601, 0.944]\n",
      "2024-01-10 18:32:04.684378: Epoch time: 41.04 s\n",
      "2024-01-10 18:32:06.248202: \n",
      "2024-01-10 18:32:06.252555: Epoch 239\n",
      "2024-01-10 18:32:06.258113: Current learning rate: 0.00782\n",
      "2024-01-10 18:32:48.120688: train_loss -0.933\n",
      "2024-01-10 18:32:48.133683: val_loss -0.8405\n",
      "2024-01-10 18:32:48.142681: Pseudo dice [0.868, 0.9589, 0.9429]\n",
      "2024-01-10 18:32:48.180212: Epoch time: 41.87 s\n",
      "2024-01-10 18:32:49.770619: \n",
      "2024-01-10 18:32:49.776624: Epoch 240\n",
      "2024-01-10 18:32:49.782623: Current learning rate: 0.00781\n",
      "2024-01-10 18:33:31.240079: train_loss -0.9335\n",
      "2024-01-10 18:33:31.253079: val_loss -0.8313\n",
      "2024-01-10 18:33:31.266082: Pseudo dice [0.8579, 0.9571, 0.9402]\n",
      "2024-01-10 18:33:31.294078: Epoch time: 41.47 s\n",
      "2024-01-10 18:33:32.765360: \n",
      "2024-01-10 18:33:32.771256: Epoch 241\n",
      "2024-01-10 18:33:32.776259: Current learning rate: 0.0078\n",
      "2024-01-10 18:34:14.688167: train_loss -0.9342\n",
      "2024-01-10 18:34:14.698317: val_loss -0.8457\n",
      "2024-01-10 18:34:14.709313: Pseudo dice [0.8699, 0.9607, 0.9458]\n",
      "2024-01-10 18:34:14.718315: Epoch time: 41.92 s\n",
      "2024-01-10 18:34:16.209920: \n",
      "2024-01-10 18:34:16.214921: Epoch 242\n",
      "2024-01-10 18:34:16.219913: Current learning rate: 0.00779\n",
      "2024-01-10 18:34:57.666259: train_loss -0.9339\n",
      "2024-01-10 18:34:57.677265: val_loss -0.8445\n",
      "2024-01-10 18:34:57.688264: Pseudo dice [0.8725, 0.9597, 0.944]\n",
      "2024-01-10 18:34:57.699777: Epoch time: 41.46 s\n",
      "2024-01-10 18:34:59.083073: \n",
      "2024-01-10 18:34:59.089051: Epoch 243\n",
      "2024-01-10 18:34:59.094564: Current learning rate: 0.00778\n",
      "2024-01-10 18:35:40.155913: train_loss -0.9333\n",
      "2024-01-10 18:35:40.165914: val_loss -0.8381\n",
      "2024-01-10 18:35:40.196913: Pseudo dice [0.865, 0.9582, 0.9416]\n",
      "2024-01-10 18:35:40.206918: Epoch time: 41.07 s\n",
      "2024-01-10 18:35:41.409784: \n",
      "2024-01-10 18:35:41.418423: Epoch 244\n",
      "2024-01-10 18:35:41.424813: Current learning rate: 0.00777\n",
      "2024-01-10 18:36:22.444135: train_loss -0.9329\n",
      "2024-01-10 18:36:22.451138: val_loss -0.8415\n",
      "2024-01-10 18:36:22.461392: Pseudo dice [0.8669, 0.9592, 0.9434]\n",
      "2024-01-10 18:36:22.469568: Epoch time: 41.03 s\n",
      "2024-01-10 18:36:24.038058: \n",
      "2024-01-10 18:36:24.046070: Epoch 245\n",
      "2024-01-10 18:36:24.051070: Current learning rate: 0.00777\n",
      "2024-01-10 18:37:05.305223: train_loss -0.9332\n",
      "2024-01-10 18:37:05.315223: val_loss -0.8379\n",
      "2024-01-10 18:37:05.322222: Pseudo dice [0.8648, 0.9583, 0.9424]\n",
      "2024-01-10 18:37:05.328223: Epoch time: 41.27 s\n",
      "2024-01-10 18:37:06.864493: \n",
      "2024-01-10 18:37:06.870018: Epoch 246\n",
      "2024-01-10 18:37:06.875012: Current learning rate: 0.00776\n",
      "2024-01-10 18:37:47.940496: train_loss -0.9332\n",
      "2024-01-10 18:37:47.949497: val_loss -0.8362\n",
      "2024-01-10 18:37:47.959500: Pseudo dice [0.8626, 0.9588, 0.9427]\n",
      "2024-01-10 18:37:47.966499: Epoch time: 41.08 s\n",
      "2024-01-10 18:37:49.380198: \n",
      "2024-01-10 18:37:49.386266: Epoch 247\n",
      "2024-01-10 18:37:49.391380: Current learning rate: 0.00775\n",
      "2024-01-10 18:38:30.235833: train_loss -0.9338\n",
      "2024-01-10 18:38:30.250836: val_loss -0.8415\n",
      "2024-01-10 18:38:30.259838: Pseudo dice [0.87, 0.9587, 0.9429]\n",
      "2024-01-10 18:38:30.291841: Epoch time: 40.86 s\n",
      "2024-01-10 18:38:31.822330: \n",
      "2024-01-10 18:38:31.831000: Epoch 248\n",
      "2024-01-10 18:38:31.837091: Current learning rate: 0.00774\n",
      "2024-01-10 18:39:12.654588: train_loss -0.9343\n",
      "2024-01-10 18:39:12.662598: val_loss -0.8416\n",
      "2024-01-10 18:39:12.669591: Pseudo dice [0.8679, 0.9595, 0.9432]\n",
      "2024-01-10 18:39:12.676592: Epoch time: 40.83 s\n",
      "2024-01-10 18:39:14.205293: \n",
      "2024-01-10 18:39:14.210288: Epoch 249\n",
      "2024-01-10 18:39:14.215288: Current learning rate: 0.00773\n",
      "2024-01-10 18:39:55.041010: train_loss -0.9333\n",
      "2024-01-10 18:39:55.052009: val_loss -0.838\n",
      "2024-01-10 18:39:55.060010: Pseudo dice [0.865, 0.9586, 0.9431]\n",
      "2024-01-10 18:39:55.070029: Epoch time: 40.84 s\n",
      "2024-01-10 18:39:56.814551: \n",
      "2024-01-10 18:39:56.822696: Epoch 250\n",
      "2024-01-10 18:39:56.827616: Current learning rate: 0.00772\n",
      "2024-01-10 18:40:37.740141: train_loss -0.9334\n",
      "2024-01-10 18:40:37.751143: val_loss -0.841\n",
      "2024-01-10 18:40:37.761142: Pseudo dice [0.8726, 0.9585, 0.9427]\n",
      "2024-01-10 18:40:37.769141: Epoch time: 40.93 s\n",
      "2024-01-10 18:40:39.265764: \n",
      "2024-01-10 18:40:39.276914: Epoch 251\n",
      "2024-01-10 18:40:39.281909: Current learning rate: 0.00771\n",
      "2024-01-10 18:41:20.384102: train_loss -0.9335\n",
      "2024-01-10 18:41:20.392100: val_loss -0.8341\n",
      "2024-01-10 18:41:20.402259: Pseudo dice [0.8559, 0.9585, 0.9436]\n",
      "2024-01-10 18:41:20.409270: Epoch time: 41.12 s\n",
      "2024-01-10 18:41:21.847358: \n",
      "2024-01-10 18:41:21.855159: Epoch 252\n",
      "2024-01-10 18:41:21.866209: Current learning rate: 0.0077\n",
      "2024-01-10 18:42:02.618894: train_loss -0.9337\n",
      "2024-01-10 18:42:02.627891: val_loss -0.8375\n",
      "2024-01-10 18:42:02.635892: Pseudo dice [0.8622, 0.9578, 0.942]\n",
      "2024-01-10 18:42:02.642892: Epoch time: 40.77 s\n",
      "2024-01-10 18:42:04.054094: \n",
      "2024-01-10 18:42:04.059523: Epoch 253\n",
      "2024-01-10 18:42:04.064199: Current learning rate: 0.00769\n",
      "2024-01-10 18:42:44.461497: train_loss -0.9329\n",
      "2024-01-10 18:42:44.468506: val_loss -0.844\n",
      "2024-01-10 18:42:44.475503: Pseudo dice [0.8712, 0.9593, 0.9447]\n",
      "2024-01-10 18:42:44.482013: Epoch time: 40.41 s\n",
      "2024-01-10 18:42:45.905490: \n",
      "2024-01-10 18:42:45.911012: Epoch 254\n",
      "2024-01-10 18:42:45.916081: Current learning rate: 0.00768\n",
      "2024-01-10 18:43:26.256991: train_loss -0.9335\n",
      "2024-01-10 18:43:26.265996: val_loss -0.8375\n",
      "2024-01-10 18:43:26.272991: Pseudo dice [0.8626, 0.9595, 0.9425]\n",
      "2024-01-10 18:43:26.279993: Epoch time: 40.35 s\n",
      "2024-01-10 18:43:27.804064: \n",
      "2024-01-10 18:43:27.810548: Epoch 255\n",
      "2024-01-10 18:43:27.815631: Current learning rate: 0.00767\n",
      "2024-01-10 18:44:08.220201: train_loss -0.934\n",
      "2024-01-10 18:44:08.229714: val_loss -0.8405\n",
      "2024-01-10 18:44:08.236712: Pseudo dice [0.863, 0.9604, 0.9445]\n",
      "2024-01-10 18:44:08.242712: Epoch time: 40.42 s\n",
      "2024-01-10 18:44:09.677465: \n",
      "2024-01-10 18:44:09.684928: Epoch 256\n",
      "2024-01-10 18:44:09.689974: Current learning rate: 0.00766\n",
      "2024-01-10 18:44:50.210792: train_loss -0.9337\n",
      "2024-01-10 18:44:50.217306: val_loss -0.8357\n",
      "2024-01-10 18:44:50.222816: Pseudo dice [0.8643, 0.9587, 0.9431]\n",
      "2024-01-10 18:44:50.229822: Epoch time: 40.54 s\n",
      "2024-01-10 18:44:51.791189: \n",
      "2024-01-10 18:44:51.797439: Epoch 257\n",
      "2024-01-10 18:44:51.801026: Current learning rate: 0.00765\n",
      "2024-01-10 18:45:32.013219: train_loss -0.9337\n",
      "2024-01-10 18:45:32.041221: val_loss -0.8383\n",
      "2024-01-10 18:45:32.050231: Pseudo dice [0.8693, 0.9587, 0.9423]\n",
      "2024-01-10 18:45:32.058232: Epoch time: 40.22 s\n",
      "2024-01-10 18:45:33.413218: \n",
      "2024-01-10 18:45:33.418267: Epoch 258\n",
      "2024-01-10 18:45:33.423403: Current learning rate: 0.00764\n",
      "2024-01-10 18:46:13.651690: train_loss -0.9338\n",
      "2024-01-10 18:46:13.661702: val_loss -0.8401\n",
      "2024-01-10 18:46:13.671705: Pseudo dice [0.8695, 0.9586, 0.9432]\n",
      "2024-01-10 18:46:13.677219: Epoch time: 40.24 s\n",
      "2024-01-10 18:46:15.050166: \n",
      "2024-01-10 18:46:15.056562: Epoch 259\n",
      "2024-01-10 18:46:15.060630: Current learning rate: 0.00764\n",
      "2024-01-10 18:46:55.238706: train_loss -0.9336\n",
      "2024-01-10 18:46:55.248704: val_loss -0.8434\n",
      "2024-01-10 18:46:55.255703: Pseudo dice [0.8732, 0.9598, 0.9439]\n",
      "2024-01-10 18:46:55.278703: Epoch time: 40.19 s\n",
      "2024-01-10 18:46:56.704381: \n",
      "2024-01-10 18:46:56.709541: Epoch 260\n",
      "2024-01-10 18:46:56.716540: Current learning rate: 0.00763\n",
      "2024-01-10 18:47:36.865583: train_loss -0.9343\n",
      "2024-01-10 18:47:36.893581: val_loss -0.8416\n",
      "2024-01-10 18:47:36.902591: Pseudo dice [0.8705, 0.9593, 0.9428]\n",
      "2024-01-10 18:47:36.911589: Epoch time: 40.16 s\n",
      "2024-01-10 18:47:38.292940: \n",
      "2024-01-10 18:47:38.298240: Epoch 261\n",
      "2024-01-10 18:47:38.303399: Current learning rate: 0.00762\n",
      "2024-01-10 18:48:18.859179: train_loss -0.9347\n",
      "2024-01-10 18:48:18.884476: val_loss -0.8408\n",
      "2024-01-10 18:48:18.891476: Pseudo dice [0.8613, 0.96, 0.9446]\n",
      "2024-01-10 18:48:18.897476: Epoch time: 40.57 s\n",
      "2024-01-10 18:48:20.222799: \n",
      "2024-01-10 18:48:20.231801: Epoch 262\n",
      "2024-01-10 18:48:20.236433: Current learning rate: 0.00761\n",
      "2024-01-10 18:49:00.521647: train_loss -0.9342\n",
      "2024-01-10 18:49:00.527645: val_loss -0.8444\n",
      "2024-01-10 18:49:00.535649: Pseudo dice [0.8718, 0.9602, 0.9447]\n",
      "2024-01-10 18:49:00.541680: Epoch time: 40.3 s\n",
      "2024-01-10 18:49:00.549649: Yayy! New best EMA pseudo Dice: 0.9233\n",
      "2024-01-10 18:49:02.267615: \n",
      "2024-01-10 18:49:02.277703: Epoch 263\n",
      "2024-01-10 18:49:02.282699: Current learning rate: 0.0076\n",
      "2024-01-10 18:49:42.656814: train_loss -0.9354\n",
      "2024-01-10 18:49:42.664822: val_loss -0.8392\n",
      "2024-01-10 18:49:42.674754: Pseudo dice [0.8724, 0.9584, 0.9422]\n",
      "2024-01-10 18:49:42.682747: Epoch time: 40.39 s\n",
      "2024-01-10 18:49:42.708746: Yayy! New best EMA pseudo Dice: 0.9234\n",
      "2024-01-10 18:49:44.378071: \n",
      "2024-01-10 18:49:44.390275: Epoch 264\n",
      "2024-01-10 18:49:44.395310: Current learning rate: 0.00759\n",
      "2024-01-10 18:50:24.739677: train_loss -0.9351\n",
      "2024-01-10 18:50:24.749678: val_loss -0.8474\n",
      "2024-01-10 18:50:24.756677: Pseudo dice [0.8708, 0.9607, 0.9465]\n",
      "2024-01-10 18:50:24.763677: Epoch time: 40.36 s\n",
      "2024-01-10 18:50:24.771677: Yayy! New best EMA pseudo Dice: 0.9236\n",
      "2024-01-10 18:50:26.505210: \n",
      "2024-01-10 18:50:26.517174: Epoch 265\n",
      "2024-01-10 18:50:26.522171: Current learning rate: 0.00758\n",
      "2024-01-10 18:51:06.997297: train_loss -0.9344\n",
      "2024-01-10 18:51:07.005295: val_loss -0.8428\n",
      "2024-01-10 18:51:07.036816: Pseudo dice [0.8668, 0.9604, 0.9446]\n",
      "2024-01-10 18:51:07.044814: Epoch time: 40.49 s\n",
      "2024-01-10 18:51:07.050816: Yayy! New best EMA pseudo Dice: 0.9237\n",
      "2024-01-10 18:51:08.854120: \n",
      "2024-01-10 18:51:08.858795: Epoch 266\n",
      "2024-01-10 18:51:08.863920: Current learning rate: 0.00757\n",
      "2024-01-10 18:51:49.286933: train_loss -0.9342\n",
      "2024-01-10 18:51:49.296932: val_loss -0.8454\n",
      "2024-01-10 18:51:49.302933: Pseudo dice [0.8752, 0.9599, 0.9445]\n",
      "2024-01-10 18:51:49.309280: Epoch time: 40.43 s\n",
      "2024-01-10 18:51:49.315281: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2024-01-10 18:51:51.073292: \n",
      "2024-01-10 18:51:51.078356: Epoch 267\n",
      "2024-01-10 18:51:51.083939: Current learning rate: 0.00756\n",
      "2024-01-10 18:52:31.708412: train_loss -0.9341\n",
      "2024-01-10 18:52:31.739932: val_loss -0.8334\n",
      "2024-01-10 18:52:31.746927: Pseudo dice [0.8603, 0.9578, 0.9415]\n",
      "2024-01-10 18:52:31.752925: Epoch time: 40.64 s\n",
      "2024-01-10 18:52:33.158385: \n",
      "2024-01-10 18:52:33.164285: Epoch 268\n",
      "2024-01-10 18:52:33.174357: Current learning rate: 0.00755\n",
      "2024-01-10 18:53:13.454555: train_loss -0.934\n",
      "2024-01-10 18:53:13.481553: val_loss -0.8388\n",
      "2024-01-10 18:53:13.488553: Pseudo dice [0.8653, 0.9596, 0.9433]\n",
      "2024-01-10 18:53:13.494554: Epoch time: 40.3 s\n",
      "2024-01-10 18:53:14.935556: \n",
      "2024-01-10 18:53:14.941920: Epoch 269\n",
      "2024-01-10 18:53:14.951610: Current learning rate: 0.00754\n",
      "2024-01-10 18:53:55.187509: train_loss -0.9347\n",
      "2024-01-10 18:53:55.196510: val_loss -0.8434\n",
      "2024-01-10 18:53:55.203508: Pseudo dice [0.8734, 0.9593, 0.9432]\n",
      "2024-01-10 18:53:55.229508: Epoch time: 40.25 s\n",
      "2024-01-10 18:53:56.764846: \n",
      "2024-01-10 18:53:56.769903: Epoch 270\n",
      "2024-01-10 18:53:56.774900: Current learning rate: 0.00753\n",
      "2024-01-10 18:54:37.025737: train_loss -0.935\n",
      "2024-01-10 18:54:37.036738: val_loss -0.8439\n",
      "2024-01-10 18:54:37.045740: Pseudo dice [0.8625, 0.9606, 0.9453]\n",
      "2024-01-10 18:54:37.051741: Epoch time: 40.26 s\n",
      "2024-01-10 18:54:38.679033: \n",
      "2024-01-10 18:54:38.686014: Epoch 271\n",
      "2024-01-10 18:54:38.689993: Current learning rate: 0.00752\n",
      "2024-01-10 18:55:18.770452: train_loss -0.9341\n",
      "2024-01-10 18:55:18.780854: val_loss -0.8421\n",
      "2024-01-10 18:55:18.807418: Pseudo dice [0.8693, 0.9598, 0.9441]\n",
      "2024-01-10 18:55:18.815418: Epoch time: 40.09 s\n",
      "2024-01-10 18:55:20.138085: \n",
      "2024-01-10 18:55:20.142617: Epoch 272\n",
      "2024-01-10 18:55:20.148692: Current learning rate: 0.00751\n",
      "2024-01-10 18:56:00.397460: train_loss -0.9348\n",
      "2024-01-10 18:56:00.407465: val_loss -0.8394\n",
      "2024-01-10 18:56:00.435984: Pseudo dice [0.863, 0.9597, 0.9454]\n",
      "2024-01-10 18:56:00.443985: Epoch time: 40.26 s\n",
      "2024-01-10 18:56:01.808528: \n",
      "2024-01-10 18:56:01.815521: Epoch 273\n",
      "2024-01-10 18:56:01.820062: Current learning rate: 0.00751\n",
      "2024-01-10 18:56:42.002265: train_loss -0.9351\n",
      "2024-01-10 18:56:42.013258: val_loss -0.8444\n",
      "2024-01-10 18:56:42.040265: Pseudo dice [0.8717, 0.96, 0.9447]\n",
      "2024-01-10 18:56:42.048499: Epoch time: 40.19 s\n",
      "2024-01-10 18:56:43.416770: \n",
      "2024-01-10 18:56:43.422966: Epoch 274\n",
      "2024-01-10 18:56:43.437458: Current learning rate: 0.0075\n",
      "2024-01-10 18:57:23.741605: train_loss -0.9357\n",
      "2024-01-10 18:57:23.768609: val_loss -0.8423\n",
      "2024-01-10 18:57:23.777543: Pseudo dice [0.8695, 0.9605, 0.9457]\n",
      "2024-01-10 18:57:23.784123: Epoch time: 40.33 s\n",
      "2024-01-10 18:57:25.188563: \n",
      "2024-01-10 18:57:25.194292: Epoch 275\n",
      "2024-01-10 18:57:25.200291: Current learning rate: 0.00749\n",
      "2024-01-10 18:58:05.591096: train_loss -0.9344\n",
      "2024-01-10 18:58:05.601122: val_loss -0.8406\n",
      "2024-01-10 18:58:05.639118: Pseudo dice [0.8694, 0.9585, 0.9428]\n",
      "2024-01-10 18:58:05.647119: Epoch time: 40.4 s\n",
      "2024-01-10 18:58:07.214205: \n",
      "2024-01-10 18:58:07.221445: Epoch 276\n",
      "2024-01-10 18:58:07.230457: Current learning rate: 0.00748\n",
      "2024-01-10 18:58:47.305553: train_loss -0.9349\n",
      "2024-01-10 18:58:47.312554: val_loss -0.8385\n",
      "2024-01-10 18:58:47.317566: Pseudo dice [0.865, 0.9589, 0.9433]\n",
      "2024-01-10 18:58:47.324563: Epoch time: 40.09 s\n",
      "2024-01-10 18:58:48.730590: \n",
      "2024-01-10 18:58:48.736226: Epoch 277\n",
      "2024-01-10 18:58:48.747168: Current learning rate: 0.00747\n",
      "2024-01-10 18:59:28.968749: train_loss -0.9365\n",
      "2024-01-10 18:59:28.976749: val_loss -0.844\n",
      "2024-01-10 18:59:28.983710: Pseudo dice [0.8734, 0.96, 0.944]\n",
      "2024-01-10 18:59:28.989710: Epoch time: 40.24 s\n",
      "2024-01-10 18:59:30.446949: \n",
      "2024-01-10 18:59:30.453471: Epoch 278\n",
      "2024-01-10 18:59:30.457705: Current learning rate: 0.00746\n",
      "2024-01-10 19:00:10.868535: train_loss -0.9362\n",
      "2024-01-10 19:00:10.877536: val_loss -0.8389\n",
      "2024-01-10 19:00:10.884538: Pseudo dice [0.8624, 0.9591, 0.9439]\n",
      "2024-01-10 19:00:10.892157: Epoch time: 40.42 s\n",
      "2024-01-10 19:00:12.311355: \n",
      "2024-01-10 19:00:12.317488: Epoch 279\n",
      "2024-01-10 19:00:12.321496: Current learning rate: 0.00745\n",
      "2024-01-10 19:00:52.529397: train_loss -0.9358\n",
      "2024-01-10 19:00:52.538109: val_loss -0.8401\n",
      "2024-01-10 19:00:52.546109: Pseudo dice [0.8672, 0.9588, 0.9427]\n",
      "2024-01-10 19:00:52.553109: Epoch time: 40.22 s\n",
      "2024-01-10 19:00:53.992229: \n",
      "2024-01-10 19:00:53.997909: Epoch 280\n",
      "2024-01-10 19:00:54.002633: Current learning rate: 0.00744\n",
      "2024-01-10 19:01:34.352261: train_loss -0.9357\n",
      "2024-01-10 19:01:34.360262: val_loss -0.8365\n",
      "2024-01-10 19:01:34.368262: Pseudo dice [0.8651, 0.9582, 0.9425]\n",
      "2024-01-10 19:01:34.373261: Epoch time: 40.36 s\n",
      "2024-01-10 19:01:35.908117: \n",
      "2024-01-10 19:01:35.914035: Epoch 281\n",
      "2024-01-10 19:01:35.918134: Current learning rate: 0.00743\n",
      "2024-01-10 19:02:16.454513: train_loss -0.9352\n",
      "2024-01-10 19:02:16.462513: val_loss -0.8426\n",
      "2024-01-10 19:02:16.468514: Pseudo dice [0.8675, 0.9591, 0.9431]\n",
      "2024-01-10 19:02:16.498515: Epoch time: 40.55 s\n",
      "2024-01-10 19:02:17.871058: \n",
      "2024-01-10 19:02:17.878524: Epoch 282\n",
      "2024-01-10 19:02:17.883531: Current learning rate: 0.00742\n",
      "2024-01-10 19:02:58.354986: train_loss -0.9362\n",
      "2024-01-10 19:02:58.393988: val_loss -0.8411\n",
      "2024-01-10 19:02:58.402985: Pseudo dice [0.8678, 0.9591, 0.9425]\n",
      "2024-01-10 19:02:58.410985: Epoch time: 40.48 s\n",
      "2024-01-10 19:02:59.887083: \n",
      "2024-01-10 19:02:59.893083: Epoch 283\n",
      "2024-01-10 19:02:59.904089: Current learning rate: 0.00741\n",
      "2024-01-10 19:03:40.266316: train_loss -0.9359\n",
      "2024-01-10 19:03:40.276827: val_loss -0.8411\n",
      "2024-01-10 19:03:40.286825: Pseudo dice [0.8664, 0.9595, 0.9432]\n",
      "2024-01-10 19:03:40.295826: Epoch time: 40.38 s\n",
      "2024-01-10 19:03:41.764277: \n",
      "2024-01-10 19:03:41.770453: Epoch 284\n",
      "2024-01-10 19:03:41.780165: Current learning rate: 0.0074\n",
      "2024-01-10 19:04:22.259961: train_loss -0.9364\n",
      "2024-01-10 19:04:22.269960: val_loss -0.8396\n",
      "2024-01-10 19:04:22.278962: Pseudo dice [0.8675, 0.9591, 0.9429]\n",
      "2024-01-10 19:04:22.285977: Epoch time: 40.5 s\n",
      "2024-01-10 19:04:23.689521: \n",
      "2024-01-10 19:04:23.700821: Epoch 285\n",
      "2024-01-10 19:04:23.705075: Current learning rate: 0.00739\n",
      "2024-01-10 19:05:04.086835: train_loss -0.9368\n",
      "2024-01-10 19:05:04.096836: val_loss -0.8434\n",
      "2024-01-10 19:05:04.102843: Pseudo dice [0.8753, 0.959, 0.9437]\n",
      "2024-01-10 19:05:04.134354: Epoch time: 40.4 s\n",
      "2024-01-10 19:05:05.611237: \n",
      "2024-01-10 19:05:05.616419: Epoch 286\n",
      "2024-01-10 19:05:05.625366: Current learning rate: 0.00738\n",
      "2024-01-10 19:05:46.047002: train_loss -0.9373\n",
      "2024-01-10 19:05:46.054514: val_loss -0.838\n",
      "2024-01-10 19:05:46.061517: Pseudo dice [0.8657, 0.9592, 0.9443]\n",
      "2024-01-10 19:05:46.069516: Epoch time: 40.44 s\n",
      "2024-01-10 19:05:47.526195: \n",
      "2024-01-10 19:05:47.534535: Epoch 287\n",
      "2024-01-10 19:05:47.539607: Current learning rate: 0.00738\n",
      "2024-01-10 19:06:27.763369: train_loss -0.9361\n",
      "2024-01-10 19:06:27.771958: val_loss -0.8379\n",
      "2024-01-10 19:06:27.778960: Pseudo dice [0.8686, 0.9586, 0.9431]\n",
      "2024-01-10 19:06:27.808958: Epoch time: 40.24 s\n",
      "2024-01-10 19:06:29.278517: \n",
      "2024-01-10 19:06:29.284517: Epoch 288\n",
      "2024-01-10 19:06:29.289521: Current learning rate: 0.00737\n",
      "2024-01-10 19:07:09.733316: train_loss -0.9364\n",
      "2024-01-10 19:07:09.741320: val_loss -0.8397\n",
      "2024-01-10 19:07:09.748317: Pseudo dice [0.8586, 0.9604, 0.9453]\n",
      "2024-01-10 19:07:09.758318: Epoch time: 40.46 s\n",
      "2024-01-10 19:07:11.132660: \n",
      "2024-01-10 19:07:11.138491: Epoch 289\n",
      "2024-01-10 19:07:11.146186: Current learning rate: 0.00736\n",
      "2024-01-10 19:07:51.504896: train_loss -0.9359\n",
      "2024-01-10 19:07:51.516410: val_loss -0.8353\n",
      "2024-01-10 19:07:51.524415: Pseudo dice [0.8606, 0.9593, 0.9432]\n",
      "2024-01-10 19:07:51.531414: Epoch time: 40.37 s\n",
      "2024-01-10 19:07:53.082870: \n",
      "2024-01-10 19:07:53.089053: Epoch 290\n",
      "2024-01-10 19:07:53.093125: Current learning rate: 0.00735\n",
      "2024-01-10 19:08:33.531024: train_loss -0.9352\n",
      "2024-01-10 19:08:33.540026: val_loss -0.844\n",
      "2024-01-10 19:08:33.548025: Pseudo dice [0.8706, 0.9596, 0.9442]\n",
      "2024-01-10 19:08:33.556026: Epoch time: 40.45 s\n",
      "2024-01-10 19:08:34.970491: \n",
      "2024-01-10 19:08:34.979585: Epoch 291\n",
      "2024-01-10 19:08:34.984581: Current learning rate: 0.00734\n",
      "2024-01-10 19:09:15.353688: train_loss -0.9357\n",
      "2024-01-10 19:09:15.363527: val_loss -0.8389\n",
      "2024-01-10 19:09:15.369528: Pseudo dice [0.8646, 0.9592, 0.9438]\n",
      "2024-01-10 19:09:15.394529: Epoch time: 40.38 s\n",
      "2024-01-10 19:09:16.917817: \n",
      "2024-01-10 19:09:16.928851: Epoch 292\n",
      "2024-01-10 19:09:16.937946: Current learning rate: 0.00733\n",
      "2024-01-10 19:09:57.294497: train_loss -0.9365\n",
      "2024-01-10 19:09:57.301491: val_loss -0.8427\n",
      "2024-01-10 19:09:57.309488: Pseudo dice [0.8667, 0.9599, 0.945]\n",
      "2024-01-10 19:09:57.316488: Epoch time: 40.38 s\n",
      "2024-01-10 19:09:58.814893: \n",
      "2024-01-10 19:09:58.819295: Epoch 293\n",
      "2024-01-10 19:09:58.823306: Current learning rate: 0.00732\n",
      "2024-01-10 19:10:39.108467: train_loss -0.9373\n",
      "2024-01-10 19:10:39.133473: val_loss -0.8426\n",
      "2024-01-10 19:10:39.142473: Pseudo dice [0.8733, 0.9591, 0.9428]\n",
      "2024-01-10 19:10:39.149473: Epoch time: 40.3 s\n",
      "2024-01-10 19:10:40.676111: \n",
      "2024-01-10 19:10:40.681739: Epoch 294\n",
      "2024-01-10 19:10:40.688740: Current learning rate: 0.00731\n",
      "2024-01-10 19:11:20.930975: train_loss -0.9366\n",
      "2024-01-10 19:11:20.939490: val_loss -0.8379\n",
      "2024-01-10 19:11:20.949490: Pseudo dice [0.8646, 0.9594, 0.9427]\n",
      "2024-01-10 19:11:20.982489: Epoch time: 40.26 s\n",
      "2024-01-10 19:11:22.431879: \n",
      "2024-01-10 19:11:22.437241: Epoch 295\n",
      "2024-01-10 19:11:22.443278: Current learning rate: 0.0073\n",
      "2024-01-10 19:12:02.805666: train_loss -0.937\n",
      "2024-01-10 19:12:02.812665: val_loss -0.8432\n",
      "2024-01-10 19:12:02.819665: Pseudo dice [0.8751, 0.9595, 0.944]\n",
      "2024-01-10 19:12:02.826666: Epoch time: 40.37 s\n",
      "2024-01-10 19:12:04.372914: \n",
      "2024-01-10 19:12:04.380299: Epoch 296\n",
      "2024-01-10 19:12:04.384310: Current learning rate: 0.00729\n",
      "2024-01-10 19:12:44.853406: train_loss -0.9367\n",
      "2024-01-10 19:12:44.862415: val_loss -0.8341\n",
      "2024-01-10 19:12:44.870411: Pseudo dice [0.8651, 0.9579, 0.9415]\n",
      "2024-01-10 19:12:44.900930: Epoch time: 40.48 s\n",
      "2024-01-10 19:12:46.390878: \n",
      "2024-01-10 19:12:46.403415: Epoch 297\n",
      "2024-01-10 19:12:46.410920: Current learning rate: 0.00728\n",
      "2024-01-10 19:13:26.796516: train_loss -0.9366\n",
      "2024-01-10 19:13:26.803517: val_loss -0.8403\n",
      "2024-01-10 19:13:26.831126: Pseudo dice [0.8653, 0.9591, 0.9439]\n",
      "2024-01-10 19:13:26.839137: Epoch time: 40.41 s\n",
      "2024-01-10 19:13:28.256311: \n",
      "2024-01-10 19:13:28.261971: Epoch 298\n",
      "2024-01-10 19:13:28.268970: Current learning rate: 0.00727\n",
      "2024-01-10 19:14:08.792239: train_loss -0.9377\n",
      "2024-01-10 19:14:08.800241: val_loss -0.8376\n",
      "2024-01-10 19:14:08.807237: Pseudo dice [0.8612, 0.9592, 0.9433]\n",
      "2024-01-10 19:14:08.815239: Epoch time: 40.54 s\n",
      "2024-01-10 19:14:10.246272: \n",
      "2024-01-10 19:14:10.251102: Epoch 299\n",
      "2024-01-10 19:14:10.255610: Current learning rate: 0.00726\n",
      "2024-01-10 19:14:50.543645: train_loss -0.9366\n",
      "2024-01-10 19:14:50.552651: val_loss -0.8404\n",
      "2024-01-10 19:14:50.561649: Pseudo dice [0.8679, 0.9594, 0.9438]\n",
      "2024-01-10 19:14:50.569649: Epoch time: 40.3 s\n",
      "2024-01-10 19:14:52.202263: \n",
      "2024-01-10 19:14:52.208998: Epoch 300\n",
      "2024-01-10 19:14:52.219004: Current learning rate: 0.00725\n",
      "2024-01-10 19:15:32.558843: train_loss -0.9374\n",
      "2024-01-10 19:15:32.566844: val_loss -0.8375\n",
      "2024-01-10 19:15:32.574844: Pseudo dice [0.8732, 0.9581, 0.9417]\n",
      "2024-01-10 19:15:32.581842: Epoch time: 40.36 s\n",
      "2024-01-10 19:15:34.202541: \n",
      "2024-01-10 19:15:34.207578: Epoch 301\n",
      "2024-01-10 19:15:34.215643: Current learning rate: 0.00724\n",
      "2024-01-10 19:16:14.671457: train_loss -0.9371\n",
      "2024-01-10 19:16:14.680455: val_loss -0.843\n",
      "2024-01-10 19:16:14.688455: Pseudo dice [0.8706, 0.9593, 0.9424]\n",
      "2024-01-10 19:16:14.695454: Epoch time: 40.47 s\n",
      "2024-01-10 19:16:16.129661: \n",
      "2024-01-10 19:16:16.141978: Epoch 302\n",
      "2024-01-10 19:16:16.149925: Current learning rate: 0.00724\n",
      "2024-01-10 19:16:56.555187: train_loss -0.9361\n",
      "2024-01-10 19:16:56.565188: val_loss -0.8337\n",
      "2024-01-10 19:16:56.573187: Pseudo dice [0.8658, 0.9577, 0.9412]\n",
      "2024-01-10 19:16:56.579188: Epoch time: 40.43 s\n",
      "2024-01-10 19:16:58.052526: \n",
      "2024-01-10 19:16:58.065466: Epoch 303\n",
      "2024-01-10 19:16:58.071545: Current learning rate: 0.00723\n",
      "2024-01-10 19:17:38.542269: train_loss -0.9369\n",
      "2024-01-10 19:17:38.551271: val_loss -0.8377\n",
      "2024-01-10 19:17:38.556296: Pseudo dice [0.8645, 0.9593, 0.9435]\n",
      "2024-01-10 19:17:38.561297: Epoch time: 40.49 s\n",
      "2024-01-10 19:17:40.036500: \n",
      "2024-01-10 19:17:40.045235: Epoch 304\n",
      "2024-01-10 19:17:40.049494: Current learning rate: 0.00722\n",
      "2024-01-10 19:18:20.425317: train_loss -0.9368\n",
      "2024-01-10 19:18:20.457317: val_loss -0.838\n",
      "2024-01-10 19:18:20.464324: Pseudo dice [0.8689, 0.9587, 0.943]\n",
      "2024-01-10 19:18:20.471322: Epoch time: 40.39 s\n",
      "2024-01-10 19:18:21.952809: \n",
      "2024-01-10 19:18:21.959162: Epoch 305\n",
      "2024-01-10 19:18:21.964229: Current learning rate: 0.00721\n",
      "2024-01-10 19:19:02.414863: train_loss -0.9373\n",
      "2024-01-10 19:19:02.423862: val_loss -0.8392\n",
      "2024-01-10 19:19:02.430863: Pseudo dice [0.8647, 0.9601, 0.9447]\n",
      "2024-01-10 19:19:02.437862: Epoch time: 40.46 s\n",
      "2024-01-10 19:19:04.035297: \n",
      "2024-01-10 19:19:04.040645: Epoch 306\n",
      "2024-01-10 19:19:04.046581: Current learning rate: 0.0072\n",
      "2024-01-10 19:19:44.355023: train_loss -0.9369\n",
      "2024-01-10 19:19:44.363022: val_loss -0.8393\n",
      "2024-01-10 19:19:44.372021: Pseudo dice [0.8657, 0.9597, 0.9433]\n",
      "2024-01-10 19:19:44.398034: Epoch time: 40.32 s\n",
      "2024-01-10 19:19:45.822117: \n",
      "2024-01-10 19:19:45.826915: Epoch 307\n",
      "2024-01-10 19:19:45.831932: Current learning rate: 0.00719\n",
      "2024-01-10 19:20:26.134097: train_loss -0.9366\n",
      "2024-01-10 19:20:26.142097: val_loss -0.8401\n",
      "2024-01-10 19:20:26.149097: Pseudo dice [0.8624, 0.9597, 0.9447]\n",
      "2024-01-10 19:20:26.176096: Epoch time: 40.31 s\n",
      "2024-01-10 19:20:27.693390: \n",
      "2024-01-10 19:20:27.699005: Epoch 308\n",
      "2024-01-10 19:20:27.704024: Current learning rate: 0.00718\n",
      "2024-01-10 19:21:08.160532: train_loss -0.9376\n",
      "2024-01-10 19:21:08.168533: val_loss -0.8371\n",
      "2024-01-10 19:21:08.175532: Pseudo dice [0.8613, 0.9592, 0.9436]\n",
      "2024-01-10 19:21:08.180533: Epoch time: 40.47 s\n",
      "2024-01-10 19:21:09.598206: \n",
      "2024-01-10 19:21:09.605280: Epoch 309\n",
      "2024-01-10 19:21:09.610201: Current learning rate: 0.00717\n",
      "2024-01-10 19:21:49.941761: train_loss -0.9361\n",
      "2024-01-10 19:21:49.949762: val_loss -0.8384\n",
      "2024-01-10 19:21:49.958761: Pseudo dice [0.8589, 0.9604, 0.9456]\n",
      "2024-01-10 19:21:49.964766: Epoch time: 40.34 s\n",
      "2024-01-10 19:21:51.435201: \n",
      "2024-01-10 19:21:51.441147: Epoch 310\n",
      "2024-01-10 19:21:51.445222: Current learning rate: 0.00716\n",
      "2024-01-10 19:22:31.831265: train_loss -0.9377\n",
      "2024-01-10 19:22:31.838266: val_loss -0.8386\n",
      "2024-01-10 19:22:31.847265: Pseudo dice [0.8617, 0.9595, 0.944]\n",
      "2024-01-10 19:22:31.854267: Epoch time: 40.4 s\n",
      "2024-01-10 19:22:33.551598: \n",
      "2024-01-10 19:22:33.557611: Epoch 311\n",
      "2024-01-10 19:22:33.565612: Current learning rate: 0.00715\n",
      "2024-01-10 19:23:14.033592: train_loss -0.9384\n",
      "2024-01-10 19:23:14.041593: val_loss -0.8375\n",
      "2024-01-10 19:23:14.047592: Pseudo dice [0.8597, 0.959, 0.9436]\n",
      "2024-01-10 19:23:14.054593: Epoch time: 40.48 s\n",
      "2024-01-10 19:23:15.528618: \n",
      "2024-01-10 19:23:15.535773: Epoch 312\n",
      "2024-01-10 19:23:15.547761: Current learning rate: 0.00714\n",
      "2024-01-10 19:23:55.716813: train_loss -0.9378\n",
      "2024-01-10 19:23:55.725318: val_loss -0.8362\n",
      "2024-01-10 19:23:55.731323: Pseudo dice [0.8693, 0.9573, 0.94]\n",
      "2024-01-10 19:23:55.749324: Epoch time: 40.19 s\n",
      "2024-01-10 19:23:57.220194: \n",
      "2024-01-10 19:23:57.225245: Epoch 313\n",
      "2024-01-10 19:23:57.230799: Current learning rate: 0.00713\n",
      "2024-01-10 19:24:37.600422: train_loss -0.9363\n",
      "2024-01-10 19:24:37.607423: val_loss -0.8424\n",
      "2024-01-10 19:24:37.616422: Pseudo dice [0.8694, 0.9595, 0.9444]\n",
      "2024-01-10 19:24:37.623422: Epoch time: 40.38 s\n",
      "2024-01-10 19:24:39.248940: \n",
      "2024-01-10 19:24:39.261282: Epoch 314\n",
      "2024-01-10 19:24:39.270347: Current learning rate: 0.00712\n",
      "2024-01-10 19:25:19.724189: train_loss -0.9371\n",
      "2024-01-10 19:25:19.730189: val_loss -0.8429\n",
      "2024-01-10 19:25:19.738190: Pseudo dice [0.8727, 0.9596, 0.9435]\n",
      "2024-01-10 19:25:19.745194: Epoch time: 40.48 s\n",
      "2024-01-10 19:25:21.249721: \n",
      "2024-01-10 19:25:21.255427: Epoch 315\n",
      "2024-01-10 19:25:21.260447: Current learning rate: 0.00711\n",
      "2024-01-10 19:26:01.611403: train_loss -0.9366\n",
      "2024-01-10 19:26:01.621277: val_loss -0.8338\n",
      "2024-01-10 19:26:01.632275: Pseudo dice [0.8536, 0.9589, 0.9428]\n",
      "2024-01-10 19:26:01.640276: Epoch time: 40.36 s\n",
      "2024-01-10 19:26:03.286839: \n",
      "2024-01-10 19:26:03.292556: Epoch 316\n",
      "2024-01-10 19:26:03.296643: Current learning rate: 0.0071\n",
      "2024-01-10 19:26:43.559515: train_loss -0.936\n",
      "2024-01-10 19:26:43.586519: val_loss -0.8398\n",
      "2024-01-10 19:26:43.593526: Pseudo dice [0.8648, 0.9591, 0.943]\n",
      "2024-01-10 19:26:43.600524: Epoch time: 40.27 s\n",
      "2024-01-10 19:26:45.079607: \n",
      "2024-01-10 19:26:45.086085: Epoch 317\n",
      "2024-01-10 19:26:45.090726: Current learning rate: 0.0071\n",
      "2024-01-10 19:27:25.164593: train_loss -0.937\n",
      "2024-01-10 19:27:25.173592: val_loss -0.8387\n",
      "2024-01-10 19:27:25.200594: Pseudo dice [0.8678, 0.9581, 0.9416]\n",
      "2024-01-10 19:27:25.207592: Epoch time: 40.09 s\n",
      "2024-01-10 19:27:26.657726: \n",
      "2024-01-10 19:27:26.663431: Epoch 318\n",
      "2024-01-10 19:27:26.667508: Current learning rate: 0.00709\n",
      "2024-01-10 19:28:06.829989: train_loss -0.9363\n",
      "2024-01-10 19:28:06.841994: val_loss -0.8401\n",
      "2024-01-10 19:28:06.851510: Pseudo dice [0.8683, 0.9588, 0.9424]\n",
      "2024-01-10 19:28:06.885510: Epoch time: 40.17 s\n",
      "2024-01-10 19:28:08.366152: \n",
      "2024-01-10 19:28:08.372111: Epoch 319\n",
      "2024-01-10 19:28:08.377113: Current learning rate: 0.00708\n",
      "2024-01-10 19:28:48.546309: train_loss -0.9377\n",
      "2024-01-10 19:28:48.554317: val_loss -0.8454\n",
      "2024-01-10 19:28:48.590661: Pseudo dice [0.8752, 0.9595, 0.9446]\n",
      "2024-01-10 19:28:48.599030: Epoch time: 40.18 s\n",
      "2024-01-10 19:28:50.084861: \n",
      "2024-01-10 19:28:50.094154: Epoch 320\n",
      "2024-01-10 19:28:50.099061: Current learning rate: 0.00707\n",
      "2024-01-10 19:29:30.135199: train_loss -0.9386\n",
      "2024-01-10 19:29:30.146206: val_loss -0.842\n",
      "2024-01-10 19:29:30.155468: Pseudo dice [0.8734, 0.9591, 0.9429]\n",
      "2024-01-10 19:29:30.164468: Epoch time: 40.05 s\n",
      "2024-01-10 19:29:31.851530: \n",
      "2024-01-10 19:29:31.863541: Epoch 321\n",
      "2024-01-10 19:29:31.869548: Current learning rate: 0.00706\n",
      "2024-01-10 19:30:12.341207: train_loss -0.9373\n",
      "2024-01-10 19:30:12.353196: val_loss -0.8445\n",
      "2024-01-10 19:30:12.359197: Pseudo dice [0.8734, 0.9597, 0.9446]\n",
      "2024-01-10 19:30:12.390205: Epoch time: 40.49 s\n",
      "2024-01-10 19:30:13.851274: \n",
      "2024-01-10 19:30:13.857318: Epoch 322\n",
      "2024-01-10 19:30:13.862333: Current learning rate: 0.00705\n",
      "2024-01-10 19:30:54.107244: train_loss -0.9376\n",
      "2024-01-10 19:30:54.115245: val_loss -0.8411\n",
      "2024-01-10 19:30:54.122250: Pseudo dice [0.8668, 0.9599, 0.9443]\n",
      "2024-01-10 19:30:54.130251: Epoch time: 40.26 s\n",
      "2024-01-10 19:30:55.663139: \n",
      "2024-01-10 19:30:55.669139: Epoch 323\n",
      "2024-01-10 19:30:55.673279: Current learning rate: 0.00704\n",
      "2024-01-10 19:31:36.054769: train_loss -0.938\n",
      "2024-01-10 19:31:36.064766: val_loss -0.8403\n",
      "2024-01-10 19:31:36.075766: Pseudo dice [0.8627, 0.9597, 0.9447]\n",
      "2024-01-10 19:31:36.109768: Epoch time: 40.39 s\n",
      "2024-01-10 19:31:37.648560: \n",
      "2024-01-10 19:31:37.658518: Epoch 324\n",
      "2024-01-10 19:31:37.669601: Current learning rate: 0.00703\n",
      "2024-01-10 19:32:17.834581: train_loss -0.9371\n",
      "2024-01-10 19:32:17.843575: val_loss -0.84\n",
      "2024-01-10 19:32:17.851575: Pseudo dice [0.8664, 0.9593, 0.944]\n",
      "2024-01-10 19:32:17.860586: Epoch time: 40.19 s\n",
      "2024-01-10 19:32:19.359548: \n",
      "2024-01-10 19:32:19.366472: Epoch 325\n",
      "2024-01-10 19:32:19.369529: Current learning rate: 0.00702\n",
      "2024-01-10 19:32:59.468741: train_loss -0.9375\n",
      "2024-01-10 19:32:59.480747: val_loss -0.8434\n",
      "2024-01-10 19:32:59.492262: Pseudo dice [0.8674, 0.9595, 0.9441]\n",
      "2024-01-10 19:32:59.500260: Epoch time: 40.11 s\n",
      "2024-01-10 19:33:01.197139: \n",
      "2024-01-10 19:33:01.204120: Epoch 326\n",
      "2024-01-10 19:33:01.208548: Current learning rate: 0.00701\n",
      "2024-01-10 19:33:41.386107: train_loss -0.9376\n",
      "2024-01-10 19:33:41.398113: val_loss -0.8354\n",
      "2024-01-10 19:33:41.436318: Pseudo dice [0.8597, 0.9594, 0.9443]\n",
      "2024-01-10 19:33:41.444322: Epoch time: 40.19 s\n",
      "2024-01-10 19:33:42.929960: \n",
      "2024-01-10 19:33:42.942595: Epoch 327\n",
      "2024-01-10 19:33:42.954655: Current learning rate: 0.007\n",
      "2024-01-10 19:34:23.151563: train_loss -0.9376\n",
      "2024-01-10 19:34:23.159566: val_loss -0.8438\n",
      "2024-01-10 19:34:23.165563: Pseudo dice [0.8676, 0.9602, 0.945]\n",
      "2024-01-10 19:34:23.200562: Epoch time: 40.22 s\n",
      "2024-01-10 19:34:24.720300: \n",
      "2024-01-10 19:34:24.725790: Epoch 328\n",
      "2024-01-10 19:34:24.730806: Current learning rate: 0.00699\n",
      "2024-01-10 19:35:04.926863: train_loss -0.9378\n",
      "2024-01-10 19:35:04.936865: val_loss -0.8419\n",
      "2024-01-10 19:35:04.947376: Pseudo dice [0.8688, 0.9602, 0.9453]\n",
      "2024-01-10 19:35:04.953884: Epoch time: 40.21 s\n",
      "2024-01-10 19:35:06.446450: \n",
      "2024-01-10 19:35:06.457011: Epoch 329\n",
      "2024-01-10 19:35:06.461007: Current learning rate: 0.00698\n",
      "2024-01-10 19:35:46.817395: train_loss -0.9379\n",
      "2024-01-10 19:35:46.848396: val_loss -0.8376\n",
      "2024-01-10 19:35:46.857395: Pseudo dice [0.866, 0.9592, 0.9447]\n",
      "2024-01-10 19:35:46.864397: Epoch time: 40.37 s\n",
      "2024-01-10 19:35:48.302242: \n",
      "2024-01-10 19:35:48.307661: Epoch 330\n",
      "2024-01-10 19:35:48.312684: Current learning rate: 0.00697\n",
      "2024-01-10 19:36:28.564663: train_loss -0.9376\n",
      "2024-01-10 19:36:28.572659: val_loss -0.8379\n",
      "2024-01-10 19:36:28.577660: Pseudo dice [0.8664, 0.9595, 0.9439]\n",
      "2024-01-10 19:36:28.583662: Epoch time: 40.26 s\n",
      "2024-01-10 19:36:30.053838: \n",
      "2024-01-10 19:36:30.058301: Epoch 331\n",
      "2024-01-10 19:36:30.063375: Current learning rate: 0.00696\n",
      "2024-01-10 19:37:10.652341: train_loss -0.9374\n",
      "2024-01-10 19:37:10.660344: val_loss -0.8391\n",
      "2024-01-10 19:37:10.666355: Pseudo dice [0.8638, 0.9593, 0.9437]\n",
      "2024-01-10 19:37:10.673339: Epoch time: 40.6 s\n",
      "2024-01-10 19:37:12.139479: \n",
      "2024-01-10 19:37:12.145173: Epoch 332\n",
      "2024-01-10 19:37:12.149403: Current learning rate: 0.00696\n",
      "2024-01-10 19:37:52.607588: train_loss -0.9382\n",
      "2024-01-10 19:37:52.617589: val_loss -0.8386\n",
      "2024-01-10 19:37:52.625587: Pseudo dice [0.8693, 0.9593, 0.9436]\n",
      "2024-01-10 19:37:52.633588: Epoch time: 40.47 s\n",
      "2024-01-10 19:37:54.113354: \n",
      "2024-01-10 19:37:54.125374: Epoch 333\n",
      "2024-01-10 19:37:54.130335: Current learning rate: 0.00695\n",
      "2024-01-10 19:38:34.396679: train_loss -0.9384\n",
      "2024-01-10 19:38:34.406679: val_loss -0.8419\n",
      "2024-01-10 19:38:34.417680: Pseudo dice [0.8719, 0.9588, 0.9434]\n",
      "2024-01-10 19:38:34.445758: Epoch time: 40.28 s\n",
      "2024-01-10 19:38:35.984582: \n",
      "2024-01-10 19:38:35.994591: Epoch 334\n",
      "2024-01-10 19:38:35.999585: Current learning rate: 0.00694\n",
      "2024-01-10 19:39:16.336664: train_loss -0.938\n",
      "2024-01-10 19:39:16.350657: val_loss -0.8347\n",
      "2024-01-10 19:39:16.358659: Pseudo dice [0.8569, 0.959, 0.944]\n",
      "2024-01-10 19:39:16.366657: Epoch time: 40.35 s\n",
      "2024-01-10 19:39:17.889018: \n",
      "2024-01-10 19:39:17.899140: Epoch 335\n",
      "2024-01-10 19:39:17.903198: Current learning rate: 0.00693\n",
      "2024-01-10 19:39:58.079734: train_loss -0.9385\n",
      "2024-01-10 19:39:58.089739: val_loss -0.8427\n",
      "2024-01-10 19:39:58.120249: Pseudo dice [0.8694, 0.9596, 0.9435]\n",
      "2024-01-10 19:39:58.128251: Epoch time: 40.19 s\n",
      "2024-01-10 19:39:59.728239: \n",
      "2024-01-10 19:39:59.734472: Epoch 336\n",
      "2024-01-10 19:39:59.738527: Current learning rate: 0.00692\n",
      "2024-01-10 19:40:40.145248: train_loss -0.9383\n",
      "2024-01-10 19:40:40.152249: val_loss -0.8398\n",
      "2024-01-10 19:40:40.161254: Pseudo dice [0.8717, 0.9587, 0.9424]\n",
      "2024-01-10 19:40:40.185250: Epoch time: 40.42 s\n",
      "2024-01-10 19:40:41.679393: \n",
      "2024-01-10 19:40:41.685393: Epoch 337\n",
      "2024-01-10 19:40:41.690393: Current learning rate: 0.00691\n",
      "2024-01-10 19:41:22.054434: train_loss -0.939\n",
      "2024-01-10 19:41:22.064948: val_loss -0.8383\n",
      "2024-01-10 19:41:22.072947: Pseudo dice [0.8676, 0.9587, 0.9425]\n",
      "2024-01-10 19:41:22.080953: Epoch time: 40.38 s\n",
      "2024-01-10 19:41:23.537294: \n",
      "2024-01-10 19:41:23.541796: Epoch 338\n",
      "2024-01-10 19:41:23.547458: Current learning rate: 0.0069\n",
      "2024-01-10 19:42:03.806231: train_loss -0.9382\n",
      "2024-01-10 19:42:03.815232: val_loss -0.8414\n",
      "2024-01-10 19:42:03.823237: Pseudo dice [0.8692, 0.9596, 0.9441]\n",
      "2024-01-10 19:42:03.830236: Epoch time: 40.27 s\n",
      "2024-01-10 19:42:05.334234: \n",
      "2024-01-10 19:42:05.341833: Epoch 339\n",
      "2024-01-10 19:42:05.345960: Current learning rate: 0.00689\n",
      "2024-01-10 19:42:45.566740: train_loss -0.9379\n",
      "2024-01-10 19:42:45.576739: val_loss -0.842\n",
      "2024-01-10 19:42:45.583247: Pseudo dice [0.8686, 0.96, 0.9443]\n",
      "2024-01-10 19:42:45.591247: Epoch time: 40.23 s\n",
      "2024-01-10 19:42:47.070561: \n",
      "2024-01-10 19:42:47.075824: Epoch 340\n",
      "2024-01-10 19:42:47.080345: Current learning rate: 0.00688\n",
      "2024-01-10 19:43:27.427403: train_loss -0.9395\n",
      "2024-01-10 19:43:27.436402: val_loss -0.838\n",
      "2024-01-10 19:43:27.474921: Pseudo dice [0.8683, 0.9582, 0.9419]\n",
      "2024-01-10 19:43:27.480925: Epoch time: 40.36 s\n",
      "2024-01-10 19:43:28.972018: \n",
      "2024-01-10 19:43:28.980897: Epoch 341\n",
      "2024-01-10 19:43:28.986829: Current learning rate: 0.00687\n",
      "2024-01-10 19:44:09.454291: train_loss -0.9388\n",
      "2024-01-10 19:44:09.465289: val_loss -0.843\n",
      "2024-01-10 19:44:09.472292: Pseudo dice [0.8711, 0.9597, 0.9446]\n",
      "2024-01-10 19:44:09.479290: Epoch time: 40.49 s\n",
      "2024-01-10 19:44:11.011766: \n",
      "2024-01-10 19:44:11.025361: Epoch 342\n",
      "2024-01-10 19:44:11.030360: Current learning rate: 0.00686\n",
      "2024-01-10 19:44:51.304409: train_loss -0.9385\n",
      "2024-01-10 19:44:51.312410: val_loss -0.8365\n",
      "2024-01-10 19:44:51.362940: Pseudo dice [0.8655, 0.9587, 0.943]\n",
      "2024-01-10 19:44:51.370940: Epoch time: 40.29 s\n",
      "2024-01-10 19:44:52.874581: \n",
      "2024-01-10 19:44:52.884650: Epoch 343\n",
      "2024-01-10 19:44:52.888644: Current learning rate: 0.00685\n",
      "2024-01-10 19:45:33.178710: train_loss -0.9385\n",
      "2024-01-10 19:45:33.188708: val_loss -0.8376\n",
      "2024-01-10 19:45:33.195720: Pseudo dice [0.8682, 0.9584, 0.9423]\n",
      "2024-01-10 19:45:33.201712: Epoch time: 40.31 s\n",
      "2024-01-10 19:45:34.640833: \n",
      "2024-01-10 19:45:34.645849: Epoch 344\n",
      "2024-01-10 19:45:34.650824: Current learning rate: 0.00684\n",
      "2024-01-10 19:46:14.980509: train_loss -0.9389\n",
      "2024-01-10 19:46:14.987510: val_loss -0.8399\n",
      "2024-01-10 19:46:14.995508: Pseudo dice [0.8712, 0.9591, 0.9434]\n",
      "2024-01-10 19:46:15.000510: Epoch time: 40.34 s\n",
      "2024-01-10 19:46:16.525549: \n",
      "2024-01-10 19:46:16.531137: Epoch 345\n",
      "2024-01-10 19:46:16.535480: Current learning rate: 0.00683\n",
      "2024-01-10 19:46:56.863142: train_loss -0.9389\n",
      "2024-01-10 19:46:56.874141: val_loss -0.8407\n",
      "2024-01-10 19:46:56.881141: Pseudo dice [0.8691, 0.9595, 0.9436]\n",
      "2024-01-10 19:46:56.890148: Epoch time: 40.34 s\n",
      "2024-01-10 19:46:58.538264: \n",
      "2024-01-10 19:46:58.547802: Epoch 346\n",
      "2024-01-10 19:46:58.554799: Current learning rate: 0.00682\n",
      "2024-01-10 19:47:38.819878: train_loss -0.9398\n",
      "2024-01-10 19:47:38.829394: val_loss -0.8411\n",
      "2024-01-10 19:47:38.861391: Pseudo dice [0.8709, 0.9605, 0.9456]\n",
      "2024-01-10 19:47:38.870394: Epoch time: 40.28 s\n",
      "2024-01-10 19:47:40.344085: \n",
      "2024-01-10 19:47:40.355987: Epoch 347\n",
      "2024-01-10 19:47:40.360907: Current learning rate: 0.00681\n",
      "2024-01-10 19:48:20.689062: train_loss -0.9407\n",
      "2024-01-10 19:48:20.699063: val_loss -0.8401\n",
      "2024-01-10 19:48:20.724063: Pseudo dice [0.8665, 0.9585, 0.9429]\n",
      "2024-01-10 19:48:20.732069: Epoch time: 40.35 s\n",
      "2024-01-10 19:48:22.223823: \n",
      "2024-01-10 19:48:22.228589: Epoch 348\n",
      "2024-01-10 19:48:22.234598: Current learning rate: 0.0068\n",
      "2024-01-10 19:49:02.618992: train_loss -0.9389\n",
      "2024-01-10 19:49:02.627181: val_loss -0.8446\n",
      "2024-01-10 19:49:02.637181: Pseudo dice [0.8762, 0.9594, 0.9437]\n",
      "2024-01-10 19:49:02.645184: Epoch time: 40.4 s\n",
      "2024-01-10 19:49:02.653191: Yayy! New best EMA pseudo Dice: 0.924\n",
      "2024-01-10 19:49:04.251428: \n",
      "2024-01-10 19:49:04.256433: Epoch 349\n",
      "2024-01-10 19:49:04.261260: Current learning rate: 0.0068\n",
      "2024-01-10 19:49:44.624090: train_loss -0.9389\n",
      "2024-01-10 19:49:44.650094: val_loss -0.8384\n",
      "2024-01-10 19:49:44.658095: Pseudo dice [0.868, 0.959, 0.9427]\n",
      "2024-01-10 19:49:44.665104: Epoch time: 40.37 s\n",
      "2024-01-10 19:49:46.359151: \n",
      "2024-01-10 19:49:46.365144: Epoch 350\n",
      "2024-01-10 19:49:46.375152: Current learning rate: 0.00679\n",
      "2024-01-10 19:50:26.762870: train_loss -0.9389\n",
      "2024-01-10 19:50:26.772869: val_loss -0.8475\n",
      "2024-01-10 19:50:26.779869: Pseudo dice [0.8748, 0.9608, 0.9466]\n",
      "2024-01-10 19:50:26.787870: Epoch time: 40.4 s\n",
      "2024-01-10 19:50:26.810869: Yayy! New best EMA pseudo Dice: 0.9242\n",
      "2024-01-10 19:50:28.690969: \n",
      "2024-01-10 19:50:28.696808: Epoch 351\n",
      "2024-01-10 19:50:28.700888: Current learning rate: 0.00678\n",
      "2024-01-10 19:51:09.116807: train_loss -0.9398\n",
      "2024-01-10 19:51:09.125807: val_loss -0.8383\n",
      "2024-01-10 19:51:09.132812: Pseudo dice [0.8638, 0.9586, 0.9428]\n",
      "2024-01-10 19:51:09.140812: Epoch time: 40.43 s\n",
      "2024-01-10 19:51:10.668868: \n",
      "2024-01-10 19:51:10.673340: Epoch 352\n",
      "2024-01-10 19:51:10.678306: Current learning rate: 0.00677\n",
      "2024-01-10 19:51:51.011311: train_loss -0.9398\n",
      "2024-01-10 19:51:51.019309: val_loss -0.84\n",
      "2024-01-10 19:51:51.027310: Pseudo dice [0.8658, 0.959, 0.9434]\n",
      "2024-01-10 19:51:51.034814: Epoch time: 40.34 s\n",
      "2024-01-10 19:51:52.536957: \n",
      "2024-01-10 19:51:52.542399: Epoch 353\n",
      "2024-01-10 19:51:52.548401: Current learning rate: 0.00676\n",
      "2024-01-10 19:52:32.909497: train_loss -0.9393\n",
      "2024-01-10 19:52:32.921007: val_loss -0.837\n",
      "2024-01-10 19:52:32.954010: Pseudo dice [0.8665, 0.9585, 0.9427]\n",
      "2024-01-10 19:52:32.961010: Epoch time: 40.37 s\n",
      "2024-01-10 19:52:34.404417: \n",
      "2024-01-10 19:52:34.411374: Epoch 354\n",
      "2024-01-10 19:52:34.417992: Current learning rate: 0.00675\n",
      "2024-01-10 19:53:14.723800: train_loss -0.9387\n",
      "2024-01-10 19:53:14.734310: val_loss -0.8426\n",
      "2024-01-10 19:53:14.742312: Pseudo dice [0.8695, 0.9602, 0.9456]\n",
      "2024-01-10 19:53:14.749313: Epoch time: 40.32 s\n",
      "2024-01-10 19:53:16.312234: \n",
      "2024-01-10 19:53:16.318269: Epoch 355\n",
      "2024-01-10 19:53:16.324274: Current learning rate: 0.00674\n",
      "2024-01-10 19:53:56.761641: train_loss -0.9387\n",
      "2024-01-10 19:53:56.771150: val_loss -0.846\n",
      "2024-01-10 19:53:56.778655: Pseudo dice [0.8742, 0.9606, 0.9453]\n",
      "2024-01-10 19:53:56.785657: Epoch time: 40.45 s\n",
      "2024-01-10 19:53:58.271317: \n",
      "2024-01-10 19:53:58.278860: Epoch 356\n",
      "2024-01-10 19:53:58.283232: Current learning rate: 0.00673\n",
      "2024-01-10 19:54:38.415224: train_loss -0.94\n",
      "2024-01-10 19:54:38.423227: val_loss -0.8414\n",
      "2024-01-10 19:54:38.430225: Pseudo dice [0.8717, 0.96, 0.9443]\n",
      "2024-01-10 19:54:38.456226: Epoch time: 40.14 s\n",
      "2024-01-10 19:54:38.464228: Yayy! New best EMA pseudo Dice: 0.9243\n",
      "2024-01-10 19:54:40.215007: \n",
      "2024-01-10 19:54:40.221629: Epoch 357\n",
      "2024-01-10 19:54:40.231836: Current learning rate: 0.00672\n",
      "2024-01-10 19:55:20.533044: train_loss -0.9396\n",
      "2024-01-10 19:55:20.559050: val_loss -0.8336\n",
      "2024-01-10 19:55:20.568041: Pseudo dice [0.858, 0.959, 0.9438]\n",
      "2024-01-10 19:55:20.577042: Epoch time: 40.32 s\n",
      "2024-01-10 19:55:22.070659: \n",
      "2024-01-10 19:55:22.077547: Epoch 358\n",
      "2024-01-10 19:55:22.083634: Current learning rate: 0.00671\n",
      "2024-01-10 19:56:02.445355: train_loss -0.9394\n",
      "2024-01-10 19:56:02.453869: val_loss -0.8402\n",
      "2024-01-10 19:56:02.461872: Pseudo dice [0.8599, 0.9594, 0.9439]\n",
      "2024-01-10 19:56:02.468871: Epoch time: 40.38 s\n",
      "2024-01-10 19:56:04.023931: \n",
      "2024-01-10 19:56:04.030766: Epoch 359\n",
      "2024-01-10 19:56:04.034848: Current learning rate: 0.0067\n",
      "2024-01-10 19:56:44.220008: train_loss -0.9398\n",
      "2024-01-10 19:56:44.227009: val_loss -0.8394\n",
      "2024-01-10 19:56:44.232013: Pseudo dice [0.8659, 0.959, 0.9434]\n",
      "2024-01-10 19:56:44.241010: Epoch time: 40.2 s\n",
      "2024-01-10 19:56:45.976458: \n",
      "2024-01-10 19:56:45.986718: Epoch 360\n",
      "2024-01-10 19:56:45.991704: Current learning rate: 0.00669\n",
      "2024-01-10 19:57:26.456088: train_loss -0.9391\n",
      "2024-01-10 19:57:26.466094: val_loss -0.8402\n",
      "2024-01-10 19:57:26.472094: Pseudo dice [0.8666, 0.9601, 0.9452]\n",
      "2024-01-10 19:57:26.497606: Epoch time: 40.48 s\n",
      "2024-01-10 19:57:27.911360: \n",
      "2024-01-10 19:57:27.916359: Epoch 361\n",
      "2024-01-10 19:57:27.921359: Current learning rate: 0.00668\n",
      "2024-01-10 19:58:08.144758: train_loss -0.9389\n",
      "2024-01-10 19:58:08.151759: val_loss -0.838\n",
      "2024-01-10 19:58:08.162758: Pseudo dice [0.8672, 0.9596, 0.9444]\n",
      "2024-01-10 19:58:08.193286: Epoch time: 40.24 s\n",
      "2024-01-10 19:58:09.653051: \n",
      "2024-01-10 19:58:09.665398: Epoch 362\n",
      "2024-01-10 19:58:09.673392: Current learning rate: 0.00667\n",
      "2024-01-10 19:58:50.020840: train_loss -0.9392\n",
      "2024-01-10 19:58:50.046839: val_loss -0.842\n",
      "2024-01-10 19:58:50.055839: Pseudo dice [0.8648, 0.9604, 0.9459]\n",
      "2024-01-10 19:58:50.062841: Epoch time: 40.37 s\n",
      "2024-01-10 19:58:51.518268: \n",
      "2024-01-10 19:58:51.523601: Epoch 363\n",
      "2024-01-10 19:58:51.527678: Current learning rate: 0.00666\n",
      "2024-01-10 19:59:31.772599: train_loss -0.9401\n",
      "2024-01-10 19:59:31.811600: val_loss -0.84\n",
      "2024-01-10 19:59:31.823597: Pseudo dice [0.8688, 0.959, 0.944]\n",
      "2024-01-10 19:59:31.832597: Epoch time: 40.26 s\n",
      "2024-01-10 19:59:33.395623: \n",
      "2024-01-10 19:59:33.406529: Epoch 364\n",
      "2024-01-10 19:59:33.418405: Current learning rate: 0.00665\n",
      "2024-01-10 20:00:13.661736: train_loss -0.9399\n",
      "2024-01-10 20:00:13.672247: val_loss -0.8443\n",
      "2024-01-10 20:00:13.681247: Pseudo dice [0.8716, 0.9605, 0.9453]\n",
      "2024-01-10 20:00:13.713245: Epoch time: 40.27 s\n",
      "2024-01-10 20:00:15.266268: \n",
      "2024-01-10 20:00:15.271800: Epoch 365\n",
      "2024-01-10 20:00:15.276680: Current learning rate: 0.00665\n",
      "2024-01-10 20:00:55.712899: train_loss -0.9396\n",
      "2024-01-10 20:00:55.720900: val_loss -0.8396\n",
      "2024-01-10 20:00:55.727901: Pseudo dice [0.8634, 0.9606, 0.9449]\n",
      "2024-01-10 20:00:55.756900: Epoch time: 40.45 s\n",
      "2024-01-10 20:00:57.253388: \n",
      "2024-01-10 20:00:57.263923: Epoch 366\n",
      "2024-01-10 20:00:57.268925: Current learning rate: 0.00664\n",
      "2024-01-10 20:01:37.593072: train_loss -0.9389\n",
      "2024-01-10 20:01:37.602074: val_loss -0.8452\n",
      "2024-01-10 20:01:37.608588: Pseudo dice [0.8734, 0.9602, 0.9451]\n",
      "2024-01-10 20:01:37.616586: Epoch time: 40.34 s\n",
      "2024-01-10 20:01:39.105069: \n",
      "2024-01-10 20:01:39.112087: Epoch 367\n",
      "2024-01-10 20:01:39.116580: Current learning rate: 0.00663\n",
      "2024-01-10 20:02:19.403717: train_loss -0.9389\n",
      "2024-01-10 20:02:19.412724: val_loss -0.8389\n",
      "2024-01-10 20:02:19.422240: Pseudo dice [0.8617, 0.9596, 0.9449]\n",
      "2024-01-10 20:02:19.431238: Epoch time: 40.3 s\n",
      "2024-01-10 20:02:20.881282: \n",
      "2024-01-10 20:02:20.893258: Epoch 368\n",
      "2024-01-10 20:02:20.897319: Current learning rate: 0.00662\n",
      "2024-01-10 20:03:00.977073: train_loss -0.9392\n",
      "2024-01-10 20:03:00.986075: val_loss -0.8415\n",
      "2024-01-10 20:03:00.993075: Pseudo dice [0.8636, 0.9603, 0.9448]\n",
      "2024-01-10 20:03:01.001073: Epoch time: 40.1 s\n",
      "2024-01-10 20:03:02.509890: \n",
      "2024-01-10 20:03:02.515453: Epoch 369\n",
      "2024-01-10 20:03:02.523513: Current learning rate: 0.00661\n",
      "2024-01-10 20:03:42.721870: train_loss -0.9402\n",
      "2024-01-10 20:03:42.729870: val_loss -0.8413\n",
      "2024-01-10 20:03:42.739868: Pseudo dice [0.8672, 0.9598, 0.9439]\n",
      "2024-01-10 20:03:42.746877: Epoch time: 40.21 s\n",
      "2024-01-10 20:03:44.450074: \n",
      "2024-01-10 20:03:44.455073: Epoch 370\n",
      "2024-01-10 20:03:44.459356: Current learning rate: 0.0066\n",
      "2024-01-10 20:04:24.760950: train_loss -0.9403\n",
      "2024-01-10 20:04:24.767957: val_loss -0.8418\n",
      "2024-01-10 20:04:24.775955: Pseudo dice [0.869, 0.9593, 0.9439]\n",
      "2024-01-10 20:04:24.783470: Epoch time: 40.31 s\n",
      "2024-01-10 20:04:26.304122: \n",
      "2024-01-10 20:04:26.310101: Epoch 371\n",
      "2024-01-10 20:04:26.314101: Current learning rate: 0.00659\n",
      "2024-01-10 20:05:06.519232: train_loss -0.9394\n",
      "2024-01-10 20:05:06.529233: val_loss -0.8433\n",
      "2024-01-10 20:05:06.540235: Pseudo dice [0.8771, 0.9594, 0.9441]\n",
      "2024-01-10 20:05:06.547236: Epoch time: 40.22 s\n",
      "2024-01-10 20:05:08.090002: \n",
      "2024-01-10 20:05:08.095984: Epoch 372\n",
      "2024-01-10 20:05:08.100049: Current learning rate: 0.00658\n",
      "2024-01-10 20:05:48.365820: train_loss -0.9405\n",
      "2024-01-10 20:05:48.373822: val_loss -0.845\n",
      "2024-01-10 20:05:48.381822: Pseudo dice [0.8709, 0.9607, 0.9465]\n",
      "2024-01-10 20:05:48.411828: Epoch time: 40.28 s\n",
      "2024-01-10 20:05:50.007368: \n",
      "2024-01-10 20:05:50.013278: Epoch 373\n",
      "2024-01-10 20:05:50.017794: Current learning rate: 0.00657\n",
      "2024-01-10 20:06:30.400868: train_loss -0.9413\n",
      "2024-01-10 20:06:30.409428: val_loss -0.838\n",
      "2024-01-10 20:06:30.417438: Pseudo dice [0.8673, 0.9591, 0.9437]\n",
      "2024-01-10 20:06:30.426440: Epoch time: 40.39 s\n",
      "2024-01-10 20:06:31.895079: \n",
      "2024-01-10 20:06:31.903007: Epoch 374\n",
      "2024-01-10 20:06:31.908045: Current learning rate: 0.00656\n",
      "2024-01-10 20:07:12.331824: train_loss -0.9406\n",
      "2024-01-10 20:07:12.339825: val_loss -0.8366\n",
      "2024-01-10 20:07:12.347832: Pseudo dice [0.8622, 0.9592, 0.9438]\n",
      "2024-01-10 20:07:12.353831: Epoch time: 40.44 s\n",
      "2024-01-10 20:07:13.925152: \n",
      "2024-01-10 20:07:13.931211: Epoch 375\n",
      "2024-01-10 20:07:13.936157: Current learning rate: 0.00655\n",
      "2024-01-10 20:07:54.198571: train_loss -0.9409\n",
      "2024-01-10 20:07:54.226093: val_loss -0.8418\n",
      "2024-01-10 20:07:54.234596: Pseudo dice [0.8681, 0.9605, 0.9454]\n",
      "2024-01-10 20:07:54.239594: Epoch time: 40.27 s\n",
      "2024-01-10 20:07:55.748546: \n",
      "2024-01-10 20:07:55.761948: Epoch 376\n",
      "2024-01-10 20:07:55.771953: Current learning rate: 0.00654\n",
      "2024-01-10 20:08:35.948575: train_loss -0.9411\n",
      "2024-01-10 20:08:35.956581: val_loss -0.8394\n",
      "2024-01-10 20:08:35.967576: Pseudo dice [0.8691, 0.9597, 0.9451]\n",
      "2024-01-10 20:08:35.998577: Epoch time: 40.2 s\n",
      "2024-01-10 20:08:37.461462: \n",
      "2024-01-10 20:08:37.472551: Epoch 377\n",
      "2024-01-10 20:08:37.476830: Current learning rate: 0.00653\n",
      "2024-01-10 20:09:17.725127: train_loss -0.9408\n",
      "2024-01-10 20:09:17.732126: val_loss -0.8372\n",
      "2024-01-10 20:09:17.738130: Pseudo dice [0.8689, 0.9597, 0.9443]\n",
      "2024-01-10 20:09:17.745642: Epoch time: 40.26 s\n",
      "2024-01-10 20:09:19.391609: \n",
      "2024-01-10 20:09:19.397542: Epoch 378\n",
      "2024-01-10 20:09:19.402508: Current learning rate: 0.00652\n",
      "2024-01-10 20:09:59.731055: train_loss -0.9407\n",
      "2024-01-10 20:09:59.756047: val_loss -0.8391\n",
      "2024-01-10 20:09:59.764050: Pseudo dice [0.8687, 0.9595, 0.9445]\n",
      "2024-01-10 20:09:59.771055: Epoch time: 40.34 s\n",
      "2024-01-10 20:10:01.191420: \n",
      "2024-01-10 20:10:01.196417: Epoch 379\n",
      "2024-01-10 20:10:01.203733: Current learning rate: 0.00651\n",
      "2024-01-10 20:10:41.662879: train_loss -0.9411\n",
      "2024-01-10 20:10:41.669878: val_loss -0.8407\n",
      "2024-01-10 20:10:41.677876: Pseudo dice [0.8684, 0.9596, 0.9443]\n",
      "2024-01-10 20:10:41.699392: Epoch time: 40.47 s\n",
      "2024-01-10 20:10:43.161658: \n",
      "2024-01-10 20:10:43.167772: Epoch 380\n",
      "2024-01-10 20:10:43.174831: Current learning rate: 0.0065\n",
      "2024-01-10 20:11:23.511730: train_loss -0.9411\n",
      "2024-01-10 20:11:23.522729: val_loss -0.8414\n",
      "2024-01-10 20:11:23.529735: Pseudo dice [0.8667, 0.9606, 0.9449]\n",
      "2024-01-10 20:11:23.537773: Epoch time: 40.35 s\n",
      "2024-01-10 20:11:25.001730: \n",
      "2024-01-10 20:11:25.007896: Epoch 381\n",
      "2024-01-10 20:11:25.012874: Current learning rate: 0.00649\n",
      "2024-01-10 20:12:05.142595: train_loss -0.9414\n",
      "2024-01-10 20:12:05.152594: val_loss -0.8389\n",
      "2024-01-10 20:12:05.159595: Pseudo dice [0.8682, 0.9597, 0.9443]\n",
      "2024-01-10 20:12:05.165596: Epoch time: 40.14 s\n",
      "2024-01-10 20:12:06.805593: \n",
      "2024-01-10 20:12:06.811219: Epoch 382\n",
      "2024-01-10 20:12:06.816268: Current learning rate: 0.00648\n",
      "2024-01-10 20:12:47.066669: train_loss -0.9407\n",
      "2024-01-10 20:12:47.072671: val_loss -0.8363\n",
      "2024-01-10 20:12:47.082671: Pseudo dice [0.8686, 0.9592, 0.9442]\n",
      "2024-01-10 20:12:47.090671: Epoch time: 40.26 s\n",
      "2024-01-10 20:12:48.630970: \n",
      "2024-01-10 20:12:48.635982: Epoch 383\n",
      "2024-01-10 20:12:48.640982: Current learning rate: 0.00648\n",
      "2024-01-10 20:13:29.029842: train_loss -0.9412\n",
      "2024-01-10 20:13:29.039356: val_loss -0.8388\n",
      "2024-01-10 20:13:29.048185: Pseudo dice [0.8716, 0.9589, 0.9435]\n",
      "2024-01-10 20:13:29.055184: Epoch time: 40.4 s\n",
      "2024-01-10 20:13:30.752603: \n",
      "2024-01-10 20:13:30.761930: Epoch 384\n",
      "2024-01-10 20:13:30.766936: Current learning rate: 0.00647\n",
      "2024-01-10 20:14:11.108163: train_loss -0.9417\n",
      "2024-01-10 20:14:11.118162: val_loss -0.8444\n",
      "2024-01-10 20:14:11.127159: Pseudo dice [0.8721, 0.96, 0.9452]\n",
      "2024-01-10 20:14:11.156456: Epoch time: 40.36 s\n",
      "2024-01-10 20:14:11.164457: Yayy! New best EMA pseudo Dice: 0.9243\n",
      "2024-01-10 20:14:12.802657: \n",
      "2024-01-10 20:14:12.808856: Epoch 385\n",
      "2024-01-10 20:14:12.812873: Current learning rate: 0.00646\n",
      "2024-01-10 20:14:53.182595: train_loss -0.9409\n",
      "2024-01-10 20:14:53.189596: val_loss -0.8429\n",
      "2024-01-10 20:14:53.195962: Pseudo dice [0.8684, 0.9608, 0.9459]\n",
      "2024-01-10 20:14:53.203967: Epoch time: 40.38 s\n",
      "2024-01-10 20:14:53.211480: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2024-01-10 20:14:54.923941: \n",
      "2024-01-10 20:14:54.931201: Epoch 386\n",
      "2024-01-10 20:14:54.940191: Current learning rate: 0.00645\n",
      "2024-01-10 20:15:35.346559: train_loss -0.9409\n",
      "2024-01-10 20:15:35.357560: val_loss -0.836\n",
      "2024-01-10 20:15:35.367074: Pseudo dice [0.8596, 0.96, 0.945]\n",
      "2024-01-10 20:15:35.398592: Epoch time: 40.42 s\n",
      "2024-01-10 20:15:36.877165: \n",
      "2024-01-10 20:15:36.889171: Epoch 387\n",
      "2024-01-10 20:15:36.902199: Current learning rate: 0.00644\n",
      "2024-01-10 20:16:17.612092: train_loss -0.9415\n",
      "2024-01-10 20:16:17.622091: val_loss -0.8439\n",
      "2024-01-10 20:16:17.630092: Pseudo dice [0.8728, 0.9602, 0.9448]\n",
      "2024-01-10 20:16:17.638092: Epoch time: 40.74 s\n",
      "2024-01-10 20:16:19.374902: \n",
      "2024-01-10 20:16:19.380904: Epoch 388\n",
      "2024-01-10 20:16:19.384979: Current learning rate: 0.00643\n",
      "2024-01-10 20:16:59.927161: train_loss -0.9411\n",
      "2024-01-10 20:16:59.958160: val_loss -0.836\n",
      "2024-01-10 20:16:59.965883: Pseudo dice [0.8629, 0.9594, 0.9443]\n",
      "2024-01-10 20:16:59.972884: Epoch time: 40.55 s\n",
      "2024-01-10 20:17:01.800529: \n",
      "2024-01-10 20:17:01.808885: Epoch 389\n",
      "2024-01-10 20:17:01.813954: Current learning rate: 0.00642\n",
      "2024-01-10 20:17:42.235305: train_loss -0.9412\n",
      "2024-01-10 20:17:42.243306: val_loss -0.8434\n",
      "2024-01-10 20:17:42.251307: Pseudo dice [0.8741, 0.9597, 0.9446]\n",
      "2024-01-10 20:17:42.258513: Epoch time: 40.44 s\n",
      "2024-01-10 20:17:43.920208: \n",
      "2024-01-10 20:17:43.931923: Epoch 390\n",
      "2024-01-10 20:17:43.939984: Current learning rate: 0.00641\n",
      "2024-01-10 20:18:24.514495: train_loss -0.9411\n",
      "2024-01-10 20:18:24.521506: val_loss -0.8408\n",
      "2024-01-10 20:18:24.530506: Pseudo dice [0.8737, 0.9593, 0.9446]\n",
      "2024-01-10 20:18:24.536021: Epoch time: 40.6 s\n",
      "2024-01-10 20:18:24.541030: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2024-01-10 20:18:26.303362: \n",
      "2024-01-10 20:18:26.308507: Epoch 391\n",
      "2024-01-10 20:18:26.314372: Current learning rate: 0.0064\n",
      "2024-01-10 20:19:06.582536: train_loss -0.9416\n",
      "2024-01-10 20:19:06.591537: val_loss -0.8392\n",
      "2024-01-10 20:19:06.601534: Pseudo dice [0.8725, 0.9593, 0.9441]\n",
      "2024-01-10 20:19:06.610535: Epoch time: 40.28 s\n",
      "2024-01-10 20:19:06.635535: Yayy! New best EMA pseudo Dice: 0.9245\n",
      "2024-01-10 20:19:08.388302: \n",
      "2024-01-10 20:19:08.394272: Epoch 392\n",
      "2024-01-10 20:19:08.402330: Current learning rate: 0.00639\n",
      "2024-01-10 20:19:48.782249: train_loss -0.94\n",
      "2024-01-10 20:19:48.792251: val_loss -0.8406\n",
      "2024-01-10 20:19:48.799250: Pseudo dice [0.8694, 0.9591, 0.9434]\n",
      "2024-01-10 20:19:48.827254: Epoch time: 40.39 s\n",
      "2024-01-10 20:19:50.242239: \n",
      "2024-01-10 20:19:50.247236: Epoch 393\n",
      "2024-01-10 20:19:50.253238: Current learning rate: 0.00638\n",
      "2024-01-10 20:20:30.564212: train_loss -0.9388\n",
      "2024-01-10 20:20:30.575211: val_loss -0.839\n",
      "2024-01-10 20:20:30.582211: Pseudo dice [0.8721, 0.9579, 0.9415]\n",
      "2024-01-10 20:20:30.589725: Epoch time: 40.32 s\n",
      "2024-01-10 20:20:32.219308: \n",
      "2024-01-10 20:20:32.226247: Epoch 394\n",
      "2024-01-10 20:20:32.230299: Current learning rate: 0.00637\n",
      "2024-01-10 20:21:12.571717: train_loss -0.94\n",
      "2024-01-10 20:21:12.579718: val_loss -0.8396\n",
      "2024-01-10 20:21:12.586717: Pseudo dice [0.8714, 0.9584, 0.9424]\n",
      "2024-01-10 20:21:12.595720: Epoch time: 40.35 s\n",
      "2024-01-10 20:21:14.135069: \n",
      "2024-01-10 20:21:14.141138: Epoch 395\n",
      "2024-01-10 20:21:14.149145: Current learning rate: 0.00636\n",
      "2024-01-10 20:21:54.306346: train_loss -0.9407\n",
      "2024-01-10 20:21:54.314345: val_loss -0.8426\n",
      "2024-01-10 20:21:54.319345: Pseudo dice [0.8723, 0.9603, 0.9453]\n",
      "2024-01-10 20:21:54.326346: Epoch time: 40.17 s\n",
      "2024-01-10 20:21:54.334346: Yayy! New best EMA pseudo Dice: 0.9245\n",
      "2024-01-10 20:21:56.125677: \n",
      "2024-01-10 20:21:56.131587: Epoch 396\n",
      "2024-01-10 20:21:56.136658: Current learning rate: 0.00635\n",
      "2024-01-10 20:22:36.481061: train_loss -0.9413\n",
      "2024-01-10 20:22:36.492062: val_loss -0.8378\n",
      "2024-01-10 20:22:36.498063: Pseudo dice [0.8671, 0.9583, 0.9434]\n",
      "2024-01-10 20:22:36.506062: Epoch time: 40.36 s\n",
      "2024-01-10 20:22:38.135521: \n",
      "2024-01-10 20:22:38.143683: Epoch 397\n",
      "2024-01-10 20:22:38.148660: Current learning rate: 0.00634\n",
      "2024-01-10 20:23:18.503081: train_loss -0.9406\n",
      "2024-01-10 20:23:18.510597: val_loss -0.8366\n",
      "2024-01-10 20:23:18.516598: Pseudo dice [0.869, 0.9586, 0.9429]\n",
      "2024-01-10 20:23:18.544973: Epoch time: 40.37 s\n",
      "2024-01-10 20:23:20.110327: \n",
      "2024-01-10 20:23:20.118770: Epoch 398\n",
      "2024-01-10 20:23:20.123855: Current learning rate: 0.00633\n",
      "2024-01-10 20:24:00.389184: train_loss -0.9414\n",
      "2024-01-10 20:24:00.399188: val_loss -0.839\n",
      "2024-01-10 20:24:00.409184: Pseudo dice [0.8687, 0.9598, 0.9442]\n",
      "2024-01-10 20:24:00.416191: Epoch time: 40.28 s\n",
      "2024-01-10 20:24:02.068784: \n",
      "2024-01-10 20:24:02.076783: Epoch 399\n",
      "2024-01-10 20:24:02.083679: Current learning rate: 0.00632\n",
      "2024-01-10 20:24:42.467357: train_loss -0.9411\n",
      "2024-01-10 20:24:42.475356: val_loss -0.8365\n",
      "2024-01-10 20:24:42.483359: Pseudo dice [0.8643, 0.9588, 0.9436]\n",
      "2024-01-10 20:24:42.488357: Epoch time: 40.4 s\n",
      "2024-01-10 20:24:44.184601: \n",
      "2024-01-10 20:24:44.190674: Epoch 400\n",
      "2024-01-10 20:24:44.194738: Current learning rate: 0.00631\n",
      "2024-01-10 20:25:24.588943: train_loss -0.941\n",
      "2024-01-10 20:25:24.595944: val_loss -0.8402\n",
      "2024-01-10 20:25:24.602944: Pseudo dice [0.8692, 0.9594, 0.9434]\n",
      "2024-01-10 20:25:24.609944: Epoch time: 40.41 s\n",
      "2024-01-10 20:25:26.077659: \n",
      "2024-01-10 20:25:26.083157: Epoch 401\n",
      "2024-01-10 20:25:26.087220: Current learning rate: 0.0063\n",
      "2024-01-10 20:26:06.225245: train_loss -0.941\n",
      "2024-01-10 20:26:06.252246: val_loss -0.8349\n",
      "2024-01-10 20:26:06.259245: Pseudo dice [0.8571, 0.9589, 0.9442]\n",
      "2024-01-10 20:26:06.267247: Epoch time: 40.15 s\n",
      "2024-01-10 20:26:07.820472: \n",
      "2024-01-10 20:26:07.827407: Epoch 402\n",
      "2024-01-10 20:26:07.831461: Current learning rate: 0.0063\n",
      "2024-01-10 20:26:48.026130: train_loss -0.9417\n",
      "2024-01-10 20:26:48.034132: val_loss -0.8411\n",
      "2024-01-10 20:26:48.050129: Pseudo dice [0.8674, 0.9604, 0.9441]\n",
      "2024-01-10 20:26:48.057132: Epoch time: 40.21 s\n",
      "2024-01-10 20:26:49.697935: \n",
      "2024-01-10 20:26:49.703552: Epoch 403\n",
      "2024-01-10 20:26:49.713630: Current learning rate: 0.00629\n",
      "2024-01-10 20:27:30.033588: train_loss -0.9413\n",
      "2024-01-10 20:27:30.070099: val_loss -0.84\n",
      "2024-01-10 20:27:30.080100: Pseudo dice [0.8717, 0.9602, 0.945]\n",
      "2024-01-10 20:27:30.087101: Epoch time: 40.34 s\n",
      "2024-01-10 20:27:31.787776: \n",
      "2024-01-10 20:27:31.795777: Epoch 404\n",
      "2024-01-10 20:27:31.801251: Current learning rate: 0.00628\n",
      "2024-01-10 20:28:11.990247: train_loss -0.9423\n",
      "2024-01-10 20:28:12.000248: val_loss -0.8367\n",
      "2024-01-10 20:28:12.006248: Pseudo dice [0.8637, 0.9585, 0.9423]\n",
      "2024-01-10 20:28:12.016247: Epoch time: 40.2 s\n",
      "2024-01-10 20:28:13.473204: \n",
      "2024-01-10 20:28:13.479332: Epoch 405\n",
      "2024-01-10 20:28:13.485334: Current learning rate: 0.00627\n",
      "2024-01-10 20:28:53.788056: train_loss -0.9413\n",
      "2024-01-10 20:28:53.797057: val_loss -0.8427\n",
      "2024-01-10 20:28:53.804058: Pseudo dice [0.8698, 0.9592, 0.9441]\n",
      "2024-01-10 20:28:53.809057: Epoch time: 40.32 s\n",
      "2024-01-10 20:28:55.292948: \n",
      "2024-01-10 20:28:55.298294: Epoch 406\n",
      "2024-01-10 20:28:55.302663: Current learning rate: 0.00626\n",
      "2024-01-10 20:29:35.636302: train_loss -0.9415\n",
      "2024-01-10 20:29:35.643302: val_loss -0.8365\n",
      "2024-01-10 20:29:35.651302: Pseudo dice [0.8683, 0.9588, 0.9431]\n",
      "2024-01-10 20:29:35.683303: Epoch time: 40.35 s\n",
      "2024-01-10 20:29:37.148387: \n",
      "2024-01-10 20:29:37.155867: Epoch 407\n",
      "2024-01-10 20:29:37.163924: Current learning rate: 0.00625\n",
      "2024-01-10 20:30:17.404639: train_loss -0.9421\n",
      "2024-01-10 20:30:17.412641: val_loss -0.8342\n",
      "2024-01-10 20:30:17.420161: Pseudo dice [0.863, 0.9591, 0.9434]\n",
      "2024-01-10 20:30:17.427167: Epoch time: 40.26 s\n",
      "2024-01-10 20:30:18.950877: \n",
      "2024-01-10 20:30:18.956791: Epoch 408\n",
      "2024-01-10 20:30:18.964795: Current learning rate: 0.00624\n",
      "2024-01-10 20:30:59.221897: train_loss -0.942\n",
      "2024-01-10 20:30:59.229904: val_loss -0.8386\n",
      "2024-01-10 20:30:59.237904: Pseudo dice [0.8653, 0.9594, 0.9446]\n",
      "2024-01-10 20:30:59.244416: Epoch time: 40.27 s\n",
      "2024-01-10 20:31:00.914097: \n",
      "2024-01-10 20:31:00.920714: Epoch 409\n",
      "2024-01-10 20:31:00.925733: Current learning rate: 0.00623\n",
      "2024-01-10 20:31:41.190392: train_loss -0.9406\n",
      "2024-01-10 20:31:41.201388: val_loss -0.8441\n",
      "2024-01-10 20:31:41.207392: Pseudo dice [0.8718, 0.9598, 0.9447]\n",
      "2024-01-10 20:31:41.214391: Epoch time: 40.28 s\n",
      "2024-01-10 20:31:42.698007: \n",
      "2024-01-10 20:31:42.706069: Epoch 410\n",
      "2024-01-10 20:31:42.711095: Current learning rate: 0.00622\n",
      "2024-01-10 20:32:22.988853: train_loss -0.9416\n",
      "2024-01-10 20:32:22.997854: val_loss -0.839\n",
      "2024-01-10 20:32:23.021858: Pseudo dice [0.8642, 0.9596, 0.9444]\n",
      "2024-01-10 20:32:23.029856: Epoch time: 40.29 s\n",
      "2024-01-10 20:32:24.535322: \n",
      "2024-01-10 20:32:24.545507: Epoch 411\n",
      "2024-01-10 20:32:24.553573: Current learning rate: 0.00621\n",
      "2024-01-10 20:33:04.767894: train_loss -0.9426\n",
      "2024-01-10 20:33:04.775895: val_loss -0.8389\n",
      "2024-01-10 20:33:04.785906: Pseudo dice [0.8637, 0.9596, 0.9445]\n",
      "2024-01-10 20:33:04.793422: Epoch time: 40.24 s\n",
      "2024-01-10 20:33:06.287285: \n",
      "2024-01-10 20:33:06.292920: Epoch 412\n",
      "2024-01-10 20:33:06.300660: Current learning rate: 0.0062\n",
      "2024-01-10 20:33:46.570829: train_loss -0.9419\n",
      "2024-01-10 20:33:46.579830: val_loss -0.8434\n",
      "2024-01-10 20:33:46.589833: Pseudo dice [0.8676, 0.9602, 0.9452]\n",
      "2024-01-10 20:33:46.599846: Epoch time: 40.28 s\n",
      "2024-01-10 20:33:48.125337: \n",
      "2024-01-10 20:33:48.131039: Epoch 413\n",
      "2024-01-10 20:33:48.141131: Current learning rate: 0.00619\n",
      "2024-01-10 20:34:28.461046: train_loss -0.9427\n",
      "2024-01-10 20:34:28.494051: val_loss -0.8357\n",
      "2024-01-10 20:34:28.503051: Pseudo dice [0.8594, 0.9593, 0.944]\n",
      "2024-01-10 20:34:28.513056: Epoch time: 40.34 s\n",
      "2024-01-10 20:34:30.093058: \n",
      "2024-01-10 20:34:30.099571: Epoch 414\n",
      "2024-01-10 20:34:30.103626: Current learning rate: 0.00618\n",
      "2024-01-10 20:35:10.295135: train_loss -0.943\n",
      "2024-01-10 20:35:10.302366: val_loss -0.8384\n",
      "2024-01-10 20:35:10.308366: Pseudo dice [0.8642, 0.96, 0.9444]\n",
      "2024-01-10 20:35:10.317388: Epoch time: 40.2 s\n",
      "2024-01-10 20:35:11.741891: \n",
      "2024-01-10 20:35:11.750006: Epoch 415\n",
      "2024-01-10 20:35:11.757013: Current learning rate: 0.00617\n",
      "2024-01-10 20:35:52.019041: train_loss -0.942\n",
      "2024-01-10 20:35:52.029048: val_loss -0.8355\n",
      "2024-01-10 20:35:52.038043: Pseudo dice [0.8676, 0.9587, 0.9425]\n",
      "2024-01-10 20:35:52.073119: Epoch time: 40.28 s\n",
      "2024-01-10 20:35:53.528126: \n",
      "2024-01-10 20:35:53.537131: Epoch 416\n",
      "2024-01-10 20:35:53.544206: Current learning rate: 0.00616\n",
      "2024-01-10 20:36:33.756368: train_loss -0.9422\n",
      "2024-01-10 20:36:33.763368: val_loss -0.8343\n",
      "2024-01-10 20:36:33.772369: Pseudo dice [0.8661, 0.9578, 0.9423]\n",
      "2024-01-10 20:36:33.796369: Epoch time: 40.23 s\n",
      "2024-01-10 20:36:35.192205: \n",
      "2024-01-10 20:36:35.202211: Epoch 417\n",
      "2024-01-10 20:36:35.206194: Current learning rate: 0.00615\n",
      "2024-01-10 20:37:15.291464: train_loss -0.9428\n",
      "2024-01-10 20:37:15.302465: val_loss -0.8389\n",
      "2024-01-10 20:37:15.310465: Pseudo dice [0.8705, 0.959, 0.9431]\n",
      "2024-01-10 20:37:15.318465: Epoch time: 40.1 s\n",
      "2024-01-10 20:37:16.746932: \n",
      "2024-01-10 20:37:16.756198: Epoch 418\n",
      "2024-01-10 20:37:16.761301: Current learning rate: 0.00614\n",
      "2024-01-10 20:37:57.147785: train_loss -0.9425\n",
      "2024-01-10 20:37:57.155785: val_loss -0.8462\n",
      "2024-01-10 20:37:57.162789: Pseudo dice [0.8716, 0.9608, 0.9455]\n",
      "2024-01-10 20:37:57.168794: Epoch time: 40.4 s\n",
      "2024-01-10 20:37:58.787475: \n",
      "2024-01-10 20:37:58.792979: Epoch 419\n",
      "2024-01-10 20:37:58.799982: Current learning rate: 0.00613\n",
      "2024-01-10 20:38:39.106968: train_loss -0.9425\n",
      "2024-01-10 20:38:39.115967: val_loss -0.8394\n",
      "2024-01-10 20:38:39.123965: Pseudo dice [0.8643, 0.9597, 0.944]\n",
      "2024-01-10 20:38:39.130965: Epoch time: 40.32 s\n",
      "2024-01-10 20:38:40.643389: \n",
      "2024-01-10 20:38:40.650385: Epoch 420\n",
      "2024-01-10 20:38:40.654446: Current learning rate: 0.00612\n",
      "2024-01-10 20:39:20.903837: train_loss -0.9426\n",
      "2024-01-10 20:39:20.912349: val_loss -0.8379\n",
      "2024-01-10 20:39:20.922349: Pseudo dice [0.8699, 0.9588, 0.9428]\n",
      "2024-01-10 20:39:20.929350: Epoch time: 40.26 s\n",
      "2024-01-10 20:39:22.471438: \n",
      "2024-01-10 20:39:22.477017: Epoch 421\n",
      "2024-01-10 20:39:22.482046: Current learning rate: 0.00612\n",
      "2024-01-10 20:40:02.803132: train_loss -0.9422\n",
      "2024-01-10 20:40:02.811132: val_loss -0.8383\n",
      "2024-01-10 20:40:02.819137: Pseudo dice [0.8681, 0.959, 0.943]\n",
      "2024-01-10 20:40:02.827138: Epoch time: 40.33 s\n",
      "2024-01-10 20:40:04.234076: \n",
      "2024-01-10 20:40:04.240081: Epoch 422\n",
      "2024-01-10 20:40:04.244135: Current learning rate: 0.00611\n",
      "2024-01-10 20:40:44.478850: train_loss -0.9428\n",
      "2024-01-10 20:40:44.511852: val_loss -0.8453\n",
      "2024-01-10 20:40:44.522893: Pseudo dice [0.8725, 0.9606, 0.9461]\n",
      "2024-01-10 20:40:44.530902: Epoch time: 40.25 s\n",
      "2024-01-10 20:40:46.079443: \n",
      "2024-01-10 20:40:46.086123: Epoch 423\n",
      "2024-01-10 20:40:46.092194: Current learning rate: 0.0061\n",
      "2024-01-10 20:41:26.349666: train_loss -0.9427\n",
      "2024-01-10 20:41:26.357652: val_loss -0.8372\n",
      "2024-01-10 20:41:26.365164: Pseudo dice [0.8681, 0.9594, 0.9439]\n",
      "2024-01-10 20:41:26.375164: Epoch time: 40.27 s\n",
      "2024-01-10 20:41:28.050745: \n",
      "2024-01-10 20:41:28.058761: Epoch 424\n",
      "2024-01-10 20:41:28.063190: Current learning rate: 0.00609\n",
      "2024-01-10 20:42:08.202502: train_loss -0.942\n",
      "2024-01-10 20:42:08.209504: val_loss -0.8436\n",
      "2024-01-10 20:42:08.217507: Pseudo dice [0.8688, 0.9604, 0.9442]\n",
      "2024-01-10 20:42:08.223505: Epoch time: 40.15 s\n",
      "2024-01-10 20:42:09.785090: \n",
      "2024-01-10 20:42:09.789966: Epoch 425\n",
      "2024-01-10 20:42:09.795037: Current learning rate: 0.00608\n",
      "2024-01-10 20:42:50.043329: train_loss -0.9418\n",
      "2024-01-10 20:42:50.051329: val_loss -0.842\n",
      "2024-01-10 20:42:50.060331: Pseudo dice [0.8704, 0.9601, 0.9456]\n",
      "2024-01-10 20:42:50.067332: Epoch time: 40.26 s\n",
      "2024-01-10 20:42:51.473188: \n",
      "2024-01-10 20:42:51.479608: Epoch 426\n",
      "2024-01-10 20:42:51.485688: Current learning rate: 0.00607\n",
      "2024-01-10 20:43:31.745052: train_loss -0.9424\n",
      "2024-01-10 20:43:31.754053: val_loss -0.8384\n",
      "2024-01-10 20:43:31.761055: Pseudo dice [0.8675, 0.9596, 0.9446]\n",
      "2024-01-10 20:43:31.767053: Epoch time: 40.27 s\n",
      "2024-01-10 20:43:33.222666: \n",
      "2024-01-10 20:43:33.228703: Epoch 427\n",
      "2024-01-10 20:43:33.235775: Current learning rate: 0.00606\n",
      "2024-01-10 20:44:13.574766: train_loss -0.9427\n",
      "2024-01-10 20:44:13.582768: val_loss -0.8402\n",
      "2024-01-10 20:44:13.611292: Pseudo dice [0.867, 0.9591, 0.9439]\n",
      "2024-01-10 20:44:13.620286: Epoch time: 40.35 s\n",
      "2024-01-10 20:44:15.034765: \n",
      "2024-01-10 20:44:15.039759: Epoch 428\n",
      "2024-01-10 20:44:15.052015: Current learning rate: 0.00605\n",
      "2024-01-10 20:44:55.407484: train_loss -0.9431\n",
      "2024-01-10 20:44:55.416485: val_loss -0.8378\n",
      "2024-01-10 20:44:55.423483: Pseudo dice [0.8673, 0.9591, 0.9443]\n",
      "2024-01-10 20:44:55.430486: Epoch time: 40.37 s\n",
      "2024-01-10 20:44:57.081429: \n",
      "2024-01-10 20:44:57.091841: Epoch 429\n",
      "2024-01-10 20:44:57.096755: Current learning rate: 0.00604\n",
      "2024-01-10 20:45:37.394168: train_loss -0.9427\n",
      "2024-01-10 20:45:37.403169: val_loss -0.8327\n",
      "2024-01-10 20:45:37.409169: Pseudo dice [0.8631, 0.9575, 0.9417]\n",
      "2024-01-10 20:45:37.416169: Epoch time: 40.31 s\n",
      "2024-01-10 20:45:38.868272: \n",
      "2024-01-10 20:45:38.879969: Epoch 430\n",
      "2024-01-10 20:45:38.889044: Current learning rate: 0.00603\n",
      "2024-01-10 20:46:19.226171: train_loss -0.9435\n",
      "2024-01-10 20:46:19.234171: val_loss -0.8403\n",
      "2024-01-10 20:46:19.244172: Pseudo dice [0.8707, 0.9601, 0.9444]\n",
      "2024-01-10 20:46:19.253173: Epoch time: 40.36 s\n",
      "2024-01-10 20:46:20.720896: \n",
      "2024-01-10 20:46:20.726387: Epoch 431\n",
      "2024-01-10 20:46:20.732452: Current learning rate: 0.00602\n",
      "2024-01-10 20:47:01.027243: train_loss -0.9436\n",
      "2024-01-10 20:47:01.036248: val_loss -0.8439\n",
      "2024-01-10 20:47:01.044243: Pseudo dice [0.8738, 0.9593, 0.9434]\n",
      "2024-01-10 20:47:01.053244: Epoch time: 40.31 s\n",
      "2024-01-10 20:47:02.454416: \n",
      "2024-01-10 20:47:02.460416: Epoch 432\n",
      "2024-01-10 20:47:02.465416: Current learning rate: 0.00601\n",
      "2024-01-10 20:47:42.674114: train_loss -0.9437\n",
      "2024-01-10 20:47:42.684116: val_loss -0.8359\n",
      "2024-01-10 20:47:42.694117: Pseudo dice [0.8668, 0.9594, 0.944]\n",
      "2024-01-10 20:47:42.701118: Epoch time: 40.22 s\n",
      "2024-01-10 20:47:44.116022: \n",
      "2024-01-10 20:47:44.122141: Epoch 433\n",
      "2024-01-10 20:47:44.127165: Current learning rate: 0.006\n",
      "2024-01-10 20:48:24.504769: train_loss -0.9435\n",
      "2024-01-10 20:48:24.514768: val_loss -0.84\n",
      "2024-01-10 20:48:24.522768: Pseudo dice [0.867, 0.9598, 0.9444]\n",
      "2024-01-10 20:48:24.550774: Epoch time: 40.39 s\n",
      "2024-01-10 20:48:26.038358: \n",
      "2024-01-10 20:48:26.043371: Epoch 434\n",
      "2024-01-10 20:48:26.048442: Current learning rate: 0.00599\n",
      "2024-01-10 20:49:06.460543: train_loss -0.9434\n",
      "2024-01-10 20:49:06.469047: val_loss -0.8434\n",
      "2024-01-10 20:49:06.493059: Pseudo dice [0.8712, 0.9605, 0.945]\n",
      "2024-01-10 20:49:06.501055: Epoch time: 40.43 s\n",
      "2024-01-10 20:49:07.865178: \n",
      "2024-01-10 20:49:07.870282: Epoch 435\n",
      "2024-01-10 20:49:07.880287: Current learning rate: 0.00598\n",
      "2024-01-10 20:49:48.045520: train_loss -0.9438\n",
      "2024-01-10 20:49:48.053520: val_loss -0.8367\n",
      "2024-01-10 20:49:48.060522: Pseudo dice [0.8653, 0.9594, 0.9442]\n",
      "2024-01-10 20:49:48.065524: Epoch time: 40.18 s\n",
      "2024-01-10 20:49:49.577621: \n",
      "2024-01-10 20:49:49.589683: Epoch 436\n",
      "2024-01-10 20:49:49.594674: Current learning rate: 0.00597\n",
      "2024-01-10 20:50:29.819832: train_loss -0.9434\n",
      "2024-01-10 20:50:29.846835: val_loss -0.8367\n",
      "2024-01-10 20:50:29.855944: Pseudo dice [0.86, 0.9595, 0.9441]\n",
      "2024-01-10 20:50:29.860945: Epoch time: 40.24 s\n",
      "2024-01-10 20:50:31.390085: \n",
      "2024-01-10 20:50:31.402010: Epoch 437\n",
      "2024-01-10 20:50:31.405796: Current learning rate: 0.00596\n",
      "2024-01-10 20:51:11.820627: train_loss -0.943\n",
      "2024-01-10 20:51:11.830136: val_loss -0.8332\n",
      "2024-01-10 20:51:11.837139: Pseudo dice [0.8588, 0.9587, 0.9427]\n",
      "2024-01-10 20:51:11.862138: Epoch time: 40.43 s\n",
      "2024-01-10 20:51:13.268859: \n",
      "2024-01-10 20:51:13.276610: Epoch 438\n",
      "2024-01-10 20:51:13.287684: Current learning rate: 0.00595\n",
      "2024-01-10 20:51:53.467955: train_loss -0.944\n",
      "2024-01-10 20:51:53.477962: val_loss -0.8385\n",
      "2024-01-10 20:51:53.484958: Pseudo dice [0.8694, 0.9592, 0.944]\n",
      "2024-01-10 20:51:53.492956: Epoch time: 40.2 s\n",
      "2024-01-10 20:51:55.067024: \n",
      "2024-01-10 20:51:55.078626: Epoch 439\n",
      "2024-01-10 20:51:55.083691: Current learning rate: 0.00594\n",
      "2024-01-10 20:52:35.400501: train_loss -0.9436\n",
      "2024-01-10 20:52:35.430503: val_loss -0.8354\n",
      "2024-01-10 20:52:35.437500: Pseudo dice [0.8655, 0.9591, 0.9433]\n",
      "2024-01-10 20:52:35.445510: Epoch time: 40.33 s\n",
      "2024-01-10 20:52:36.935591: \n",
      "2024-01-10 20:52:36.948663: Epoch 440\n",
      "2024-01-10 20:52:36.961368: Current learning rate: 0.00593\n",
      "2024-01-10 20:53:17.121807: train_loss -0.9434\n",
      "2024-01-10 20:53:17.150322: val_loss -0.8329\n",
      "2024-01-10 20:53:17.159323: Pseudo dice [0.8622, 0.9585, 0.9419]\n",
      "2024-01-10 20:53:17.166331: Epoch time: 40.19 s\n",
      "2024-01-10 20:53:18.662613: \n",
      "2024-01-10 20:53:18.668326: Epoch 441\n",
      "2024-01-10 20:53:18.672331: Current learning rate: 0.00592\n",
      "2024-01-10 20:53:58.853908: train_loss -0.9435\n",
      "2024-01-10 20:53:58.861909: val_loss -0.8371\n",
      "2024-01-10 20:53:58.869910: Pseudo dice [0.8644, 0.9592, 0.9434]\n",
      "2024-01-10 20:53:58.876537: Epoch time: 40.19 s\n",
      "2024-01-10 20:54:00.304287: \n",
      "2024-01-10 20:54:00.311845: Epoch 442\n",
      "2024-01-10 20:54:00.316926: Current learning rate: 0.00592\n",
      "2024-01-10 20:54:40.624405: train_loss -0.9438\n",
      "2024-01-10 20:54:40.633405: val_loss -0.8361\n",
      "2024-01-10 20:54:40.642406: Pseudo dice [0.8634, 0.9585, 0.9421]\n",
      "2024-01-10 20:54:40.674408: Epoch time: 40.32 s\n",
      "2024-01-10 20:54:42.087209: \n",
      "2024-01-10 20:54:42.095109: Epoch 443\n",
      "2024-01-10 20:54:42.102175: Current learning rate: 0.00591\n",
      "2024-01-10 20:55:22.344342: train_loss -0.9427\n",
      "2024-01-10 20:55:22.351339: val_loss -0.8349\n",
      "2024-01-10 20:55:22.361340: Pseudo dice [0.8634, 0.9582, 0.9421]\n",
      "2024-01-10 20:55:22.366340: Epoch time: 40.26 s\n",
      "2024-01-10 20:55:24.016508: \n",
      "2024-01-10 20:55:24.022047: Epoch 444\n",
      "2024-01-10 20:55:24.026135: Current learning rate: 0.0059\n",
      "2024-01-10 20:56:04.338118: train_loss -0.9421\n",
      "2024-01-10 20:56:04.346118: val_loss -0.8375\n",
      "2024-01-10 20:56:04.352123: Pseudo dice [0.8675, 0.9595, 0.9445]\n",
      "2024-01-10 20:56:04.360124: Epoch time: 40.32 s\n",
      "2024-01-10 20:56:05.746248: \n",
      "2024-01-10 20:56:05.751852: Epoch 445\n",
      "2024-01-10 20:56:05.756238: Current learning rate: 0.00589\n",
      "2024-01-10 20:56:46.009244: train_loss -0.9432\n",
      "2024-01-10 20:56:46.017244: val_loss -0.848\n",
      "2024-01-10 20:56:46.024750: Pseudo dice [0.8772, 0.9608, 0.9457]\n",
      "2024-01-10 20:56:46.031755: Epoch time: 40.26 s\n",
      "2024-01-10 20:56:47.408389: \n",
      "2024-01-10 20:56:47.413818: Epoch 446\n",
      "2024-01-10 20:56:47.421872: Current learning rate: 0.00588\n",
      "2024-01-10 20:57:27.692228: train_loss -0.943\n",
      "2024-01-10 20:57:27.699229: val_loss -0.8427\n",
      "2024-01-10 20:57:27.706235: Pseudo dice [0.8745, 0.9594, 0.9435]\n",
      "2024-01-10 20:57:27.714234: Epoch time: 40.29 s\n",
      "2024-01-10 20:57:29.165612: \n",
      "2024-01-10 20:57:29.171052: Epoch 447\n",
      "2024-01-10 20:57:29.175134: Current learning rate: 0.00587\n",
      "2024-01-10 20:58:09.524875: train_loss -0.9427\n",
      "2024-01-10 20:58:09.534879: val_loss -0.8431\n",
      "2024-01-10 20:58:09.540880: Pseudo dice [0.8714, 0.9603, 0.9451]\n",
      "2024-01-10 20:58:09.546880: Epoch time: 40.36 s\n",
      "2024-01-10 20:58:10.984491: \n",
      "2024-01-10 20:58:10.989857: Epoch 448\n",
      "2024-01-10 20:58:10.994931: Current learning rate: 0.00586\n",
      "2024-01-10 20:58:51.289183: train_loss -0.9433\n",
      "2024-01-10 20:58:51.324529: val_loss -0.8392\n",
      "2024-01-10 20:58:51.334525: Pseudo dice [0.8658, 0.9591, 0.9443]\n",
      "2024-01-10 20:58:51.343529: Epoch time: 40.31 s\n",
      "2024-01-10 20:58:52.660221: \n",
      "2024-01-10 20:58:52.667243: Epoch 449\n",
      "2024-01-10 20:58:52.671809: Current learning rate: 0.00585\n",
      "2024-01-10 20:59:33.104251: train_loss -0.9436\n",
      "2024-01-10 20:59:33.113252: val_loss -0.8386\n",
      "2024-01-10 20:59:33.120252: Pseudo dice [0.8671, 0.9592, 0.9435]\n",
      "2024-01-10 20:59:33.126252: Epoch time: 40.45 s\n",
      "2024-01-10 20:59:34.762429: \n",
      "2024-01-10 20:59:34.773882: Epoch 450\n",
      "2024-01-10 20:59:34.778959: Current learning rate: 0.00584\n",
      "2024-01-10 21:00:15.172079: train_loss -0.9437\n",
      "2024-01-10 21:00:15.197085: val_loss -0.8407\n",
      "2024-01-10 21:00:15.205605: Pseudo dice [0.8728, 0.96, 0.9446]\n",
      "2024-01-10 21:00:15.212607: Epoch time: 40.41 s\n",
      "2024-01-10 21:00:16.570910: \n",
      "2024-01-10 21:00:16.583476: Epoch 451\n",
      "2024-01-10 21:00:16.588478: Current learning rate: 0.00583\n",
      "2024-01-10 21:00:56.870480: train_loss -0.9413\n",
      "2024-01-10 21:00:56.880479: val_loss -0.8329\n",
      "2024-01-10 21:00:56.886478: Pseudo dice [0.8639, 0.958, 0.9411]\n",
      "2024-01-10 21:00:56.895479: Epoch time: 40.3 s\n",
      "2024-01-10 21:00:58.392617: \n",
      "2024-01-10 21:00:58.397616: Epoch 452\n",
      "2024-01-10 21:00:58.402624: Current learning rate: 0.00582\n",
      "2024-01-10 21:01:38.721027: train_loss -0.942\n",
      "2024-01-10 21:01:38.729031: val_loss -0.8341\n",
      "2024-01-10 21:01:38.735032: Pseudo dice [0.8538, 0.9593, 0.9436]\n",
      "2024-01-10 21:01:38.742545: Epoch time: 40.33 s\n",
      "2024-01-10 21:01:40.116948: \n",
      "2024-01-10 21:01:40.123591: Epoch 453\n",
      "2024-01-10 21:01:40.135680: Current learning rate: 0.00581\n",
      "2024-01-10 21:02:20.291767: train_loss -0.9431\n",
      "2024-01-10 21:02:20.318767: val_loss -0.8413\n",
      "2024-01-10 21:02:20.327766: Pseudo dice [0.8675, 0.9596, 0.9436]\n",
      "2024-01-10 21:02:20.336772: Epoch time: 40.17 s\n",
      "2024-01-10 21:02:21.722996: \n",
      "2024-01-10 21:02:21.734951: Epoch 454\n",
      "2024-01-10 21:02:21.739990: Current learning rate: 0.0058\n",
      "2024-01-10 21:03:02.210940: train_loss -0.9432\n",
      "2024-01-10 21:03:02.220944: val_loss -0.8404\n",
      "2024-01-10 21:03:02.246943: Pseudo dice [0.8672, 0.9602, 0.9451]\n",
      "2024-01-10 21:03:02.255943: Epoch time: 40.49 s\n",
      "2024-01-10 21:03:03.745666: \n",
      "2024-01-10 21:03:03.751614: Epoch 455\n",
      "2024-01-10 21:03:03.756638: Current learning rate: 0.00579\n",
      "2024-01-10 21:03:44.010206: train_loss -0.9435\n",
      "2024-01-10 21:03:44.037205: val_loss -0.8415\n",
      "2024-01-10 21:03:44.045205: Pseudo dice [0.8733, 0.9598, 0.9448]\n",
      "2024-01-10 21:03:44.051206: Epoch time: 40.27 s\n",
      "2024-01-10 21:03:45.424558: \n",
      "2024-01-10 21:03:45.430436: Epoch 456\n",
      "2024-01-10 21:03:45.440511: Current learning rate: 0.00578\n",
      "2024-01-10 21:04:25.800151: train_loss -0.9437\n",
      "2024-01-10 21:04:25.809164: val_loss -0.8376\n",
      "2024-01-10 21:04:25.817165: Pseudo dice [0.8717, 0.9588, 0.9421]\n",
      "2024-01-10 21:04:25.843685: Epoch time: 40.38 s\n",
      "2024-01-10 21:04:27.205369: \n",
      "2024-01-10 21:04:27.213359: Epoch 457\n",
      "2024-01-10 21:04:27.218359: Current learning rate: 0.00577\n",
      "2024-01-10 21:05:07.512596: train_loss -0.9439\n",
      "2024-01-10 21:05:07.520598: val_loss -0.8367\n",
      "2024-01-10 21:05:07.530604: Pseudo dice [0.8747, 0.9584, 0.9427]\n",
      "2024-01-10 21:05:07.561112: Epoch time: 40.31 s\n",
      "2024-01-10 21:05:09.039763: \n",
      "2024-01-10 21:05:09.050266: Epoch 458\n",
      "2024-01-10 21:05:09.057344: Current learning rate: 0.00576\n",
      "2024-01-10 21:05:49.248859: train_loss -0.9441\n",
      "2024-01-10 21:05:49.257861: val_loss -0.8406\n",
      "2024-01-10 21:05:49.267899: Pseudo dice [0.8679, 0.9597, 0.9435]\n",
      "2024-01-10 21:05:49.274896: Epoch time: 40.21 s\n",
      "2024-01-10 21:05:50.700302: \n",
      "2024-01-10 21:05:50.705310: Epoch 459\n",
      "2024-01-10 21:05:50.710310: Current learning rate: 0.00575\n",
      "2024-01-10 21:06:31.201982: train_loss -0.9429\n",
      "2024-01-10 21:06:31.209980: val_loss -0.8368\n",
      "2024-01-10 21:06:31.217981: Pseudo dice [0.8667, 0.9595, 0.944]\n",
      "2024-01-10 21:06:31.242980: Epoch time: 40.5 s\n",
      "2024-01-10 21:06:32.616660: \n",
      "2024-01-10 21:06:32.622131: Epoch 460\n",
      "2024-01-10 21:06:32.627134: Current learning rate: 0.00574\n",
      "2024-01-10 21:07:13.063763: train_loss -0.9433\n",
      "2024-01-10 21:07:13.073498: val_loss -0.8369\n",
      "2024-01-10 21:07:13.082497: Pseudo dice [0.8731, 0.9581, 0.9421]\n",
      "2024-01-10 21:07:13.111012: Epoch time: 40.45 s\n",
      "2024-01-10 21:07:14.467627: \n",
      "2024-01-10 21:07:14.474781: Epoch 461\n",
      "2024-01-10 21:07:14.479779: Current learning rate: 0.00573\n",
      "2024-01-10 21:07:54.715643: train_loss -0.9444\n",
      "2024-01-10 21:07:54.726643: val_loss -0.8423\n",
      "2024-01-10 21:07:54.734644: Pseudo dice [0.8711, 0.9601, 0.9443]\n",
      "2024-01-10 21:07:54.739657: Epoch time: 40.25 s\n",
      "2024-01-10 21:07:56.239071: \n",
      "2024-01-10 21:07:56.247169: Epoch 462\n",
      "2024-01-10 21:07:56.252148: Current learning rate: 0.00572\n",
      "2024-01-10 21:08:36.620424: train_loss -0.9436\n",
      "2024-01-10 21:08:36.629155: val_loss -0.8387\n",
      "2024-01-10 21:08:36.636157: Pseudo dice [0.869, 0.9593, 0.9439]\n",
      "2024-01-10 21:08:36.644157: Epoch time: 40.38 s\n",
      "2024-01-10 21:08:38.133679: \n",
      "2024-01-10 21:08:38.141299: Epoch 463\n",
      "2024-01-10 21:08:38.152385: Current learning rate: 0.00571\n",
      "2024-01-10 21:09:18.395273: train_loss -0.9441\n",
      "2024-01-10 21:09:18.422778: val_loss -0.8414\n",
      "2024-01-10 21:09:18.431785: Pseudo dice [0.8685, 0.9595, 0.944]\n",
      "2024-01-10 21:09:18.439783: Epoch time: 40.26 s\n",
      "2024-01-10 21:09:19.778290: \n",
      "2024-01-10 21:09:19.783922: Epoch 464\n",
      "2024-01-10 21:09:19.788001: Current learning rate: 0.0057\n",
      "2024-01-10 21:10:00.348359: train_loss -0.9433\n",
      "2024-01-10 21:10:00.360868: val_loss -0.8404\n",
      "2024-01-10 21:10:00.369869: Pseudo dice [0.8666, 0.9598, 0.9446]\n",
      "2024-01-10 21:10:00.396421: Epoch time: 40.57 s\n",
      "2024-01-10 21:10:01.853085: \n",
      "2024-01-10 21:10:01.864846: Epoch 465\n",
      "2024-01-10 21:10:01.869899: Current learning rate: 0.0057\n",
      "2024-01-10 21:10:42.168138: train_loss -0.9441\n",
      "2024-01-10 21:10:42.179652: val_loss -0.8398\n",
      "2024-01-10 21:10:42.215899: Pseudo dice [0.868, 0.9595, 0.9446]\n",
      "2024-01-10 21:10:42.223899: Epoch time: 40.32 s\n",
      "2024-01-10 21:10:43.589025: \n",
      "2024-01-10 21:10:43.597024: Epoch 466\n",
      "2024-01-10 21:10:43.602048: Current learning rate: 0.00569\n",
      "2024-01-10 21:11:23.908257: train_loss -0.944\n",
      "2024-01-10 21:11:23.932767: val_loss -0.8331\n",
      "2024-01-10 21:11:23.941765: Pseudo dice [0.8715, 0.9572, 0.9406]\n",
      "2024-01-10 21:11:23.948766: Epoch time: 40.32 s\n",
      "2024-01-10 21:11:25.342067: \n",
      "2024-01-10 21:11:25.347885: Epoch 467\n",
      "2024-01-10 21:11:25.352905: Current learning rate: 0.00568\n",
      "2024-01-10 21:12:05.673308: train_loss -0.9439\n",
      "2024-01-10 21:12:05.682307: val_loss -0.8434\n",
      "2024-01-10 21:12:05.688308: Pseudo dice [0.8713, 0.9603, 0.9455]\n",
      "2024-01-10 21:12:05.693307: Epoch time: 40.33 s\n",
      "2024-01-10 21:12:07.199869: \n",
      "2024-01-10 21:12:07.204947: Epoch 468\n",
      "2024-01-10 21:12:07.209921: Current learning rate: 0.00567\n",
      "2024-01-10 21:12:47.648661: train_loss -0.9445\n",
      "2024-01-10 21:12:47.657659: val_loss -0.8389\n",
      "2024-01-10 21:12:47.665658: Pseudo dice [0.862, 0.9591, 0.944]\n",
      "2024-01-10 21:12:47.671664: Epoch time: 40.45 s\n",
      "2024-01-10 21:12:49.134268: \n",
      "2024-01-10 21:12:49.140259: Epoch 469\n",
      "2024-01-10 21:12:49.144539: Current learning rate: 0.00566\n",
      "2024-01-10 21:13:29.503442: train_loss -0.9445\n",
      "2024-01-10 21:13:29.512445: val_loss -0.8381\n",
      "2024-01-10 21:13:29.518443: Pseudo dice [0.8613, 0.9595, 0.9446]\n",
      "2024-01-10 21:13:29.523444: Epoch time: 40.37 s\n",
      "2024-01-10 21:13:30.930423: \n",
      "2024-01-10 21:13:30.939411: Epoch 470\n",
      "2024-01-10 21:13:30.947487: Current learning rate: 0.00565\n",
      "2024-01-10 21:14:11.247971: train_loss -0.9437\n",
      "2024-01-10 21:14:11.255970: val_loss -0.8365\n",
      "2024-01-10 21:14:11.265968: Pseudo dice [0.8656, 0.9588, 0.9427]\n",
      "2024-01-10 21:14:11.271976: Epoch time: 40.32 s\n",
      "2024-01-10 21:14:12.758094: \n",
      "2024-01-10 21:14:12.766181: Epoch 471\n",
      "2024-01-10 21:14:12.771447: Current learning rate: 0.00564\n",
      "2024-01-10 21:14:52.981306: train_loss -0.9446\n",
      "2024-01-10 21:14:53.014820: val_loss -0.8387\n",
      "2024-01-10 21:14:53.021821: Pseudo dice [0.866, 0.9595, 0.9442]\n",
      "2024-01-10 21:14:53.027821: Epoch time: 40.22 s\n",
      "2024-01-10 21:14:54.490775: \n",
      "2024-01-10 21:14:54.496864: Epoch 472\n",
      "2024-01-10 21:14:54.500787: Current learning rate: 0.00563\n",
      "2024-01-10 21:15:34.834653: train_loss -0.9442\n",
      "2024-01-10 21:15:34.841166: val_loss -0.838\n",
      "2024-01-10 21:15:34.847175: Pseudo dice [0.8654, 0.9595, 0.9442]\n",
      "2024-01-10 21:15:34.852174: Epoch time: 40.34 s\n",
      "2024-01-10 21:15:36.338875: \n",
      "2024-01-10 21:15:36.345398: Epoch 473\n",
      "2024-01-10 21:15:36.350211: Current learning rate: 0.00562\n",
      "2024-01-10 21:16:16.686347: train_loss -0.9451\n",
      "2024-01-10 21:16:16.696349: val_loss -0.839\n",
      "2024-01-10 21:16:16.704349: Pseudo dice [0.8649, 0.9595, 0.9438]\n",
      "2024-01-10 21:16:16.713349: Epoch time: 40.35 s\n",
      "2024-01-10 21:16:18.143420: \n",
      "2024-01-10 21:16:18.148837: Epoch 474\n",
      "2024-01-10 21:16:18.153837: Current learning rate: 0.00561\n",
      "2024-01-10 21:16:58.506141: train_loss -0.9445\n",
      "2024-01-10 21:16:58.514144: val_loss -0.839\n",
      "2024-01-10 21:16:58.523141: Pseudo dice [0.8683, 0.9599, 0.9441]\n",
      "2024-01-10 21:16:58.546140: Epoch time: 40.36 s\n",
      "2024-01-10 21:17:00.009646: \n",
      "2024-01-10 21:17:00.017574: Epoch 475\n",
      "2024-01-10 21:17:00.022782: Current learning rate: 0.0056\n",
      "2024-01-10 21:17:40.225649: train_loss -0.9442\n",
      "2024-01-10 21:17:40.233649: val_loss -0.8352\n",
      "2024-01-10 21:17:40.240649: Pseudo dice [0.8636, 0.9591, 0.9433]\n",
      "2024-01-10 21:17:40.248649: Epoch time: 40.22 s\n",
      "2024-01-10 21:17:41.675749: \n",
      "2024-01-10 21:17:41.681089: Epoch 476\n",
      "2024-01-10 21:17:41.685087: Current learning rate: 0.00559\n",
      "2024-01-10 21:18:21.904080: train_loss -0.9448\n",
      "2024-01-10 21:18:21.915088: val_loss -0.834\n",
      "2024-01-10 21:18:21.945599: Pseudo dice [0.8666, 0.9579, 0.9421]\n",
      "2024-01-10 21:18:21.954596: Epoch time: 40.23 s\n",
      "2024-01-10 21:18:23.285374: \n",
      "2024-01-10 21:18:23.289785: Epoch 477\n",
      "2024-01-10 21:18:23.297862: Current learning rate: 0.00558\n",
      "2024-01-10 21:19:03.455591: train_loss -0.9432\n",
      "2024-01-10 21:19:03.463590: val_loss -0.8409\n",
      "2024-01-10 21:19:03.471591: Pseudo dice [0.8719, 0.9602, 0.9444]\n",
      "2024-01-10 21:19:03.479588: Epoch time: 40.17 s\n",
      "2024-01-10 21:19:04.942965: \n",
      "2024-01-10 21:19:04.949546: Epoch 478\n",
      "2024-01-10 21:19:04.953608: Current learning rate: 0.00557\n",
      "2024-01-10 21:19:45.180241: train_loss -0.9437\n",
      "2024-01-10 21:19:45.187858: val_loss -0.8383\n",
      "2024-01-10 21:19:45.220859: Pseudo dice [0.8605, 0.9599, 0.945]\n",
      "2024-01-10 21:19:45.231857: Epoch time: 40.24 s\n",
      "2024-01-10 21:19:46.715889: \n",
      "2024-01-10 21:19:46.721066: Epoch 479\n",
      "2024-01-10 21:19:46.725081: Current learning rate: 0.00556\n",
      "2024-01-10 21:20:27.265060: train_loss -0.943\n",
      "2024-01-10 21:20:27.275061: val_loss -0.8367\n",
      "2024-01-10 21:20:27.283568: Pseudo dice [0.8675, 0.9599, 0.9448]\n",
      "2024-01-10 21:20:27.288569: Epoch time: 40.55 s\n",
      "2024-01-10 21:20:28.719969: \n",
      "2024-01-10 21:20:28.724969: Epoch 480\n",
      "2024-01-10 21:20:28.728968: Current learning rate: 0.00555\n",
      "2024-01-10 21:21:08.997929: train_loss -0.9444\n",
      "2024-01-10 21:21:09.006933: val_loss -0.8398\n",
      "2024-01-10 21:21:09.014933: Pseudo dice [0.8691, 0.9599, 0.9441]\n",
      "2024-01-10 21:21:09.019934: Epoch time: 40.28 s\n",
      "2024-01-10 21:21:10.441496: \n",
      "2024-01-10 21:21:10.452788: Epoch 481\n",
      "2024-01-10 21:21:10.461855: Current learning rate: 0.00554\n",
      "2024-01-10 21:21:50.808222: train_loss -0.9444\n",
      "2024-01-10 21:21:50.817736: val_loss -0.8338\n",
      "2024-01-10 21:21:50.823736: Pseudo dice [0.8615, 0.9584, 0.9418]\n",
      "2024-01-10 21:21:50.831737: Epoch time: 40.37 s\n",
      "2024-01-10 21:21:52.288788: \n",
      "2024-01-10 21:21:52.293702: Epoch 482\n",
      "2024-01-10 21:21:52.299850: Current learning rate: 0.00553\n",
      "2024-01-10 21:22:32.582289: train_loss -0.9447\n",
      "2024-01-10 21:22:32.590291: val_loss -0.838\n",
      "2024-01-10 21:22:32.600062: Pseudo dice [0.8677, 0.9598, 0.9451]\n",
      "2024-01-10 21:22:32.609062: Epoch time: 40.29 s\n",
      "2024-01-10 21:22:34.137555: \n",
      "2024-01-10 21:22:34.146005: Epoch 483\n",
      "2024-01-10 21:22:34.150430: Current learning rate: 0.00552\n",
      "2024-01-10 21:23:14.387536: train_loss -0.9447\n",
      "2024-01-10 21:23:14.397534: val_loss -0.839\n",
      "2024-01-10 21:23:14.407537: Pseudo dice [0.8659, 0.9592, 0.9437]\n",
      "2024-01-10 21:23:14.414536: Epoch time: 40.25 s\n",
      "2024-01-10 21:23:16.139584: \n",
      "2024-01-10 21:23:16.152578: Epoch 484\n",
      "2024-01-10 21:23:16.158588: Current learning rate: 0.00551\n",
      "2024-01-10 21:23:56.344374: train_loss -0.9436\n",
      "2024-01-10 21:23:56.372896: val_loss -0.8329\n",
      "2024-01-10 21:23:56.381901: Pseudo dice [0.866, 0.9579, 0.9408]\n",
      "2024-01-10 21:23:56.388899: Epoch time: 40.21 s\n",
      "2024-01-10 21:23:57.820658: \n",
      "2024-01-10 21:23:57.832371: Epoch 485\n",
      "2024-01-10 21:23:57.842372: Current learning rate: 0.0055\n",
      "2024-01-10 21:24:38.140585: train_loss -0.9444\n",
      "2024-01-10 21:24:38.152586: val_loss -0.8398\n",
      "2024-01-10 21:24:38.158585: Pseudo dice [0.8696, 0.9592, 0.9439]\n",
      "2024-01-10 21:24:38.165585: Epoch time: 40.32 s\n",
      "2024-01-10 21:24:39.665932: \n",
      "2024-01-10 21:24:39.676641: Epoch 486\n",
      "2024-01-10 21:24:39.681711: Current learning rate: 0.00549\n",
      "2024-01-10 21:25:19.973270: train_loss -0.9446\n",
      "2024-01-10 21:25:19.981272: val_loss -0.8422\n",
      "2024-01-10 21:25:19.987793: Pseudo dice [0.8735, 0.9595, 0.945]\n",
      "2024-01-10 21:25:20.010802: Epoch time: 40.31 s\n",
      "2024-01-10 21:25:21.471666: \n",
      "2024-01-10 21:25:21.477366: Epoch 487\n",
      "2024-01-10 21:25:21.483348: Current learning rate: 0.00548\n",
      "2024-01-10 21:26:01.760158: train_loss -0.9441\n",
      "2024-01-10 21:26:01.770160: val_loss -0.8354\n",
      "2024-01-10 21:26:01.801159: Pseudo dice [0.8646, 0.959, 0.9431]\n",
      "2024-01-10 21:26:01.812157: Epoch time: 40.29 s\n",
      "2024-01-10 21:26:03.190716: \n",
      "2024-01-10 21:26:03.198473: Epoch 488\n",
      "2024-01-10 21:26:03.209494: Current learning rate: 0.00547\n",
      "2024-01-10 21:26:43.697125: train_loss -0.9434\n",
      "2024-01-10 21:26:43.705127: val_loss -0.8352\n",
      "2024-01-10 21:26:43.730126: Pseudo dice [0.8677, 0.9584, 0.9419]\n",
      "2024-01-10 21:26:43.737128: Epoch time: 40.51 s\n",
      "2024-01-10 21:26:45.262559: \n",
      "2024-01-10 21:26:45.268492: Epoch 489\n",
      "2024-01-10 21:26:45.275490: Current learning rate: 0.00546\n",
      "2024-01-10 21:27:25.653709: train_loss -0.9446\n",
      "2024-01-10 21:27:25.662715: val_loss -0.8421\n",
      "2024-01-10 21:27:25.671715: Pseudo dice [0.8773, 0.9587, 0.9431]\n",
      "2024-01-10 21:27:25.682228: Epoch time: 40.39 s\n",
      "2024-01-10 21:27:27.133719: \n",
      "2024-01-10 21:27:27.144001: Epoch 490\n",
      "2024-01-10 21:27:27.149980: Current learning rate: 0.00546\n",
      "2024-01-10 21:28:07.344681: train_loss -0.9448\n",
      "2024-01-10 21:28:07.355681: val_loss -0.8331\n",
      "2024-01-10 21:28:07.361687: Pseudo dice [0.8605, 0.9587, 0.9437]\n",
      "2024-01-10 21:28:07.368682: Epoch time: 40.21 s\n",
      "2024-01-10 21:28:08.864216: \n",
      "2024-01-10 21:28:08.869301: Epoch 491\n",
      "2024-01-10 21:28:08.874278: Current learning rate: 0.00545\n",
      "2024-01-10 21:28:49.070633: train_loss -0.9453\n",
      "2024-01-10 21:28:49.079637: val_loss -0.8386\n",
      "2024-01-10 21:28:49.087636: Pseudo dice [0.8648, 0.9596, 0.9438]\n",
      "2024-01-10 21:28:49.095796: Epoch time: 40.21 s\n",
      "2024-01-10 21:28:50.490745: \n",
      "2024-01-10 21:28:50.496104: Epoch 492\n",
      "2024-01-10 21:28:50.501094: Current learning rate: 0.00544\n",
      "2024-01-10 21:29:30.763214: train_loss -0.9454\n",
      "2024-01-10 21:29:30.773215: val_loss -0.8362\n",
      "2024-01-10 21:29:30.781216: Pseudo dice [0.8692, 0.9588, 0.9431]\n",
      "2024-01-10 21:29:30.788212: Epoch time: 40.28 s\n",
      "2024-01-10 21:29:32.203273: \n",
      "2024-01-10 21:29:32.209517: Epoch 493\n",
      "2024-01-10 21:29:32.216557: Current learning rate: 0.00543\n",
      "2024-01-10 21:30:12.524612: train_loss -0.9461\n",
      "2024-01-10 21:30:12.533614: val_loss -0.8419\n",
      "2024-01-10 21:30:12.541506: Pseudo dice [0.8745, 0.9602, 0.9453]\n",
      "2024-01-10 21:30:12.570023: Epoch time: 40.32 s\n",
      "2024-01-10 21:30:13.971748: \n",
      "2024-01-10 21:30:13.982640: Epoch 494\n",
      "2024-01-10 21:30:13.993316: Current learning rate: 0.00542\n",
      "2024-01-10 21:30:54.371244: train_loss -0.9438\n",
      "2024-01-10 21:30:54.405521: val_loss -0.8341\n",
      "2024-01-10 21:30:54.413516: Pseudo dice [0.8633, 0.9583, 0.943]\n",
      "2024-01-10 21:30:54.423515: Epoch time: 40.4 s\n",
      "2024-01-10 21:30:55.911328: \n",
      "2024-01-10 21:30:55.918792: Epoch 495\n",
      "2024-01-10 21:30:55.923102: Current learning rate: 0.00541\n",
      "2024-01-10 21:31:36.213725: train_loss -0.9451\n",
      "2024-01-10 21:31:36.221725: val_loss -0.8407\n",
      "2024-01-10 21:31:36.230723: Pseudo dice [0.8679, 0.9599, 0.9452]\n",
      "2024-01-10 21:31:36.236723: Epoch time: 40.3 s\n",
      "2024-01-10 21:31:37.595002: \n",
      "2024-01-10 21:31:37.601846: Epoch 496\n",
      "2024-01-10 21:31:37.605856: Current learning rate: 0.0054\n",
      "2024-01-10 21:32:17.880424: train_loss -0.9454\n",
      "2024-01-10 21:32:17.917591: val_loss -0.8407\n",
      "2024-01-10 21:32:17.924104: Pseudo dice [0.8702, 0.9594, 0.944]\n",
      "2024-01-10 21:32:17.930105: Epoch time: 40.29 s\n",
      "2024-01-10 21:32:19.300825: \n",
      "2024-01-10 21:32:19.309994: Epoch 497\n",
      "2024-01-10 21:32:19.314928: Current learning rate: 0.00539\n",
      "2024-01-10 21:32:59.684808: train_loss -0.9451\n",
      "2024-01-10 21:32:59.693812: val_loss -0.8444\n",
      "2024-01-10 21:32:59.701808: Pseudo dice [0.8753, 0.9605, 0.9455]\n",
      "2024-01-10 21:32:59.732820: Epoch time: 40.38 s\n",
      "2024-01-10 21:33:01.180766: \n",
      "2024-01-10 21:33:01.186561: Epoch 498\n",
      "2024-01-10 21:33:01.193615: Current learning rate: 0.00538\n",
      "2024-01-10 21:33:41.489027: train_loss -0.9455\n",
      "2024-01-10 21:33:41.496027: val_loss -0.8422\n",
      "2024-01-10 21:33:41.504025: Pseudo dice [0.8692, 0.9609, 0.9456]\n",
      "2024-01-10 21:33:41.511026: Epoch time: 40.31 s\n",
      "2024-01-10 21:33:42.912301: \n",
      "2024-01-10 21:33:42.922379: Epoch 499\n",
      "2024-01-10 21:33:42.928303: Current learning rate: 0.00537\n",
      "2024-01-10 21:34:23.336347: train_loss -0.9456\n",
      "2024-01-10 21:34:23.371364: val_loss -0.8393\n",
      "2024-01-10 21:34:23.377866: Pseudo dice [0.8669, 0.9598, 0.9446]\n",
      "2024-01-10 21:34:23.384872: Epoch time: 40.43 s\n",
      "2024-01-10 21:34:25.028657: \n",
      "2024-01-10 21:34:25.039662: Epoch 500\n",
      "2024-01-10 21:34:25.043871: Current learning rate: 0.00536\n",
      "2024-01-10 21:35:05.557926: train_loss -0.9458\n",
      "2024-01-10 21:35:05.600222: val_loss -0.8397\n",
      "2024-01-10 21:35:05.609227: Pseudo dice [0.8705, 0.9589, 0.9429]\n",
      "2024-01-10 21:35:05.615227: Epoch time: 40.53 s\n",
      "2024-01-10 21:35:07.043092: \n",
      "2024-01-10 21:35:07.055020: Epoch 501\n",
      "2024-01-10 21:35:07.060588: Current learning rate: 0.00535\n",
      "2024-01-10 21:35:47.483207: train_loss -0.9451\n",
      "2024-01-10 21:35:47.492207: val_loss -0.8382\n",
      "2024-01-10 21:35:47.499209: Pseudo dice [0.8723, 0.9588, 0.9432]\n",
      "2024-01-10 21:35:47.506213: Epoch time: 40.44 s\n",
      "2024-01-10 21:35:49.089213: \n",
      "2024-01-10 21:35:49.098208: Epoch 502\n",
      "2024-01-10 21:35:49.103146: Current learning rate: 0.00534\n",
      "2024-01-10 21:36:29.325918: train_loss -0.9459\n",
      "2024-01-10 21:36:29.333920: val_loss -0.8383\n",
      "2024-01-10 21:36:29.342433: Pseudo dice [0.8675, 0.96, 0.9449]\n",
      "2024-01-10 21:36:29.351435: Epoch time: 40.24 s\n",
      "2024-01-10 21:36:30.825495: \n",
      "2024-01-10 21:36:30.831577: Epoch 503\n",
      "2024-01-10 21:36:30.836574: Current learning rate: 0.00533\n",
      "2024-01-10 21:37:11.020965: train_loss -0.9453\n",
      "2024-01-10 21:37:11.030405: val_loss -0.8304\n",
      "2024-01-10 21:37:11.040401: Pseudo dice [0.8531, 0.9589, 0.9431]\n",
      "2024-01-10 21:37:11.048409: Epoch time: 40.2 s\n",
      "2024-01-10 21:37:12.503256: \n",
      "2024-01-10 21:37:12.509194: Epoch 504\n",
      "2024-01-10 21:37:12.519250: Current learning rate: 0.00532\n",
      "2024-01-10 21:37:53.013279: train_loss -0.9455\n",
      "2024-01-10 21:37:53.022279: val_loss -0.8371\n",
      "2024-01-10 21:37:53.028279: Pseudo dice [0.8665, 0.9594, 0.9436]\n",
      "2024-01-10 21:37:53.036277: Epoch time: 40.51 s\n",
      "2024-01-10 21:37:54.561185: \n",
      "2024-01-10 21:37:54.567249: Epoch 505\n",
      "2024-01-10 21:37:54.571242: Current learning rate: 0.00531\n",
      "2024-01-10 21:38:34.885491: train_loss -0.9458\n",
      "2024-01-10 21:38:34.913499: val_loss -0.8421\n",
      "2024-01-10 21:38:34.922495: Pseudo dice [0.869, 0.96, 0.9449]\n",
      "2024-01-10 21:38:34.930006: Epoch time: 40.33 s\n",
      "2024-01-10 21:38:36.332276: \n",
      "2024-01-10 21:38:36.340342: Epoch 506\n",
      "2024-01-10 21:38:36.345335: Current learning rate: 0.0053\n",
      "2024-01-10 21:39:16.632655: train_loss -0.9453\n",
      "2024-01-10 21:39:16.646169: val_loss -0.8381\n",
      "2024-01-10 21:39:16.654185: Pseudo dice [0.8619, 0.9593, 0.9439]\n",
      "2024-01-10 21:39:16.661171: Epoch time: 40.3 s\n",
      "2024-01-10 21:39:18.032795: \n",
      "2024-01-10 21:39:18.038811: Epoch 507\n",
      "2024-01-10 21:39:18.043384: Current learning rate: 0.00529\n",
      "2024-01-10 21:39:58.270739: train_loss -0.946\n",
      "2024-01-10 21:39:58.282741: val_loss -0.8385\n",
      "2024-01-10 21:39:58.292739: Pseudo dice [0.8665, 0.959, 0.9433]\n",
      "2024-01-10 21:39:58.301738: Epoch time: 40.24 s\n",
      "2024-01-10 21:39:59.818749: \n",
      "2024-01-10 21:39:59.824947: Epoch 508\n",
      "2024-01-10 21:39:59.829967: Current learning rate: 0.00528\n",
      "2024-01-10 21:40:40.189945: train_loss -0.9464\n",
      "2024-01-10 21:40:40.215943: val_loss -0.8375\n",
      "2024-01-10 21:40:40.223943: Pseudo dice [0.8669, 0.9596, 0.9445]\n",
      "2024-01-10 21:40:40.230945: Epoch time: 40.37 s\n",
      "2024-01-10 21:40:41.639654: \n",
      "2024-01-10 21:40:41.645653: Epoch 509\n",
      "2024-01-10 21:40:41.650654: Current learning rate: 0.00527\n",
      "2024-01-10 21:41:21.942456: train_loss -0.9452\n",
      "2024-01-10 21:41:21.952457: val_loss -0.8394\n",
      "2024-01-10 21:41:21.963455: Pseudo dice [0.8693, 0.9592, 0.943]\n",
      "2024-01-10 21:41:21.968457: Epoch time: 40.3 s\n",
      "2024-01-10 21:41:23.382392: \n",
      "2024-01-10 21:41:23.387521: Epoch 510\n",
      "2024-01-10 21:41:23.392465: Current learning rate: 0.00526\n",
      "2024-01-10 21:42:03.691992: train_loss -0.9454\n",
      "2024-01-10 21:42:03.698994: val_loss -0.8411\n",
      "2024-01-10 21:42:03.708500: Pseudo dice [0.8732, 0.9598, 0.9448]\n",
      "2024-01-10 21:42:03.736519: Epoch time: 40.31 s\n",
      "2024-01-10 21:42:05.146666: \n",
      "2024-01-10 21:42:05.152204: Epoch 511\n",
      "2024-01-10 21:42:05.164009: Current learning rate: 0.00525\n",
      "2024-01-10 21:42:45.445646: train_loss -0.9462\n",
      "2024-01-10 21:42:45.454650: val_loss -0.8378\n",
      "2024-01-10 21:42:45.461638: Pseudo dice [0.8664, 0.9596, 0.944]\n",
      "2024-01-10 21:42:45.491158: Epoch time: 40.3 s\n",
      "2024-01-10 21:42:46.920721: \n",
      "2024-01-10 21:42:46.927119: Epoch 512\n",
      "2024-01-10 21:42:46.931522: Current learning rate: 0.00524\n",
      "2024-01-10 21:43:27.186934: train_loss -0.9455\n",
      "2024-01-10 21:43:27.197942: val_loss -0.8398\n",
      "2024-01-10 21:43:27.205959: Pseudo dice [0.8663, 0.9599, 0.9449]\n",
      "2024-01-10 21:43:27.239365: Epoch time: 40.27 s\n",
      "2024-01-10 21:43:28.627574: \n",
      "2024-01-10 21:43:28.633816: Epoch 513\n",
      "2024-01-10 21:43:28.637785: Current learning rate: 0.00523\n",
      "2024-01-10 21:44:08.830833: train_loss -0.9458\n",
      "2024-01-10 21:44:08.840348: val_loss -0.8375\n",
      "2024-01-10 21:44:08.848861: Pseudo dice [0.8672, 0.9593, 0.9443]\n",
      "2024-01-10 21:44:08.856863: Epoch time: 40.2 s\n",
      "2024-01-10 21:44:10.220027: \n",
      "2024-01-10 21:44:10.232248: Epoch 514\n",
      "2024-01-10 21:44:10.240980: Current learning rate: 0.00522\n",
      "2024-01-10 21:44:50.723893: train_loss -0.946\n",
      "2024-01-10 21:44:50.731907: val_loss -0.839\n",
      "2024-01-10 21:44:50.742895: Pseudo dice [0.8708, 0.9591, 0.943]\n",
      "2024-01-10 21:44:50.748901: Epoch time: 40.5 s\n",
      "2024-01-10 21:44:52.213464: \n",
      "2024-01-10 21:44:52.224465: Epoch 515\n",
      "2024-01-10 21:44:52.230482: Current learning rate: 0.00521\n",
      "2024-01-10 21:45:32.433643: train_loss -0.946\n",
      "2024-01-10 21:45:32.442644: val_loss -0.8379\n",
      "2024-01-10 21:45:32.449646: Pseudo dice [0.8665, 0.9589, 0.9429]\n",
      "2024-01-10 21:45:32.457645: Epoch time: 40.22 s\n",
      "2024-01-10 21:45:33.855439: \n",
      "2024-01-10 21:45:33.861370: Epoch 516\n",
      "2024-01-10 21:45:33.865379: Current learning rate: 0.0052\n",
      "2024-01-10 21:46:14.156454: train_loss -0.9458\n",
      "2024-01-10 21:46:14.166457: val_loss -0.8424\n",
      "2024-01-10 21:46:14.177667: Pseudo dice [0.8692, 0.9598, 0.9447]\n",
      "2024-01-10 21:46:14.212181: Epoch time: 40.3 s\n",
      "2024-01-10 21:46:15.666324: \n",
      "2024-01-10 21:46:15.672335: Epoch 517\n",
      "2024-01-10 21:46:15.676305: Current learning rate: 0.00519\n",
      "2024-01-10 21:46:55.925088: train_loss -0.9457\n",
      "2024-01-10 21:46:55.934093: val_loss -0.8401\n",
      "2024-01-10 21:46:55.941609: Pseudo dice [0.8738, 0.9591, 0.9434]\n",
      "2024-01-10 21:46:55.946618: Epoch time: 40.26 s\n",
      "2024-01-10 21:46:57.350993: \n",
      "2024-01-10 21:46:57.355519: Epoch 518\n",
      "2024-01-10 21:46:57.360521: Current learning rate: 0.00518\n",
      "2024-01-10 21:47:37.718975: train_loss -0.9462\n",
      "2024-01-10 21:47:37.728977: val_loss -0.8425\n",
      "2024-01-10 21:47:37.736974: Pseudo dice [0.8697, 0.9596, 0.9444]\n",
      "2024-01-10 21:47:37.742975: Epoch time: 40.37 s\n",
      "2024-01-10 21:47:39.259451: \n",
      "2024-01-10 21:47:39.264452: Epoch 519\n",
      "2024-01-10 21:47:39.271455: Current learning rate: 0.00518\n",
      "2024-01-10 21:48:19.825346: train_loss -0.9462\n",
      "2024-01-10 21:48:19.836846: val_loss -0.8389\n",
      "2024-01-10 21:48:19.843854: Pseudo dice [0.8681, 0.9605, 0.9464]\n",
      "2024-01-10 21:48:19.878178: Epoch time: 40.57 s\n",
      "2024-01-10 21:48:21.346173: \n",
      "2024-01-10 21:48:21.354098: Epoch 520\n",
      "2024-01-10 21:48:21.365103: Current learning rate: 0.00517\n",
      "2024-01-10 21:49:01.675021: train_loss -0.9468\n",
      "2024-01-10 21:49:01.683021: val_loss -0.8375\n",
      "2024-01-10 21:49:01.690019: Pseudo dice [0.8645, 0.9598, 0.9447]\n",
      "2024-01-10 21:49:01.696023: Epoch time: 40.33 s\n",
      "2024-01-10 21:49:03.112427: \n",
      "2024-01-10 21:49:03.118601: Epoch 521\n",
      "2024-01-10 21:49:03.122671: Current learning rate: 0.00516\n",
      "2024-01-10 21:49:43.339131: train_loss -0.9465\n",
      "2024-01-10 21:49:43.349422: val_loss -0.8398\n",
      "2024-01-10 21:49:43.355900: Pseudo dice [0.8695, 0.9589, 0.9427]\n",
      "2024-01-10 21:49:43.363410: Epoch time: 40.23 s\n",
      "2024-01-10 21:49:44.815880: \n",
      "2024-01-10 21:49:44.823268: Epoch 522\n",
      "2024-01-10 21:49:44.828341: Current learning rate: 0.00515\n",
      "2024-01-10 21:50:25.154793: train_loss -0.9457\n",
      "2024-01-10 21:50:25.164795: val_loss -0.8407\n",
      "2024-01-10 21:50:25.196794: Pseudo dice [0.8681, 0.9591, 0.9439]\n",
      "2024-01-10 21:50:25.204806: Epoch time: 40.34 s\n",
      "2024-01-10 21:50:26.560533: \n",
      "2024-01-10 21:50:26.567240: Epoch 523\n",
      "2024-01-10 21:50:26.572299: Current learning rate: 0.00514\n",
      "2024-01-10 21:51:07.042277: train_loss -0.9448\n",
      "2024-01-10 21:51:07.065413: val_loss -0.83\n",
      "2024-01-10 21:51:07.080405: Pseudo dice [0.8591, 0.9575, 0.9406]\n",
      "2024-01-10 21:51:07.087439: Epoch time: 40.48 s\n",
      "2024-01-10 21:51:08.501750: \n",
      "2024-01-10 21:51:08.505749: Epoch 524\n",
      "2024-01-10 21:51:08.509748: Current learning rate: 0.00513\n",
      "2024-01-10 21:51:48.952483: train_loss -0.9459\n",
      "2024-01-10 21:51:48.961485: val_loss -0.8422\n",
      "2024-01-10 21:51:48.970489: Pseudo dice [0.8699, 0.9606, 0.9456]\n",
      "2024-01-10 21:51:48.976485: Epoch time: 40.45 s\n",
      "2024-01-10 21:51:50.389091: \n",
      "2024-01-10 21:51:50.394830: Epoch 525\n",
      "2024-01-10 21:51:50.399489: Current learning rate: 0.00512\n",
      "2024-01-10 21:52:30.547117: train_loss -0.946\n",
      "2024-01-10 21:52:30.556853: val_loss -0.8398\n",
      "2024-01-10 21:52:30.587845: Pseudo dice [0.8644, 0.9596, 0.9446]\n",
      "2024-01-10 21:52:30.595854: Epoch time: 40.16 s\n",
      "2024-01-10 21:52:31.959495: \n",
      "2024-01-10 21:52:31.964568: Epoch 526\n",
      "2024-01-10 21:52:31.969495: Current learning rate: 0.00511\n",
      "2024-01-10 21:53:12.152379: train_loss -0.9456\n",
      "2024-01-10 21:53:12.162880: val_loss -0.839\n",
      "2024-01-10 21:53:12.169883: Pseudo dice [0.8709, 0.9587, 0.9433]\n",
      "2024-01-10 21:53:12.176880: Epoch time: 40.19 s\n",
      "2024-01-10 21:53:13.604159: \n",
      "2024-01-10 21:53:13.611159: Epoch 527\n",
      "2024-01-10 21:53:13.615288: Current learning rate: 0.0051\n",
      "2024-01-10 21:53:54.069336: train_loss -0.9447\n",
      "2024-01-10 21:53:54.080347: val_loss -0.8433\n",
      "2024-01-10 21:53:54.089344: Pseudo dice [0.8727, 0.9596, 0.9448]\n",
      "2024-01-10 21:53:54.096520: Epoch time: 40.47 s\n",
      "2024-01-10 21:53:55.568078: \n",
      "2024-01-10 21:53:55.573140: Epoch 528\n",
      "2024-01-10 21:53:55.578079: Current learning rate: 0.00509\n",
      "2024-01-10 21:54:35.785479: train_loss -0.9451\n",
      "2024-01-10 21:54:35.822495: val_loss -0.8434\n",
      "2024-01-10 21:54:35.830510: Pseudo dice [0.8674, 0.9608, 0.9456]\n",
      "2024-01-10 21:54:35.836510: Epoch time: 40.22 s\n",
      "2024-01-10 21:54:37.231714: \n",
      "2024-01-10 21:54:37.243204: Epoch 529\n",
      "2024-01-10 21:54:37.248198: Current learning rate: 0.00508\n",
      "2024-01-10 21:55:17.605045: train_loss -0.9455\n",
      "2024-01-10 21:55:17.613039: val_loss -0.8364\n",
      "2024-01-10 21:55:17.647552: Pseudo dice [0.8682, 0.9587, 0.9431]\n",
      "2024-01-10 21:55:17.657552: Epoch time: 40.37 s\n",
      "2024-01-10 21:55:19.089506: \n",
      "2024-01-10 21:55:19.102321: Epoch 530\n",
      "2024-01-10 21:55:19.109353: Current learning rate: 0.00507\n",
      "2024-01-10 21:55:59.342422: train_loss -0.9451\n",
      "2024-01-10 21:55:59.349426: val_loss -0.8365\n",
      "2024-01-10 21:55:59.355424: Pseudo dice [0.8655, 0.9592, 0.9436]\n",
      "2024-01-10 21:55:59.363809: Epoch time: 40.25 s\n",
      "2024-01-10 21:56:00.813777: \n",
      "2024-01-10 21:56:00.820060: Epoch 531\n",
      "2024-01-10 21:56:00.824743: Current learning rate: 0.00506\n",
      "2024-01-10 21:56:41.131709: train_loss -0.9455\n",
      "2024-01-10 21:56:41.140709: val_loss -0.8372\n",
      "2024-01-10 21:56:41.148709: Pseudo dice [0.8658, 0.9592, 0.9436]\n",
      "2024-01-10 21:56:41.157710: Epoch time: 40.32 s\n",
      "2024-01-10 21:56:42.564725: \n",
      "2024-01-10 21:56:42.576195: Epoch 532\n",
      "2024-01-10 21:56:42.585195: Current learning rate: 0.00505\n",
      "2024-01-10 21:57:23.007114: train_loss -0.9453\n",
      "2024-01-10 21:57:23.017122: val_loss -0.8345\n",
      "2024-01-10 21:57:23.025636: Pseudo dice [0.8648, 0.9588, 0.9424]\n",
      "2024-01-10 21:57:23.052635: Epoch time: 40.44 s\n",
      "2024-01-10 21:57:24.516985: \n",
      "2024-01-10 21:57:24.526245: Epoch 533\n",
      "2024-01-10 21:57:24.534332: Current learning rate: 0.00504\n",
      "2024-01-10 21:58:04.756441: train_loss -0.945\n",
      "2024-01-10 21:58:04.764445: val_loss -0.8414\n",
      "2024-01-10 21:58:04.771445: Pseudo dice [0.8661, 0.9602, 0.9454]\n",
      "2024-01-10 21:58:04.779441: Epoch time: 40.24 s\n",
      "2024-01-10 21:58:06.380688: \n",
      "2024-01-10 21:58:06.390382: Epoch 534\n",
      "2024-01-10 21:58:06.394380: Current learning rate: 0.00503\n",
      "2024-01-10 21:58:46.738736: train_loss -0.9459\n",
      "2024-01-10 21:58:46.747737: val_loss -0.8384\n",
      "2024-01-10 21:58:46.755736: Pseudo dice [0.868, 0.9592, 0.9432]\n",
      "2024-01-10 21:58:46.762738: Epoch time: 40.36 s\n",
      "2024-01-10 21:58:48.119792: \n",
      "2024-01-10 21:58:48.125781: Epoch 535\n",
      "2024-01-10 21:58:48.137808: Current learning rate: 0.00502\n",
      "2024-01-10 21:59:28.389832: train_loss -0.945\n",
      "2024-01-10 21:59:28.397845: val_loss -0.845\n",
      "2024-01-10 21:59:28.404845: Pseudo dice [0.8742, 0.9605, 0.9461]\n",
      "2024-01-10 21:59:28.414353: Epoch time: 40.27 s\n",
      "2024-01-10 21:59:29.926022: \n",
      "2024-01-10 21:59:29.941024: Epoch 536\n",
      "2024-01-10 21:59:29.949036: Current learning rate: 0.00501\n",
      "2024-01-10 22:00:10.260393: train_loss -0.9456\n",
      "2024-01-10 22:00:10.270902: val_loss -0.8339\n",
      "2024-01-10 22:00:10.277902: Pseudo dice [0.8613, 0.9587, 0.9429]\n",
      "2024-01-10 22:00:10.306903: Epoch time: 40.34 s\n",
      "2024-01-10 22:00:11.770087: \n",
      "2024-01-10 22:00:11.775087: Epoch 537\n",
      "2024-01-10 22:00:11.779087: Current learning rate: 0.005\n",
      "2024-01-10 22:00:52.078581: train_loss -0.9459\n",
      "2024-01-10 22:00:52.088583: val_loss -0.8403\n",
      "2024-01-10 22:00:52.098093: Pseudo dice [0.8688, 0.9599, 0.945]\n",
      "2024-01-10 22:00:52.106091: Epoch time: 40.31 s\n",
      "2024-01-10 22:00:53.552323: \n",
      "2024-01-10 22:00:53.559464: Epoch 538\n",
      "2024-01-10 22:00:53.564465: Current learning rate: 0.00499\n",
      "2024-01-10 22:01:33.968011: train_loss -0.9458\n",
      "2024-01-10 22:01:33.976010: val_loss -0.8418\n",
      "2024-01-10 22:01:33.984013: Pseudo dice [0.8684, 0.9598, 0.9451]\n",
      "2024-01-10 22:01:34.014709: Epoch time: 40.42 s\n",
      "2024-01-10 22:01:35.468369: \n",
      "2024-01-10 22:01:35.473583: Epoch 539\n",
      "2024-01-10 22:01:35.480671: Current learning rate: 0.00498\n",
      "2024-01-10 22:02:16.156021: train_loss -0.9463\n",
      "2024-01-10 22:02:16.166022: val_loss -0.8352\n",
      "2024-01-10 22:02:16.174022: Pseudo dice [0.865, 0.9589, 0.9435]\n",
      "2024-01-10 22:02:16.183021: Epoch time: 40.69 s\n",
      "2024-01-10 22:02:17.617324: \n",
      "2024-01-10 22:02:17.623249: Epoch 540\n",
      "2024-01-10 22:02:17.628327: Current learning rate: 0.00497\n",
      "2024-01-10 22:02:57.970454: train_loss -0.945\n",
      "2024-01-10 22:02:57.978457: val_loss -0.8341\n",
      "2024-01-10 22:02:57.987695: Pseudo dice [0.8663, 0.9586, 0.9424]\n",
      "2024-01-10 22:02:57.995693: Epoch time: 40.35 s\n",
      "2024-01-10 22:02:59.449352: \n",
      "2024-01-10 22:02:59.455188: Epoch 541\n",
      "2024-01-10 22:02:59.464267: Current learning rate: 0.00496\n",
      "2024-01-10 22:03:39.713931: train_loss -0.9453\n",
      "2024-01-10 22:03:39.723922: val_loss -0.8427\n",
      "2024-01-10 22:03:39.731920: Pseudo dice [0.8703, 0.9593, 0.9439]\n",
      "2024-01-10 22:03:39.758922: Epoch time: 40.27 s\n",
      "2024-01-10 22:03:41.139288: \n",
      "2024-01-10 22:03:41.144365: Epoch 542\n",
      "2024-01-10 22:03:41.149448: Current learning rate: 0.00495\n",
      "2024-01-10 22:04:21.470078: train_loss -0.9457\n",
      "2024-01-10 22:04:21.499603: val_loss -0.838\n",
      "2024-01-10 22:04:21.507603: Pseudo dice [0.8614, 0.9595, 0.9449]\n",
      "2024-01-10 22:04:21.514602: Epoch time: 40.33 s\n",
      "2024-01-10 22:04:22.931664: \n",
      "2024-01-10 22:04:22.938932: Epoch 543\n",
      "2024-01-10 22:04:22.943012: Current learning rate: 0.00494\n",
      "2024-01-10 22:05:03.288834: train_loss -0.9468\n",
      "2024-01-10 22:05:03.298834: val_loss -0.8387\n",
      "2024-01-10 22:05:03.307837: Pseudo dice [0.8677, 0.9589, 0.943]\n",
      "2024-01-10 22:05:03.341856: Epoch time: 40.36 s\n",
      "2024-01-10 22:05:05.054551: \n",
      "2024-01-10 22:05:05.060489: Epoch 544\n",
      "2024-01-10 22:05:05.064489: Current learning rate: 0.00493\n",
      "2024-01-10 22:05:45.512098: train_loss -0.9468\n",
      "2024-01-10 22:05:45.537072: val_loss -0.842\n",
      "2024-01-10 22:05:45.544073: Pseudo dice [0.8674, 0.9606, 0.9455]\n",
      "2024-01-10 22:05:45.551081: Epoch time: 40.46 s\n",
      "2024-01-10 22:05:47.020809: \n",
      "2024-01-10 22:05:47.025893: Epoch 545\n",
      "2024-01-10 22:05:47.029978: Current learning rate: 0.00492\n",
      "2024-01-10 22:06:27.276546: train_loss -0.9463\n",
      "2024-01-10 22:06:27.286551: val_loss -0.8416\n",
      "2024-01-10 22:06:27.293552: Pseudo dice [0.8649, 0.9605, 0.9454]\n",
      "2024-01-10 22:06:27.299067: Epoch time: 40.26 s\n",
      "2024-01-10 22:06:28.721064: \n",
      "2024-01-10 22:06:28.733085: Epoch 546\n",
      "2024-01-10 22:06:28.738012: Current learning rate: 0.00491\n",
      "2024-01-10 22:07:09.089031: train_loss -0.9468\n",
      "2024-01-10 22:07:09.098030: val_loss -0.8377\n",
      "2024-01-10 22:07:09.105030: Pseudo dice [0.863, 0.9589, 0.9439]\n",
      "2024-01-10 22:07:09.114042: Epoch time: 40.37 s\n",
      "2024-01-10 22:07:10.572003: \n",
      "2024-01-10 22:07:10.578010: Epoch 547\n",
      "2024-01-10 22:07:10.582648: Current learning rate: 0.0049\n",
      "2024-01-10 22:07:50.825230: train_loss -0.9464\n",
      "2024-01-10 22:07:50.833750: val_loss -0.8392\n",
      "2024-01-10 22:07:50.842755: Pseudo dice [0.8719, 0.9588, 0.9432]\n",
      "2024-01-10 22:07:50.850269: Epoch time: 40.25 s\n",
      "2024-01-10 22:07:52.254095: \n",
      "2024-01-10 22:07:52.262316: Epoch 548\n",
      "2024-01-10 22:07:52.267387: Current learning rate: 0.00489\n",
      "2024-01-10 22:08:32.574008: train_loss -0.9451\n",
      "2024-01-10 22:08:32.583009: val_loss -0.8375\n",
      "2024-01-10 22:08:32.592008: Pseudo dice [0.861, 0.9593, 0.9439]\n",
      "2024-01-10 22:08:32.599010: Epoch time: 40.32 s\n",
      "2024-01-10 22:08:34.069346: \n",
      "2024-01-10 22:08:34.079934: Epoch 549\n",
      "2024-01-10 22:08:34.084980: Current learning rate: 0.00488\n",
      "2024-01-10 22:09:14.436226: train_loss -0.9457\n",
      "2024-01-10 22:09:14.443229: val_loss -0.8399\n",
      "2024-01-10 22:09:14.450229: Pseudo dice [0.8694, 0.9589, 0.9436]\n",
      "2024-01-10 22:09:14.457228: Epoch time: 40.37 s\n",
      "2024-01-10 22:09:16.109972: \n",
      "2024-01-10 22:09:16.121117: Epoch 550\n",
      "2024-01-10 22:09:16.129117: Current learning rate: 0.00487\n",
      "2024-01-10 22:09:56.533629: train_loss -0.9461\n",
      "2024-01-10 22:09:56.541630: val_loss -0.8377\n",
      "2024-01-10 22:09:56.548630: Pseudo dice [0.8627, 0.9592, 0.9433]\n",
      "2024-01-10 22:09:56.554631: Epoch time: 40.43 s\n",
      "2024-01-10 22:09:58.004055: \n",
      "2024-01-10 22:09:58.010081: Epoch 551\n",
      "2024-01-10 22:09:58.015039: Current learning rate: 0.00486\n",
      "2024-01-10 22:10:38.390865: train_loss -0.9461\n",
      "2024-01-10 22:10:38.398865: val_loss -0.8379\n",
      "2024-01-10 22:10:38.406917: Pseudo dice [0.8671, 0.9592, 0.9444]\n",
      "2024-01-10 22:10:38.414919: Epoch time: 40.39 s\n",
      "2024-01-10 22:10:39.919760: \n",
      "2024-01-10 22:10:39.925574: Epoch 552\n",
      "2024-01-10 22:10:39.930261: Current learning rate: 0.00485\n",
      "2024-01-10 22:11:20.282459: train_loss -0.9464\n",
      "2024-01-10 22:11:20.293458: val_loss -0.8372\n",
      "2024-01-10 22:11:20.303459: Pseudo dice [0.8669, 0.9599, 0.9448]\n",
      "2024-01-10 22:11:20.338465: Epoch time: 40.36 s\n",
      "2024-01-10 22:11:21.744471: \n",
      "2024-01-10 22:11:21.750462: Epoch 553\n",
      "2024-01-10 22:11:21.755147: Current learning rate: 0.00484\n",
      "2024-01-10 22:12:01.970939: train_loss -0.9469\n",
      "2024-01-10 22:12:01.980939: val_loss -0.8375\n",
      "2024-01-10 22:12:01.987937: Pseudo dice [0.8636, 0.959, 0.9439]\n",
      "2024-01-10 22:12:01.997936: Epoch time: 40.23 s\n",
      "2024-01-10 22:12:03.401477: \n",
      "2024-01-10 22:12:03.407712: Epoch 554\n",
      "2024-01-10 22:12:03.417798: Current learning rate: 0.00484\n",
      "2024-01-10 22:12:43.886656: train_loss -0.9478\n",
      "2024-01-10 22:12:43.896163: val_loss -0.8395\n",
      "2024-01-10 22:12:43.905164: Pseudo dice [0.8667, 0.9595, 0.9438]\n",
      "2024-01-10 22:12:43.911164: Epoch time: 40.49 s\n",
      "2024-01-10 22:12:45.368873: \n",
      "2024-01-10 22:12:45.373899: Epoch 555\n",
      "2024-01-10 22:12:45.380248: Current learning rate: 0.00483\n",
      "2024-01-10 22:13:25.600627: train_loss -0.9466\n",
      "2024-01-10 22:13:25.608627: val_loss -0.842\n",
      "2024-01-10 22:13:25.614657: Pseudo dice [0.8683, 0.96, 0.9447]\n",
      "2024-01-10 22:13:25.623634: Epoch time: 40.23 s\n",
      "2024-01-10 22:13:27.139400: \n",
      "2024-01-10 22:13:27.151686: Epoch 556\n",
      "2024-01-10 22:13:27.156752: Current learning rate: 0.00482\n",
      "2024-01-10 22:14:07.469080: train_loss -0.9465\n",
      "2024-01-10 22:14:07.478082: val_loss -0.8389\n",
      "2024-01-10 22:14:07.485081: Pseudo dice [0.8697, 0.9594, 0.9448]\n",
      "2024-01-10 22:14:07.490081: Epoch time: 40.33 s\n",
      "2024-01-10 22:14:08.855905: \n",
      "2024-01-10 22:14:08.861305: Epoch 557\n",
      "2024-01-10 22:14:08.866308: Current learning rate: 0.00481\n",
      "2024-01-10 22:14:49.149445: train_loss -0.9464\n",
      "2024-01-10 22:14:49.161956: val_loss -0.8361\n",
      "2024-01-10 22:14:49.170954: Pseudo dice [0.8688, 0.9588, 0.9429]\n",
      "2024-01-10 22:14:49.207958: Epoch time: 40.29 s\n",
      "2024-01-10 22:14:50.660391: \n",
      "2024-01-10 22:14:50.667343: Epoch 558\n",
      "2024-01-10 22:14:50.672425: Current learning rate: 0.0048\n",
      "2024-01-10 22:15:30.921871: train_loss -0.9468\n",
      "2024-01-10 22:15:30.931871: val_loss -0.8329\n",
      "2024-01-10 22:15:30.937870: Pseudo dice [0.8663, 0.9582, 0.9417]\n",
      "2024-01-10 22:15:30.945872: Epoch time: 40.26 s\n",
      "2024-01-10 22:15:32.398068: \n",
      "2024-01-10 22:15:32.404042: Epoch 559\n",
      "2024-01-10 22:15:32.411317: Current learning rate: 0.00479\n",
      "2024-01-10 22:16:12.894802: train_loss -0.9474\n",
      "2024-01-10 22:16:12.903803: val_loss -0.843\n",
      "2024-01-10 22:16:12.909804: Pseudo dice [0.8689, 0.9592, 0.9434]\n",
      "2024-01-10 22:16:12.938802: Epoch time: 40.5 s\n",
      "2024-01-10 22:16:14.319252: \n",
      "2024-01-10 22:16:14.330201: Epoch 560\n",
      "2024-01-10 22:16:14.335256: Current learning rate: 0.00478\n",
      "2024-01-10 22:16:54.444446: train_loss -0.947\n",
      "2024-01-10 22:16:54.453443: val_loss -0.834\n",
      "2024-01-10 22:16:54.466441: Pseudo dice [0.8534, 0.9596, 0.9442]\n",
      "2024-01-10 22:16:54.471442: Epoch time: 40.13 s\n",
      "2024-01-10 22:16:55.952888: \n",
      "2024-01-10 22:16:55.957877: Epoch 561\n",
      "2024-01-10 22:16:55.962878: Current learning rate: 0.00477\n",
      "2024-01-10 22:17:36.247149: train_loss -0.9468\n",
      "2024-01-10 22:17:36.253150: val_loss -0.8361\n",
      "2024-01-10 22:17:36.261152: Pseudo dice [0.8579, 0.96, 0.9445]\n",
      "2024-01-10 22:17:36.269150: Epoch time: 40.3 s\n",
      "2024-01-10 22:17:37.915751: \n",
      "2024-01-10 22:17:37.921751: Epoch 562\n",
      "2024-01-10 22:17:37.928494: Current learning rate: 0.00476\n",
      "2024-01-10 22:18:18.239779: train_loss -0.9466\n",
      "2024-01-10 22:18:18.249781: val_loss -0.8424\n",
      "2024-01-10 22:18:18.257298: Pseudo dice [0.8675, 0.96, 0.9449]\n",
      "2024-01-10 22:18:18.263300: Epoch time: 40.33 s\n",
      "2024-01-10 22:18:19.671207: \n",
      "2024-01-10 22:18:19.676863: Epoch 563\n",
      "2024-01-10 22:18:19.686949: Current learning rate: 0.00475\n",
      "2024-01-10 22:18:59.960395: train_loss -0.9472\n",
      "2024-01-10 22:18:59.969395: val_loss -0.8399\n",
      "2024-01-10 22:18:59.976924: Pseudo dice [0.8735, 0.9596, 0.9439]\n",
      "2024-01-10 22:18:59.982923: Epoch time: 40.29 s\n",
      "2024-01-10 22:19:01.514039: \n",
      "2024-01-10 22:19:01.525622: Epoch 564\n",
      "2024-01-10 22:19:01.530168: Current learning rate: 0.00474\n",
      "2024-01-10 22:19:41.895413: train_loss -0.9477\n",
      "2024-01-10 22:19:41.904919: val_loss -0.8391\n",
      "2024-01-10 22:19:41.912926: Pseudo dice [0.8638, 0.9598, 0.9447]\n",
      "2024-01-10 22:19:41.919926: Epoch time: 40.38 s\n",
      "2024-01-10 22:19:43.459753: \n",
      "2024-01-10 22:19:43.465762: Epoch 565\n",
      "2024-01-10 22:19:43.469836: Current learning rate: 0.00473\n",
      "2024-01-10 22:20:23.774699: train_loss -0.9473\n",
      "2024-01-10 22:20:23.783698: val_loss -0.8368\n",
      "2024-01-10 22:20:23.790698: Pseudo dice [0.8622, 0.9601, 0.9457]\n",
      "2024-01-10 22:20:23.818709: Epoch time: 40.32 s\n",
      "2024-01-10 22:20:25.171853: \n",
      "2024-01-10 22:20:25.178163: Epoch 566\n",
      "2024-01-10 22:20:25.189238: Current learning rate: 0.00472\n",
      "2024-01-10 22:21:05.749139: train_loss -0.9461\n",
      "2024-01-10 22:21:05.756143: val_loss -0.8395\n",
      "2024-01-10 22:21:05.762749: Pseudo dice [0.8636, 0.9594, 0.9436]\n",
      "2024-01-10 22:21:05.768756: Epoch time: 40.58 s\n",
      "2024-01-10 22:21:07.197506: \n",
      "2024-01-10 22:21:07.203623: Epoch 567\n",
      "2024-01-10 22:21:07.208648: Current learning rate: 0.00471\n",
      "2024-01-10 22:21:47.444006: train_loss -0.9474\n",
      "2024-01-10 22:21:47.454008: val_loss -0.8395\n",
      "2024-01-10 22:21:47.460019: Pseudo dice [0.8707, 0.9599, 0.9441]\n",
      "2024-01-10 22:21:47.471024: Epoch time: 40.25 s\n",
      "2024-01-10 22:21:48.867136: \n",
      "2024-01-10 22:21:48.879251: Epoch 568\n",
      "2024-01-10 22:21:48.883193: Current learning rate: 0.0047\n",
      "2024-01-10 22:22:29.192854: train_loss -0.9471\n",
      "2024-01-10 22:22:29.226365: val_loss -0.8405\n",
      "2024-01-10 22:22:29.236366: Pseudo dice [0.8709, 0.9597, 0.9438]\n",
      "2024-01-10 22:22:29.243368: Epoch time: 40.33 s\n",
      "2024-01-10 22:22:30.660255: \n",
      "2024-01-10 22:22:30.671736: Epoch 569\n",
      "2024-01-10 22:22:30.676649: Current learning rate: 0.00469\n",
      "2024-01-10 22:23:11.214362: train_loss -0.947\n",
      "2024-01-10 22:23:11.222367: val_loss -0.8378\n",
      "2024-01-10 22:23:11.229368: Pseudo dice [0.8669, 0.9588, 0.9428]\n",
      "2024-01-10 22:23:11.255890: Epoch time: 40.56 s\n",
      "2024-01-10 22:23:12.620673: \n",
      "2024-01-10 22:23:12.625526: Epoch 570\n",
      "2024-01-10 22:23:12.630539: Current learning rate: 0.00468\n",
      "2024-01-10 22:23:52.659319: train_loss -0.9473\n",
      "2024-01-10 22:23:52.667319: val_loss -0.8376\n",
      "2024-01-10 22:23:52.675320: Pseudo dice [0.8641, 0.9594, 0.9438]\n",
      "2024-01-10 22:23:52.681319: Epoch time: 40.04 s\n",
      "2024-01-10 22:23:54.154086: \n",
      "2024-01-10 22:23:54.160357: Epoch 571\n",
      "2024-01-10 22:23:54.169485: Current learning rate: 0.00467\n",
      "2024-01-10 22:24:34.343224: train_loss -0.9477\n",
      "2024-01-10 22:24:34.352225: val_loss -0.8361\n",
      "2024-01-10 22:24:34.360229: Pseudo dice [0.8637, 0.9594, 0.9439]\n",
      "2024-01-10 22:24:34.414270: Epoch time: 40.19 s\n",
      "2024-01-10 22:24:35.992716: \n",
      "2024-01-10 22:24:35.998461: Epoch 572\n",
      "2024-01-10 22:24:36.003183: Current learning rate: 0.00466\n",
      "2024-01-10 22:25:16.236867: train_loss -0.9474\n",
      "2024-01-10 22:25:16.243867: val_loss -0.8376\n",
      "2024-01-10 22:25:16.250866: Pseudo dice [0.8646, 0.96, 0.9444]\n",
      "2024-01-10 22:25:16.274872: Epoch time: 40.25 s\n",
      "2024-01-10 22:25:17.737386: \n",
      "2024-01-10 22:25:17.743667: Epoch 573\n",
      "2024-01-10 22:25:17.752403: Current learning rate: 0.00465\n",
      "2024-01-10 22:25:58.077360: train_loss -0.9478\n",
      "2024-01-10 22:25:58.090368: val_loss -0.8338\n",
      "2024-01-10 22:25:58.097371: Pseudo dice [0.8597, 0.9589, 0.9437]\n",
      "2024-01-10 22:25:58.106883: Epoch time: 40.34 s\n",
      "2024-01-10 22:25:59.549588: \n",
      "2024-01-10 22:25:59.556609: Epoch 574\n",
      "2024-01-10 22:25:59.561519: Current learning rate: 0.00464\n",
      "2024-01-10 22:26:40.259408: train_loss -0.9475\n",
      "2024-01-10 22:26:40.268407: val_loss -0.8417\n",
      "2024-01-10 22:26:40.274408: Pseudo dice [0.8696, 0.9597, 0.9445]\n",
      "2024-01-10 22:26:40.279409: Epoch time: 40.71 s\n",
      "2024-01-10 22:26:41.691409: \n",
      "2024-01-10 22:26:41.703410: Epoch 575\n",
      "2024-01-10 22:26:41.715994: Current learning rate: 0.00463\n",
      "2024-01-10 22:27:21.995878: train_loss -0.9481\n",
      "2024-01-10 22:27:22.004879: val_loss -0.842\n",
      "2024-01-10 22:27:22.013876: Pseudo dice [0.8687, 0.96, 0.9447]\n",
      "2024-01-10 22:27:22.019879: Epoch time: 40.31 s\n",
      "2024-01-10 22:27:23.485817: \n",
      "2024-01-10 22:27:23.492189: Epoch 576\n",
      "2024-01-10 22:27:23.497195: Current learning rate: 0.00462\n",
      "2024-01-10 22:28:03.697580: train_loss -0.9477\n",
      "2024-01-10 22:28:03.705581: val_loss -0.8408\n",
      "2024-01-10 22:28:03.713581: Pseudo dice [0.8703, 0.9596, 0.9452]\n",
      "2024-01-10 22:28:03.742580: Epoch time: 40.21 s\n",
      "2024-01-10 22:28:05.206545: \n",
      "2024-01-10 22:28:05.213231: Epoch 577\n",
      "2024-01-10 22:28:05.217281: Current learning rate: 0.00461\n",
      "2024-01-10 22:28:45.414780: train_loss -0.9473\n",
      "2024-01-10 22:28:45.424783: val_loss -0.8409\n",
      "2024-01-10 22:28:45.433781: Pseudo dice [0.8655, 0.9598, 0.9448]\n",
      "2024-01-10 22:28:45.441782: Epoch time: 40.21 s\n",
      "2024-01-10 22:28:46.820655: \n",
      "2024-01-10 22:28:46.827521: Epoch 578\n",
      "2024-01-10 22:28:46.831619: Current learning rate: 0.0046\n",
      "2024-01-10 22:29:27.186028: train_loss -0.9474\n",
      "2024-01-10 22:29:27.195026: val_loss -0.8419\n",
      "2024-01-10 22:29:27.216536: Pseudo dice [0.8738, 0.9589, 0.9435]\n",
      "2024-01-10 22:29:27.222544: Epoch time: 40.37 s\n",
      "2024-01-10 22:29:28.854649: \n",
      "2024-01-10 22:29:28.865201: Epoch 579\n",
      "2024-01-10 22:29:28.870181: Current learning rate: 0.00459\n",
      "2024-01-10 22:30:09.148334: train_loss -0.9477\n",
      "2024-01-10 22:30:09.157336: val_loss -0.8392\n",
      "2024-01-10 22:30:09.165333: Pseudo dice [0.8672, 0.9595, 0.9457]\n",
      "2024-01-10 22:30:09.172334: Epoch time: 40.3 s\n",
      "2024-01-10 22:30:10.651003: \n",
      "2024-01-10 22:30:10.661083: Epoch 580\n",
      "2024-01-10 22:30:10.668137: Current learning rate: 0.00458\n",
      "2024-01-10 22:30:51.013455: train_loss -0.9476\n",
      "2024-01-10 22:30:51.021454: val_loss -0.8437\n",
      "2024-01-10 22:30:51.030454: Pseudo dice [0.8723, 0.9599, 0.9451]\n",
      "2024-01-10 22:30:51.037454: Epoch time: 40.36 s\n",
      "2024-01-10 22:30:52.557346: \n",
      "2024-01-10 22:30:52.563021: Epoch 581\n",
      "2024-01-10 22:30:52.568072: Current learning rate: 0.00457\n",
      "2024-01-10 22:31:32.850093: train_loss -0.9475\n",
      "2024-01-10 22:31:32.875612: val_loss -0.842\n",
      "2024-01-10 22:31:32.883610: Pseudo dice [0.8738, 0.9594, 0.9445]\n",
      "2024-01-10 22:31:32.890611: Epoch time: 40.29 s\n",
      "2024-01-10 22:31:34.322114: \n",
      "2024-01-10 22:31:34.330188: Epoch 582\n",
      "2024-01-10 22:31:34.335219: Current learning rate: 0.00456\n",
      "2024-01-10 22:32:14.580817: train_loss -0.9477\n",
      "2024-01-10 22:32:14.590342: val_loss -0.8417\n",
      "2024-01-10 22:32:14.600339: Pseudo dice [0.8695, 0.9602, 0.9455]\n",
      "2024-01-10 22:32:14.631343: Epoch time: 40.26 s\n",
      "2024-01-10 22:32:16.141208: \n",
      "2024-01-10 22:32:16.150869: Epoch 583\n",
      "2024-01-10 22:32:16.160887: Current learning rate: 0.00455\n",
      "2024-01-10 22:32:56.364735: train_loss -0.9476\n",
      "2024-01-10 22:32:56.390738: val_loss -0.8384\n",
      "2024-01-10 22:32:56.399744: Pseudo dice [0.8744, 0.9594, 0.944]\n",
      "2024-01-10 22:32:56.408745: Epoch time: 40.23 s\n",
      "2024-01-10 22:32:57.892758: \n",
      "2024-01-10 22:32:57.898670: Epoch 584\n",
      "2024-01-10 22:32:57.902688: Current learning rate: 0.00454\n",
      "2024-01-10 22:33:38.370088: train_loss -0.9481\n",
      "2024-01-10 22:33:38.380080: val_loss -0.8407\n",
      "2024-01-10 22:33:38.386080: Pseudo dice [0.8681, 0.9604, 0.946]\n",
      "2024-01-10 22:33:38.392080: Epoch time: 40.48 s\n",
      "2024-01-10 22:33:39.875463: \n",
      "2024-01-10 22:33:39.881326: Epoch 585\n",
      "2024-01-10 22:33:39.885396: Current learning rate: 0.00453\n",
      "2024-01-10 22:34:20.132202: train_loss -0.947\n",
      "2024-01-10 22:34:20.141212: val_loss -0.84\n",
      "2024-01-10 22:34:20.149211: Pseudo dice [0.8726, 0.9596, 0.9444]\n",
      "2024-01-10 22:34:20.156724: Epoch time: 40.26 s\n",
      "2024-01-10 22:34:21.607167: \n",
      "2024-01-10 22:34:21.614244: Epoch 586\n",
      "2024-01-10 22:34:21.618230: Current learning rate: 0.00452\n",
      "2024-01-10 22:35:02.041538: train_loss -0.9478\n",
      "2024-01-10 22:35:02.052518: val_loss -0.8412\n",
      "2024-01-10 22:35:02.058525: Pseudo dice [0.8729, 0.9599, 0.9447]\n",
      "2024-01-10 22:35:02.063523: Epoch time: 40.44 s\n",
      "2024-01-10 22:35:02.070031: Yayy! New best EMA pseudo Dice: 0.9246\n",
      "2024-01-10 22:35:03.618063: \n",
      "2024-01-10 22:35:03.627214: Epoch 587\n",
      "2024-01-10 22:35:03.635238: Current learning rate: 0.00451\n",
      "2024-01-10 22:35:43.995641: train_loss -0.9477\n",
      "2024-01-10 22:35:44.005647: val_loss -0.846\n",
      "2024-01-10 22:35:44.012656: Pseudo dice [0.8729, 0.9602, 0.9451]\n",
      "2024-01-10 22:35:44.021698: Epoch time: 40.38 s\n",
      "2024-01-10 22:35:44.050726: Yayy! New best EMA pseudo Dice: 0.9248\n",
      "2024-01-10 22:35:45.780170: \n",
      "2024-01-10 22:35:45.785888: Epoch 588\n",
      "2024-01-10 22:35:45.790950: Current learning rate: 0.0045\n",
      "2024-01-10 22:36:26.312088: train_loss -0.9476\n",
      "2024-01-10 22:36:26.319092: val_loss -0.8442\n",
      "2024-01-10 22:36:26.325093: Pseudo dice [0.8743, 0.961, 0.9463]\n",
      "2024-01-10 22:36:26.332097: Epoch time: 40.53 s\n",
      "2024-01-10 22:36:26.339098: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2024-01-10 22:36:28.096607: \n",
      "2024-01-10 22:36:28.105588: Epoch 589\n",
      "2024-01-10 22:36:28.109607: Current learning rate: 0.00449\n",
      "2024-01-10 22:37:08.529558: train_loss -0.9479\n",
      "2024-01-10 22:37:08.555068: val_loss -0.8398\n",
      "2024-01-10 22:37:08.562580: Pseudo dice [0.8708, 0.959, 0.9436]\n",
      "2024-01-10 22:37:08.567580: Epoch time: 40.43 s\n",
      "2024-01-10 22:37:10.087971: \n",
      "2024-01-10 22:37:10.094086: Epoch 590\n",
      "2024-01-10 22:37:10.100076: Current learning rate: 0.00448\n",
      "2024-01-10 22:37:50.386507: train_loss -0.9479\n",
      "2024-01-10 22:37:50.395026: val_loss -0.8419\n",
      "2024-01-10 22:37:50.403026: Pseudo dice [0.8666, 0.9605, 0.9458]\n",
      "2024-01-10 22:37:50.410024: Epoch time: 40.3 s\n",
      "2024-01-10 22:37:51.821310: \n",
      "2024-01-10 22:37:51.827309: Epoch 591\n",
      "2024-01-10 22:37:51.832542: Current learning rate: 0.00447\n",
      "2024-01-10 22:38:32.119872: train_loss -0.948\n",
      "2024-01-10 22:38:32.128871: val_loss -0.8399\n",
      "2024-01-10 22:38:32.135870: Pseudo dice [0.8702, 0.9591, 0.9437]\n",
      "2024-01-10 22:38:32.141870: Epoch time: 40.3 s\n",
      "2024-01-10 22:38:33.528010: \n",
      "2024-01-10 22:38:33.535756: Epoch 592\n",
      "2024-01-10 22:38:33.540812: Current learning rate: 0.00446\n",
      "2024-01-10 22:39:13.984377: train_loss -0.9484\n",
      "2024-01-10 22:39:13.992377: val_loss -0.8454\n",
      "2024-01-10 22:39:13.998378: Pseudo dice [0.8732, 0.9602, 0.9455]\n",
      "2024-01-10 22:39:14.024898: Epoch time: 40.46 s\n",
      "2024-01-10 22:39:15.393902: \n",
      "2024-01-10 22:39:15.406360: Epoch 593\n",
      "2024-01-10 22:39:15.411483: Current learning rate: 0.00445\n",
      "2024-01-10 22:39:55.584405: train_loss -0.9476\n",
      "2024-01-10 22:39:55.592915: val_loss -0.8391\n",
      "2024-01-10 22:39:55.599916: Pseudo dice [0.8702, 0.96, 0.945]\n",
      "2024-01-10 22:39:55.607915: Epoch time: 40.19 s\n",
      "2024-01-10 22:39:57.201886: \n",
      "2024-01-10 22:39:57.208886: Epoch 594\n",
      "2024-01-10 22:39:57.212886: Current learning rate: 0.00444\n",
      "2024-01-10 22:40:37.444475: train_loss -0.9477\n",
      "2024-01-10 22:40:37.479986: val_loss -0.8415\n",
      "2024-01-10 22:40:37.486986: Pseudo dice [0.869, 0.9599, 0.9447]\n",
      "2024-01-10 22:40:37.492986: Epoch time: 40.24 s\n",
      "2024-01-10 22:40:38.939290: \n",
      "2024-01-10 22:40:38.952491: Epoch 595\n",
      "2024-01-10 22:40:38.957558: Current learning rate: 0.00443\n",
      "2024-01-10 22:41:19.212785: train_loss -0.9489\n",
      "2024-01-10 22:41:19.224786: val_loss -0.8402\n",
      "2024-01-10 22:41:19.230791: Pseudo dice [0.8663, 0.9604, 0.946]\n",
      "2024-01-10 22:41:19.262786: Epoch time: 40.27 s\n",
      "2024-01-10 22:41:20.680223: \n",
      "2024-01-10 22:41:20.685612: Epoch 596\n",
      "2024-01-10 22:41:20.690439: Current learning rate: 0.00442\n",
      "2024-01-10 22:42:00.905257: train_loss -0.9486\n",
      "2024-01-10 22:42:00.915257: val_loss -0.8399\n",
      "2024-01-10 22:42:00.922260: Pseudo dice [0.867, 0.9596, 0.9441]\n",
      "2024-01-10 22:42:00.952257: Epoch time: 40.23 s\n",
      "2024-01-10 22:42:02.385267: \n",
      "2024-01-10 22:42:02.396561: Epoch 597\n",
      "2024-01-10 22:42:02.400500: Current learning rate: 0.00441\n",
      "2024-01-10 22:42:42.578130: train_loss -0.948\n",
      "2024-01-10 22:42:42.609131: val_loss -0.8406\n",
      "2024-01-10 22:42:42.616132: Pseudo dice [0.871, 0.9601, 0.9446]\n",
      "2024-01-10 22:42:42.623643: Epoch time: 40.19 s\n",
      "2024-01-10 22:42:44.151614: \n",
      "2024-01-10 22:42:44.157738: Epoch 598\n",
      "2024-01-10 22:42:44.165752: Current learning rate: 0.0044\n",
      "2024-01-10 22:43:24.481003: train_loss -0.9485\n",
      "2024-01-10 22:43:24.490003: val_loss -0.8341\n",
      "2024-01-10 22:43:24.496006: Pseudo dice [0.8628, 0.9583, 0.9429]\n",
      "2024-01-10 22:43:24.525912: Epoch time: 40.33 s\n",
      "2024-01-10 22:43:26.141921: \n",
      "2024-01-10 22:43:26.147273: Epoch 599\n",
      "2024-01-10 22:43:26.152279: Current learning rate: 0.00439\n",
      "2024-01-10 22:44:06.316594: train_loss -0.9481\n",
      "2024-01-10 22:44:06.324594: val_loss -0.8419\n",
      "2024-01-10 22:44:06.332594: Pseudo dice [0.873, 0.9596, 0.9439]\n",
      "2024-01-10 22:44:06.339594: Epoch time: 40.18 s\n",
      "2024-01-10 22:44:08.082870: \n",
      "2024-01-10 22:44:08.089986: Epoch 600\n",
      "2024-01-10 22:44:08.094957: Current learning rate: 0.00438\n",
      "2024-01-10 22:44:48.419999: train_loss -0.948\n",
      "2024-01-10 22:44:48.429000: val_loss -0.8416\n",
      "2024-01-10 22:44:48.436509: Pseudo dice [0.8678, 0.96, 0.9448]\n",
      "2024-01-10 22:44:48.444512: Epoch time: 40.34 s\n",
      "2024-01-10 22:44:49.890101: \n",
      "2024-01-10 22:44:49.895630: Epoch 601\n",
      "2024-01-10 22:44:49.900719: Current learning rate: 0.00437\n",
      "2024-01-10 22:45:30.071126: train_loss -0.9477\n",
      "2024-01-10 22:45:30.081125: val_loss -0.8402\n",
      "2024-01-10 22:45:30.105635: Pseudo dice [0.8664, 0.9602, 0.9454]\n",
      "2024-01-10 22:45:30.113643: Epoch time: 40.18 s\n",
      "2024-01-10 22:45:31.616686: \n",
      "2024-01-10 22:45:31.623330: Epoch 602\n",
      "2024-01-10 22:45:31.633337: Current learning rate: 0.00436\n",
      "2024-01-10 22:46:12.021947: train_loss -0.9475\n",
      "2024-01-10 22:46:12.033458: val_loss -0.8384\n",
      "2024-01-10 22:46:12.041461: Pseudo dice [0.8693, 0.9592, 0.9438]\n",
      "2024-01-10 22:46:12.049460: Epoch time: 40.41 s\n",
      "2024-01-10 22:46:13.621222: \n",
      "2024-01-10 22:46:13.629302: Epoch 603\n",
      "2024-01-10 22:46:13.633948: Current learning rate: 0.00435\n",
      "2024-01-10 22:46:54.015816: train_loss -0.9483\n",
      "2024-01-10 22:46:54.023815: val_loss -0.8397\n",
      "2024-01-10 22:46:54.030816: Pseudo dice [0.8697, 0.9595, 0.9443]\n",
      "2024-01-10 22:46:54.036817: Epoch time: 40.4 s\n",
      "2024-01-10 22:46:55.505160: \n",
      "2024-01-10 22:46:55.511100: Epoch 604\n",
      "2024-01-10 22:46:55.516284: Current learning rate: 0.00434\n",
      "2024-01-10 22:47:35.663693: train_loss -0.9485\n",
      "2024-01-10 22:47:35.672203: val_loss -0.8371\n",
      "2024-01-10 22:47:35.682204: Pseudo dice [0.869, 0.9589, 0.9441]\n",
      "2024-01-10 22:47:35.689204: Epoch time: 40.16 s\n",
      "2024-01-10 22:47:37.107780: \n",
      "2024-01-10 22:47:37.115699: Epoch 605\n",
      "2024-01-10 22:47:37.121371: Current learning rate: 0.00433\n",
      "2024-01-10 22:48:17.378715: train_loss -0.948\n",
      "2024-01-10 22:48:17.421226: val_loss -0.8446\n",
      "2024-01-10 22:48:17.430228: Pseudo dice [0.8694, 0.9607, 0.9457]\n",
      "2024-01-10 22:48:17.438226: Epoch time: 40.27 s\n",
      "2024-01-10 22:48:18.931613: \n",
      "2024-01-10 22:48:18.937171: Epoch 606\n",
      "2024-01-10 22:48:18.941166: Current learning rate: 0.00432\n",
      "2024-01-10 22:48:59.322076: train_loss -0.9485\n",
      "2024-01-10 22:48:59.329076: val_loss -0.8353\n",
      "2024-01-10 22:48:59.338078: Pseudo dice [0.8702, 0.9586, 0.943]\n",
      "2024-01-10 22:48:59.346077: Epoch time: 40.39 s\n",
      "2024-01-10 22:49:00.865520: \n",
      "2024-01-10 22:49:00.871599: Epoch 607\n",
      "2024-01-10 22:49:00.876627: Current learning rate: 0.00431\n",
      "2024-01-10 22:49:41.093688: train_loss -0.9476\n",
      "2024-01-10 22:49:41.100690: val_loss -0.8345\n",
      "2024-01-10 22:49:41.107689: Pseudo dice [0.8637, 0.9589, 0.9434]\n",
      "2024-01-10 22:49:41.133214: Epoch time: 40.23 s\n",
      "2024-01-10 22:49:42.645955: \n",
      "2024-01-10 22:49:42.651955: Epoch 608\n",
      "2024-01-10 22:49:42.656492: Current learning rate: 0.0043\n",
      "2024-01-10 22:50:23.089203: train_loss -0.948\n",
      "2024-01-10 22:50:23.098200: val_loss -0.8392\n",
      "2024-01-10 22:50:23.107199: Pseudo dice [0.8664, 0.9591, 0.9439]\n",
      "2024-01-10 22:50:23.114203: Epoch time: 40.44 s\n",
      "2024-01-10 22:50:24.539300: \n",
      "2024-01-10 22:50:24.545300: Epoch 609\n",
      "2024-01-10 22:50:24.549300: Current learning rate: 0.00429\n",
      "2024-01-10 22:51:04.770869: train_loss -0.9471\n",
      "2024-01-10 22:51:04.781384: val_loss -0.8381\n",
      "2024-01-10 22:51:04.788382: Pseudo dice [0.8632, 0.9602, 0.9462]\n",
      "2024-01-10 22:51:04.809381: Epoch time: 40.23 s\n",
      "2024-01-10 22:51:06.343945: \n",
      "2024-01-10 22:51:06.349800: Epoch 610\n",
      "2024-01-10 22:51:06.353871: Current learning rate: 0.00429\n",
      "2024-01-10 22:51:46.523299: train_loss -0.9478\n",
      "2024-01-10 22:51:46.557309: val_loss -0.839\n",
      "2024-01-10 22:51:46.564302: Pseudo dice [0.87, 0.9593, 0.9441]\n",
      "2024-01-10 22:51:46.580565: Epoch time: 40.18 s\n",
      "2024-01-10 22:51:48.088707: \n",
      "2024-01-10 22:51:48.093213: Epoch 611\n",
      "2024-01-10 22:51:48.100295: Current learning rate: 0.00428\n",
      "2024-01-10 22:52:28.393401: train_loss -0.9484\n",
      "2024-01-10 22:52:28.401399: val_loss -0.8385\n",
      "2024-01-10 22:52:28.409410: Pseudo dice [0.8703, 0.96, 0.9459]\n",
      "2024-01-10 22:52:28.429914: Epoch time: 40.31 s\n",
      "2024-01-10 22:52:29.849814: \n",
      "2024-01-10 22:52:29.860436: Epoch 612\n",
      "2024-01-10 22:52:29.865306: Current learning rate: 0.00427\n",
      "2024-01-10 22:53:10.084737: train_loss -0.9478\n",
      "2024-01-10 22:53:10.092739: val_loss -0.831\n",
      "2024-01-10 22:53:10.123268: Pseudo dice [0.8651, 0.9589, 0.9433]\n",
      "2024-01-10 22:53:10.130268: Epoch time: 40.23 s\n",
      "2024-01-10 22:53:11.588823: \n",
      "2024-01-10 22:53:11.602868: Epoch 613\n",
      "2024-01-10 22:53:11.607941: Current learning rate: 0.00426\n",
      "2024-01-10 22:53:52.033843: train_loss -0.9489\n",
      "2024-01-10 22:53:52.041846: val_loss -0.8392\n",
      "2024-01-10 22:53:52.068847: Pseudo dice [0.8687, 0.9597, 0.945]\n",
      "2024-01-10 22:53:52.078358: Epoch time: 40.45 s\n",
      "2024-01-10 22:53:53.512570: \n",
      "2024-01-10 22:53:53.525578: Epoch 614\n",
      "2024-01-10 22:53:53.532605: Current learning rate: 0.00425\n",
      "2024-01-10 22:54:33.811608: train_loss -0.9485\n",
      "2024-01-10 22:54:33.841608: val_loss -0.8357\n",
      "2024-01-10 22:54:33.850610: Pseudo dice [0.8696, 0.9591, 0.9442]\n",
      "2024-01-10 22:54:33.858610: Epoch time: 40.3 s\n",
      "2024-01-10 22:54:35.270036: \n",
      "2024-01-10 22:54:35.275576: Epoch 615\n",
      "2024-01-10 22:54:35.281645: Current learning rate: 0.00424\n",
      "2024-01-10 22:55:15.447132: train_loss -0.9482\n",
      "2024-01-10 22:55:15.478133: val_loss -0.8403\n",
      "2024-01-10 22:55:15.488131: Pseudo dice [0.8697, 0.9593, 0.9451]\n",
      "2024-01-10 22:55:15.496131: Epoch time: 40.18 s\n",
      "2024-01-10 22:55:16.948706: \n",
      "2024-01-10 22:55:16.954706: Epoch 616\n",
      "2024-01-10 22:55:16.959882: Current learning rate: 0.00423\n",
      "2024-01-10 22:55:57.195849: train_loss -0.9484\n",
      "2024-01-10 22:55:57.206850: val_loss -0.8386\n",
      "2024-01-10 22:55:57.215849: Pseudo dice [0.8679, 0.9587, 0.9435]\n",
      "2024-01-10 22:55:57.224852: Epoch time: 40.25 s\n",
      "2024-01-10 22:55:58.663705: \n",
      "2024-01-10 22:55:58.676678: Epoch 617\n",
      "2024-01-10 22:55:58.686677: Current learning rate: 0.00422\n",
      "2024-01-10 22:56:39.008461: train_loss -0.9487\n",
      "2024-01-10 22:56:39.018461: val_loss -0.8394\n",
      "2024-01-10 22:56:39.025839: Pseudo dice [0.8726, 0.9593, 0.9437]\n",
      "2024-01-10 22:56:39.034841: Epoch time: 40.35 s\n",
      "2024-01-10 22:56:40.556906: \n",
      "2024-01-10 22:56:40.562483: Epoch 618\n",
      "2024-01-10 22:56:40.572001: Current learning rate: 0.00421\n",
      "2024-01-10 22:57:21.048795: train_loss -0.9481\n",
      "2024-01-10 22:57:21.086310: val_loss -0.8372\n",
      "2024-01-10 22:57:21.093311: Pseudo dice [0.8668, 0.9587, 0.9426]\n",
      "2024-01-10 22:57:21.102311: Epoch time: 40.49 s\n",
      "2024-01-10 22:57:22.524123: \n",
      "2024-01-10 22:57:22.530107: Epoch 619\n",
      "2024-01-10 22:57:22.535173: Current learning rate: 0.0042\n",
      "2024-01-10 22:58:02.733895: train_loss -0.9479\n",
      "2024-01-10 22:58:02.745894: val_loss -0.8366\n",
      "2024-01-10 22:58:02.754893: Pseudo dice [0.867, 0.959, 0.9444]\n",
      "2024-01-10 22:58:02.786899: Epoch time: 40.21 s\n",
      "2024-01-10 22:58:04.279940: \n",
      "2024-01-10 22:58:04.285278: Epoch 620\n",
      "2024-01-10 22:58:04.293344: Current learning rate: 0.00419\n",
      "2024-01-10 22:58:44.577117: train_loss -0.9474\n",
      "2024-01-10 22:58:44.587124: val_loss -0.8398\n",
      "2024-01-10 22:58:44.610126: Pseudo dice [0.87, 0.9595, 0.9439]\n",
      "2024-01-10 22:58:44.618637: Epoch time: 40.3 s\n",
      "2024-01-10 22:58:46.122079: \n",
      "2024-01-10 22:58:46.134732: Epoch 621\n",
      "2024-01-10 22:58:46.140712: Current learning rate: 0.00418\n",
      "2024-01-10 22:59:26.564759: train_loss -0.9467\n",
      "2024-01-10 22:59:26.603136: val_loss -0.8338\n",
      "2024-01-10 22:59:26.611137: Pseudo dice [0.8639, 0.9589, 0.9439]\n",
      "2024-01-10 22:59:26.618136: Epoch time: 40.44 s\n",
      "2024-01-10 22:59:28.099732: \n",
      "2024-01-10 22:59:28.106706: Epoch 622\n",
      "2024-01-10 22:59:28.111683: Current learning rate: 0.00417\n",
      "2024-01-10 23:00:08.414457: train_loss -0.9469\n",
      "2024-01-10 23:00:08.423457: val_loss -0.8371\n",
      "2024-01-10 23:00:08.431458: Pseudo dice [0.8661, 0.9596, 0.9447]\n",
      "2024-01-10 23:00:08.459573: Epoch time: 40.32 s\n",
      "2024-01-10 23:00:09.963707: \n",
      "2024-01-10 23:00:09.969967: Epoch 623\n",
      "2024-01-10 23:00:09.974892: Current learning rate: 0.00416\n",
      "2024-01-10 23:00:50.403434: train_loss -0.9473\n",
      "2024-01-10 23:00:50.412440: val_loss -0.8367\n",
      "2024-01-10 23:00:50.418435: Pseudo dice [0.8648, 0.9593, 0.9439]\n",
      "2024-01-10 23:00:50.425947: Epoch time: 40.44 s\n",
      "2024-01-10 23:00:51.904728: \n",
      "2024-01-10 23:00:51.914729: Epoch 624\n",
      "2024-01-10 23:00:51.923780: Current learning rate: 0.00415\n",
      "2024-01-10 23:01:32.358558: train_loss -0.9483\n",
      "2024-01-10 23:01:32.368560: val_loss -0.8435\n",
      "2024-01-10 23:01:32.376559: Pseudo dice [0.8731, 0.9599, 0.9454]\n",
      "2024-01-10 23:01:32.408580: Epoch time: 40.45 s\n",
      "2024-01-10 23:01:33.864537: \n",
      "2024-01-10 23:01:33.877172: Epoch 625\n",
      "2024-01-10 23:01:33.882232: Current learning rate: 0.00414\n",
      "2024-01-10 23:02:14.081836: train_loss -0.9486\n",
      "2024-01-10 23:02:14.090845: val_loss -0.8358\n",
      "2024-01-10 23:02:14.099839: Pseudo dice [0.8695, 0.9582, 0.9432]\n",
      "2024-01-10 23:02:14.110844: Epoch time: 40.22 s\n",
      "2024-01-10 23:02:15.692309: \n",
      "2024-01-10 23:02:15.704411: Epoch 626\n",
      "2024-01-10 23:02:15.709391: Current learning rate: 0.00413\n",
      "2024-01-10 23:02:56.111159: train_loss -0.9485\n",
      "2024-01-10 23:02:56.118163: val_loss -0.8385\n",
      "2024-01-10 23:02:56.123176: Pseudo dice [0.8689, 0.9584, 0.9431]\n",
      "2024-01-10 23:02:56.131177: Epoch time: 40.42 s\n",
      "2024-01-10 23:02:57.619437: \n",
      "2024-01-10 23:02:57.624945: Epoch 627\n",
      "2024-01-10 23:02:57.637032: Current learning rate: 0.00412\n",
      "2024-01-10 23:03:37.872156: train_loss -0.9487\n",
      "2024-01-10 23:03:37.880157: val_loss -0.8414\n",
      "2024-01-10 23:03:37.887157: Pseudo dice [0.873, 0.9591, 0.9439]\n",
      "2024-01-10 23:03:37.894158: Epoch time: 40.25 s\n",
      "2024-01-10 23:03:39.553551: \n",
      "2024-01-10 23:03:39.559017: Epoch 628\n",
      "2024-01-10 23:03:39.565113: Current learning rate: 0.00411\n",
      "2024-01-10 23:04:20.064368: train_loss -0.9488\n",
      "2024-01-10 23:04:20.087882: val_loss -0.842\n",
      "2024-01-10 23:04:20.094886: Pseudo dice [0.8721, 0.9597, 0.9444]\n",
      "2024-01-10 23:04:20.105881: Epoch time: 40.51 s\n",
      "2024-01-10 23:04:21.569755: \n",
      "2024-01-10 23:04:21.576215: Epoch 629\n",
      "2024-01-10 23:04:21.581280: Current learning rate: 0.0041\n",
      "2024-01-10 23:05:01.844109: train_loss -0.9488\n",
      "2024-01-10 23:05:01.855108: val_loss -0.8417\n",
      "2024-01-10 23:05:01.884111: Pseudo dice [0.8722, 0.959, 0.9433]\n",
      "2024-01-10 23:05:01.892627: Epoch time: 40.28 s\n",
      "2024-01-10 23:05:03.387294: \n",
      "2024-01-10 23:05:03.394880: Epoch 630\n",
      "2024-01-10 23:05:03.400194: Current learning rate: 0.00409\n",
      "2024-01-10 23:05:43.659564: train_loss -0.9487\n",
      "2024-01-10 23:05:43.668564: val_loss -0.8347\n",
      "2024-01-10 23:05:43.676564: Pseudo dice [0.871, 0.958, 0.9421]\n",
      "2024-01-10 23:05:43.686565: Epoch time: 40.27 s\n",
      "2024-01-10 23:05:45.117916: \n",
      "2024-01-10 23:05:45.127002: Epoch 631\n",
      "2024-01-10 23:05:45.133935: Current learning rate: 0.00408\n",
      "2024-01-10 23:06:25.490277: train_loss -0.9487\n",
      "2024-01-10 23:06:25.499279: val_loss -0.8408\n",
      "2024-01-10 23:06:25.505277: Pseudo dice [0.8735, 0.9593, 0.9439]\n",
      "2024-01-10 23:06:25.511295: Epoch time: 40.38 s\n",
      "2024-01-10 23:06:26.977062: \n",
      "2024-01-10 23:06:26.985356: Epoch 632\n",
      "2024-01-10 23:06:26.993444: Current learning rate: 0.00407\n",
      "2024-01-10 23:07:07.428011: train_loss -0.9487\n",
      "2024-01-10 23:07:07.438012: val_loss -0.8408\n",
      "2024-01-10 23:07:07.446014: Pseudo dice [0.8697, 0.96, 0.9452]\n",
      "2024-01-10 23:07:07.454015: Epoch time: 40.45 s\n",
      "2024-01-10 23:07:08.940710: \n",
      "2024-01-10 23:07:08.948718: Epoch 633\n",
      "2024-01-10 23:07:08.954716: Current learning rate: 0.00406\n",
      "2024-01-10 23:07:49.403758: train_loss -0.9486\n",
      "2024-01-10 23:07:49.411757: val_loss -0.834\n",
      "2024-01-10 23:07:49.421759: Pseudo dice [0.8632, 0.9591, 0.9439]\n",
      "2024-01-10 23:07:49.431364: Epoch time: 40.46 s\n",
      "2024-01-10 23:07:50.924347: \n",
      "2024-01-10 23:07:50.930617: Epoch 634\n",
      "2024-01-10 23:07:50.934678: Current learning rate: 0.00405\n",
      "2024-01-10 23:08:31.376138: train_loss -0.9489\n",
      "2024-01-10 23:08:31.418652: val_loss -0.8373\n",
      "2024-01-10 23:08:31.428656: Pseudo dice [0.8652, 0.9596, 0.9443]\n",
      "2024-01-10 23:08:31.434653: Epoch time: 40.45 s\n",
      "2024-01-10 23:08:32.931578: \n",
      "2024-01-10 23:08:32.943873: Epoch 635\n",
      "2024-01-10 23:08:32.948948: Current learning rate: 0.00404\n",
      "2024-01-10 23:09:13.307376: train_loss -0.9481\n",
      "2024-01-10 23:09:13.314379: val_loss -0.8347\n",
      "2024-01-10 23:09:13.320384: Pseudo dice [0.8538, 0.96, 0.9447]\n",
      "2024-01-10 23:09:13.328382: Epoch time: 40.38 s\n",
      "2024-01-10 23:09:14.819656: \n",
      "2024-01-10 23:09:14.827450: Epoch 636\n",
      "2024-01-10 23:09:14.835860: Current learning rate: 0.00403\n",
      "2024-01-10 23:09:55.285284: train_loss -0.9485\n",
      "2024-01-10 23:09:55.294285: val_loss -0.84\n",
      "2024-01-10 23:09:55.302287: Pseudo dice [0.873, 0.9588, 0.9425]\n",
      "2024-01-10 23:09:55.309286: Epoch time: 40.47 s\n",
      "2024-01-10 23:09:56.731979: \n",
      "2024-01-10 23:09:56.738146: Epoch 637\n",
      "2024-01-10 23:09:56.743156: Current learning rate: 0.00402\n",
      "2024-01-10 23:10:36.871767: train_loss -0.9497\n",
      "2024-01-10 23:10:36.901761: val_loss -0.834\n",
      "2024-01-10 23:10:36.910758: Pseudo dice [0.8622, 0.9588, 0.9432]\n",
      "2024-01-10 23:10:36.917760: Epoch time: 40.14 s\n",
      "2024-01-10 23:10:38.587921: \n",
      "2024-01-10 23:10:38.593451: Epoch 638\n",
      "2024-01-10 23:10:38.600635: Current learning rate: 0.00401\n",
      "2024-01-10 23:11:18.995935: train_loss -0.949\n",
      "2024-01-10 23:11:19.002444: val_loss -0.8364\n",
      "2024-01-10 23:11:19.011444: Pseudo dice [0.8645, 0.9594, 0.9439]\n",
      "2024-01-10 23:11:19.017445: Epoch time: 40.41 s\n",
      "2024-01-10 23:11:20.469820: \n",
      "2024-01-10 23:11:20.477537: Epoch 639\n",
      "2024-01-10 23:11:20.482598: Current learning rate: 0.004\n",
      "2024-01-10 23:12:00.699643: train_loss -0.9488\n",
      "2024-01-10 23:12:00.710651: val_loss -0.8381\n",
      "2024-01-10 23:12:00.721170: Pseudo dice [0.868, 0.9597, 0.9442]\n",
      "2024-01-10 23:12:00.728169: Epoch time: 40.23 s\n",
      "2024-01-10 23:12:02.257015: \n",
      "2024-01-10 23:12:02.262657: Epoch 640\n",
      "2024-01-10 23:12:02.268657: Current learning rate: 0.00399\n",
      "2024-01-10 23:12:42.458949: train_loss -0.9486\n",
      "2024-01-10 23:12:42.468951: val_loss -0.8383\n",
      "2024-01-10 23:12:42.478461: Pseudo dice [0.8697, 0.9592, 0.944]\n",
      "2024-01-10 23:12:42.487464: Epoch time: 40.2 s\n",
      "2024-01-10 23:12:43.879904: \n",
      "2024-01-10 23:12:43.885904: Epoch 641\n",
      "2024-01-10 23:12:43.890383: Current learning rate: 0.00398\n",
      "2024-01-10 23:13:23.984092: train_loss -0.9478\n",
      "2024-01-10 23:13:23.994093: val_loss -0.8355\n",
      "2024-01-10 23:13:24.002095: Pseudo dice [0.8619, 0.9592, 0.9435]\n",
      "2024-01-10 23:13:24.010093: Epoch time: 40.11 s\n",
      "2024-01-10 23:13:25.517720: \n",
      "2024-01-10 23:13:25.528320: Epoch 642\n",
      "2024-01-10 23:13:25.533384: Current learning rate: 0.00397\n",
      "2024-01-10 23:14:05.730776: train_loss -0.9491\n",
      "2024-01-10 23:14:05.741781: val_loss -0.8344\n",
      "2024-01-10 23:14:05.748778: Pseudo dice [0.8675, 0.9584, 0.9424]\n",
      "2024-01-10 23:14:05.777297: Epoch time: 40.22 s\n",
      "2024-01-10 23:14:07.270120: \n",
      "2024-01-10 23:14:07.280630: Epoch 643\n",
      "2024-01-10 23:14:07.290630: Current learning rate: 0.00396\n",
      "2024-01-10 23:14:47.560455: train_loss -0.9488\n",
      "2024-01-10 23:14:47.568456: val_loss -0.8386\n",
      "2024-01-10 23:14:47.575456: Pseudo dice [0.8665, 0.9602, 0.9453]\n",
      "2024-01-10 23:14:47.580463: Epoch time: 40.29 s\n",
      "2024-01-10 23:14:49.081678: \n",
      "2024-01-10 23:14:49.088699: Epoch 644\n",
      "2024-01-10 23:14:49.098381: Current learning rate: 0.00395\n",
      "2024-01-10 23:15:29.208071: train_loss -0.9481\n",
      "2024-01-10 23:15:29.214078: val_loss -0.8368\n",
      "2024-01-10 23:15:29.222079: Pseudo dice [0.8732, 0.9587, 0.9437]\n",
      "2024-01-10 23:15:29.229247: Epoch time: 40.13 s\n",
      "2024-01-10 23:15:30.617833: \n",
      "2024-01-10 23:15:30.629454: Epoch 645\n",
      "2024-01-10 23:15:30.635960: Current learning rate: 0.00394\n",
      "2024-01-10 23:16:10.694439: train_loss -0.9489\n",
      "2024-01-10 23:16:10.706440: val_loss -0.8408\n",
      "2024-01-10 23:16:10.730441: Pseudo dice [0.8666, 0.9605, 0.9459]\n",
      "2024-01-10 23:16:10.739480: Epoch time: 40.08 s\n",
      "2024-01-10 23:16:12.324351: \n",
      "2024-01-10 23:16:12.330348: Epoch 646\n",
      "2024-01-10 23:16:12.335285: Current learning rate: 0.00393\n",
      "2024-01-10 23:16:52.500688: train_loss -0.9503\n",
      "2024-01-10 23:16:52.507686: val_loss -0.8369\n",
      "2024-01-10 23:16:52.515691: Pseudo dice [0.8693, 0.9595, 0.9447]\n",
      "2024-01-10 23:16:52.544690: Epoch time: 40.18 s\n",
      "2024-01-10 23:16:53.961000: \n",
      "2024-01-10 23:16:53.966982: Epoch 647\n",
      "2024-01-10 23:16:53.974001: Current learning rate: 0.00392\n",
      "2024-01-10 23:17:34.086277: train_loss -0.9487\n",
      "2024-01-10 23:17:34.097285: val_loss -0.8397\n",
      "2024-01-10 23:17:34.124371: Pseudo dice [0.8749, 0.9586, 0.9436]\n",
      "2024-01-10 23:17:34.132371: Epoch time: 40.13 s\n",
      "2024-01-10 23:17:35.822991: \n",
      "2024-01-10 23:17:35.829573: Epoch 648\n",
      "2024-01-10 23:17:35.833649: Current learning rate: 0.00391\n",
      "2024-01-10 23:18:16.012647: train_loss -0.9494\n",
      "2024-01-10 23:18:16.019652: val_loss -0.8414\n",
      "2024-01-10 23:18:16.028658: Pseudo dice [0.8714, 0.9595, 0.9441]\n",
      "2024-01-10 23:18:16.037167: Epoch time: 40.19 s\n",
      "2024-01-10 23:18:17.516715: \n",
      "2024-01-10 23:18:17.522959: Epoch 649\n",
      "2024-01-10 23:18:17.527022: Current learning rate: 0.0039\n",
      "2024-01-10 23:18:57.793989: train_loss -0.95\n",
      "2024-01-10 23:18:57.803941: val_loss -0.8363\n",
      "2024-01-10 23:18:57.811940: Pseudo dice [0.8677, 0.9577, 0.9415]\n",
      "2024-01-10 23:18:57.839946: Epoch time: 40.28 s\n",
      "2024-01-10 23:18:59.555871: \n",
      "2024-01-10 23:18:59.561881: Epoch 650\n",
      "2024-01-10 23:18:59.566872: Current learning rate: 0.00389\n",
      "2024-01-10 23:19:39.826520: train_loss -0.9494\n",
      "2024-01-10 23:19:39.835523: val_loss -0.8334\n",
      "2024-01-10 23:19:39.841522: Pseudo dice [0.8684, 0.9584, 0.9422]\n",
      "2024-01-10 23:19:39.848684: Epoch time: 40.27 s\n",
      "2024-01-10 23:19:41.336426: \n",
      "2024-01-10 23:19:41.341985: Epoch 651\n",
      "2024-01-10 23:19:41.349993: Current learning rate: 0.00388\n",
      "2024-01-10 23:20:21.401118: train_loss -0.9487\n",
      "2024-01-10 23:20:21.411119: val_loss -0.8378\n",
      "2024-01-10 23:20:21.417121: Pseudo dice [0.8668, 0.9592, 0.9442]\n",
      "2024-01-10 23:20:21.446117: Epoch time: 40.07 s\n",
      "2024-01-10 23:20:22.859186: \n",
      "2024-01-10 23:20:22.864896: Epoch 652\n",
      "2024-01-10 23:20:22.871950: Current learning rate: 0.00387\n",
      "2024-01-10 23:21:03.013015: train_loss -0.9496\n",
      "2024-01-10 23:21:03.023524: val_loss -0.8378\n",
      "2024-01-10 23:21:03.051532: Pseudo dice [0.8715, 0.9595, 0.9442]\n",
      "2024-01-10 23:21:03.060120: Epoch time: 40.16 s\n",
      "2024-01-10 23:21:04.661407: \n",
      "2024-01-10 23:21:04.668439: Epoch 653\n",
      "2024-01-10 23:21:04.678452: Current learning rate: 0.00386\n",
      "2024-01-10 23:21:45.024897: train_loss -0.9498\n",
      "2024-01-10 23:21:45.037411: val_loss -0.8385\n",
      "2024-01-10 23:21:45.047410: Pseudo dice [0.8653, 0.9592, 0.9432]\n",
      "2024-01-10 23:21:45.080409: Epoch time: 40.36 s\n",
      "2024-01-10 23:21:46.609143: \n",
      "2024-01-10 23:21:46.616284: Epoch 654\n",
      "2024-01-10 23:21:46.620272: Current learning rate: 0.00385\n",
      "2024-01-10 23:22:26.754135: train_loss -0.9495\n",
      "2024-01-10 23:22:26.761135: val_loss -0.8327\n",
      "2024-01-10 23:22:26.768136: Pseudo dice [0.8626, 0.9584, 0.9431]\n",
      "2024-01-10 23:22:26.774625: Epoch time: 40.15 s\n",
      "2024-01-10 23:22:28.274093: \n",
      "2024-01-10 23:22:28.279804: Epoch 655\n",
      "2024-01-10 23:22:28.284895: Current learning rate: 0.00384\n",
      "2024-01-10 23:23:08.457205: train_loss -0.949\n",
      "2024-01-10 23:23:08.467209: val_loss -0.8378\n",
      "2024-01-10 23:23:08.477716: Pseudo dice [0.8704, 0.9595, 0.9435]\n",
      "2024-01-10 23:23:08.486717: Epoch time: 40.18 s\n",
      "2024-01-10 23:23:10.026065: \n",
      "2024-01-10 23:23:10.032077: Epoch 656\n",
      "2024-01-10 23:23:10.037010: Current learning rate: 0.00383\n",
      "2024-01-10 23:23:50.245708: train_loss -0.9493\n",
      "2024-01-10 23:23:50.254709: val_loss -0.8387\n",
      "2024-01-10 23:23:50.261710: Pseudo dice [0.8694, 0.9588, 0.9433]\n",
      "2024-01-10 23:23:50.283829: Epoch time: 40.22 s\n",
      "2024-01-10 23:23:51.751782: \n",
      "2024-01-10 23:23:51.761866: Epoch 657\n",
      "2024-01-10 23:23:51.768830: Current learning rate: 0.00382\n",
      "2024-01-10 23:24:32.153250: train_loss -0.9494\n",
      "2024-01-10 23:24:32.161251: val_loss -0.8398\n",
      "2024-01-10 23:24:32.170254: Pseudo dice [0.8667, 0.9599, 0.9457]\n",
      "2024-01-10 23:24:32.175254: Epoch time: 40.4 s\n",
      "2024-01-10 23:24:33.677763: \n",
      "2024-01-10 23:24:33.683763: Epoch 658\n",
      "2024-01-10 23:24:33.693763: Current learning rate: 0.00381\n",
      "2024-01-10 23:25:13.956556: train_loss -0.9495\n",
      "2024-01-10 23:25:13.964563: val_loss -0.839\n",
      "2024-01-10 23:25:13.972083: Pseudo dice [0.8667, 0.9602, 0.9452]\n",
      "2024-01-10 23:25:13.980090: Epoch time: 40.28 s\n",
      "2024-01-10 23:25:15.483765: \n",
      "2024-01-10 23:25:15.496635: Epoch 659\n",
      "2024-01-10 23:25:15.509627: Current learning rate: 0.0038\n",
      "2024-01-10 23:25:55.672413: train_loss -0.9489\n",
      "2024-01-10 23:25:55.682421: val_loss -0.8395\n",
      "2024-01-10 23:25:55.713935: Pseudo dice [0.8742, 0.9593, 0.9436]\n",
      "2024-01-10 23:25:55.722938: Epoch time: 40.19 s\n",
      "2024-01-10 23:25:57.205544: \n",
      "2024-01-10 23:25:57.211547: Epoch 660\n",
      "2024-01-10 23:25:57.215975: Current learning rate: 0.00379\n",
      "2024-01-10 23:26:37.531491: train_loss -0.9488\n",
      "2024-01-10 23:26:37.545489: val_loss -0.8391\n",
      "2024-01-10 23:26:37.554492: Pseudo dice [0.8694, 0.9594, 0.9444]\n",
      "2024-01-10 23:26:37.561492: Epoch time: 40.33 s\n",
      "2024-01-10 23:26:39.009867: \n",
      "2024-01-10 23:26:39.015867: Epoch 661\n",
      "2024-01-10 23:26:39.021317: Current learning rate: 0.00378\n",
      "2024-01-10 23:27:19.340125: train_loss -0.95\n",
      "2024-01-10 23:27:19.349124: val_loss -0.8404\n",
      "2024-01-10 23:27:19.374127: Pseudo dice [0.8673, 0.9599, 0.9452]\n",
      "2024-01-10 23:27:19.383125: Epoch time: 40.33 s\n",
      "2024-01-10 23:27:20.818418: \n",
      "2024-01-10 23:27:20.824931: Epoch 662\n",
      "2024-01-10 23:27:20.834936: Current learning rate: 0.00377\n",
      "2024-01-10 23:28:01.308777: train_loss -0.9497\n",
      "2024-01-10 23:28:01.320985: val_loss -0.8371\n",
      "2024-01-10 23:28:01.326982: Pseudo dice [0.8718, 0.9588, 0.9428]\n",
      "2024-01-10 23:28:01.333991: Epoch time: 40.49 s\n",
      "2024-01-10 23:28:02.865079: \n",
      "2024-01-10 23:28:02.875892: Epoch 663\n",
      "2024-01-10 23:28:02.880963: Current learning rate: 0.00376\n",
      "2024-01-10 23:28:43.134529: train_loss -0.9496\n",
      "2024-01-10 23:28:43.146539: val_loss -0.8429\n",
      "2024-01-10 23:28:43.154793: Pseudo dice [0.872, 0.9602, 0.9454]\n",
      "2024-01-10 23:28:43.184798: Epoch time: 40.27 s\n",
      "2024-01-10 23:28:44.645941: \n",
      "2024-01-10 23:28:44.651378: Epoch 664\n",
      "2024-01-10 23:28:44.657022: Current learning rate: 0.00375\n",
      "2024-01-10 23:29:24.785356: train_loss -0.949\n",
      "2024-01-10 23:29:24.793356: val_loss -0.8388\n",
      "2024-01-10 23:29:24.803357: Pseudo dice [0.8693, 0.9595, 0.9448]\n",
      "2024-01-10 23:29:24.811357: Epoch time: 40.14 s\n",
      "2024-01-10 23:29:26.340585: \n",
      "2024-01-10 23:29:26.346587: Epoch 665\n",
      "2024-01-10 23:29:26.351677: Current learning rate: 0.00374\n",
      "2024-01-10 23:30:06.465666: train_loss -0.9494\n",
      "2024-01-10 23:30:06.471663: val_loss -0.8371\n",
      "2024-01-10 23:30:06.478670: Pseudo dice [0.8679, 0.9595, 0.9449]\n",
      "2024-01-10 23:30:06.483670: Epoch time: 40.13 s\n",
      "2024-01-10 23:30:08.061574: \n",
      "2024-01-10 23:30:08.066968: Epoch 666\n",
      "2024-01-10 23:30:08.070969: Current learning rate: 0.00373\n",
      "2024-01-10 23:30:48.252696: train_loss -0.9503\n",
      "2024-01-10 23:30:48.262696: val_loss -0.8403\n",
      "2024-01-10 23:30:48.272696: Pseudo dice [0.8697, 0.96, 0.9452]\n",
      "2024-01-10 23:30:48.281700: Epoch time: 40.19 s\n",
      "2024-01-10 23:30:49.952679: \n",
      "2024-01-10 23:30:49.959677: Epoch 667\n",
      "2024-01-10 23:30:49.964620: Current learning rate: 0.00372\n",
      "2024-01-10 23:31:30.139140: train_loss -0.95\n",
      "2024-01-10 23:31:30.180142: val_loss -0.8392\n",
      "2024-01-10 23:31:30.189140: Pseudo dice [0.8731, 0.9593, 0.9442]\n",
      "2024-01-10 23:31:30.195142: Epoch time: 40.19 s\n",
      "2024-01-10 23:31:31.641512: \n",
      "2024-01-10 23:31:31.647831: Epoch 668\n",
      "2024-01-10 23:31:31.653902: Current learning rate: 0.00371\n",
      "2024-01-10 23:32:11.811131: train_loss -0.9497\n",
      "2024-01-10 23:32:11.824131: val_loss -0.835\n",
      "2024-01-10 23:32:11.830139: Pseudo dice [0.8702, 0.9579, 0.9425]\n",
      "2024-01-10 23:32:11.838138: Epoch time: 40.17 s\n",
      "2024-01-10 23:32:13.387867: \n",
      "2024-01-10 23:32:13.392473: Epoch 669\n",
      "2024-01-10 23:32:13.399549: Current learning rate: 0.0037\n",
      "2024-01-10 23:32:53.444278: train_loss -0.9497\n",
      "2024-01-10 23:32:53.453282: val_loss -0.8385\n",
      "2024-01-10 23:32:53.459788: Pseudo dice [0.8682, 0.9599, 0.9443]\n",
      "2024-01-10 23:32:53.469794: Epoch time: 40.06 s\n",
      "2024-01-10 23:32:55.012953: \n",
      "2024-01-10 23:32:55.017952: Epoch 670\n",
      "2024-01-10 23:32:55.022969: Current learning rate: 0.00369\n",
      "2024-01-10 23:33:35.312737: train_loss -0.9493\n",
      "2024-01-10 23:33:35.320736: val_loss -0.8405\n",
      "2024-01-10 23:33:35.327743: Pseudo dice [0.8727, 0.9592, 0.9441]\n",
      "2024-01-10 23:33:35.335734: Epoch time: 40.3 s\n",
      "2024-01-10 23:33:36.809873: \n",
      "2024-01-10 23:33:36.816023: Epoch 671\n",
      "2024-01-10 23:33:36.824042: Current learning rate: 0.00368\n",
      "2024-01-10 23:34:16.958328: train_loss -0.9502\n",
      "2024-01-10 23:34:16.967323: val_loss -0.8393\n",
      "2024-01-10 23:34:16.975178: Pseudo dice [0.8753, 0.959, 0.9426]\n",
      "2024-01-10 23:34:16.985177: Epoch time: 40.15 s\n",
      "2024-01-10 23:34:18.628747: \n",
      "2024-01-10 23:34:18.634747: Epoch 672\n",
      "2024-01-10 23:34:18.639747: Current learning rate: 0.00367\n",
      "2024-01-10 23:34:58.819766: train_loss -0.9505\n",
      "2024-01-10 23:34:58.829285: val_loss -0.8411\n",
      "2024-01-10 23:34:58.837282: Pseudo dice [0.8728, 0.9603, 0.9449]\n",
      "2024-01-10 23:34:58.845282: Epoch time: 40.19 s\n",
      "2024-01-10 23:35:00.332970: \n",
      "2024-01-10 23:35:00.338665: Epoch 673\n",
      "2024-01-10 23:35:00.353750: Current learning rate: 0.00366\n",
      "2024-01-10 23:35:40.582564: train_loss -0.9497\n",
      "2024-01-10 23:35:40.590564: val_loss -0.8411\n",
      "2024-01-10 23:35:40.621565: Pseudo dice [0.8741, 0.9598, 0.9451]\n",
      "2024-01-10 23:35:40.629576: Epoch time: 40.25 s\n",
      "2024-01-10 23:35:42.054668: \n",
      "2024-01-10 23:35:42.063601: Epoch 674\n",
      "2024-01-10 23:35:42.069601: Current learning rate: 0.00365\n",
      "2024-01-10 23:36:22.344656: train_loss -0.9503\n",
      "2024-01-10 23:36:22.353659: val_loss -0.8422\n",
      "2024-01-10 23:36:22.360657: Pseudo dice [0.8724, 0.9601, 0.9451]\n",
      "2024-01-10 23:36:22.386681: Epoch time: 40.29 s\n",
      "2024-01-10 23:36:23.826358: \n",
      "2024-01-10 23:36:23.832311: Epoch 675\n",
      "2024-01-10 23:36:23.836383: Current learning rate: 0.00364\n",
      "2024-01-10 23:37:04.009652: train_loss -0.9509\n",
      "2024-01-10 23:37:04.019653: val_loss -0.8416\n",
      "2024-01-10 23:37:04.051653: Pseudo dice [0.8717, 0.9598, 0.9446]\n",
      "2024-01-10 23:37:04.058653: Epoch time: 40.18 s\n",
      "2024-01-10 23:37:05.540136: \n",
      "2024-01-10 23:37:05.550854: Epoch 676\n",
      "2024-01-10 23:37:05.555842: Current learning rate: 0.00363\n",
      "2024-01-10 23:37:45.914244: train_loss -0.95\n",
      "2024-01-10 23:37:45.921241: val_loss -0.8384\n",
      "2024-01-10 23:37:45.931248: Pseudo dice [0.8686, 0.9593, 0.9444]\n",
      "2024-01-10 23:37:45.940246: Epoch time: 40.38 s\n",
      "2024-01-10 23:37:47.508878: \n",
      "2024-01-10 23:37:47.513722: Epoch 677\n",
      "2024-01-10 23:37:47.517722: Current learning rate: 0.00362\n",
      "2024-01-10 23:38:27.644340: train_loss -0.9498\n",
      "2024-01-10 23:38:27.651650: val_loss -0.8405\n",
      "2024-01-10 23:38:27.660650: Pseudo dice [0.8714, 0.9597, 0.9445]\n",
      "2024-01-10 23:38:27.666649: Epoch time: 40.14 s\n",
      "2024-01-10 23:38:29.170393: \n",
      "2024-01-10 23:38:29.176979: Epoch 678\n",
      "2024-01-10 23:38:29.181916: Current learning rate: 0.00361\n",
      "2024-01-10 23:39:09.294485: train_loss -0.9504\n",
      "2024-01-10 23:39:09.303486: val_loss -0.8413\n",
      "2024-01-10 23:39:09.309486: Pseudo dice [0.8731, 0.9602, 0.9447]\n",
      "2024-01-10 23:39:09.317487: Epoch time: 40.13 s\n",
      "2024-01-10 23:39:09.323487: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2024-01-10 23:39:11.045159: \n",
      "2024-01-10 23:39:11.051506: Epoch 679\n",
      "2024-01-10 23:39:11.059508: Current learning rate: 0.0036\n",
      "2024-01-10 23:39:51.137254: train_loss -0.9499\n",
      "2024-01-10 23:39:51.148254: val_loss -0.84\n",
      "2024-01-10 23:39:51.157254: Pseudo dice [0.8705, 0.9598, 0.9451]\n",
      "2024-01-10 23:39:51.167254: Epoch time: 40.09 s\n",
      "2024-01-10 23:39:51.174254: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2024-01-10 23:39:52.952858: \n",
      "2024-01-10 23:39:52.958224: Epoch 680\n",
      "2024-01-10 23:39:52.962617: Current learning rate: 0.00359\n",
      "2024-01-10 23:40:33.454160: train_loss -0.9507\n",
      "2024-01-10 23:40:33.461162: val_loss -0.8426\n",
      "2024-01-10 23:40:33.466162: Pseudo dice [0.872, 0.96, 0.9449]\n",
      "2024-01-10 23:40:33.473677: Epoch time: 40.5 s\n",
      "2024-01-10 23:40:33.479678: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2024-01-10 23:40:35.196950: \n",
      "2024-01-10 23:40:35.210291: Epoch 681\n",
      "2024-01-10 23:40:35.219346: Current learning rate: 0.00358\n",
      "2024-01-10 23:41:15.618568: train_loss -0.9497\n",
      "2024-01-10 23:41:15.627567: val_loss -0.8408\n",
      "2024-01-10 23:41:15.636567: Pseudo dice [0.8705, 0.96, 0.945]\n",
      "2024-01-10 23:41:15.644568: Epoch time: 40.42 s\n",
      "2024-01-10 23:41:15.652568: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2024-01-10 23:41:17.549921: \n",
      "2024-01-10 23:41:17.555979: Epoch 682\n",
      "2024-01-10 23:41:17.560997: Current learning rate: 0.00357\n",
      "2024-01-10 23:41:57.836805: train_loss -0.9492\n",
      "2024-01-10 23:41:57.849808: val_loss -0.8365\n",
      "2024-01-10 23:41:57.881809: Pseudo dice [0.8676, 0.9595, 0.9437]\n",
      "2024-01-10 23:41:57.890809: Epoch time: 40.29 s\n",
      "2024-01-10 23:41:59.410844: \n",
      "2024-01-10 23:41:59.420729: Epoch 683\n",
      "2024-01-10 23:41:59.427752: Current learning rate: 0.00356\n",
      "2024-01-10 23:42:39.573132: train_loss -0.9509\n",
      "2024-01-10 23:42:39.580132: val_loss -0.8396\n",
      "2024-01-10 23:42:39.588133: Pseudo dice [0.8632, 0.9602, 0.9443]\n",
      "2024-01-10 23:42:39.622651: Epoch time: 40.16 s\n",
      "2024-01-10 23:42:41.084314: \n",
      "2024-01-10 23:42:41.090641: Epoch 684\n",
      "2024-01-10 23:42:41.095643: Current learning rate: 0.00355\n",
      "2024-01-10 23:43:21.343250: train_loss -0.9504\n",
      "2024-01-10 23:43:21.350249: val_loss -0.842\n",
      "2024-01-10 23:43:21.357250: Pseudo dice [0.8763, 0.96, 0.9448]\n",
      "2024-01-10 23:43:21.365251: Epoch time: 40.26 s\n",
      "2024-01-10 23:43:22.888238: \n",
      "2024-01-10 23:43:22.893165: Epoch 685\n",
      "2024-01-10 23:43:22.899228: Current learning rate: 0.00354\n",
      "2024-01-10 23:44:03.148248: train_loss -0.9501\n",
      "2024-01-10 23:44:03.180248: val_loss -0.8393\n",
      "2024-01-10 23:44:03.188248: Pseudo dice [0.8682, 0.9598, 0.9447]\n",
      "2024-01-10 23:44:03.195250: Epoch time: 40.26 s\n",
      "2024-01-10 23:44:04.747976: \n",
      "2024-01-10 23:44:04.757792: Epoch 686\n",
      "2024-01-10 23:44:04.763876: Current learning rate: 0.00353\n",
      "2024-01-10 23:44:45.057733: train_loss -0.95\n",
      "2024-01-10 23:44:45.065734: val_loss -0.8376\n",
      "2024-01-10 23:44:45.073242: Pseudo dice [0.8721, 0.9587, 0.9432]\n",
      "2024-01-10 23:44:45.081113: Epoch time: 40.31 s\n",
      "2024-01-10 23:44:46.605609: \n",
      "2024-01-10 23:44:46.611896: Epoch 687\n",
      "2024-01-10 23:44:46.615989: Current learning rate: 0.00352\n",
      "2024-01-10 23:45:27.076034: train_loss -0.9502\n",
      "2024-01-10 23:45:27.084033: val_loss -0.8314\n",
      "2024-01-10 23:45:27.095038: Pseudo dice [0.8686, 0.9575, 0.9412]\n",
      "2024-01-10 23:45:27.105553: Epoch time: 40.47 s\n",
      "2024-01-10 23:45:28.804097: \n",
      "2024-01-10 23:45:28.809614: Epoch 688\n",
      "2024-01-10 23:45:28.814607: Current learning rate: 0.00351\n",
      "2024-01-10 23:46:09.198809: train_loss -0.95\n",
      "2024-01-10 23:46:09.204810: val_loss -0.8401\n",
      "2024-01-10 23:46:09.211816: Pseudo dice [0.8659, 0.9606, 0.9456]\n",
      "2024-01-10 23:46:09.218819: Epoch time: 40.4 s\n",
      "2024-01-10 23:46:10.795628: \n",
      "2024-01-10 23:46:10.802636: Epoch 689\n",
      "2024-01-10 23:46:10.807636: Current learning rate: 0.0035\n",
      "2024-01-10 23:46:51.006797: train_loss -0.9493\n",
      "2024-01-10 23:46:51.014795: val_loss -0.8397\n",
      "2024-01-10 23:46:51.021797: Pseudo dice [0.8765, 0.9593, 0.9436]\n",
      "2024-01-10 23:46:51.031798: Epoch time: 40.21 s\n",
      "2024-01-10 23:46:52.532421: \n",
      "2024-01-10 23:46:52.538728: Epoch 690\n",
      "2024-01-10 23:46:52.543799: Current learning rate: 0.00349\n",
      "2024-01-10 23:47:32.768397: train_loss -0.9504\n",
      "2024-01-10 23:47:32.775398: val_loss -0.8449\n",
      "2024-01-10 23:47:32.780402: Pseudo dice [0.873, 0.9609, 0.9466]\n",
      "2024-01-10 23:47:32.813926: Epoch time: 40.24 s\n",
      "2024-01-10 23:47:34.290849: \n",
      "2024-01-10 23:47:34.302545: Epoch 691\n",
      "2024-01-10 23:47:34.314561: Current learning rate: 0.00348\n",
      "2024-01-10 23:48:14.659837: train_loss -0.95\n",
      "2024-01-10 23:48:14.670838: val_loss -0.8418\n",
      "2024-01-10 23:48:14.676838: Pseudo dice [0.8747, 0.9599, 0.9442]\n",
      "2024-01-10 23:48:14.684065: Epoch time: 40.37 s\n",
      "2024-01-10 23:48:16.126594: \n",
      "2024-01-10 23:48:16.132593: Epoch 692\n",
      "2024-01-10 23:48:16.136662: Current learning rate: 0.00346\n",
      "2024-01-10 23:48:56.357675: train_loss -0.9503\n",
      "2024-01-10 23:48:56.383676: val_loss -0.8424\n",
      "2024-01-10 23:48:56.391676: Pseudo dice [0.8733, 0.9598, 0.945]\n",
      "2024-01-10 23:48:56.398675: Epoch time: 40.23 s\n",
      "2024-01-10 23:48:56.405682: Yayy! New best EMA pseudo Dice: 0.9252\n",
      "2024-01-10 23:48:58.052029: \n",
      "2024-01-10 23:48:58.062671: Epoch 693\n",
      "2024-01-10 23:48:58.072744: Current learning rate: 0.00345\n",
      "2024-01-10 23:49:38.340471: train_loss -0.9501\n",
      "2024-01-10 23:49:38.349472: val_loss -0.8414\n",
      "2024-01-10 23:49:38.379470: Pseudo dice [0.8727, 0.9603, 0.9448]\n",
      "2024-01-10 23:49:38.387473: Epoch time: 40.29 s\n",
      "2024-01-10 23:49:38.394471: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-10 23:49:40.091243: \n",
      "2024-01-10 23:49:40.100088: Epoch 694\n",
      "2024-01-10 23:49:40.105108: Current learning rate: 0.00344\n",
      "2024-01-10 23:50:20.584957: train_loss -0.9499\n",
      "2024-01-10 23:50:20.610956: val_loss -0.8384\n",
      "2024-01-10 23:50:20.618958: Pseudo dice [0.8689, 0.9592, 0.9434]\n",
      "2024-01-10 23:50:20.626958: Epoch time: 40.49 s\n",
      "2024-01-10 23:50:22.112008: \n",
      "2024-01-10 23:50:22.119105: Epoch 695\n",
      "2024-01-10 23:50:22.124302: Current learning rate: 0.00343\n",
      "2024-01-10 23:51:02.256469: train_loss -0.9509\n",
      "2024-01-10 23:51:02.265476: val_loss -0.8376\n",
      "2024-01-10 23:51:02.288983: Pseudo dice [0.8683, 0.9591, 0.9433]\n",
      "2024-01-10 23:51:02.301984: Epoch time: 40.15 s\n",
      "2024-01-10 23:51:03.957803: \n",
      "2024-01-10 23:51:03.964201: Epoch 696\n",
      "2024-01-10 23:51:03.972267: Current learning rate: 0.00342\n",
      "2024-01-10 23:51:44.303787: train_loss -0.9499\n",
      "2024-01-10 23:51:44.314074: val_loss -0.8411\n",
      "2024-01-10 23:51:44.321074: Pseudo dice [0.8725, 0.9601, 0.9443]\n",
      "2024-01-10 23:51:44.327075: Epoch time: 40.35 s\n",
      "2024-01-10 23:51:45.862406: \n",
      "2024-01-10 23:51:45.875710: Epoch 697\n",
      "2024-01-10 23:51:45.882082: Current learning rate: 0.00341\n",
      "2024-01-10 23:52:26.191242: train_loss -0.9507\n",
      "2024-01-10 23:52:26.203243: val_loss -0.8405\n",
      "2024-01-10 23:52:26.211242: Pseudo dice [0.8708, 0.9593, 0.9442]\n",
      "2024-01-10 23:52:26.220246: Epoch time: 40.33 s\n",
      "2024-01-10 23:52:27.745895: \n",
      "2024-01-10 23:52:27.751207: Epoch 698\n",
      "2024-01-10 23:52:27.761284: Current learning rate: 0.0034\n",
      "2024-01-10 23:53:07.894064: train_loss -0.9512\n",
      "2024-01-10 23:53:07.904066: val_loss -0.8347\n",
      "2024-01-10 23:53:07.911075: Pseudo dice [0.8626, 0.9596, 0.9445]\n",
      "2024-01-10 23:53:07.920065: Epoch time: 40.15 s\n",
      "2024-01-10 23:53:09.358358: \n",
      "2024-01-10 23:53:09.365080: Epoch 699\n",
      "2024-01-10 23:53:09.371084: Current learning rate: 0.00339\n",
      "2024-01-10 23:53:49.709500: train_loss -0.9492\n",
      "2024-01-10 23:53:49.717500: val_loss -0.8386\n",
      "2024-01-10 23:53:49.745501: Pseudo dice [0.8666, 0.9591, 0.9441]\n",
      "2024-01-10 23:53:49.754500: Epoch time: 40.35 s\n",
      "2024-01-10 23:53:51.527936: \n",
      "2024-01-10 23:53:51.534768: Epoch 700\n",
      "2024-01-10 23:53:51.543770: Current learning rate: 0.00338\n",
      "2024-01-10 23:54:32.029022: train_loss -0.9507\n",
      "2024-01-10 23:54:32.040025: val_loss -0.8374\n",
      "2024-01-10 23:54:32.047022: Pseudo dice [0.8663, 0.9596, 0.9443]\n",
      "2024-01-10 23:54:32.056094: Epoch time: 40.5 s\n",
      "2024-01-10 23:54:33.737633: \n",
      "2024-01-10 23:54:33.743162: Epoch 701\n",
      "2024-01-10 23:54:33.747250: Current learning rate: 0.00337\n",
      "2024-01-10 23:55:14.111027: train_loss -0.9501\n",
      "2024-01-10 23:55:14.135552: val_loss -0.8401\n",
      "2024-01-10 23:55:14.144086: Pseudo dice [0.8684, 0.9605, 0.9463]\n",
      "2024-01-10 23:55:14.151273: Epoch time: 40.37 s\n",
      "2024-01-10 23:55:15.641597: \n",
      "2024-01-10 23:55:15.654165: Epoch 702\n",
      "2024-01-10 23:55:15.659214: Current learning rate: 0.00336\n",
      "2024-01-10 23:55:55.805922: train_loss -0.9509\n",
      "2024-01-10 23:55:55.814923: val_loss -0.8394\n",
      "2024-01-10 23:55:55.824924: Pseudo dice [0.8669, 0.9599, 0.9447]\n",
      "2024-01-10 23:55:55.834923: Epoch time: 40.17 s\n",
      "2024-01-10 23:55:57.329539: \n",
      "2024-01-10 23:55:57.342562: Epoch 703\n",
      "2024-01-10 23:55:57.350707: Current learning rate: 0.00335\n",
      "2024-01-10 23:56:37.691042: train_loss -0.9506\n",
      "2024-01-10 23:56:37.701044: val_loss -0.841\n",
      "2024-01-10 23:56:37.708044: Pseudo dice [0.8693, 0.9595, 0.9446]\n",
      "2024-01-10 23:56:37.735043: Epoch time: 40.36 s\n",
      "2024-01-10 23:56:39.151720: \n",
      "2024-01-10 23:56:39.158852: Epoch 704\n",
      "2024-01-10 23:56:39.163931: Current learning rate: 0.00334\n",
      "2024-01-10 23:57:19.398296: train_loss -0.9505\n",
      "2024-01-10 23:57:19.429806: val_loss -0.8392\n",
      "2024-01-10 23:57:19.438814: Pseudo dice [0.8687, 0.9596, 0.9444]\n",
      "2024-01-10 23:57:19.445806: Epoch time: 40.25 s\n",
      "2024-01-10 23:57:20.924100: \n",
      "2024-01-10 23:57:20.929857: Epoch 705\n",
      "2024-01-10 23:57:20.935946: Current learning rate: 0.00333\n",
      "2024-01-10 23:58:01.211369: train_loss -0.9507\n",
      "2024-01-10 23:58:01.238880: val_loss -0.8408\n",
      "2024-01-10 23:58:01.246878: Pseudo dice [0.8682, 0.96, 0.9454]\n",
      "2024-01-10 23:58:01.253880: Epoch time: 40.29 s\n",
      "2024-01-10 23:58:02.707149: \n",
      "2024-01-10 23:58:02.715871: Epoch 706\n",
      "2024-01-10 23:58:02.720776: Current learning rate: 0.00332\n",
      "2024-01-10 23:58:42.917314: train_loss -0.9509\n",
      "2024-01-10 23:58:42.926323: val_loss -0.8387\n",
      "2024-01-10 23:58:42.958836: Pseudo dice [0.8661, 0.9595, 0.9449]\n",
      "2024-01-10 23:58:42.968836: Epoch time: 40.21 s\n",
      "2024-01-10 23:58:44.441048: \n",
      "2024-01-10 23:58:44.451898: Epoch 707\n",
      "2024-01-10 23:58:44.456901: Current learning rate: 0.00331\n",
      "2024-01-10 23:59:24.639532: train_loss -0.9516\n",
      "2024-01-10 23:59:24.649530: val_loss -0.8387\n",
      "2024-01-10 23:59:24.657047: Pseudo dice [0.87, 0.9598, 0.9444]\n",
      "2024-01-10 23:59:24.670707: Epoch time: 40.2 s\n",
      "2024-01-10 23:59:26.161116: \n",
      "2024-01-10 23:59:26.171642: Epoch 708\n",
      "2024-01-10 23:59:26.176628: Current learning rate: 0.0033\n",
      "2024-01-11 00:00:06.492563: train_loss -0.9514\n",
      "2024-01-11 00:00:06.502569: val_loss -0.8403\n",
      "2024-01-11 00:00:06.511637: Pseudo dice [0.8716, 0.9596, 0.9443]\n",
      "2024-01-11 00:00:06.517633: Epoch time: 40.33 s\n",
      "2024-01-11 00:00:08.044913: \n",
      "2024-01-11 00:00:08.050432: Epoch 709\n",
      "2024-01-11 00:00:08.055518: Current learning rate: 0.00329\n",
      "2024-01-11 00:00:48.418965: train_loss -0.9505\n",
      "2024-01-11 00:00:48.426971: val_loss -0.8426\n",
      "2024-01-11 00:00:48.454966: Pseudo dice [0.8664, 0.9601, 0.944]\n",
      "2024-01-11 00:00:48.462965: Epoch time: 40.38 s\n",
      "2024-01-11 00:00:49.955380: \n",
      "2024-01-11 00:00:49.960390: Epoch 710\n",
      "2024-01-11 00:00:49.965391: Current learning rate: 0.00328\n",
      "2024-01-11 00:01:30.523833: train_loss -0.9511\n",
      "2024-01-11 00:01:30.533853: val_loss -0.8405\n",
      "2024-01-11 00:01:30.541850: Pseudo dice [0.8692, 0.9603, 0.9457]\n",
      "2024-01-11 00:01:30.568374: Epoch time: 40.57 s\n",
      "2024-01-11 00:01:32.069349: \n",
      "2024-01-11 00:01:32.080614: Epoch 711\n",
      "2024-01-11 00:01:32.085674: Current learning rate: 0.00327\n",
      "2024-01-11 00:02:12.332777: train_loss -0.9508\n",
      "2024-01-11 00:02:12.344786: val_loss -0.8386\n",
      "2024-01-11 00:02:12.382291: Pseudo dice [0.869, 0.9593, 0.9441]\n",
      "2024-01-11 00:02:12.391439: Epoch time: 40.26 s\n",
      "2024-01-11 00:02:13.855740: \n",
      "2024-01-11 00:02:13.861797: Epoch 712\n",
      "2024-01-11 00:02:13.866170: Current learning rate: 0.00326\n",
      "2024-01-11 00:02:54.046092: train_loss -0.9521\n",
      "2024-01-11 00:02:54.054092: val_loss -0.8351\n",
      "2024-01-11 00:02:54.084611: Pseudo dice [0.8655, 0.9587, 0.9436]\n",
      "2024-01-11 00:02:54.092608: Epoch time: 40.19 s\n",
      "2024-01-11 00:02:55.498538: \n",
      "2024-01-11 00:02:55.508047: Epoch 713\n",
      "2024-01-11 00:02:55.515078: Current learning rate: 0.00325\n",
      "2024-01-11 00:03:35.764490: train_loss -0.9514\n",
      "2024-01-11 00:03:35.776391: val_loss -0.8383\n",
      "2024-01-11 00:03:35.785394: Pseudo dice [0.8684, 0.9587, 0.9429]\n",
      "2024-01-11 00:03:35.791910: Epoch time: 40.27 s\n",
      "2024-01-11 00:03:37.291378: \n",
      "2024-01-11 00:03:37.300398: Epoch 714\n",
      "2024-01-11 00:03:37.305466: Current learning rate: 0.00324\n",
      "2024-01-11 00:04:17.558106: train_loss -0.951\n",
      "2024-01-11 00:04:17.567107: val_loss -0.8395\n",
      "2024-01-11 00:04:17.574110: Pseudo dice [0.8678, 0.9595, 0.9447]\n",
      "2024-01-11 00:04:17.581107: Epoch time: 40.27 s\n",
      "2024-01-11 00:04:19.053767: \n",
      "2024-01-11 00:04:19.063542: Epoch 715\n",
      "2024-01-11 00:04:19.074442: Current learning rate: 0.00323\n",
      "2024-01-11 00:04:59.384445: train_loss -0.9514\n",
      "2024-01-11 00:04:59.391441: val_loss -0.8349\n",
      "2024-01-11 00:04:59.412006: Pseudo dice [0.8674, 0.959, 0.9428]\n",
      "2024-01-11 00:04:59.418006: Epoch time: 40.33 s\n",
      "2024-01-11 00:05:01.025973: \n",
      "2024-01-11 00:05:01.031787: Epoch 716\n",
      "2024-01-11 00:05:01.036864: Current learning rate: 0.00322\n",
      "2024-01-11 00:05:41.244561: train_loss -0.9507\n",
      "2024-01-11 00:05:41.255562: val_loss -0.8387\n",
      "2024-01-11 00:05:41.263572: Pseudo dice [0.8704, 0.9594, 0.9442]\n",
      "2024-01-11 00:05:41.269561: Epoch time: 40.22 s\n",
      "2024-01-11 00:05:42.767926: \n",
      "2024-01-11 00:05:42.774453: Epoch 717\n",
      "2024-01-11 00:05:42.784532: Current learning rate: 0.00321\n",
      "2024-01-11 00:06:22.944560: train_loss -0.951\n",
      "2024-01-11 00:06:22.955092: val_loss -0.8344\n",
      "2024-01-11 00:06:22.961088: Pseudo dice [0.8642, 0.9594, 0.9438]\n",
      "2024-01-11 00:06:22.968601: Epoch time: 40.18 s\n",
      "2024-01-11 00:06:24.477418: \n",
      "2024-01-11 00:06:24.489197: Epoch 718\n",
      "2024-01-11 00:06:24.498250: Current learning rate: 0.0032\n",
      "2024-01-11 00:07:04.779030: train_loss -0.9512\n",
      "2024-01-11 00:07:04.808177: val_loss -0.8403\n",
      "2024-01-11 00:07:04.818168: Pseudo dice [0.8655, 0.9603, 0.9459]\n",
      "2024-01-11 00:07:04.827168: Epoch time: 40.3 s\n",
      "2024-01-11 00:07:06.312489: \n",
      "2024-01-11 00:07:06.318028: Epoch 719\n",
      "2024-01-11 00:07:06.323064: Current learning rate: 0.00319\n",
      "2024-01-11 00:07:46.671829: train_loss -0.9515\n",
      "2024-01-11 00:07:46.679831: val_loss -0.8382\n",
      "2024-01-11 00:07:46.689591: Pseudo dice [0.8681, 0.9593, 0.9442]\n",
      "2024-01-11 00:07:46.696593: Epoch time: 40.36 s\n",
      "2024-01-11 00:07:48.281522: \n",
      "2024-01-11 00:07:48.288017: Epoch 720\n",
      "2024-01-11 00:07:48.292102: Current learning rate: 0.00318\n",
      "2024-01-11 00:08:28.592624: train_loss -0.9517\n",
      "2024-01-11 00:08:28.601637: val_loss -0.8413\n",
      "2024-01-11 00:08:28.611774: Pseudo dice [0.8704, 0.9597, 0.9449]\n",
      "2024-01-11 00:08:28.624776: Epoch time: 40.31 s\n",
      "2024-01-11 00:08:30.126804: \n",
      "2024-01-11 00:08:30.132288: Epoch 721\n",
      "2024-01-11 00:08:30.137377: Current learning rate: 0.00317\n",
      "2024-01-11 00:09:10.515442: train_loss -0.9509\n",
      "2024-01-11 00:09:10.524444: val_loss -0.8342\n",
      "2024-01-11 00:09:10.532956: Pseudo dice [0.8651, 0.959, 0.9432]\n",
      "2024-01-11 00:09:10.541960: Epoch time: 40.39 s\n",
      "2024-01-11 00:09:12.046274: \n",
      "2024-01-11 00:09:12.052526: Epoch 722\n",
      "2024-01-11 00:09:12.060600: Current learning rate: 0.00316\n",
      "2024-01-11 00:09:52.322558: train_loss -0.9508\n",
      "2024-01-11 00:09:52.349588: val_loss -0.8317\n",
      "2024-01-11 00:09:52.359014: Pseudo dice [0.8612, 0.9589, 0.9431]\n",
      "2024-01-11 00:09:52.365015: Epoch time: 40.28 s\n",
      "2024-01-11 00:09:53.835994: \n",
      "2024-01-11 00:09:53.843113: Epoch 723\n",
      "2024-01-11 00:09:53.848128: Current learning rate: 0.00315\n",
      "2024-01-11 00:10:34.087638: train_loss -0.9509\n",
      "2024-01-11 00:10:34.096646: val_loss -0.8345\n",
      "2024-01-11 00:10:34.122984: Pseudo dice [0.863, 0.9595, 0.9447]\n",
      "2024-01-11 00:10:34.132982: Epoch time: 40.25 s\n",
      "2024-01-11 00:10:35.552552: \n",
      "2024-01-11 00:10:35.563616: Epoch 724\n",
      "2024-01-11 00:10:35.569590: Current learning rate: 0.00314\n",
      "2024-01-11 00:11:15.788970: train_loss -0.9512\n",
      "2024-01-11 00:11:15.794972: val_loss -0.8376\n",
      "2024-01-11 00:11:15.804970: Pseudo dice [0.8616, 0.96, 0.9454]\n",
      "2024-01-11 00:11:15.813976: Epoch time: 40.24 s\n",
      "2024-01-11 00:11:17.431160: \n",
      "2024-01-11 00:11:17.439125: Epoch 725\n",
      "2024-01-11 00:11:17.444133: Current learning rate: 0.00313\n",
      "2024-01-11 00:11:57.632684: train_loss -0.9512\n",
      "2024-01-11 00:11:57.640687: val_loss -0.8355\n",
      "2024-01-11 00:11:57.647191: Pseudo dice [0.8612, 0.9596, 0.9447]\n",
      "2024-01-11 00:11:57.654197: Epoch time: 40.2 s\n",
      "2024-01-11 00:11:59.172082: \n",
      "2024-01-11 00:11:59.182341: Epoch 726\n",
      "2024-01-11 00:11:59.191857: Current learning rate: 0.00312\n",
      "2024-01-11 00:12:40.063696: train_loss -0.951\n",
      "2024-01-11 00:12:40.073198: val_loss -0.8403\n",
      "2024-01-11 00:12:40.098204: Pseudo dice [0.8709, 0.9601, 0.9445]\n",
      "2024-01-11 00:12:40.107203: Epoch time: 40.89 s\n",
      "2024-01-11 00:12:41.579795: \n",
      "2024-01-11 00:12:41.591480: Epoch 727\n",
      "2024-01-11 00:12:41.598546: Current learning rate: 0.00311\n",
      "2024-01-11 00:13:22.368863: train_loss -0.9514\n",
      "2024-01-11 00:13:22.381876: val_loss -0.8395\n",
      "2024-01-11 00:13:22.405494: Pseudo dice [0.8745, 0.9595, 0.9452]\n",
      "2024-01-11 00:13:22.413495: Epoch time: 40.79 s\n",
      "2024-01-11 00:13:23.848285: \n",
      "2024-01-11 00:13:23.854357: Epoch 728\n",
      "2024-01-11 00:13:23.863422: Current learning rate: 0.0031\n",
      "2024-01-11 00:14:04.727009: train_loss -0.9517\n",
      "2024-01-11 00:14:04.736010: val_loss -0.8398\n",
      "2024-01-11 00:14:04.742009: Pseudo dice [0.8738, 0.9592, 0.9442]\n",
      "2024-01-11 00:14:04.748008: Epoch time: 40.88 s\n",
      "2024-01-11 00:14:06.204880: \n",
      "2024-01-11 00:14:06.211137: Epoch 729\n",
      "2024-01-11 00:14:06.218144: Current learning rate: 0.00309\n",
      "2024-01-11 00:14:46.947381: train_loss -0.9512\n",
      "2024-01-11 00:14:46.958384: val_loss -0.8413\n",
      "2024-01-11 00:14:46.966382: Pseudo dice [0.8756, 0.9599, 0.9452]\n",
      "2024-01-11 00:14:46.974383: Epoch time: 40.74 s\n",
      "2024-01-11 00:14:48.638951: \n",
      "2024-01-11 00:14:48.652197: Epoch 730\n",
      "2024-01-11 00:14:48.660255: Current learning rate: 0.00308\n",
      "2024-01-11 00:15:29.443776: train_loss -0.9516\n",
      "2024-01-11 00:15:29.454782: val_loss -0.839\n",
      "2024-01-11 00:15:29.461993: Pseudo dice [0.8699, 0.9594, 0.9439]\n",
      "2024-01-11 00:15:29.469508: Epoch time: 40.81 s\n",
      "2024-01-11 00:15:30.901435: \n",
      "2024-01-11 00:15:30.908617: Epoch 731\n",
      "2024-01-11 00:15:30.918636: Current learning rate: 0.00307\n",
      "2024-01-11 00:16:12.583471: train_loss -0.9519\n",
      "2024-01-11 00:16:12.614625: val_loss -0.8372\n",
      "2024-01-11 00:16:12.628987: Pseudo dice [0.8712, 0.9593, 0.9437]\n",
      "2024-01-11 00:16:12.640154: Epoch time: 41.68 s\n",
      "2024-01-11 00:16:14.389794: \n",
      "2024-01-11 00:16:14.395797: Epoch 732\n",
      "2024-01-11 00:16:14.401812: Current learning rate: 0.00306\n",
      "2024-01-11 00:16:58.664359: train_loss -0.952\n",
      "2024-01-11 00:16:58.677453: val_loss -0.8389\n",
      "2024-01-11 00:16:58.723758: Pseudo dice [0.8712, 0.9585, 0.9429]\n",
      "2024-01-11 00:16:58.734690: Epoch time: 44.28 s\n",
      "2024-01-11 00:17:00.448777: \n",
      "2024-01-11 00:17:00.454786: Epoch 733\n",
      "2024-01-11 00:17:00.460417: Current learning rate: 0.00305\n",
      "2024-01-11 00:17:44.648397: train_loss -0.9515\n",
      "2024-01-11 00:17:44.658396: val_loss -0.8381\n",
      "2024-01-11 00:17:44.664396: Pseudo dice [0.872, 0.9592, 0.9442]\n",
      "2024-01-11 00:17:44.671403: Epoch time: 44.2 s\n",
      "2024-01-11 00:17:46.234333: \n",
      "2024-01-11 00:17:46.239414: Epoch 734\n",
      "2024-01-11 00:17:46.244420: Current learning rate: 0.00304\n",
      "2024-01-11 00:18:30.013697: train_loss -0.9517\n",
      "2024-01-11 00:18:30.026197: val_loss -0.836\n",
      "2024-01-11 00:18:30.039670: Pseudo dice [0.8685, 0.9589, 0.9437]\n",
      "2024-01-11 00:18:30.059664: Epoch time: 43.78 s\n",
      "2024-01-11 00:18:32.010311: \n",
      "2024-01-11 00:18:32.016831: Epoch 735\n",
      "2024-01-11 00:18:32.023843: Current learning rate: 0.00303\n",
      "2024-01-11 00:19:14.746519: train_loss -0.9516\n",
      "2024-01-11 00:19:14.754030: val_loss -0.838\n",
      "2024-01-11 00:19:14.762030: Pseudo dice [0.8726, 0.9592, 0.9449]\n",
      "2024-01-11 00:19:14.797032: Epoch time: 42.74 s\n",
      "2024-01-11 00:19:16.197305: \n",
      "2024-01-11 00:19:16.205311: Epoch 736\n",
      "2024-01-11 00:19:16.211306: Current learning rate: 0.00302\n",
      "2024-01-11 00:19:58.404818: train_loss -0.9513\n",
      "2024-01-11 00:19:58.443821: val_loss -0.8385\n",
      "2024-01-11 00:19:58.454817: Pseudo dice [0.8696, 0.9592, 0.9445]\n",
      "2024-01-11 00:19:58.465823: Epoch time: 42.21 s\n",
      "2024-01-11 00:20:00.023226: \n",
      "2024-01-11 00:20:00.029162: Epoch 737\n",
      "2024-01-11 00:20:00.034238: Current learning rate: 0.00301\n",
      "2024-01-11 00:20:41.676415: train_loss -0.9516\n",
      "2024-01-11 00:20:41.688427: val_loss -0.8432\n",
      "2024-01-11 00:20:41.698421: Pseudo dice [0.871, 0.9605, 0.9452]\n",
      "2024-01-11 00:20:41.706961: Epoch time: 41.65 s\n",
      "2024-01-11 00:20:43.250127: \n",
      "2024-01-11 00:20:43.256115: Epoch 738\n",
      "2024-01-11 00:20:43.261125: Current learning rate: 0.003\n",
      "2024-01-11 00:21:24.639875: train_loss -0.951\n",
      "2024-01-11 00:21:24.687878: val_loss -0.8373\n",
      "2024-01-11 00:21:24.699404: Pseudo dice [0.875, 0.9587, 0.9421]\n",
      "2024-01-11 00:21:24.708402: Epoch time: 41.39 s\n",
      "2024-01-11 00:21:26.105494: \n",
      "2024-01-11 00:21:26.116488: Epoch 739\n",
      "2024-01-11 00:21:26.122483: Current learning rate: 0.00299\n",
      "2024-01-11 00:22:08.276729: train_loss -0.9515\n",
      "2024-01-11 00:22:08.287730: val_loss -0.8374\n",
      "2024-01-11 00:22:08.295731: Pseudo dice [0.8705, 0.9596, 0.9445]\n",
      "2024-01-11 00:22:08.303729: Epoch time: 42.17 s\n",
      "2024-01-11 00:22:09.987535: \n",
      "2024-01-11 00:22:09.994546: Epoch 740\n",
      "2024-01-11 00:22:10.001536: Current learning rate: 0.00297\n",
      "2024-01-11 00:22:52.602954: train_loss -0.9519\n",
      "2024-01-11 00:22:52.612468: val_loss -0.8398\n",
      "2024-01-11 00:22:52.621473: Pseudo dice [0.8692, 0.9596, 0.9435]\n",
      "2024-01-11 00:22:52.632468: Epoch time: 42.62 s\n",
      "2024-01-11 00:22:54.413912: \n",
      "2024-01-11 00:22:54.420929: Epoch 741\n",
      "2024-01-11 00:22:54.428911: Current learning rate: 0.00296\n",
      "2024-01-11 00:23:36.575537: train_loss -0.9514\n",
      "2024-01-11 00:23:36.590071: val_loss -0.8405\n",
      "2024-01-11 00:23:36.633495: Pseudo dice [0.8648, 0.9595, 0.9442]\n",
      "2024-01-11 00:23:36.650931: Epoch time: 42.16 s\n",
      "2024-01-11 00:23:38.285848: \n",
      "2024-01-11 00:23:38.292775: Epoch 742\n",
      "2024-01-11 00:23:38.298785: Current learning rate: 0.00295\n",
      "2024-01-11 00:24:19.372691: train_loss -0.9516\n",
      "2024-01-11 00:24:19.405749: val_loss -0.8385\n",
      "2024-01-11 00:24:19.416007: Pseudo dice [0.8708, 0.9587, 0.9431]\n",
      "2024-01-11 00:24:19.425004: Epoch time: 41.09 s\n",
      "2024-01-11 00:24:20.973289: \n",
      "2024-01-11 00:24:20.982807: Epoch 743\n",
      "2024-01-11 00:24:20.987752: Current learning rate: 0.00294\n",
      "2024-01-11 00:25:03.773502: train_loss -0.9519\n",
      "2024-01-11 00:25:03.785503: val_loss -0.8334\n",
      "2024-01-11 00:25:03.793510: Pseudo dice [0.8661, 0.9592, 0.9433]\n",
      "2024-01-11 00:25:03.802011: Epoch time: 42.8 s\n",
      "2024-01-11 00:25:05.314451: \n",
      "2024-01-11 00:25:05.327299: Epoch 744\n",
      "2024-01-11 00:25:05.333377: Current learning rate: 0.00293\n",
      "2024-01-11 00:25:47.061050: train_loss -0.9517\n",
      "2024-01-11 00:25:47.069050: val_loss -0.8365\n",
      "2024-01-11 00:25:47.078050: Pseudo dice [0.8701, 0.9585, 0.9425]\n",
      "2024-01-11 00:25:47.085050: Epoch time: 41.75 s\n",
      "2024-01-11 00:25:48.678989: \n",
      "2024-01-11 00:25:48.685967: Epoch 745\n",
      "2024-01-11 00:25:48.691960: Current learning rate: 0.00292\n",
      "2024-01-11 00:26:29.486738: train_loss -0.9515\n",
      "2024-01-11 00:26:29.496736: val_loss -0.837\n",
      "2024-01-11 00:26:29.507736: Pseudo dice [0.8677, 0.9595, 0.9438]\n",
      "2024-01-11 00:26:29.515737: Epoch time: 40.81 s\n",
      "2024-01-11 00:26:30.996965: \n",
      "2024-01-11 00:26:31.005228: Epoch 746\n",
      "2024-01-11 00:26:31.009316: Current learning rate: 0.00291\n",
      "2024-01-11 00:27:12.310262: train_loss -0.9514\n",
      "2024-01-11 00:27:12.344262: val_loss -0.8436\n",
      "2024-01-11 00:27:12.353261: Pseudo dice [0.8756, 0.9604, 0.9451]\n",
      "2024-01-11 00:27:12.361261: Epoch time: 41.31 s\n",
      "2024-01-11 00:27:14.132549: \n",
      "2024-01-11 00:27:14.139549: Epoch 747\n",
      "2024-01-11 00:27:14.144550: Current learning rate: 0.0029\n",
      "2024-01-11 00:27:56.647476: train_loss -0.9517\n",
      "2024-01-11 00:27:56.673043: val_loss -0.8388\n",
      "2024-01-11 00:27:56.726564: Pseudo dice [0.8731, 0.9599, 0.9444]\n",
      "2024-01-11 00:27:56.759380: Epoch time: 42.52 s\n",
      "2024-01-11 00:27:58.354954: \n",
      "2024-01-11 00:27:58.361951: Epoch 748\n",
      "2024-01-11 00:27:58.368951: Current learning rate: 0.00289\n",
      "2024-01-11 00:28:42.212536: train_loss -0.9521\n",
      "2024-01-11 00:28:42.226805: val_loss -0.8408\n",
      "2024-01-11 00:28:42.238291: Pseudo dice [0.8678, 0.9599, 0.945]\n",
      "2024-01-11 00:28:42.250042: Epoch time: 43.86 s\n",
      "2024-01-11 00:28:44.020557: \n",
      "2024-01-11 00:28:44.029547: Epoch 749\n",
      "2024-01-11 00:28:44.035573: Current learning rate: 0.00288\n",
      "2024-01-11 00:29:26.986950: train_loss -0.9513\n",
      "2024-01-11 00:29:26.995951: val_loss -0.8366\n",
      "2024-01-11 00:29:27.003088: Pseudo dice [0.868, 0.9584, 0.9421]\n",
      "2024-01-11 00:29:27.009088: Epoch time: 42.97 s\n",
      "2024-01-11 00:29:28.926130: \n",
      "2024-01-11 00:29:28.932118: Epoch 750\n",
      "2024-01-11 00:29:28.940052: Current learning rate: 0.00287\n",
      "2024-01-11 00:30:11.245589: train_loss -0.9519\n",
      "2024-01-11 00:30:11.255592: val_loss -0.8363\n",
      "2024-01-11 00:30:11.264798: Pseudo dice [0.8661, 0.9591, 0.9431]\n",
      "2024-01-11 00:30:11.294919: Epoch time: 42.32 s\n",
      "2024-01-11 00:30:12.893009: \n",
      "2024-01-11 00:30:12.900007: Epoch 751\n",
      "2024-01-11 00:30:12.910030: Current learning rate: 0.00286\n",
      "2024-01-11 00:30:55.903840: train_loss -0.9511\n",
      "2024-01-11 00:30:55.913841: val_loss -0.8381\n",
      "2024-01-11 00:30:55.945865: Pseudo dice [0.8709, 0.959, 0.9436]\n",
      "2024-01-11 00:30:55.956858: Epoch time: 43.01 s\n",
      "2024-01-11 00:30:57.661633: \n",
      "2024-01-11 00:30:57.667457: Epoch 752\n",
      "2024-01-11 00:30:57.673356: Current learning rate: 0.00285\n",
      "2024-01-11 00:31:38.864248: train_loss -0.9523\n",
      "2024-01-11 00:31:38.875566: val_loss -0.8412\n",
      "2024-01-11 00:31:38.885079: Pseudo dice [0.8718, 0.9599, 0.9444]\n",
      "2024-01-11 00:31:38.892080: Epoch time: 41.2 s\n",
      "2024-01-11 00:31:40.363802: \n",
      "2024-01-11 00:31:40.370810: Epoch 753\n",
      "2024-01-11 00:31:40.375808: Current learning rate: 0.00284\n",
      "2024-01-11 00:32:22.033298: train_loss -0.9513\n",
      "2024-01-11 00:32:22.043298: val_loss -0.8386\n",
      "2024-01-11 00:32:22.053298: Pseudo dice [0.87, 0.9599, 0.9451]\n",
      "2024-01-11 00:32:22.065505: Epoch time: 41.67 s\n",
      "2024-01-11 00:32:24.088625: \n",
      "2024-01-11 00:32:24.097622: Epoch 754\n",
      "2024-01-11 00:32:24.104175: Current learning rate: 0.00283\n",
      "2024-01-11 00:33:06.951061: train_loss -0.9515\n",
      "2024-01-11 00:33:06.968882: val_loss -0.8335\n",
      "2024-01-11 00:33:06.981400: Pseudo dice [0.8629, 0.9588, 0.943]\n",
      "2024-01-11 00:33:06.991638: Epoch time: 42.86 s\n",
      "2024-01-11 00:33:08.891721: \n",
      "2024-01-11 00:33:08.900448: Epoch 755\n",
      "2024-01-11 00:33:08.907054: Current learning rate: 0.00282\n",
      "2024-01-11 00:33:51.818633: train_loss -0.952\n",
      "2024-01-11 00:33:51.825639: val_loss -0.8372\n",
      "2024-01-11 00:33:51.834633: Pseudo dice [0.8712, 0.959, 0.9441]\n",
      "2024-01-11 00:33:51.842633: Epoch time: 42.93 s\n",
      "2024-01-11 00:33:53.466234: \n",
      "2024-01-11 00:33:53.472627: Epoch 756\n",
      "2024-01-11 00:33:53.480307: Current learning rate: 0.00281\n",
      "2024-01-11 00:34:35.800463: train_loss -0.9518\n",
      "2024-01-11 00:34:35.815828: val_loss -0.837\n",
      "2024-01-11 00:34:35.829192: Pseudo dice [0.8685, 0.9596, 0.9442]\n",
      "2024-01-11 00:34:35.839190: Epoch time: 42.34 s\n",
      "2024-01-11 00:34:37.703161: \n",
      "2024-01-11 00:34:37.710257: Epoch 757\n",
      "2024-01-11 00:34:37.715265: Current learning rate: 0.0028\n",
      "2024-01-11 00:35:19.057707: train_loss -0.952\n",
      "2024-01-11 00:35:19.068706: val_loss -0.8343\n",
      "2024-01-11 00:35:19.077709: Pseudo dice [0.8648, 0.9597, 0.9446]\n",
      "2024-01-11 00:35:19.085707: Epoch time: 41.36 s\n",
      "2024-01-11 00:35:20.499786: \n",
      "2024-01-11 00:35:20.507884: Epoch 758\n",
      "2024-01-11 00:35:20.513804: Current learning rate: 0.00279\n",
      "2024-01-11 00:36:01.552465: train_loss -0.9524\n",
      "2024-01-11 00:36:01.562470: val_loss -0.837\n",
      "2024-01-11 00:36:01.568977: Pseudo dice [0.8693, 0.9586, 0.9431]\n",
      "2024-01-11 00:36:01.574978: Epoch time: 41.05 s\n",
      "2024-01-11 00:36:03.017086: \n",
      "2024-01-11 00:36:03.024808: Epoch 759\n",
      "2024-01-11 00:36:03.032880: Current learning rate: 0.00278\n",
      "2024-01-11 00:36:44.174585: train_loss -0.9525\n",
      "2024-01-11 00:36:44.184282: val_loss -0.8373\n",
      "2024-01-11 00:36:44.195979: Pseudo dice [0.8666, 0.9597, 0.944]\n",
      "2024-01-11 00:36:44.207272: Epoch time: 41.16 s\n",
      "2024-01-11 00:36:45.846212: \n",
      "2024-01-11 00:36:45.852203: Epoch 760\n",
      "2024-01-11 00:36:45.857203: Current learning rate: 0.00277\n",
      "2024-01-11 00:37:26.591222: train_loss -0.9519\n",
      "2024-01-11 00:37:26.599225: val_loss -0.8374\n",
      "2024-01-11 00:37:26.606222: Pseudo dice [0.8674, 0.9598, 0.9449]\n",
      "2024-01-11 00:37:26.613221: Epoch time: 40.75 s\n",
      "2024-01-11 00:37:28.041538: \n",
      "2024-01-11 00:37:28.053485: Epoch 761\n",
      "2024-01-11 00:37:28.059480: Current learning rate: 0.00276\n",
      "2024-01-11 00:38:10.145502: train_loss -0.9513\n",
      "2024-01-11 00:38:10.155521: val_loss -0.8379\n",
      "2024-01-11 00:38:10.167038: Pseudo dice [0.8641, 0.9594, 0.9435]\n",
      "2024-01-11 00:38:10.196298: Epoch time: 42.11 s\n",
      "2024-01-11 00:38:11.711864: \n",
      "2024-01-11 00:38:11.719374: Epoch 762\n",
      "2024-01-11 00:38:11.724380: Current learning rate: 0.00275\n",
      "2024-01-11 00:38:53.283804: train_loss -0.9514\n",
      "2024-01-11 00:38:53.296835: val_loss -0.837\n",
      "2024-01-11 00:38:53.305423: Pseudo dice [0.863, 0.9593, 0.9441]\n",
      "2024-01-11 00:38:53.312354: Epoch time: 41.57 s\n",
      "2024-01-11 00:38:55.151168: \n",
      "2024-01-11 00:38:55.157696: Epoch 763\n",
      "2024-01-11 00:38:55.164222: Current learning rate: 0.00274\n",
      "2024-01-11 00:39:37.114232: train_loss -0.9517\n",
      "2024-01-11 00:39:37.126230: val_loss -0.831\n",
      "2024-01-11 00:39:37.136233: Pseudo dice [0.8641, 0.9584, 0.9426]\n",
      "2024-01-11 00:39:37.148232: Epoch time: 41.96 s\n",
      "2024-01-11 00:39:38.958845: \n",
      "2024-01-11 00:39:38.966849: Epoch 764\n",
      "2024-01-11 00:39:38.971853: Current learning rate: 0.00273\n",
      "2024-01-11 00:40:21.278030: train_loss -0.9517\n",
      "2024-01-11 00:40:21.286030: val_loss -0.8366\n",
      "2024-01-11 00:40:21.295029: Pseudo dice [0.8678, 0.9589, 0.943]\n",
      "2024-01-11 00:40:21.329033: Epoch time: 42.32 s\n",
      "2024-01-11 00:40:22.837231: \n",
      "2024-01-11 00:40:22.844104: Epoch 765\n",
      "2024-01-11 00:40:22.850316: Current learning rate: 0.00272\n",
      "2024-01-11 00:41:04.611425: train_loss -0.9516\n",
      "2024-01-11 00:41:04.648434: val_loss -0.8373\n",
      "2024-01-11 00:41:04.657425: Pseudo dice [0.8679, 0.9592, 0.9437]\n",
      "2024-01-11 00:41:04.666533: Epoch time: 41.78 s\n",
      "2024-01-11 00:41:06.223448: \n",
      "2024-01-11 00:41:06.232918: Epoch 766\n",
      "2024-01-11 00:41:06.243139: Current learning rate: 0.00271\n",
      "2024-01-11 00:41:47.896765: train_loss -0.9521\n",
      "2024-01-11 00:41:47.915427: val_loss -0.8301\n",
      "2024-01-11 00:41:47.931599: Pseudo dice [0.864, 0.9581, 0.9417]\n",
      "2024-01-11 00:41:47.944084: Epoch time: 41.67 s\n",
      "2024-01-11 00:41:49.894762: \n",
      "2024-01-11 00:41:49.904776: Epoch 767\n",
      "2024-01-11 00:41:49.911775: Current learning rate: 0.0027\n",
      "2024-01-11 00:42:32.175563: train_loss -0.9515\n",
      "2024-01-11 00:42:32.194218: val_loss -0.8341\n",
      "2024-01-11 00:42:32.214467: Pseudo dice [0.8696, 0.9585, 0.9435]\n",
      "2024-01-11 00:42:32.233869: Epoch time: 42.28 s\n",
      "2024-01-11 00:42:33.960564: \n",
      "2024-01-11 00:42:33.967558: Epoch 768\n",
      "2024-01-11 00:42:33.972564: Current learning rate: 0.00268\n",
      "2024-01-11 00:43:16.460853: train_loss -0.9521\n",
      "2024-01-11 00:43:16.468863: val_loss -0.8327\n",
      "2024-01-11 00:43:16.503393: Pseudo dice [0.8651, 0.9583, 0.9424]\n",
      "2024-01-11 00:43:16.511388: Epoch time: 42.5 s\n",
      "2024-01-11 00:43:18.311729: \n",
      "2024-01-11 00:43:18.317730: Epoch 769\n",
      "2024-01-11 00:43:18.323729: Current learning rate: 0.00267\n",
      "2024-01-11 00:43:59.941123: train_loss -0.9528\n",
      "2024-01-11 00:43:59.952634: val_loss -0.8343\n",
      "2024-01-11 00:43:59.964662: Pseudo dice [0.8681, 0.9587, 0.9424]\n",
      "2024-01-11 00:44:00.002405: Epoch time: 41.63 s\n",
      "2024-01-11 00:44:01.738347: \n",
      "2024-01-11 00:44:01.746927: Epoch 770\n",
      "2024-01-11 00:44:01.751920: Current learning rate: 0.00266\n",
      "2024-01-11 00:44:44.041907: train_loss -0.9523\n",
      "2024-01-11 00:44:44.084279: val_loss -0.8332\n",
      "2024-01-11 00:44:44.095525: Pseudo dice [0.8668, 0.9584, 0.9429]\n",
      "2024-01-11 00:44:44.108478: Epoch time: 42.3 s\n",
      "2024-01-11 00:44:45.834503: \n",
      "2024-01-11 00:44:45.841527: Epoch 771\n",
      "2024-01-11 00:44:45.848067: Current learning rate: 0.00265\n",
      "2024-01-11 00:45:27.370892: train_loss -0.953\n",
      "2024-01-11 00:45:27.384894: val_loss -0.8354\n",
      "2024-01-11 00:45:27.397741: Pseudo dice [0.8637, 0.9593, 0.9438]\n",
      "2024-01-11 00:45:27.410740: Epoch time: 41.54 s\n",
      "2024-01-11 00:45:29.006460: \n",
      "2024-01-11 00:45:29.012460: Epoch 772\n",
      "2024-01-11 00:45:29.017460: Current learning rate: 0.00264\n",
      "2024-01-11 00:46:10.451303: train_loss -0.9521\n",
      "2024-01-11 00:46:10.461302: val_loss -0.8404\n",
      "2024-01-11 00:46:10.467309: Pseudo dice [0.8728, 0.9598, 0.9449]\n",
      "2024-01-11 00:46:10.474308: Epoch time: 41.45 s\n",
      "2024-01-11 00:46:12.086390: \n",
      "2024-01-11 00:46:12.091857: Epoch 773\n",
      "2024-01-11 00:46:12.096798: Current learning rate: 0.00263\n",
      "2024-01-11 00:46:53.436728: train_loss -0.9517\n",
      "2024-01-11 00:46:53.446726: val_loss -0.8365\n",
      "2024-01-11 00:46:53.480872: Pseudo dice [0.8677, 0.9593, 0.9433]\n",
      "2024-01-11 00:46:53.490133: Epoch time: 41.35 s\n",
      "2024-01-11 00:46:55.239965: \n",
      "2024-01-11 00:46:55.247964: Epoch 774\n",
      "2024-01-11 00:46:55.253962: Current learning rate: 0.00262\n",
      "2024-01-11 00:47:37.362964: train_loss -0.9525\n",
      "2024-01-11 00:47:37.372964: val_loss -0.8395\n",
      "2024-01-11 00:47:37.382964: Pseudo dice [0.8706, 0.9602, 0.9453]\n",
      "2024-01-11 00:47:37.390962: Epoch time: 42.12 s\n",
      "2024-01-11 00:47:38.877432: \n",
      "2024-01-11 00:47:38.887444: Epoch 775\n",
      "2024-01-11 00:47:38.892514: Current learning rate: 0.00261\n",
      "2024-01-11 00:48:21.811405: train_loss -0.9526\n",
      "2024-01-11 00:48:21.823854: val_loss -0.8362\n",
      "2024-01-11 00:48:21.864517: Pseudo dice [0.8688, 0.9588, 0.9434]\n",
      "2024-01-11 00:48:21.874058: Epoch time: 42.93 s\n",
      "2024-01-11 00:48:23.484703: \n",
      "2024-01-11 00:48:23.491225: Epoch 776\n",
      "2024-01-11 00:48:23.496225: Current learning rate: 0.0026\n",
      "2024-01-11 00:49:05.159832: train_loss -0.9524\n",
      "2024-01-11 00:49:05.173070: val_loss -0.8392\n",
      "2024-01-11 00:49:05.185272: Pseudo dice [0.8699, 0.9591, 0.9436]\n",
      "2024-01-11 00:49:05.194292: Epoch time: 41.68 s\n",
      "2024-01-11 00:49:06.753516: \n",
      "2024-01-11 00:49:06.759052: Epoch 777\n",
      "2024-01-11 00:49:06.765053: Current learning rate: 0.00259\n",
      "2024-01-11 00:49:48.327785: train_loss -0.9529\n",
      "2024-01-11 00:49:48.337301: val_loss -0.8399\n",
      "2024-01-11 00:49:48.346307: Pseudo dice [0.8692, 0.96, 0.9447]\n",
      "2024-01-11 00:49:48.353819: Epoch time: 41.58 s\n",
      "2024-01-11 00:49:49.946649: \n",
      "2024-01-11 00:49:49.954479: Epoch 778\n",
      "2024-01-11 00:49:49.960144: Current learning rate: 0.00258\n",
      "2024-01-11 00:50:31.669799: train_loss -0.9525\n",
      "2024-01-11 00:50:31.680827: val_loss -0.8383\n",
      "2024-01-11 00:50:31.688335: Pseudo dice [0.8699, 0.9593, 0.9437]\n",
      "2024-01-11 00:50:31.695349: Epoch time: 41.72 s\n",
      "2024-01-11 00:50:33.173042: \n",
      "2024-01-11 00:50:33.180562: Epoch 779\n",
      "2024-01-11 00:50:33.187672: Current learning rate: 0.00257\n",
      "2024-01-11 00:51:14.736017: train_loss -0.9525\n",
      "2024-01-11 00:51:14.776550: val_loss -0.8364\n",
      "2024-01-11 00:51:14.785078: Pseudo dice [0.8668, 0.9594, 0.9444]\n",
      "2024-01-11 00:51:14.794925: Epoch time: 41.56 s\n",
      "2024-01-11 00:51:16.350814: \n",
      "2024-01-11 00:51:16.360960: Epoch 780\n",
      "2024-01-11 00:51:16.372582: Current learning rate: 0.00256\n",
      "2024-01-11 00:51:57.615224: train_loss -0.9533\n",
      "2024-01-11 00:51:57.623744: val_loss -0.8403\n",
      "2024-01-11 00:51:57.632275: Pseudo dice [0.8734, 0.9596, 0.9441]\n",
      "2024-01-11 00:51:57.656466: Epoch time: 41.27 s\n",
      "2024-01-11 00:51:59.185220: \n",
      "2024-01-11 00:51:59.192745: Epoch 781\n",
      "2024-01-11 00:51:59.199057: Current learning rate: 0.00255\n",
      "2024-01-11 00:52:40.529944: train_loss -0.9523\n",
      "2024-01-11 00:52:40.539469: val_loss -0.8377\n",
      "2024-01-11 00:52:40.550437: Pseudo dice [0.8719, 0.9599, 0.9443]\n",
      "2024-01-11 00:52:40.559487: Epoch time: 41.35 s\n",
      "2024-01-11 00:52:42.147380: \n",
      "2024-01-11 00:52:42.157828: Epoch 782\n",
      "2024-01-11 00:52:42.162981: Current learning rate: 0.00254\n",
      "2024-01-11 00:53:23.516755: train_loss -0.9519\n",
      "2024-01-11 00:53:23.528790: val_loss -0.8384\n",
      "2024-01-11 00:53:23.537299: Pseudo dice [0.8686, 0.9596, 0.9441]\n",
      "2024-01-11 00:53:23.544311: Epoch time: 41.37 s\n",
      "2024-01-11 00:53:25.242227: \n",
      "2024-01-11 00:53:25.249739: Epoch 783\n",
      "2024-01-11 00:53:25.254802: Current learning rate: 0.00253\n",
      "2024-01-11 00:54:06.609457: train_loss -0.9521\n",
      "2024-01-11 00:54:06.620981: val_loss -0.8383\n",
      "2024-01-11 00:54:06.632496: Pseudo dice [0.8693, 0.9593, 0.9443]\n",
      "2024-01-11 00:54:06.643554: Epoch time: 41.37 s\n",
      "2024-01-11 00:54:08.158196: \n",
      "2024-01-11 00:54:08.166217: Epoch 784\n",
      "2024-01-11 00:54:08.171732: Current learning rate: 0.00252\n",
      "2024-01-11 00:54:49.452184: train_loss -0.9521\n",
      "2024-01-11 00:54:49.488763: val_loss -0.8381\n",
      "2024-01-11 00:54:49.500345: Pseudo dice [0.869, 0.9596, 0.9442]\n",
      "2024-01-11 00:54:49.510204: Epoch time: 41.3 s\n",
      "2024-01-11 00:54:51.091374: \n",
      "2024-01-11 00:54:51.097402: Epoch 785\n",
      "2024-01-11 00:54:51.102672: Current learning rate: 0.00251\n",
      "2024-01-11 00:55:32.363818: train_loss -0.9524\n",
      "2024-01-11 00:55:32.379001: val_loss -0.8318\n",
      "2024-01-11 00:55:32.408018: Pseudo dice [0.8655, 0.9581, 0.9422]\n",
      "2024-01-11 00:55:32.418036: Epoch time: 41.27 s\n",
      "2024-01-11 00:55:33.996054: \n",
      "2024-01-11 00:55:34.002103: Epoch 786\n",
      "2024-01-11 00:55:34.007631: Current learning rate: 0.0025\n",
      "2024-01-11 00:56:15.430187: train_loss -0.9528\n",
      "2024-01-11 00:56:15.474099: val_loss -0.8411\n",
      "2024-01-11 00:56:15.486011: Pseudo dice [0.8684, 0.9599, 0.9455]\n",
      "2024-01-11 00:56:15.495545: Epoch time: 41.44 s\n",
      "2024-01-11 00:56:17.075611: \n",
      "2024-01-11 00:56:17.083394: Epoch 787\n",
      "2024-01-11 00:56:17.088621: Current learning rate: 0.00249\n",
      "2024-01-11 00:56:58.645303: train_loss -0.9524\n",
      "2024-01-11 00:56:58.658168: val_loss -0.8344\n",
      "2024-01-11 00:56:58.668687: Pseudo dice [0.8661, 0.9594, 0.9446]\n",
      "2024-01-11 00:56:58.707592: Epoch time: 41.57 s\n",
      "2024-01-11 00:57:00.401280: \n",
      "2024-01-11 00:57:00.406276: Epoch 788\n",
      "2024-01-11 00:57:00.412791: Current learning rate: 0.00248\n",
      "2024-01-11 00:57:41.688205: train_loss -0.9523\n",
      "2024-01-11 00:57:41.698228: val_loss -0.8378\n",
      "2024-01-11 00:57:41.709766: Pseudo dice [0.8654, 0.96, 0.9454]\n",
      "2024-01-11 00:57:41.718192: Epoch time: 41.29 s\n",
      "2024-01-11 00:57:43.239651: \n",
      "2024-01-11 00:57:43.249841: Epoch 789\n",
      "2024-01-11 00:57:43.254909: Current learning rate: 0.00247\n",
      "2024-01-11 00:58:24.034382: train_loss -0.9525\n",
      "2024-01-11 00:58:24.046370: val_loss -0.8403\n",
      "2024-01-11 00:58:24.058369: Pseudo dice [0.8704, 0.9602, 0.9452]\n",
      "2024-01-11 00:58:24.093371: Epoch time: 40.8 s\n",
      "2024-01-11 00:58:25.511663: \n",
      "2024-01-11 00:58:25.521322: Epoch 790\n",
      "2024-01-11 00:58:25.526265: Current learning rate: 0.00245\n",
      "2024-01-11 00:59:06.015480: train_loss -0.9535\n",
      "2024-01-11 00:59:06.027485: val_loss -0.8401\n",
      "2024-01-11 00:59:06.037485: Pseudo dice [0.8742, 0.9598, 0.9455]\n",
      "2024-01-11 00:59:06.069997: Epoch time: 40.5 s\n",
      "2024-01-11 00:59:07.578177: \n",
      "2024-01-11 00:59:07.585264: Epoch 791\n",
      "2024-01-11 00:59:07.589355: Current learning rate: 0.00244\n",
      "2024-01-11 00:59:48.108323: train_loss -0.9531\n",
      "2024-01-11 00:59:48.117323: val_loss -0.8401\n",
      "2024-01-11 00:59:48.125324: Pseudo dice [0.8711, 0.9597, 0.9446]\n",
      "2024-01-11 00:59:48.159897: Epoch time: 40.53 s\n",
      "2024-01-11 00:59:49.683038: \n",
      "2024-01-11 00:59:49.690014: Epoch 792\n",
      "2024-01-11 00:59:49.695035: Current learning rate: 0.00243\n",
      "2024-01-11 01:00:30.311539: train_loss -0.9527\n",
      "2024-01-11 01:00:30.322539: val_loss -0.8394\n",
      "2024-01-11 01:00:30.331540: Pseudo dice [0.8735, 0.959, 0.9438]\n",
      "2024-01-11 01:00:30.358546: Epoch time: 40.63 s\n",
      "2024-01-11 01:00:31.942190: \n",
      "2024-01-11 01:00:31.948368: Epoch 793\n",
      "2024-01-11 01:00:31.952447: Current learning rate: 0.00242\n",
      "2024-01-11 01:01:12.466072: train_loss -0.9532\n",
      "2024-01-11 01:01:12.492503: val_loss -0.8379\n",
      "2024-01-11 01:01:12.500016: Pseudo dice [0.8736, 0.9595, 0.9447]\n",
      "2024-01-11 01:01:12.507012: Epoch time: 40.52 s\n",
      "2024-01-11 01:01:13.997812: \n",
      "2024-01-11 01:01:14.004106: Epoch 794\n",
      "2024-01-11 01:01:14.011081: Current learning rate: 0.00241\n",
      "2024-01-11 01:01:54.723048: train_loss -0.953\n",
      "2024-01-11 01:01:54.733047: val_loss -0.8402\n",
      "2024-01-11 01:01:54.744047: Pseudo dice [0.8762, 0.9595, 0.9449]\n",
      "2024-01-11 01:01:54.753047: Epoch time: 40.73 s\n",
      "2024-01-11 01:01:56.211259: \n",
      "2024-01-11 01:01:56.218719: Epoch 795\n",
      "2024-01-11 01:01:56.226152: Current learning rate: 0.0024\n",
      "2024-01-11 01:02:36.802882: train_loss -0.9523\n",
      "2024-01-11 01:02:36.809883: val_loss -0.8393\n",
      "2024-01-11 01:02:36.822882: Pseudo dice [0.8699, 0.9591, 0.9432]\n",
      "2024-01-11 01:02:36.832886: Epoch time: 40.59 s\n",
      "2024-01-11 01:02:38.328336: \n",
      "2024-01-11 01:02:38.334721: Epoch 796\n",
      "2024-01-11 01:02:38.339331: Current learning rate: 0.00239\n",
      "2024-01-11 01:03:18.640633: train_loss -0.9533\n",
      "2024-01-11 01:03:18.649638: val_loss -0.8348\n",
      "2024-01-11 01:03:18.658638: Pseudo dice [0.8642, 0.9592, 0.9435]\n",
      "2024-01-11 01:03:18.666146: Epoch time: 40.31 s\n",
      "2024-01-11 01:03:20.147868: \n",
      "2024-01-11 01:03:20.159707: Epoch 797\n",
      "2024-01-11 01:03:20.168212: Current learning rate: 0.00238\n",
      "2024-01-11 01:04:00.541352: train_loss -0.9527\n",
      "2024-01-11 01:04:00.550352: val_loss -0.8369\n",
      "2024-01-11 01:04:00.560354: Pseudo dice [0.8654, 0.9594, 0.9438]\n",
      "2024-01-11 01:04:00.567355: Epoch time: 40.39 s\n",
      "2024-01-11 01:04:02.217721: \n",
      "2024-01-11 01:04:02.226652: Epoch 798\n",
      "2024-01-11 01:04:02.232651: Current learning rate: 0.00237\n",
      "2024-01-11 01:04:43.309300: train_loss -0.9535\n",
      "2024-01-11 01:04:43.327117: val_loss -0.8376\n",
      "2024-01-11 01:04:43.365258: Pseudo dice [0.8673, 0.9594, 0.9442]\n",
      "2024-01-11 01:04:43.378304: Epoch time: 41.09 s\n",
      "2024-01-11 01:04:44.987249: \n",
      "2024-01-11 01:04:44.993003: Epoch 799\n",
      "2024-01-11 01:04:44.998034: Current learning rate: 0.00236\n",
      "2024-01-11 01:05:25.339482: train_loss -0.9536\n",
      "2024-01-11 01:05:25.349482: val_loss -0.8365\n",
      "2024-01-11 01:05:25.357481: Pseudo dice [0.8657, 0.9588, 0.943]\n",
      "2024-01-11 01:05:25.367481: Epoch time: 40.35 s\n",
      "2024-01-11 01:05:27.182914: \n",
      "2024-01-11 01:05:27.188914: Epoch 800\n",
      "2024-01-11 01:05:27.194914: Current learning rate: 0.00235\n",
      "2024-01-11 01:06:07.767158: train_loss -0.9528\n",
      "2024-01-11 01:06:07.783158: val_loss -0.8409\n",
      "2024-01-11 01:06:07.822166: Pseudo dice [0.8713, 0.9599, 0.9452]\n",
      "2024-01-11 01:06:07.833165: Epoch time: 40.59 s\n",
      "2024-01-11 01:06:09.363092: \n",
      "2024-01-11 01:06:09.372949: Epoch 801\n",
      "2024-01-11 01:06:09.382007: Current learning rate: 0.00234\n",
      "2024-01-11 01:06:49.803499: train_loss -0.9535\n",
      "2024-01-11 01:06:49.812954: val_loss -0.8368\n",
      "2024-01-11 01:06:49.821463: Pseudo dice [0.8685, 0.9589, 0.943]\n",
      "2024-01-11 01:06:49.845469: Epoch time: 40.44 s\n",
      "2024-01-11 01:06:51.206944: \n",
      "2024-01-11 01:06:51.215943: Epoch 802\n",
      "2024-01-11 01:06:51.221940: Current learning rate: 0.00233\n",
      "2024-01-11 01:07:31.663949: train_loss -0.9529\n",
      "2024-01-11 01:07:31.693983: val_loss -0.8369\n",
      "2024-01-11 01:07:31.703982: Pseudo dice [0.8702, 0.9583, 0.9427]\n",
      "2024-01-11 01:07:31.712980: Epoch time: 40.46 s\n",
      "2024-01-11 01:07:33.306028: \n",
      "2024-01-11 01:07:33.312227: Epoch 803\n",
      "2024-01-11 01:07:33.317171: Current learning rate: 0.00232\n",
      "2024-01-11 01:08:13.718138: train_loss -0.9528\n",
      "2024-01-11 01:08:13.726645: val_loss -0.8387\n",
      "2024-01-11 01:08:13.735646: Pseudo dice [0.8689, 0.9598, 0.9449]\n",
      "2024-01-11 01:08:13.767645: Epoch time: 40.41 s\n",
      "2024-01-11 01:08:15.246280: \n",
      "2024-01-11 01:08:15.252342: Epoch 804\n",
      "2024-01-11 01:08:15.257345: Current learning rate: 0.00231\n",
      "2024-01-11 01:08:55.701521: train_loss -0.9529\n",
      "2024-01-11 01:08:55.739039: val_loss -0.8371\n",
      "2024-01-11 01:08:55.749040: Pseudo dice [0.8726, 0.9589, 0.9429]\n",
      "2024-01-11 01:08:55.756041: Epoch time: 40.46 s\n",
      "2024-01-11 01:08:57.245615: \n",
      "2024-01-11 01:08:57.252549: Epoch 805\n",
      "2024-01-11 01:08:57.256548: Current learning rate: 0.0023\n",
      "2024-01-11 01:09:37.738311: train_loss -0.9536\n",
      "2024-01-11 01:09:37.760312: val_loss -0.8415\n",
      "2024-01-11 01:09:37.770312: Pseudo dice [0.8779, 0.9598, 0.9449]\n",
      "2024-01-11 01:09:37.777314: Epoch time: 40.49 s\n",
      "2024-01-11 01:09:39.207290: \n",
      "2024-01-11 01:09:39.215363: Epoch 806\n",
      "2024-01-11 01:09:39.223296: Current learning rate: 0.00229\n",
      "2024-01-11 01:10:19.327743: train_loss -0.9536\n",
      "2024-01-11 01:10:19.339749: val_loss -0.8385\n",
      "2024-01-11 01:10:19.366259: Pseudo dice [0.8745, 0.959, 0.9435]\n",
      "2024-01-11 01:10:19.375259: Epoch time: 40.12 s\n",
      "2024-01-11 01:10:20.875087: \n",
      "2024-01-11 01:10:20.880837: Epoch 807\n",
      "2024-01-11 01:10:20.887893: Current learning rate: 0.00228\n",
      "2024-01-11 01:11:01.138413: train_loss -0.9529\n",
      "2024-01-11 01:11:01.147415: val_loss -0.8412\n",
      "2024-01-11 01:11:01.173415: Pseudo dice [0.8739, 0.96, 0.9445]\n",
      "2024-01-11 01:11:01.181917: Epoch time: 40.27 s\n",
      "2024-01-11 01:11:02.704361: \n",
      "2024-01-11 01:11:02.710361: Epoch 808\n",
      "2024-01-11 01:11:02.715360: Current learning rate: 0.00226\n",
      "2024-01-11 01:11:42.828317: train_loss -0.9536\n",
      "2024-01-11 01:11:42.839318: val_loss -0.8436\n",
      "2024-01-11 01:11:42.845318: Pseudo dice [0.8745, 0.9598, 0.9451]\n",
      "2024-01-11 01:11:42.852318: Epoch time: 40.13 s\n",
      "2024-01-11 01:11:44.353883: \n",
      "2024-01-11 01:11:44.359882: Epoch 809\n",
      "2024-01-11 01:11:44.364875: Current learning rate: 0.00225\n",
      "2024-01-11 01:12:24.537892: train_loss -0.9535\n",
      "2024-01-11 01:12:24.546893: val_loss -0.8388\n",
      "2024-01-11 01:12:24.552891: Pseudo dice [0.8777, 0.959, 0.9434]\n",
      "2024-01-11 01:12:24.584893: Epoch time: 40.19 s\n",
      "2024-01-11 01:12:26.070372: \n",
      "2024-01-11 01:12:26.076768: Epoch 810\n",
      "2024-01-11 01:12:26.085770: Current learning rate: 0.00224\n",
      "2024-01-11 01:13:06.066412: train_loss -0.9535\n",
      "2024-01-11 01:13:06.075410: val_loss -0.8362\n",
      "2024-01-11 01:13:06.116411: Pseudo dice [0.8669, 0.9593, 0.9441]\n",
      "2024-01-11 01:13:06.127413: Epoch time: 40.0 s\n",
      "2024-01-11 01:13:07.515440: \n",
      "2024-01-11 01:13:07.522684: Epoch 811\n",
      "2024-01-11 01:13:07.529702: Current learning rate: 0.00223\n",
      "2024-01-11 01:13:47.668084: train_loss -0.9531\n",
      "2024-01-11 01:13:47.678086: val_loss -0.8432\n",
      "2024-01-11 01:13:47.684095: Pseudo dice [0.8744, 0.9604, 0.9451]\n",
      "2024-01-11 01:13:47.690093: Epoch time: 40.15 s\n",
      "2024-01-11 01:13:49.398201: \n",
      "2024-01-11 01:13:49.410348: Epoch 812\n",
      "2024-01-11 01:13:49.415359: Current learning rate: 0.00222\n",
      "2024-01-11 01:14:29.613440: train_loss -0.9532\n",
      "2024-01-11 01:14:29.642948: val_loss -0.8374\n",
      "2024-01-11 01:14:29.649949: Pseudo dice [0.8693, 0.9593, 0.9435]\n",
      "2024-01-11 01:14:29.655954: Epoch time: 40.22 s\n",
      "2024-01-11 01:14:31.053882: \n",
      "2024-01-11 01:14:31.060735: Epoch 813\n",
      "2024-01-11 01:14:31.065731: Current learning rate: 0.00221\n",
      "2024-01-11 01:15:10.929158: train_loss -0.9535\n",
      "2024-01-11 01:15:10.938666: val_loss -0.8372\n",
      "2024-01-11 01:15:10.965670: Pseudo dice [0.872, 0.9595, 0.9444]\n",
      "2024-01-11 01:15:10.973673: Epoch time: 39.88 s\n",
      "2024-01-11 01:15:12.398969: \n",
      "2024-01-11 01:15:12.407973: Epoch 814\n",
      "2024-01-11 01:15:12.412990: Current learning rate: 0.0022\n",
      "2024-01-11 01:15:52.342678: train_loss -0.9534\n",
      "2024-01-11 01:15:52.350686: val_loss -0.8426\n",
      "2024-01-11 01:15:52.359677: Pseudo dice [0.8763, 0.9601, 0.9454]\n",
      "2024-01-11 01:15:52.367706: Epoch time: 39.94 s\n",
      "2024-01-11 01:15:52.391727: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-11 01:15:54.209967: \n",
      "2024-01-11 01:15:54.215967: Epoch 815\n",
      "2024-01-11 01:15:54.221967: Current learning rate: 0.00219\n",
      "2024-01-11 01:16:34.383677: train_loss -0.9528\n",
      "2024-01-11 01:16:34.393676: val_loss -0.8345\n",
      "2024-01-11 01:16:34.423185: Pseudo dice [0.8667, 0.9586, 0.9429]\n",
      "2024-01-11 01:16:34.434186: Epoch time: 40.18 s\n",
      "2024-01-11 01:16:35.907888: \n",
      "2024-01-11 01:16:35.920547: Epoch 816\n",
      "2024-01-11 01:16:35.925993: Current learning rate: 0.00218\n",
      "2024-01-11 01:17:16.085882: train_loss -0.9542\n",
      "2024-01-11 01:17:16.094882: val_loss -0.8398\n",
      "2024-01-11 01:17:16.124394: Pseudo dice [0.8745, 0.9602, 0.9454]\n",
      "2024-01-11 01:17:16.132392: Epoch time: 40.18 s\n",
      "2024-01-11 01:17:17.723766: \n",
      "2024-01-11 01:17:17.729774: Epoch 817\n",
      "2024-01-11 01:17:17.734783: Current learning rate: 0.00217\n",
      "2024-01-11 01:17:57.779438: train_loss -0.9541\n",
      "2024-01-11 01:17:57.787388: val_loss -0.8406\n",
      "2024-01-11 01:17:57.793387: Pseudo dice [0.8701, 0.9597, 0.9442]\n",
      "2024-01-11 01:17:57.798907: Epoch time: 40.06 s\n",
      "2024-01-11 01:17:59.282047: \n",
      "2024-01-11 01:17:59.289047: Epoch 818\n",
      "2024-01-11 01:17:59.294047: Current learning rate: 0.00216\n",
      "2024-01-11 01:18:39.303168: train_loss -0.9531\n",
      "2024-01-11 01:18:39.310126: val_loss -0.8367\n",
      "2024-01-11 01:18:39.338135: Pseudo dice [0.8697, 0.9593, 0.9446]\n",
      "2024-01-11 01:18:39.353221: Epoch time: 40.02 s\n",
      "2024-01-11 01:18:40.814596: \n",
      "2024-01-11 01:18:40.824431: Epoch 819\n",
      "2024-01-11 01:18:40.832479: Current learning rate: 0.00215\n",
      "2024-01-11 01:19:20.771137: train_loss -0.9532\n",
      "2024-01-11 01:19:20.779650: val_loss -0.8382\n",
      "2024-01-11 01:19:20.789060: Pseudo dice [0.8682, 0.9593, 0.9444]\n",
      "2024-01-11 01:19:20.797163: Epoch time: 39.96 s\n",
      "2024-01-11 01:19:22.308142: \n",
      "2024-01-11 01:19:22.314042: Epoch 820\n",
      "2024-01-11 01:19:22.320577: Current learning rate: 0.00214\n",
      "2024-01-11 01:20:02.400259: train_loss -0.9542\n",
      "2024-01-11 01:20:02.408259: val_loss -0.8333\n",
      "2024-01-11 01:20:02.415258: Pseudo dice [0.8684, 0.9582, 0.942]\n",
      "2024-01-11 01:20:02.420258: Epoch time: 40.1 s\n",
      "2024-01-11 01:20:03.760474: \n",
      "2024-01-11 01:20:03.768964: Epoch 821\n",
      "2024-01-11 01:20:03.777969: Current learning rate: 0.00213\n",
      "2024-01-11 01:20:43.958880: train_loss -0.9538\n",
      "2024-01-11 01:20:43.968399: val_loss -0.8367\n",
      "2024-01-11 01:20:43.975400: Pseudo dice [0.8715, 0.9591, 0.9437]\n",
      "2024-01-11 01:20:43.981401: Epoch time: 40.2 s\n",
      "2024-01-11 01:20:45.298405: \n",
      "2024-01-11 01:20:45.307576: Epoch 822\n",
      "2024-01-11 01:20:45.312520: Current learning rate: 0.00212\n",
      "2024-01-11 01:21:25.235866: train_loss -0.9537\n",
      "2024-01-11 01:21:25.244862: val_loss -0.8372\n",
      "2024-01-11 01:21:25.255863: Pseudo dice [0.8678, 0.9594, 0.944]\n",
      "2024-01-11 01:21:25.261865: Epoch time: 39.94 s\n",
      "2024-01-11 01:21:26.712750: \n",
      "2024-01-11 01:21:26.718734: Epoch 823\n",
      "2024-01-11 01:21:26.723771: Current learning rate: 0.0021\n",
      "2024-01-11 01:22:06.693804: train_loss -0.9537\n",
      "2024-01-11 01:22:06.703803: val_loss -0.8388\n",
      "2024-01-11 01:22:06.709802: Pseudo dice [0.8702, 0.96, 0.9455]\n",
      "2024-01-11 01:22:06.715809: Epoch time: 39.98 s\n",
      "2024-01-11 01:22:08.114587: \n",
      "2024-01-11 01:22:08.120649: Epoch 824\n",
      "2024-01-11 01:22:08.125650: Current learning rate: 0.00209\n",
      "2024-01-11 01:22:48.098890: train_loss -0.9538\n",
      "2024-01-11 01:22:48.107890: val_loss -0.838\n",
      "2024-01-11 01:22:48.115892: Pseudo dice [0.8687, 0.9602, 0.9455]\n",
      "2024-01-11 01:22:48.123693: Epoch time: 39.99 s\n",
      "2024-01-11 01:22:49.558621: \n",
      "2024-01-11 01:22:49.567631: Epoch 825\n",
      "2024-01-11 01:22:49.572398: Current learning rate: 0.00208\n",
      "2024-01-11 01:23:29.809785: train_loss -0.9541\n",
      "2024-01-11 01:23:29.818787: val_loss -0.8381\n",
      "2024-01-11 01:23:29.826786: Pseudo dice [0.8707, 0.9597, 0.9446]\n",
      "2024-01-11 01:23:29.833787: Epoch time: 40.25 s\n",
      "2024-01-11 01:23:31.249428: \n",
      "2024-01-11 01:23:31.256495: Epoch 826\n",
      "2024-01-11 01:23:31.262496: Current learning rate: 0.00207\n",
      "2024-01-11 01:24:11.434744: train_loss -0.9535\n",
      "2024-01-11 01:24:11.443746: val_loss -0.8412\n",
      "2024-01-11 01:24:11.451745: Pseudo dice [0.8749, 0.9599, 0.9448]\n",
      "2024-01-11 01:24:11.473745: Epoch time: 40.19 s\n",
      "2024-01-11 01:24:12.830786: \n",
      "2024-01-11 01:24:12.837857: Epoch 827\n",
      "2024-01-11 01:24:12.845866: Current learning rate: 0.00206\n",
      "2024-01-11 01:24:52.872432: train_loss -0.9535\n",
      "2024-01-11 01:24:52.902434: val_loss -0.839\n",
      "2024-01-11 01:24:52.910433: Pseudo dice [0.8718, 0.9597, 0.9444]\n",
      "2024-01-11 01:24:52.919431: Epoch time: 40.04 s\n",
      "2024-01-11 01:24:54.330741: \n",
      "2024-01-11 01:24:54.336914: Epoch 828\n",
      "2024-01-11 01:24:54.347668: Current learning rate: 0.00205\n",
      "2024-01-11 01:25:34.380529: train_loss -0.9538\n",
      "2024-01-11 01:25:34.392532: val_loss -0.8371\n",
      "2024-01-11 01:25:34.417530: Pseudo dice [0.8711, 0.9595, 0.9438]\n",
      "2024-01-11 01:25:34.426530: Epoch time: 40.05 s\n",
      "2024-01-11 01:25:35.790988: \n",
      "2024-01-11 01:25:35.797055: Epoch 829\n",
      "2024-01-11 01:25:35.806059: Current learning rate: 0.00204\n",
      "2024-01-11 01:26:15.771957: train_loss -0.9532\n",
      "2024-01-11 01:26:15.779954: val_loss -0.8387\n",
      "2024-01-11 01:26:15.789954: Pseudo dice [0.8706, 0.9599, 0.9446]\n",
      "2024-01-11 01:26:15.817618: Epoch time: 39.98 s\n",
      "2024-01-11 01:26:17.178577: \n",
      "2024-01-11 01:26:17.186577: Epoch 830\n",
      "2024-01-11 01:26:17.193585: Current learning rate: 0.00203\n",
      "2024-01-11 01:26:57.163813: train_loss -0.954\n",
      "2024-01-11 01:26:57.172811: val_loss -0.8383\n",
      "2024-01-11 01:26:57.180811: Pseudo dice [0.8722, 0.9593, 0.9446]\n",
      "2024-01-11 01:26:57.186809: Epoch time: 39.99 s\n",
      "2024-01-11 01:26:58.568089: \n",
      "2024-01-11 01:26:58.577735: Epoch 831\n",
      "2024-01-11 01:26:58.583795: Current learning rate: 0.00202\n",
      "2024-01-11 01:27:38.629576: train_loss -0.9536\n",
      "2024-01-11 01:27:38.637090: val_loss -0.8364\n",
      "2024-01-11 01:27:38.645090: Pseudo dice [0.8643, 0.9596, 0.9444]\n",
      "2024-01-11 01:27:38.677090: Epoch time: 40.06 s\n",
      "2024-01-11 01:27:40.151989: \n",
      "2024-01-11 01:27:40.157921: Epoch 832\n",
      "2024-01-11 01:27:40.169919: Current learning rate: 0.00201\n",
      "2024-01-11 01:28:20.175088: train_loss -0.954\n",
      "2024-01-11 01:28:20.185085: val_loss -0.8353\n",
      "2024-01-11 01:28:20.193086: Pseudo dice [0.8661, 0.9593, 0.9436]\n",
      "2024-01-11 01:28:20.200087: Epoch time: 40.02 s\n",
      "2024-01-11 01:28:21.656130: \n",
      "2024-01-11 01:28:21.668460: Epoch 833\n",
      "2024-01-11 01:28:21.673459: Current learning rate: 0.002\n",
      "2024-01-11 01:29:01.617214: train_loss -0.9543\n",
      "2024-01-11 01:29:01.626216: val_loss -0.8371\n",
      "2024-01-11 01:29:01.634215: Pseudo dice [0.8686, 0.9591, 0.9436]\n",
      "2024-01-11 01:29:01.642216: Epoch time: 39.96 s\n",
      "2024-01-11 01:29:03.029223: \n",
      "2024-01-11 01:29:03.035230: Epoch 834\n",
      "2024-01-11 01:29:03.039280: Current learning rate: 0.00199\n",
      "2024-01-11 01:29:43.059955: train_loss -0.9534\n",
      "2024-01-11 01:29:43.066956: val_loss -0.8322\n",
      "2024-01-11 01:29:43.072957: Pseudo dice [0.8685, 0.9587, 0.9425]\n",
      "2024-01-11 01:29:43.078957: Epoch time: 40.03 s\n",
      "2024-01-11 01:29:44.432922: \n",
      "2024-01-11 01:29:44.439036: Epoch 835\n",
      "2024-01-11 01:29:44.445053: Current learning rate: 0.00198\n",
      "2024-01-11 01:30:24.552403: train_loss -0.9537\n",
      "2024-01-11 01:30:24.565403: val_loss -0.8352\n",
      "2024-01-11 01:30:24.576938: Pseudo dice [0.8721, 0.9595, 0.9442]\n",
      "2024-01-11 01:30:24.613449: Epoch time: 40.12 s\n",
      "2024-01-11 01:30:26.231155: \n",
      "2024-01-11 01:30:26.239176: Epoch 836\n",
      "2024-01-11 01:30:26.245243: Current learning rate: 0.00196\n",
      "2024-01-11 01:31:06.241329: train_loss -0.9531\n",
      "2024-01-11 01:31:06.251690: val_loss -0.8389\n",
      "2024-01-11 01:31:06.286696: Pseudo dice [0.8682, 0.9599, 0.9452]\n",
      "2024-01-11 01:31:06.295697: Epoch time: 40.01 s\n",
      "2024-01-11 01:31:07.678740: \n",
      "2024-01-11 01:31:07.684752: Epoch 837\n",
      "2024-01-11 01:31:07.689742: Current learning rate: 0.00195\n",
      "2024-01-11 01:31:47.668269: train_loss -0.954\n",
      "2024-01-11 01:31:47.675271: val_loss -0.8352\n",
      "2024-01-11 01:31:47.685790: Pseudo dice [0.8652, 0.9591, 0.9431]\n",
      "2024-01-11 01:31:47.692792: Epoch time: 39.99 s\n",
      "2024-01-11 01:31:49.068231: \n",
      "2024-01-11 01:31:49.074119: Epoch 838\n",
      "2024-01-11 01:31:49.078200: Current learning rate: 0.00194\n",
      "2024-01-11 01:32:29.129403: train_loss -0.9537\n",
      "2024-01-11 01:32:29.139395: val_loss -0.8347\n",
      "2024-01-11 01:32:29.147395: Pseudo dice [0.8674, 0.9592, 0.9433]\n",
      "2024-01-11 01:32:29.153395: Epoch time: 40.06 s\n",
      "2024-01-11 01:32:30.472832: \n",
      "2024-01-11 01:32:30.478331: Epoch 839\n",
      "2024-01-11 01:32:30.491393: Current learning rate: 0.00193\n",
      "2024-01-11 01:33:10.407384: train_loss -0.9536\n",
      "2024-01-11 01:33:10.416889: val_loss -0.8342\n",
      "2024-01-11 01:33:10.424893: Pseudo dice [0.866, 0.9593, 0.944]\n",
      "2024-01-11 01:33:10.459894: Epoch time: 39.94 s\n",
      "2024-01-11 01:33:11.891407: \n",
      "2024-01-11 01:33:11.897177: Epoch 840\n",
      "2024-01-11 01:33:11.903261: Current learning rate: 0.00192\n",
      "2024-01-11 01:33:52.001663: train_loss -0.954\n",
      "2024-01-11 01:33:52.036186: val_loss -0.8355\n",
      "2024-01-11 01:33:52.048185: Pseudo dice [0.8704, 0.959, 0.9433]\n",
      "2024-01-11 01:33:52.058187: Epoch time: 40.11 s\n",
      "2024-01-11 01:33:53.431652: \n",
      "2024-01-11 01:33:53.437554: Epoch 841\n",
      "2024-01-11 01:33:53.445805: Current learning rate: 0.00191\n",
      "2024-01-11 01:34:33.555385: train_loss -0.9541\n",
      "2024-01-11 01:34:33.565388: val_loss -0.8386\n",
      "2024-01-11 01:34:33.589386: Pseudo dice [0.867, 0.9598, 0.9447]\n",
      "2024-01-11 01:34:33.598385: Epoch time: 40.12 s\n",
      "2024-01-11 01:34:34.976967: \n",
      "2024-01-11 01:34:34.983260: Epoch 842\n",
      "2024-01-11 01:34:34.989324: Current learning rate: 0.0019\n",
      "2024-01-11 01:35:15.005907: train_loss -0.9539\n",
      "2024-01-11 01:35:15.014907: val_loss -0.829\n",
      "2024-01-11 01:35:15.025906: Pseudo dice [0.8619, 0.9589, 0.9445]\n",
      "2024-01-11 01:35:15.033908: Epoch time: 40.03 s\n",
      "2024-01-11 01:35:16.474266: \n",
      "2024-01-11 01:35:16.482624: Epoch 843\n",
      "2024-01-11 01:35:16.493513: Current learning rate: 0.00189\n",
      "2024-01-11 01:35:56.536965: train_loss -0.9542\n",
      "2024-01-11 01:35:56.544965: val_loss -0.8348\n",
      "2024-01-11 01:35:56.576964: Pseudo dice [0.8686, 0.9592, 0.9437]\n",
      "2024-01-11 01:35:56.587965: Epoch time: 40.06 s\n",
      "2024-01-11 01:35:57.985444: \n",
      "2024-01-11 01:35:57.993102: Epoch 844\n",
      "2024-01-11 01:35:57.998173: Current learning rate: 0.00188\n",
      "2024-01-11 01:36:37.892593: train_loss -0.9544\n",
      "2024-01-11 01:36:37.905604: val_loss -0.8376\n",
      "2024-01-11 01:36:37.913605: Pseudo dice [0.8713, 0.9593, 0.9447]\n",
      "2024-01-11 01:36:37.960156: Epoch time: 39.91 s\n",
      "2024-01-11 01:36:39.419620: \n",
      "2024-01-11 01:36:39.425631: Epoch 845\n",
      "2024-01-11 01:36:39.430612: Current learning rate: 0.00187\n",
      "2024-01-11 01:37:19.468570: train_loss -0.9544\n",
      "2024-01-11 01:37:19.476085: val_loss -0.8366\n",
      "2024-01-11 01:37:19.483086: Pseudo dice [0.8669, 0.9593, 0.9435]\n",
      "2024-01-11 01:37:19.488083: Epoch time: 40.05 s\n",
      "2024-01-11 01:37:20.918094: \n",
      "2024-01-11 01:37:20.928250: Epoch 846\n",
      "2024-01-11 01:37:20.937274: Current learning rate: 0.00186\n",
      "2024-01-11 01:38:01.047449: train_loss -0.9539\n",
      "2024-01-11 01:38:01.055451: val_loss -0.836\n",
      "2024-01-11 01:38:01.065459: Pseudo dice [0.8683, 0.9588, 0.943]\n",
      "2024-01-11 01:38:01.074447: Epoch time: 40.13 s\n",
      "2024-01-11 01:38:02.463001: \n",
      "2024-01-11 01:38:02.470946: Epoch 847\n",
      "2024-01-11 01:38:02.475946: Current learning rate: 0.00185\n",
      "2024-01-11 01:38:42.377940: train_loss -0.9537\n",
      "2024-01-11 01:38:42.385941: val_loss -0.8371\n",
      "2024-01-11 01:38:42.399945: Pseudo dice [0.8715, 0.9592, 0.944]\n",
      "2024-01-11 01:38:42.434138: Epoch time: 39.92 s\n",
      "2024-01-11 01:38:43.813740: \n",
      "2024-01-11 01:38:43.820175: Epoch 848\n",
      "2024-01-11 01:38:43.825193: Current learning rate: 0.00184\n",
      "2024-01-11 01:39:23.903890: train_loss -0.9548\n",
      "2024-01-11 01:39:23.915891: val_loss -0.8371\n",
      "2024-01-11 01:39:23.925888: Pseudo dice [0.8697, 0.9597, 0.9439]\n",
      "2024-01-11 01:39:23.934402: Epoch time: 40.09 s\n",
      "2024-01-11 01:39:25.295002: \n",
      "2024-01-11 01:39:25.302076: Epoch 849\n",
      "2024-01-11 01:39:25.307056: Current learning rate: 0.00182\n",
      "2024-01-11 01:40:05.271037: train_loss -0.9548\n",
      "2024-01-11 01:40:05.280037: val_loss -0.8335\n",
      "2024-01-11 01:40:05.289058: Pseudo dice [0.8674, 0.9595, 0.9442]\n",
      "2024-01-11 01:40:05.295061: Epoch time: 39.98 s\n",
      "2024-01-11 01:40:06.982147: \n",
      "2024-01-11 01:40:06.989067: Epoch 850\n",
      "2024-01-11 01:40:06.997125: Current learning rate: 0.00181\n",
      "2024-01-11 01:40:46.896755: train_loss -0.9548\n",
      "2024-01-11 01:40:46.905752: val_loss -0.8391\n",
      "2024-01-11 01:40:46.917755: Pseudo dice [0.8716, 0.9601, 0.9451]\n",
      "2024-01-11 01:40:46.941755: Epoch time: 39.92 s\n",
      "2024-01-11 01:40:48.353635: \n",
      "2024-01-11 01:40:48.363647: Epoch 851\n",
      "2024-01-11 01:40:48.369121: Current learning rate: 0.0018\n",
      "2024-01-11 01:41:28.428939: train_loss -0.9545\n",
      "2024-01-11 01:41:28.441940: val_loss -0.8344\n",
      "2024-01-11 01:41:28.449939: Pseudo dice [0.8683, 0.9591, 0.9426]\n",
      "2024-01-11 01:41:28.457939: Epoch time: 40.08 s\n",
      "2024-01-11 01:41:29.899063: \n",
      "2024-01-11 01:41:29.908235: Epoch 852\n",
      "2024-01-11 01:41:29.914246: Current learning rate: 0.00179\n",
      "2024-01-11 01:42:09.792984: train_loss -0.9547\n",
      "2024-01-11 01:42:09.800986: val_loss -0.8382\n",
      "2024-01-11 01:42:09.812494: Pseudo dice [0.8718, 0.9597, 0.945]\n",
      "2024-01-11 01:42:09.840499: Epoch time: 39.9 s\n",
      "2024-01-11 01:42:11.246023: \n",
      "2024-01-11 01:42:11.251476: Epoch 853\n",
      "2024-01-11 01:42:11.256850: Current learning rate: 0.00178\n",
      "2024-01-11 01:42:51.298636: train_loss -0.9554\n",
      "2024-01-11 01:42:51.305636: val_loss -0.8344\n",
      "2024-01-11 01:42:51.314634: Pseudo dice [0.8704, 0.9593, 0.9434]\n",
      "2024-01-11 01:42:51.323635: Epoch time: 40.05 s\n",
      "2024-01-11 01:42:52.685974: \n",
      "2024-01-11 01:42:52.694968: Epoch 854\n",
      "2024-01-11 01:42:52.705971: Current learning rate: 0.00177\n",
      "2024-01-11 01:43:32.630217: train_loss -0.9548\n",
      "2024-01-11 01:43:32.640489: val_loss -0.8388\n",
      "2024-01-11 01:43:32.648501: Pseudo dice [0.8714, 0.9597, 0.9449]\n",
      "2024-01-11 01:43:32.654509: Epoch time: 39.95 s\n",
      "2024-01-11 01:43:34.002226: \n",
      "2024-01-11 01:43:34.012515: Epoch 855\n",
      "2024-01-11 01:43:34.020581: Current learning rate: 0.00176\n",
      "2024-01-11 01:44:13.988960: train_loss -0.9551\n",
      "2024-01-11 01:44:13.998961: val_loss -0.8386\n",
      "2024-01-11 01:44:14.006960: Pseudo dice [0.8738, 0.9593, 0.9442]\n",
      "2024-01-11 01:44:14.015966: Epoch time: 39.99 s\n",
      "2024-01-11 01:44:15.369809: \n",
      "2024-01-11 01:44:15.378694: Epoch 856\n",
      "2024-01-11 01:44:15.382778: Current learning rate: 0.00175\n",
      "2024-01-11 01:44:55.555212: train_loss -0.9545\n",
      "2024-01-11 01:44:55.583318: val_loss -0.8372\n",
      "2024-01-11 01:44:55.593845: Pseudo dice [0.8737, 0.9591, 0.9435]\n",
      "2024-01-11 01:44:55.601833: Epoch time: 40.19 s\n",
      "2024-01-11 01:44:57.080973: \n",
      "2024-01-11 01:44:57.087220: Epoch 857\n",
      "2024-01-11 01:44:57.091282: Current learning rate: 0.00174\n",
      "2024-01-11 01:45:36.994557: train_loss -0.9551\n",
      "2024-01-11 01:45:37.009549: val_loss -0.8381\n",
      "2024-01-11 01:45:37.017553: Pseudo dice [0.8709, 0.9592, 0.9437]\n",
      "2024-01-11 01:45:37.038551: Epoch time: 39.91 s\n",
      "2024-01-11 01:45:38.366795: \n",
      "2024-01-11 01:45:38.373455: Epoch 858\n",
      "2024-01-11 01:45:38.379538: Current learning rate: 0.00173\n",
      "2024-01-11 01:46:18.303972: train_loss -0.9544\n",
      "2024-01-11 01:46:18.311972: val_loss -0.8374\n",
      "2024-01-11 01:46:18.319974: Pseudo dice [0.8721, 0.959, 0.9442]\n",
      "2024-01-11 01:46:18.326985: Epoch time: 39.94 s\n",
      "2024-01-11 01:46:19.798963: \n",
      "2024-01-11 01:46:19.805147: Epoch 859\n",
      "2024-01-11 01:46:19.811229: Current learning rate: 0.00172\n",
      "2024-01-11 01:46:59.842052: train_loss -0.9546\n",
      "2024-01-11 01:46:59.849055: val_loss -0.8368\n",
      "2024-01-11 01:46:59.857057: Pseudo dice [0.8689, 0.9595, 0.9445]\n",
      "2024-01-11 01:46:59.865885: Epoch time: 40.04 s\n",
      "2024-01-11 01:47:01.293715: \n",
      "2024-01-11 01:47:01.299780: Epoch 860\n",
      "2024-01-11 01:47:01.304787: Current learning rate: 0.0017\n",
      "2024-01-11 01:47:41.155309: train_loss -0.9546\n",
      "2024-01-11 01:47:41.166310: val_loss -0.8338\n",
      "2024-01-11 01:47:41.174309: Pseudo dice [0.8718, 0.959, 0.9431]\n",
      "2024-01-11 01:47:41.181312: Epoch time: 39.86 s\n",
      "2024-01-11 01:47:42.598950: \n",
      "2024-01-11 01:47:42.604542: Epoch 861\n",
      "2024-01-11 01:47:42.611614: Current learning rate: 0.00169\n",
      "2024-01-11 01:48:22.851923: train_loss -0.9545\n",
      "2024-01-11 01:48:22.861925: val_loss -0.834\n",
      "2024-01-11 01:48:22.871924: Pseudo dice [0.8703, 0.9582, 0.9434]\n",
      "2024-01-11 01:48:22.879924: Epoch time: 40.25 s\n",
      "2024-01-11 01:48:24.348675: \n",
      "2024-01-11 01:48:24.355675: Epoch 862\n",
      "2024-01-11 01:48:24.361675: Current learning rate: 0.00168\n",
      "2024-01-11 01:49:04.308642: train_loss -0.955\n",
      "2024-01-11 01:49:04.315641: val_loss -0.8382\n",
      "2024-01-11 01:49:04.322640: Pseudo dice [0.8727, 0.9597, 0.9446]\n",
      "2024-01-11 01:49:04.352640: Epoch time: 39.96 s\n",
      "2024-01-11 01:49:05.658513: \n",
      "2024-01-11 01:49:05.663837: Epoch 863\n",
      "2024-01-11 01:49:05.668837: Current learning rate: 0.00167\n",
      "2024-01-11 01:49:45.596879: train_loss -0.9549\n",
      "2024-01-11 01:49:45.605396: val_loss -0.8314\n",
      "2024-01-11 01:49:45.612394: Pseudo dice [0.866, 0.9586, 0.9432]\n",
      "2024-01-11 01:49:45.620396: Epoch time: 39.94 s\n",
      "2024-01-11 01:49:46.994766: \n",
      "2024-01-11 01:49:47.002806: Epoch 864\n",
      "2024-01-11 01:49:47.007435: Current learning rate: 0.00166\n",
      "2024-01-11 01:50:26.921664: train_loss -0.9544\n",
      "2024-01-11 01:50:26.931182: val_loss -0.8343\n",
      "2024-01-11 01:50:26.940177: Pseudo dice [0.8658, 0.9589, 0.9436]\n",
      "2024-01-11 01:50:26.976690: Epoch time: 39.93 s\n",
      "2024-01-11 01:50:28.425772: \n",
      "2024-01-11 01:50:28.433487: Epoch 865\n",
      "2024-01-11 01:50:28.438482: Current learning rate: 0.00165\n",
      "2024-01-11 01:51:08.495063: train_loss -0.9545\n",
      "2024-01-11 01:51:08.505064: val_loss -0.8367\n",
      "2024-01-11 01:51:08.513064: Pseudo dice [0.8693, 0.9596, 0.9444]\n",
      "2024-01-11 01:51:08.520062: Epoch time: 40.07 s\n",
      "2024-01-11 01:51:10.032370: \n",
      "2024-01-11 01:51:10.043419: Epoch 866\n",
      "2024-01-11 01:51:10.048150: Current learning rate: 0.00164\n",
      "2024-01-11 01:51:50.200951: train_loss -0.9553\n",
      "2024-01-11 01:51:50.211953: val_loss -0.8387\n",
      "2024-01-11 01:51:50.236950: Pseudo dice [0.8741, 0.9599, 0.9448]\n",
      "2024-01-11 01:51:50.245952: Epoch time: 40.17 s\n",
      "2024-01-11 01:51:51.610460: \n",
      "2024-01-11 01:51:51.617459: Epoch 867\n",
      "2024-01-11 01:51:51.622461: Current learning rate: 0.00163\n",
      "2024-01-11 01:52:31.519169: train_loss -0.9548\n",
      "2024-01-11 01:52:31.526171: val_loss -0.8354\n",
      "2024-01-11 01:52:31.536170: Pseudo dice [0.8687, 0.9597, 0.9446]\n",
      "2024-01-11 01:52:31.542172: Epoch time: 39.91 s\n",
      "2024-01-11 01:52:32.898154: \n",
      "2024-01-11 01:52:32.904379: Epoch 868\n",
      "2024-01-11 01:52:32.912373: Current learning rate: 0.00162\n",
      "2024-01-11 01:53:12.867665: train_loss -0.9546\n",
      "2024-01-11 01:53:12.876666: val_loss -0.8343\n",
      "2024-01-11 01:53:12.882665: Pseudo dice [0.8693, 0.959, 0.9432]\n",
      "2024-01-11 01:53:12.889677: Epoch time: 39.97 s\n",
      "2024-01-11 01:53:14.278852: \n",
      "2024-01-11 01:53:14.286988: Epoch 869\n",
      "2024-01-11 01:53:14.291965: Current learning rate: 0.00161\n",
      "2024-01-11 01:53:54.205894: train_loss -0.9554\n",
      "2024-01-11 01:53:54.214896: val_loss -0.8343\n",
      "2024-01-11 01:53:54.223410: Pseudo dice [0.8677, 0.9593, 0.9437]\n",
      "2024-01-11 01:53:54.251413: Epoch time: 39.93 s\n",
      "2024-01-11 01:53:55.621062: \n",
      "2024-01-11 01:53:55.627062: Epoch 870\n",
      "2024-01-11 01:53:55.631062: Current learning rate: 0.00159\n",
      "2024-01-11 01:54:35.655715: train_loss -0.955\n",
      "2024-01-11 01:54:35.663715: val_loss -0.8392\n",
      "2024-01-11 01:54:35.671715: Pseudo dice [0.8723, 0.9597, 0.9449]\n",
      "2024-01-11 01:54:35.678717: Epoch time: 40.04 s\n",
      "2024-01-11 01:54:37.059283: \n",
      "2024-01-11 01:54:37.068800: Epoch 871\n",
      "2024-01-11 01:54:37.074875: Current learning rate: 0.00158\n",
      "2024-01-11 01:55:17.176139: train_loss -0.9552\n",
      "2024-01-11 01:55:17.184140: val_loss -0.8386\n",
      "2024-01-11 01:55:17.192138: Pseudo dice [0.8709, 0.9594, 0.9446]\n",
      "2024-01-11 01:55:17.199140: Epoch time: 40.12 s\n",
      "2024-01-11 01:55:18.580411: \n",
      "2024-01-11 01:55:18.586484: Epoch 872\n",
      "2024-01-11 01:55:18.591448: Current learning rate: 0.00157\n",
      "2024-01-11 01:55:58.531226: train_loss -0.9551\n",
      "2024-01-11 01:55:58.538229: val_loss -0.8346\n",
      "2024-01-11 01:55:58.567225: Pseudo dice [0.8651, 0.9595, 0.9434]\n",
      "2024-01-11 01:55:58.576231: Epoch time: 39.95 s\n",
      "2024-01-11 01:56:00.006509: \n",
      "2024-01-11 01:56:00.013509: Epoch 873\n",
      "2024-01-11 01:56:00.019509: Current learning rate: 0.00156\n",
      "2024-01-11 01:56:39.970404: train_loss -0.9547\n",
      "2024-01-11 01:56:39.980915: val_loss -0.8367\n",
      "2024-01-11 01:56:39.989916: Pseudo dice [0.8731, 0.9592, 0.9443]\n",
      "2024-01-11 01:56:39.995920: Epoch time: 39.96 s\n",
      "2024-01-11 01:56:41.372505: \n",
      "2024-01-11 01:56:41.383077: Epoch 874\n",
      "2024-01-11 01:56:41.387416: Current learning rate: 0.00155\n",
      "2024-01-11 01:57:21.307173: train_loss -0.9553\n",
      "2024-01-11 01:57:21.316173: val_loss -0.8362\n",
      "2024-01-11 01:57:21.323172: Pseudo dice [0.8671, 0.9592, 0.9446]\n",
      "2024-01-11 01:57:21.331171: Epoch time: 39.94 s\n",
      "2024-01-11 01:57:22.663146: \n",
      "2024-01-11 01:57:22.673604: Epoch 875\n",
      "2024-01-11 01:57:22.680393: Current learning rate: 0.00154\n",
      "2024-01-11 01:58:02.762176: train_loss -0.9553\n",
      "2024-01-11 01:58:02.768697: val_loss -0.8375\n",
      "2024-01-11 01:58:02.774696: Pseudo dice [0.8686, 0.9592, 0.9436]\n",
      "2024-01-11 01:58:02.782693: Epoch time: 40.1 s\n",
      "2024-01-11 01:58:04.223017: \n",
      "2024-01-11 01:58:04.229071: Epoch 876\n",
      "2024-01-11 01:58:04.235013: Current learning rate: 0.00153\n",
      "2024-01-11 01:58:44.421959: train_loss -0.9547\n",
      "2024-01-11 01:58:44.430960: val_loss -0.836\n",
      "2024-01-11 01:58:44.461959: Pseudo dice [0.8698, 0.9593, 0.9441]\n",
      "2024-01-11 01:58:44.472961: Epoch time: 40.2 s\n",
      "2024-01-11 01:58:45.842368: \n",
      "2024-01-11 01:58:45.848808: Epoch 877\n",
      "2024-01-11 01:58:45.856827: Current learning rate: 0.00152\n",
      "2024-01-11 01:59:25.729271: train_loss -0.956\n",
      "2024-01-11 01:59:25.736266: val_loss -0.8348\n",
      "2024-01-11 01:59:25.746271: Pseudo dice [0.8658, 0.959, 0.9432]\n",
      "2024-01-11 01:59:25.751270: Epoch time: 39.89 s\n",
      "2024-01-11 01:59:27.219333: \n",
      "2024-01-11 01:59:27.227336: Epoch 878\n",
      "2024-01-11 01:59:27.232334: Current learning rate: 0.00151\n",
      "2024-01-11 02:00:07.195345: train_loss -0.9554\n",
      "2024-01-11 02:00:07.204342: val_loss -0.837\n",
      "2024-01-11 02:00:07.232870: Pseudo dice [0.8701, 0.9593, 0.9441]\n",
      "2024-01-11 02:00:07.240872: Epoch time: 39.98 s\n",
      "2024-01-11 02:00:08.686814: \n",
      "2024-01-11 02:00:08.695882: Epoch 879\n",
      "2024-01-11 02:00:08.701288: Current learning rate: 0.00149\n",
      "2024-01-11 02:00:48.824724: train_loss -0.9557\n",
      "2024-01-11 02:00:48.832725: val_loss -0.8341\n",
      "2024-01-11 02:00:48.840730: Pseudo dice [0.867, 0.9588, 0.9431]\n",
      "2024-01-11 02:00:48.847816: Epoch time: 40.14 s\n",
      "2024-01-11 02:00:50.158737: \n",
      "2024-01-11 02:00:50.164865: Epoch 880\n",
      "2024-01-11 02:00:50.170794: Current learning rate: 0.00148\n",
      "2024-01-11 02:01:30.239908: train_loss -0.9561\n",
      "2024-01-11 02:01:30.250906: val_loss -0.8351\n",
      "2024-01-11 02:01:30.257908: Pseudo dice [0.8692, 0.9584, 0.9435]\n",
      "2024-01-11 02:01:30.290995: Epoch time: 40.08 s\n",
      "2024-01-11 02:01:31.784048: \n",
      "2024-01-11 02:01:31.790394: Epoch 881\n",
      "2024-01-11 02:01:31.796440: Current learning rate: 0.00147\n",
      "2024-01-11 02:02:11.909950: train_loss -0.9551\n",
      "2024-01-11 02:02:11.916952: val_loss -0.8393\n",
      "2024-01-11 02:02:11.923949: Pseudo dice [0.8715, 0.9592, 0.9438]\n",
      "2024-01-11 02:02:11.929953: Epoch time: 40.13 s\n",
      "2024-01-11 02:02:13.369646: \n",
      "2024-01-11 02:02:13.378693: Epoch 882\n",
      "2024-01-11 02:02:13.383646: Current learning rate: 0.00146\n",
      "2024-01-11 02:02:53.369787: train_loss -0.9555\n",
      "2024-01-11 02:02:53.381788: val_loss -0.8317\n",
      "2024-01-11 02:02:53.391787: Pseudo dice [0.8697, 0.9586, 0.9432]\n",
      "2024-01-11 02:02:53.426097: Epoch time: 40.0 s\n",
      "2024-01-11 02:02:54.830183: \n",
      "2024-01-11 02:02:54.836392: Epoch 883\n",
      "2024-01-11 02:02:54.843509: Current learning rate: 0.00145\n",
      "2024-01-11 02:03:34.806538: train_loss -0.9555\n",
      "2024-01-11 02:03:34.818542: val_loss -0.8334\n",
      "2024-01-11 02:03:34.828542: Pseudo dice [0.8678, 0.9593, 0.9439]\n",
      "2024-01-11 02:03:34.834749: Epoch time: 39.98 s\n",
      "2024-01-11 02:03:36.247386: \n",
      "2024-01-11 02:03:36.256903: Epoch 884\n",
      "2024-01-11 02:03:36.264903: Current learning rate: 0.00144\n",
      "2024-01-11 02:04:16.161088: train_loss -0.9561\n",
      "2024-01-11 02:04:16.170092: val_loss -0.8343\n",
      "2024-01-11 02:04:16.193088: Pseudo dice [0.8693, 0.9591, 0.9435]\n",
      "2024-01-11 02:04:16.203088: Epoch time: 39.92 s\n",
      "2024-01-11 02:04:17.650903: \n",
      "2024-01-11 02:04:17.657087: Epoch 885\n",
      "2024-01-11 02:04:17.666091: Current learning rate: 0.00143\n",
      "2024-01-11 02:04:57.829290: train_loss -0.9557\n",
      "2024-01-11 02:04:57.839293: val_loss -0.8337\n",
      "2024-01-11 02:04:57.845299: Pseudo dice [0.8628, 0.9593, 0.9432]\n",
      "2024-01-11 02:04:57.880808: Epoch time: 40.18 s\n",
      "2024-01-11 02:04:59.257499: \n",
      "2024-01-11 02:04:59.264496: Epoch 886\n",
      "2024-01-11 02:04:59.272012: Current learning rate: 0.00142\n",
      "2024-01-11 02:05:39.383737: train_loss -0.9551\n",
      "2024-01-11 02:05:39.394245: val_loss -0.8352\n",
      "2024-01-11 02:05:39.426755: Pseudo dice [0.8668, 0.9588, 0.943]\n",
      "2024-01-11 02:05:39.435755: Epoch time: 40.13 s\n",
      "2024-01-11 02:05:40.877477: \n",
      "2024-01-11 02:05:40.889537: Epoch 887\n",
      "2024-01-11 02:05:40.900065: Current learning rate: 0.00141\n",
      "2024-01-11 02:06:20.793352: train_loss -0.9557\n",
      "2024-01-11 02:06:20.800353: val_loss -0.8335\n",
      "2024-01-11 02:06:20.808866: Pseudo dice [0.8683, 0.9587, 0.9431]\n",
      "2024-01-11 02:06:20.816869: Epoch time: 39.92 s\n",
      "2024-01-11 02:06:22.145649: \n",
      "2024-01-11 02:06:22.157724: Epoch 888\n",
      "2024-01-11 02:06:22.162646: Current learning rate: 0.00139\n",
      "2024-01-11 02:07:02.068187: train_loss -0.9549\n",
      "2024-01-11 02:07:02.106189: val_loss -0.8346\n",
      "2024-01-11 02:07:02.114189: Pseudo dice [0.8662, 0.9589, 0.9427]\n",
      "2024-01-11 02:07:02.120703: Epoch time: 39.92 s\n",
      "2024-01-11 02:07:03.464107: \n",
      "2024-01-11 02:07:03.473453: Epoch 889\n",
      "2024-01-11 02:07:03.478364: Current learning rate: 0.00138\n",
      "2024-01-11 02:07:43.459082: train_loss -0.9551\n",
      "2024-01-11 02:07:43.487083: val_loss -0.8379\n",
      "2024-01-11 02:07:43.495588: Pseudo dice [0.8681, 0.9596, 0.9441]\n",
      "2024-01-11 02:07:43.500596: Epoch time: 40.0 s\n",
      "2024-01-11 02:07:44.876986: \n",
      "2024-01-11 02:07:44.882982: Epoch 890\n",
      "2024-01-11 02:07:44.888989: Current learning rate: 0.00137\n",
      "2024-01-11 02:08:24.909451: train_loss -0.9563\n",
      "2024-01-11 02:08:24.918453: val_loss -0.8359\n",
      "2024-01-11 02:08:24.926454: Pseudo dice [0.8722, 0.9594, 0.9431]\n",
      "2024-01-11 02:08:24.933452: Epoch time: 40.03 s\n",
      "2024-01-11 02:08:26.371447: \n",
      "2024-01-11 02:08:26.379011: Epoch 891\n",
      "2024-01-11 02:08:26.384016: Current learning rate: 0.00136\n",
      "2024-01-11 02:09:06.894207: train_loss -0.9548\n",
      "2024-01-11 02:09:06.905208: val_loss -0.8314\n",
      "2024-01-11 02:09:06.912209: Pseudo dice [0.8662, 0.9586, 0.9435]\n",
      "2024-01-11 02:09:06.921209: Epoch time: 40.52 s\n",
      "2024-01-11 02:09:08.322647: \n",
      "2024-01-11 02:09:08.327647: Epoch 892\n",
      "2024-01-11 02:09:08.332656: Current learning rate: 0.00135\n",
      "2024-01-11 02:09:48.758427: train_loss -0.9563\n",
      "2024-01-11 02:09:48.766424: val_loss -0.8362\n",
      "2024-01-11 02:09:48.773425: Pseudo dice [0.8687, 0.9601, 0.9449]\n",
      "2024-01-11 02:09:48.780424: Epoch time: 40.44 s\n",
      "2024-01-11 02:09:50.314232: \n",
      "2024-01-11 02:09:50.321232: Epoch 893\n",
      "2024-01-11 02:09:50.326232: Current learning rate: 0.00134\n",
      "2024-01-11 02:10:29.942660: train_loss -0.9556\n",
      "2024-01-11 02:10:29.948661: val_loss -0.8355\n",
      "2024-01-11 02:10:29.955663: Pseudo dice [0.8682, 0.9593, 0.9437]\n",
      "2024-01-11 02:10:29.960664: Epoch time: 39.63 s\n",
      "2024-01-11 02:10:31.344757: \n",
      "2024-01-11 02:10:31.351146: Epoch 894\n",
      "2024-01-11 02:10:31.359203: Current learning rate: 0.00133\n",
      "2024-01-11 02:11:11.079322: train_loss -0.9565\n",
      "2024-01-11 02:11:11.085321: val_loss -0.84\n",
      "2024-01-11 02:11:11.093321: Pseudo dice [0.8736, 0.9595, 0.9445]\n",
      "2024-01-11 02:11:11.100321: Epoch time: 39.74 s\n",
      "2024-01-11 02:11:12.473543: \n",
      "2024-01-11 02:11:12.478708: Epoch 895\n",
      "2024-01-11 02:11:12.482778: Current learning rate: 0.00132\n",
      "2024-01-11 02:11:52.102555: train_loss -0.9551\n",
      "2024-01-11 02:11:52.112556: val_loss -0.8378\n",
      "2024-01-11 02:11:52.121558: Pseudo dice [0.872, 0.96, 0.9454]\n",
      "2024-01-11 02:11:52.126560: Epoch time: 39.63 s\n",
      "2024-01-11 02:11:53.472069: \n",
      "2024-01-11 02:11:53.478431: Epoch 896\n",
      "2024-01-11 02:11:53.487504: Current learning rate: 0.0013\n",
      "2024-01-11 02:12:33.568867: train_loss -0.9555\n",
      "2024-01-11 02:12:33.576879: val_loss -0.8413\n",
      "2024-01-11 02:12:33.583868: Pseudo dice [0.8712, 0.9598, 0.945]\n",
      "2024-01-11 02:12:33.589868: Epoch time: 40.1 s\n",
      "2024-01-11 02:12:34.978696: \n",
      "2024-01-11 02:12:34.984593: Epoch 897\n",
      "2024-01-11 02:12:34.989528: Current learning rate: 0.00129\n",
      "2024-01-11 02:13:14.986368: train_loss -0.9559\n",
      "2024-01-11 02:13:14.994889: val_loss -0.8352\n",
      "2024-01-11 02:13:15.002875: Pseudo dice [0.8684, 0.959, 0.9436]\n",
      "2024-01-11 02:13:15.009875: Epoch time: 40.01 s\n",
      "2024-01-11 02:13:16.379535: \n",
      "2024-01-11 02:13:16.385534: Epoch 898\n",
      "2024-01-11 02:13:16.391066: Current learning rate: 0.00128\n",
      "2024-01-11 02:13:56.570130: train_loss -0.9554\n",
      "2024-01-11 02:13:56.579129: val_loss -0.8339\n",
      "2024-01-11 02:13:56.589133: Pseudo dice [0.8708, 0.9585, 0.9432]\n",
      "2024-01-11 02:13:56.596135: Epoch time: 40.19 s\n",
      "2024-01-11 02:13:58.023215: \n",
      "2024-01-11 02:13:58.030380: Epoch 899\n",
      "2024-01-11 02:13:58.036461: Current learning rate: 0.00127\n",
      "2024-01-11 02:14:37.995380: train_loss -0.9567\n",
      "2024-01-11 02:14:38.004384: val_loss -0.837\n",
      "2024-01-11 02:14:38.010391: Pseudo dice [0.8677, 0.9593, 0.9445]\n",
      "2024-01-11 02:14:38.016390: Epoch time: 39.97 s\n",
      "2024-01-11 02:14:39.647495: \n",
      "2024-01-11 02:14:39.656566: Epoch 900\n",
      "2024-01-11 02:14:39.660500: Current learning rate: 0.00126\n",
      "2024-01-11 02:15:19.748920: train_loss -0.956\n",
      "2024-01-11 02:15:19.757433: val_loss -0.8384\n",
      "2024-01-11 02:15:19.763235: Pseudo dice [0.8705, 0.9598, 0.9441]\n",
      "2024-01-11 02:15:19.768234: Epoch time: 40.1 s\n",
      "2024-01-11 02:15:21.265124: \n",
      "2024-01-11 02:15:21.273125: Epoch 901\n",
      "2024-01-11 02:15:21.279124: Current learning rate: 0.00125\n",
      "2024-01-11 02:16:01.541082: train_loss -0.9561\n",
      "2024-01-11 02:16:01.574579: val_loss -0.8348\n",
      "2024-01-11 02:16:01.583579: Pseudo dice [0.8669, 0.9589, 0.9435]\n",
      "2024-01-11 02:16:01.588592: Epoch time: 40.28 s\n",
      "2024-01-11 02:16:03.008920: \n",
      "2024-01-11 02:16:03.015404: Epoch 902\n",
      "2024-01-11 02:16:03.020477: Current learning rate: 0.00124\n",
      "2024-01-11 02:16:43.002516: train_loss -0.9561\n",
      "2024-01-11 02:16:43.012513: val_loss -0.8384\n",
      "2024-01-11 02:16:43.052033: Pseudo dice [0.8741, 0.9595, 0.9448]\n",
      "2024-01-11 02:16:43.060033: Epoch time: 39.99 s\n",
      "2024-01-11 02:16:44.404986: \n",
      "2024-01-11 02:16:44.410935: Epoch 903\n",
      "2024-01-11 02:16:44.414935: Current learning rate: 0.00122\n",
      "2024-01-11 02:17:24.498842: train_loss -0.9557\n",
      "2024-01-11 02:17:24.507847: val_loss -0.8326\n",
      "2024-01-11 02:17:24.513843: Pseudo dice [0.8681, 0.959, 0.9431]\n",
      "2024-01-11 02:17:24.522847: Epoch time: 40.09 s\n",
      "2024-01-11 02:17:25.895625: \n",
      "2024-01-11 02:17:25.901497: Epoch 904\n",
      "2024-01-11 02:17:25.905571: Current learning rate: 0.00121\n",
      "2024-01-11 02:18:05.804854: train_loss -0.9558\n",
      "2024-01-11 02:18:05.813862: val_loss -0.8371\n",
      "2024-01-11 02:18:05.822855: Pseudo dice [0.8728, 0.9592, 0.9436]\n",
      "2024-01-11 02:18:05.832854: Epoch time: 39.91 s\n",
      "2024-01-11 02:18:07.257108: \n",
      "2024-01-11 02:18:07.262108: Epoch 905\n",
      "2024-01-11 02:18:07.266761: Current learning rate: 0.0012\n",
      "2024-01-11 02:18:47.129787: train_loss -0.9562\n",
      "2024-01-11 02:18:47.142787: val_loss -0.8349\n",
      "2024-01-11 02:18:47.169797: Pseudo dice [0.8693, 0.9586, 0.9425]\n",
      "2024-01-11 02:18:47.178301: Epoch time: 39.87 s\n",
      "2024-01-11 02:18:48.523533: \n",
      "2024-01-11 02:18:48.531060: Epoch 906\n",
      "2024-01-11 02:18:48.539112: Current learning rate: 0.00119\n",
      "2024-01-11 02:19:28.643682: train_loss -0.9569\n",
      "2024-01-11 02:19:28.653683: val_loss -0.8344\n",
      "2024-01-11 02:19:28.689688: Pseudo dice [0.8694, 0.9593, 0.9441]\n",
      "2024-01-11 02:19:28.700244: Epoch time: 40.12 s\n",
      "2024-01-11 02:19:30.021648: \n",
      "2024-01-11 02:19:30.028594: Epoch 907\n",
      "2024-01-11 02:19:30.037390: Current learning rate: 0.00118\n",
      "2024-01-11 02:20:10.164109: train_loss -0.9556\n",
      "2024-01-11 02:20:10.172108: val_loss -0.8348\n",
      "2024-01-11 02:20:10.183084: Pseudo dice [0.8702, 0.9591, 0.9433]\n",
      "2024-01-11 02:20:10.190084: Epoch time: 40.14 s\n",
      "2024-01-11 02:20:11.535407: \n",
      "2024-01-11 02:20:11.545039: Epoch 908\n",
      "2024-01-11 02:20:11.555020: Current learning rate: 0.00117\n",
      "2024-01-11 02:20:51.629070: train_loss -0.9569\n",
      "2024-01-11 02:20:51.640583: val_loss -0.8343\n",
      "2024-01-11 02:20:51.646582: Pseudo dice [0.8708, 0.9588, 0.9436]\n",
      "2024-01-11 02:20:51.702241: Epoch time: 40.09 s\n",
      "2024-01-11 02:20:52.997371: \n",
      "2024-01-11 02:20:53.005438: Epoch 909\n",
      "2024-01-11 02:20:53.011797: Current learning rate: 0.00116\n",
      "2024-01-11 02:21:33.007146: train_loss -0.9563\n",
      "2024-01-11 02:21:33.014147: val_loss -0.8376\n",
      "2024-01-11 02:21:33.021147: Pseudo dice [0.8706, 0.9592, 0.9439]\n",
      "2024-01-11 02:21:33.029147: Epoch time: 40.01 s\n",
      "2024-01-11 02:21:34.388060: \n",
      "2024-01-11 02:21:34.394127: Epoch 910\n",
      "2024-01-11 02:21:34.403097: Current learning rate: 0.00115\n",
      "2024-01-11 02:22:14.433582: train_loss -0.9565\n",
      "2024-01-11 02:22:14.442580: val_loss -0.8327\n",
      "2024-01-11 02:22:14.450582: Pseudo dice [0.8682, 0.9582, 0.9429]\n",
      "2024-01-11 02:22:14.479625: Epoch time: 40.05 s\n",
      "2024-01-11 02:22:15.858909: \n",
      "2024-01-11 02:22:15.867674: Epoch 911\n",
      "2024-01-11 02:22:15.877547: Current learning rate: 0.00113\n",
      "2024-01-11 02:22:55.960229: train_loss -0.9558\n",
      "2024-01-11 02:22:55.973231: val_loss -0.8374\n",
      "2024-01-11 02:22:56.006749: Pseudo dice [0.8681, 0.9597, 0.945]\n",
      "2024-01-11 02:22:56.013749: Epoch time: 40.1 s\n",
      "2024-01-11 02:22:57.378421: \n",
      "2024-01-11 02:22:57.384501: Epoch 912\n",
      "2024-01-11 02:22:57.389459: Current learning rate: 0.00112\n",
      "2024-01-11 02:23:37.277663: train_loss -0.9565\n",
      "2024-01-11 02:23:37.285663: val_loss -0.8383\n",
      "2024-01-11 02:23:37.292664: Pseudo dice [0.8734, 0.9597, 0.9445]\n",
      "2024-01-11 02:23:37.298671: Epoch time: 39.9 s\n",
      "2024-01-11 02:23:38.651877: \n",
      "2024-01-11 02:23:38.661906: Epoch 913\n",
      "2024-01-11 02:23:38.666916: Current learning rate: 0.00111\n",
      "2024-01-11 02:24:18.627286: train_loss -0.9565\n",
      "2024-01-11 02:24:18.648795: val_loss -0.8375\n",
      "2024-01-11 02:24:18.657796: Pseudo dice [0.8718, 0.9591, 0.9443]\n",
      "2024-01-11 02:24:18.665798: Epoch time: 39.98 s\n",
      "2024-01-11 02:24:20.043750: \n",
      "2024-01-11 02:24:20.051094: Epoch 914\n",
      "2024-01-11 02:24:20.058100: Current learning rate: 0.0011\n",
      "2024-01-11 02:24:59.995779: train_loss -0.9562\n",
      "2024-01-11 02:25:00.029782: val_loss -0.8332\n",
      "2024-01-11 02:25:00.040787: Pseudo dice [0.8693, 0.9585, 0.9434]\n",
      "2024-01-11 02:25:00.048307: Epoch time: 39.95 s\n",
      "2024-01-11 02:25:01.483439: \n",
      "2024-01-11 02:25:01.489550: Epoch 915\n",
      "2024-01-11 02:25:01.494620: Current learning rate: 0.00109\n",
      "2024-01-11 02:25:41.542129: train_loss -0.9568\n",
      "2024-01-11 02:25:41.551129: val_loss -0.8351\n",
      "2024-01-11 02:25:41.559131: Pseudo dice [0.8725, 0.9589, 0.9437]\n",
      "2024-01-11 02:25:41.568128: Epoch time: 40.06 s\n",
      "2024-01-11 02:25:42.989845: \n",
      "2024-01-11 02:25:42.996266: Epoch 916\n",
      "2024-01-11 02:25:43.005271: Current learning rate: 0.00108\n",
      "2024-01-11 02:26:23.056309: train_loss -0.9564\n",
      "2024-01-11 02:26:23.065309: val_loss -0.8343\n",
      "2024-01-11 02:26:23.076312: Pseudo dice [0.8698, 0.9586, 0.9435]\n",
      "2024-01-11 02:26:23.084308: Epoch time: 40.07 s\n",
      "2024-01-11 02:26:24.435220: \n",
      "2024-01-11 02:26:24.446146: Epoch 917\n",
      "2024-01-11 02:26:24.454211: Current learning rate: 0.00106\n",
      "2024-01-11 02:27:04.365819: train_loss -0.9569\n",
      "2024-01-11 02:27:04.372819: val_loss -0.8378\n",
      "2024-01-11 02:27:04.379819: Pseudo dice [0.8723, 0.959, 0.9443]\n",
      "2024-01-11 02:27:04.399825: Epoch time: 39.93 s\n",
      "2024-01-11 02:27:05.753387: \n",
      "2024-01-11 02:27:05.759330: Epoch 918\n",
      "2024-01-11 02:27:05.763338: Current learning rate: 0.00105\n",
      "2024-01-11 02:27:45.707016: train_loss -0.9568\n",
      "2024-01-11 02:27:45.716024: val_loss -0.838\n",
      "2024-01-11 02:27:45.746529: Pseudo dice [0.873, 0.9591, 0.9437]\n",
      "2024-01-11 02:27:45.754530: Epoch time: 39.95 s\n",
      "2024-01-11 02:27:47.287933: \n",
      "2024-01-11 02:27:47.293004: Epoch 919\n",
      "2024-01-11 02:27:47.297995: Current learning rate: 0.00104\n",
      "2024-01-11 02:28:27.362584: train_loss -0.9573\n",
      "2024-01-11 02:28:27.373583: val_loss -0.8373\n",
      "2024-01-11 02:28:27.379585: Pseudo dice [0.8678, 0.9594, 0.9444]\n",
      "2024-01-11 02:28:27.384584: Epoch time: 40.08 s\n",
      "2024-01-11 02:28:28.786339: \n",
      "2024-01-11 02:28:28.793790: Epoch 920\n",
      "2024-01-11 02:28:28.802290: Current learning rate: 0.00103\n",
      "2024-01-11 02:29:08.814776: train_loss -0.9563\n",
      "2024-01-11 02:29:08.823776: val_loss -0.8333\n",
      "2024-01-11 02:29:08.831775: Pseudo dice [0.8654, 0.9589, 0.944]\n",
      "2024-01-11 02:29:08.858779: Epoch time: 40.03 s\n",
      "2024-01-11 02:29:10.187424: \n",
      "2024-01-11 02:29:10.194430: Epoch 921\n",
      "2024-01-11 02:29:10.201366: Current learning rate: 0.00102\n",
      "2024-01-11 02:29:50.353960: train_loss -0.9564\n",
      "2024-01-11 02:29:50.360971: val_loss -0.833\n",
      "2024-01-11 02:29:50.366974: Pseudo dice [0.8621, 0.959, 0.9431]\n",
      "2024-01-11 02:29:50.376558: Epoch time: 40.17 s\n",
      "2024-01-11 02:29:51.760918: \n",
      "2024-01-11 02:29:51.772283: Epoch 922\n",
      "2024-01-11 02:29:51.776949: Current learning rate: 0.00101\n",
      "2024-01-11 02:30:31.842087: train_loss -0.9561\n",
      "2024-01-11 02:30:31.849088: val_loss -0.8339\n",
      "2024-01-11 02:30:31.855086: Pseudo dice [0.87, 0.9588, 0.9441]\n",
      "2024-01-11 02:30:31.861086: Epoch time: 40.08 s\n",
      "2024-01-11 02:30:33.225142: \n",
      "2024-01-11 02:30:33.233141: Epoch 923\n",
      "2024-01-11 02:30:33.242142: Current learning rate: 0.001\n",
      "2024-01-11 02:31:13.296757: train_loss -0.9563\n",
      "2024-01-11 02:31:13.303767: val_loss -0.832\n",
      "2024-01-11 02:31:13.314275: Pseudo dice [0.8652, 0.9584, 0.9432]\n",
      "2024-01-11 02:31:13.320275: Epoch time: 40.07 s\n",
      "2024-01-11 02:31:14.761323: \n",
      "2024-01-11 02:31:14.769568: Epoch 924\n",
      "2024-01-11 02:31:14.774570: Current learning rate: 0.00098\n",
      "2024-01-11 02:31:54.845153: train_loss -0.956\n",
      "2024-01-11 02:31:54.851666: val_loss -0.8321\n",
      "2024-01-11 02:31:54.858668: Pseudo dice [0.8679, 0.9586, 0.943]\n",
      "2024-01-11 02:31:54.864669: Epoch time: 40.09 s\n",
      "2024-01-11 02:31:56.307666: \n",
      "2024-01-11 02:31:56.316671: Epoch 925\n",
      "2024-01-11 02:31:56.320668: Current learning rate: 0.00097\n",
      "2024-01-11 02:32:36.341421: train_loss -0.9566\n",
      "2024-01-11 02:32:36.348422: val_loss -0.8335\n",
      "2024-01-11 02:32:36.357426: Pseudo dice [0.8686, 0.9593, 0.9443]\n",
      "2024-01-11 02:32:36.365939: Epoch time: 40.04 s\n",
      "2024-01-11 02:32:37.736542: \n",
      "2024-01-11 02:32:37.743330: Epoch 926\n",
      "2024-01-11 02:32:37.748334: Current learning rate: 0.00096\n",
      "2024-01-11 02:33:17.890911: train_loss -0.9574\n",
      "2024-01-11 02:33:17.897911: val_loss -0.8341\n",
      "2024-01-11 02:33:17.904912: Pseudo dice [0.8664, 0.9593, 0.9446]\n",
      "2024-01-11 02:33:17.910913: Epoch time: 40.16 s\n",
      "2024-01-11 02:33:19.286505: \n",
      "2024-01-11 02:33:19.291575: Epoch 927\n",
      "2024-01-11 02:33:19.295569: Current learning rate: 0.00095\n",
      "2024-01-11 02:33:59.413488: train_loss -0.9567\n",
      "2024-01-11 02:33:59.425999: val_loss -0.8376\n",
      "2024-01-11 02:33:59.461193: Pseudo dice [0.8714, 0.9595, 0.9442]\n",
      "2024-01-11 02:33:59.471214: Epoch time: 40.13 s\n",
      "2024-01-11 02:34:00.864282: \n",
      "2024-01-11 02:34:00.870042: Epoch 928\n",
      "2024-01-11 02:34:00.876045: Current learning rate: 0.00094\n",
      "2024-01-11 02:34:40.920822: train_loss -0.957\n",
      "2024-01-11 02:34:40.928823: val_loss -0.8334\n",
      "2024-01-11 02:34:40.937341: Pseudo dice [0.8671, 0.9585, 0.9426]\n",
      "2024-01-11 02:34:40.944341: Epoch time: 40.06 s\n",
      "2024-01-11 02:34:42.287957: \n",
      "2024-01-11 02:34:42.296263: Epoch 929\n",
      "2024-01-11 02:34:42.304331: Current learning rate: 0.00092\n",
      "2024-01-11 02:35:22.185108: train_loss -0.9563\n",
      "2024-01-11 02:35:22.193109: val_loss -0.8337\n",
      "2024-01-11 02:35:22.200109: Pseudo dice [0.867, 0.9587, 0.9433]\n",
      "2024-01-11 02:35:22.208109: Epoch time: 39.9 s\n",
      "2024-01-11 02:35:23.656157: \n",
      "2024-01-11 02:35:23.666737: Epoch 930\n",
      "2024-01-11 02:35:23.676738: Current learning rate: 0.00091\n",
      "2024-01-11 02:36:03.543622: train_loss -0.9574\n",
      "2024-01-11 02:36:03.549621: val_loss -0.8319\n",
      "2024-01-11 02:36:03.559627: Pseudo dice [0.8627, 0.9588, 0.9433]\n",
      "2024-01-11 02:36:03.567627: Epoch time: 39.89 s\n",
      "2024-01-11 02:36:04.938324: \n",
      "2024-01-11 02:36:04.947848: Epoch 931\n",
      "2024-01-11 02:36:04.954906: Current learning rate: 0.0009\n",
      "2024-01-11 02:36:45.081574: train_loss -0.9567\n",
      "2024-01-11 02:36:45.091084: val_loss -0.8357\n",
      "2024-01-11 02:36:45.097084: Pseudo dice [0.8712, 0.9589, 0.9437]\n",
      "2024-01-11 02:36:45.106083: Epoch time: 40.14 s\n",
      "2024-01-11 02:36:46.465800: \n",
      "2024-01-11 02:36:46.471178: Epoch 932\n",
      "2024-01-11 02:36:46.478223: Current learning rate: 0.00089\n",
      "2024-01-11 02:37:26.433384: train_loss -0.9571\n",
      "2024-01-11 02:37:26.444383: val_loss -0.8302\n",
      "2024-01-11 02:37:26.453383: Pseudo dice [0.8637, 0.9589, 0.9427]\n",
      "2024-01-11 02:37:26.461384: Epoch time: 39.97 s\n",
      "2024-01-11 02:37:27.842300: \n",
      "2024-01-11 02:37:27.848034: Epoch 933\n",
      "2024-01-11 02:37:27.856126: Current learning rate: 0.00088\n",
      "2024-01-11 02:38:07.860372: train_loss -0.9568\n",
      "2024-01-11 02:38:07.871371: val_loss -0.8393\n",
      "2024-01-11 02:38:07.880371: Pseudo dice [0.8729, 0.9596, 0.9442]\n",
      "2024-01-11 02:38:07.901370: Epoch time: 40.02 s\n",
      "2024-01-11 02:38:09.571880: \n",
      "2024-01-11 02:38:09.579947: Epoch 934\n",
      "2024-01-11 02:38:09.585947: Current learning rate: 0.00087\n",
      "2024-01-11 02:38:49.682261: train_loss -0.9571\n",
      "2024-01-11 02:38:49.692262: val_loss -0.8329\n",
      "2024-01-11 02:38:49.703263: Pseudo dice [0.8661, 0.9589, 0.9434]\n",
      "2024-01-11 02:38:49.710262: Epoch time: 40.11 s\n",
      "2024-01-11 02:38:51.183793: \n",
      "2024-01-11 02:38:51.190793: Epoch 935\n",
      "2024-01-11 02:38:51.199739: Current learning rate: 0.00085\n",
      "2024-01-11 02:39:31.239231: train_loss -0.9571\n",
      "2024-01-11 02:39:31.246246: val_loss -0.8338\n",
      "2024-01-11 02:39:31.251234: Pseudo dice [0.8688, 0.9591, 0.9431]\n",
      "2024-01-11 02:39:31.257556: Epoch time: 40.06 s\n",
      "2024-01-11 02:39:32.591637: \n",
      "2024-01-11 02:39:32.596638: Epoch 936\n",
      "2024-01-11 02:39:32.605705: Current learning rate: 0.00084\n",
      "2024-01-11 02:40:12.575712: train_loss -0.9571\n",
      "2024-01-11 02:40:12.582713: val_loss -0.832\n",
      "2024-01-11 02:40:12.590714: Pseudo dice [0.867, 0.9587, 0.9428]\n",
      "2024-01-11 02:40:12.597714: Epoch time: 39.99 s\n",
      "2024-01-11 02:40:14.152120: \n",
      "2024-01-11 02:40:14.161251: Epoch 937\n",
      "2024-01-11 02:40:14.167737: Current learning rate: 0.00083\n",
      "2024-01-11 02:40:54.055188: train_loss -0.957\n",
      "2024-01-11 02:40:54.062182: val_loss -0.8371\n",
      "2024-01-11 02:40:54.067185: Pseudo dice [0.8658, 0.9595, 0.9443]\n",
      "2024-01-11 02:40:54.075185: Epoch time: 39.91 s\n",
      "2024-01-11 02:40:55.376285: \n",
      "2024-01-11 02:40:55.382565: Epoch 938\n",
      "2024-01-11 02:40:55.388119: Current learning rate: 0.00082\n",
      "2024-01-11 02:41:35.308706: train_loss -0.9569\n",
      "2024-01-11 02:41:35.315715: val_loss -0.8358\n",
      "2024-01-11 02:41:35.330289: Pseudo dice [0.8696, 0.9588, 0.943]\n",
      "2024-01-11 02:41:35.336805: Epoch time: 39.93 s\n",
      "2024-01-11 02:41:36.838562: \n",
      "2024-01-11 02:41:36.848276: Epoch 939\n",
      "2024-01-11 02:41:36.853215: Current learning rate: 0.00081\n",
      "2024-01-11 02:42:16.773650: train_loss -0.9573\n",
      "2024-01-11 02:42:16.782651: val_loss -0.8327\n",
      "2024-01-11 02:42:16.806649: Pseudo dice [0.8671, 0.9587, 0.9431]\n",
      "2024-01-11 02:42:16.813650: Epoch time: 39.94 s\n",
      "2024-01-11 02:42:18.237456: \n",
      "2024-01-11 02:42:18.244936: Epoch 940\n",
      "2024-01-11 02:42:18.250925: Current learning rate: 0.00079\n",
      "2024-01-11 02:42:58.163566: train_loss -0.9572\n",
      "2024-01-11 02:42:58.171567: val_loss -0.8359\n",
      "2024-01-11 02:42:58.179569: Pseudo dice [0.8679, 0.9596, 0.9444]\n",
      "2024-01-11 02:42:58.187566: Epoch time: 39.93 s\n",
      "2024-01-11 02:42:59.551095: \n",
      "2024-01-11 02:42:59.559097: Epoch 941\n",
      "2024-01-11 02:42:59.563584: Current learning rate: 0.00078\n",
      "2024-01-11 02:43:39.601742: train_loss -0.9571\n",
      "2024-01-11 02:43:39.613742: val_loss -0.8342\n",
      "2024-01-11 02:43:39.641747: Pseudo dice [0.8698, 0.9587, 0.9429]\n",
      "2024-01-11 02:43:39.650256: Epoch time: 40.05 s\n",
      "2024-01-11 02:43:41.160111: \n",
      "2024-01-11 02:43:41.166105: Epoch 942\n",
      "2024-01-11 02:43:41.171103: Current learning rate: 0.00077\n",
      "2024-01-11 02:44:21.232530: train_loss -0.957\n",
      "2024-01-11 02:44:21.242529: val_loss -0.8349\n",
      "2024-01-11 02:44:21.250529: Pseudo dice [0.8714, 0.959, 0.9437]\n",
      "2024-01-11 02:44:21.290064: Epoch time: 40.07 s\n",
      "2024-01-11 02:44:22.701504: \n",
      "2024-01-11 02:44:22.711335: Epoch 943\n",
      "2024-01-11 02:44:22.716403: Current learning rate: 0.00076\n",
      "2024-01-11 02:45:02.778182: train_loss -0.9581\n",
      "2024-01-11 02:45:02.786180: val_loss -0.833\n",
      "2024-01-11 02:45:02.793694: Pseudo dice [0.867, 0.9589, 0.9441]\n",
      "2024-01-11 02:45:02.819695: Epoch time: 40.08 s\n",
      "2024-01-11 02:45:04.234267: \n",
      "2024-01-11 02:45:04.243270: Epoch 944\n",
      "2024-01-11 02:45:04.250268: Current learning rate: 0.00075\n",
      "2024-01-11 02:45:44.335616: train_loss -0.9572\n",
      "2024-01-11 02:45:44.347623: val_loss -0.8337\n",
      "2024-01-11 02:45:44.376613: Pseudo dice [0.865, 0.9586, 0.9429]\n",
      "2024-01-11 02:45:44.388615: Epoch time: 40.1 s\n",
      "2024-01-11 02:45:45.763687: \n",
      "2024-01-11 02:45:45.772790: Epoch 945\n",
      "2024-01-11 02:45:45.778881: Current learning rate: 0.00074\n",
      "2024-01-11 02:46:25.608366: train_loss -0.9576\n",
      "2024-01-11 02:46:25.635370: val_loss -0.8337\n",
      "2024-01-11 02:46:25.642881: Pseudo dice [0.8631, 0.9591, 0.9442]\n",
      "2024-01-11 02:46:25.648879: Epoch time: 39.85 s\n",
      "2024-01-11 02:46:27.053558: \n",
      "2024-01-11 02:46:27.061445: Epoch 946\n",
      "2024-01-11 02:46:27.070517: Current learning rate: 0.00072\n",
      "2024-01-11 02:47:07.001394: train_loss -0.9578\n",
      "2024-01-11 02:47:07.008397: val_loss -0.8335\n",
      "2024-01-11 02:47:07.018393: Pseudo dice [0.8692, 0.9585, 0.9433]\n",
      "2024-01-11 02:47:07.024394: Epoch time: 39.95 s\n",
      "2024-01-11 02:47:08.615675: \n",
      "2024-01-11 02:47:08.621105: Epoch 947\n",
      "2024-01-11 02:47:08.629172: Current learning rate: 0.00071\n",
      "2024-01-11 02:47:48.708508: train_loss -0.9572\n",
      "2024-01-11 02:47:48.716509: val_loss -0.8333\n",
      "2024-01-11 02:47:48.724507: Pseudo dice [0.8644, 0.9591, 0.944]\n",
      "2024-01-11 02:47:48.730509: Epoch time: 40.09 s\n",
      "2024-01-11 02:47:50.158083: \n",
      "2024-01-11 02:47:50.165595: Epoch 948\n",
      "2024-01-11 02:47:50.173332: Current learning rate: 0.0007\n",
      "2024-01-11 02:48:30.196358: train_loss -0.9582\n",
      "2024-01-11 02:48:30.237352: val_loss -0.836\n",
      "2024-01-11 02:48:30.248351: Pseudo dice [0.8742, 0.9589, 0.9437]\n",
      "2024-01-11 02:48:30.256353: Epoch time: 40.04 s\n",
      "2024-01-11 02:48:31.582408: \n",
      "2024-01-11 02:48:31.588418: Epoch 949\n",
      "2024-01-11 02:48:31.595137: Current learning rate: 0.00069\n",
      "2024-01-11 02:49:11.570807: train_loss -0.957\n",
      "2024-01-11 02:49:11.579808: val_loss -0.8344\n",
      "2024-01-11 02:49:11.587807: Pseudo dice [0.8692, 0.9591, 0.9441]\n",
      "2024-01-11 02:49:11.595812: Epoch time: 39.99 s\n",
      "2024-01-11 02:49:13.267311: \n",
      "2024-01-11 02:49:13.274312: Epoch 950\n",
      "2024-01-11 02:49:13.279312: Current learning rate: 0.00067\n",
      "2024-01-11 02:49:53.510344: train_loss -0.9569\n",
      "2024-01-11 02:49:53.520357: val_loss -0.83\n",
      "2024-01-11 02:49:53.529859: Pseudo dice [0.8682, 0.9585, 0.9429]\n",
      "2024-01-11 02:49:53.538860: Epoch time: 40.24 s\n",
      "2024-01-11 02:49:54.971785: \n",
      "2024-01-11 02:49:54.977995: Epoch 951\n",
      "2024-01-11 02:49:54.983011: Current learning rate: 0.00066\n",
      "2024-01-11 02:50:35.157874: train_loss -0.9581\n",
      "2024-01-11 02:50:35.165950: val_loss -0.8383\n",
      "2024-01-11 02:50:35.172879: Pseudo dice [0.871, 0.9594, 0.9441]\n",
      "2024-01-11 02:50:35.196873: Epoch time: 40.19 s\n",
      "2024-01-11 02:50:36.453766: \n",
      "2024-01-11 02:50:36.460336: Epoch 952\n",
      "2024-01-11 02:50:36.465340: Current learning rate: 0.00065\n",
      "2024-01-11 02:51:16.270210: train_loss -0.9576\n",
      "2024-01-11 02:51:16.278213: val_loss -0.8368\n",
      "2024-01-11 02:51:16.287724: Pseudo dice [0.8686, 0.9593, 0.9442]\n",
      "2024-01-11 02:51:16.294726: Epoch time: 39.82 s\n",
      "2024-01-11 02:51:17.611907: \n",
      "2024-01-11 02:51:17.618252: Epoch 953\n",
      "2024-01-11 02:51:17.626321: Current learning rate: 0.00064\n",
      "2024-01-11 02:51:57.657870: train_loss -0.9571\n",
      "2024-01-11 02:51:57.666867: val_loss -0.8358\n",
      "2024-01-11 02:51:57.675873: Pseudo dice [0.871, 0.9594, 0.9439]\n",
      "2024-01-11 02:51:57.702391: Epoch time: 40.05 s\n",
      "2024-01-11 02:51:59.108385: \n",
      "2024-01-11 02:51:59.118327: Epoch 954\n",
      "2024-01-11 02:51:59.123321: Current learning rate: 0.00063\n",
      "2024-01-11 02:52:39.137774: train_loss -0.9581\n",
      "2024-01-11 02:52:39.146852: val_loss -0.8357\n",
      "2024-01-11 02:52:39.155774: Pseudo dice [0.8711, 0.9592, 0.9444]\n",
      "2024-01-11 02:52:39.160774: Epoch time: 40.03 s\n",
      "2024-01-11 02:52:40.545834: \n",
      "2024-01-11 02:52:40.552633: Epoch 955\n",
      "2024-01-11 02:52:40.559646: Current learning rate: 0.00061\n",
      "2024-01-11 02:53:20.520342: train_loss -0.9577\n",
      "2024-01-11 02:53:20.527341: val_loss -0.8337\n",
      "2024-01-11 02:53:20.534349: Pseudo dice [0.8706, 0.9586, 0.942]\n",
      "2024-01-11 02:53:20.541347: Epoch time: 39.98 s\n",
      "2024-01-11 02:53:21.936355: \n",
      "2024-01-11 02:53:21.943367: Epoch 956\n",
      "2024-01-11 02:53:21.951023: Current learning rate: 0.0006\n",
      "2024-01-11 02:54:02.141894: train_loss -0.9573\n",
      "2024-01-11 02:54:02.150901: val_loss -0.8351\n",
      "2024-01-11 02:54:02.158414: Pseudo dice [0.8712, 0.9587, 0.9437]\n",
      "2024-01-11 02:54:02.166414: Epoch time: 40.21 s\n",
      "2024-01-11 02:54:03.802960: \n",
      "2024-01-11 02:54:03.808568: Epoch 957\n",
      "2024-01-11 02:54:03.812575: Current learning rate: 0.00059\n",
      "2024-01-11 02:54:43.671264: train_loss -0.9571\n",
      "2024-01-11 02:54:43.682267: val_loss -0.8365\n",
      "2024-01-11 02:54:43.690777: Pseudo dice [0.8716, 0.9588, 0.9436]\n",
      "2024-01-11 02:54:43.719776: Epoch time: 39.87 s\n",
      "2024-01-11 02:54:45.103170: \n",
      "2024-01-11 02:54:45.108902: Epoch 958\n",
      "2024-01-11 02:54:45.112887: Current learning rate: 0.00058\n",
      "2024-01-11 02:55:25.148315: train_loss -0.9577\n",
      "2024-01-11 02:55:25.158316: val_loss -0.8348\n",
      "2024-01-11 02:55:25.166318: Pseudo dice [0.8711, 0.9593, 0.9436]\n",
      "2024-01-11 02:55:25.173220: Epoch time: 40.05 s\n",
      "2024-01-11 02:55:26.598546: \n",
      "2024-01-11 02:55:26.604794: Epoch 959\n",
      "2024-01-11 02:55:26.613869: Current learning rate: 0.00056\n",
      "2024-01-11 02:56:06.451693: train_loss -0.9584\n",
      "2024-01-11 02:56:06.457695: val_loss -0.8347\n",
      "2024-01-11 02:56:06.464695: Pseudo dice [0.8695, 0.9589, 0.9433]\n",
      "2024-01-11 02:56:06.470695: Epoch time: 39.85 s\n",
      "2024-01-11 02:56:07.889458: \n",
      "2024-01-11 02:56:07.897679: Epoch 960\n",
      "2024-01-11 02:56:07.904467: Current learning rate: 0.00055\n",
      "2024-01-11 02:56:47.820811: train_loss -0.9582\n",
      "2024-01-11 02:56:47.846327: val_loss -0.8331\n",
      "2024-01-11 02:56:47.855326: Pseudo dice [0.8678, 0.9587, 0.9435]\n",
      "2024-01-11 02:56:47.863327: Epoch time: 39.93 s\n",
      "2024-01-11 02:56:49.284843: \n",
      "2024-01-11 02:56:49.289852: Epoch 961\n",
      "2024-01-11 02:56:49.294852: Current learning rate: 0.00054\n",
      "2024-01-11 02:57:29.306258: train_loss -0.9583\n",
      "2024-01-11 02:57:29.317259: val_loss -0.8358\n",
      "2024-01-11 02:57:29.345265: Pseudo dice [0.8737, 0.9592, 0.9431]\n",
      "2024-01-11 02:57:29.353271: Epoch time: 40.02 s\n",
      "2024-01-11 02:57:30.827861: \n",
      "2024-01-11 02:57:30.837631: Epoch 962\n",
      "2024-01-11 02:57:30.842713: Current learning rate: 0.00053\n",
      "2024-01-11 02:58:10.816247: train_loss -0.9578\n",
      "2024-01-11 02:58:10.826248: val_loss -0.8353\n",
      "2024-01-11 02:58:10.836246: Pseudo dice [0.8731, 0.9591, 0.9436]\n",
      "2024-01-11 02:58:10.842248: Epoch time: 39.99 s\n",
      "2024-01-11 02:58:12.322894: \n",
      "2024-01-11 02:58:12.329823: Epoch 963\n",
      "2024-01-11 02:58:12.334880: Current learning rate: 0.00051\n",
      "2024-01-11 02:58:52.279305: train_loss -0.9577\n",
      "2024-01-11 02:58:52.308847: val_loss -0.8356\n",
      "2024-01-11 02:58:52.319852: Pseudo dice [0.8714, 0.9588, 0.9431]\n",
      "2024-01-11 02:58:52.325849: Epoch time: 39.96 s\n",
      "2024-01-11 02:58:53.780617: \n",
      "2024-01-11 02:58:53.786089: Epoch 964\n",
      "2024-01-11 02:58:53.793100: Current learning rate: 0.0005\n",
      "2024-01-11 02:59:33.747965: train_loss -0.9579\n",
      "2024-01-11 02:59:33.757968: val_loss -0.8342\n",
      "2024-01-11 02:59:33.765963: Pseudo dice [0.869, 0.9589, 0.9435]\n",
      "2024-01-11 02:59:33.792963: Epoch time: 39.97 s\n",
      "2024-01-11 02:59:35.185876: \n",
      "2024-01-11 02:59:35.191944: Epoch 965\n",
      "2024-01-11 02:59:35.197017: Current learning rate: 0.00049\n",
      "2024-01-11 03:00:15.155431: train_loss -0.9577\n",
      "2024-01-11 03:00:15.164430: val_loss -0.8333\n",
      "2024-01-11 03:00:15.170424: Pseudo dice [0.8626, 0.9592, 0.9438]\n",
      "2024-01-11 03:00:15.180425: Epoch time: 39.97 s\n",
      "2024-01-11 03:00:16.547291: \n",
      "2024-01-11 03:00:16.557671: Epoch 966\n",
      "2024-01-11 03:00:16.565737: Current learning rate: 0.00048\n",
      "2024-01-11 03:00:56.827981: train_loss -0.9578\n",
      "2024-01-11 03:00:56.836982: val_loss -0.8326\n",
      "2024-01-11 03:00:56.844982: Pseudo dice [0.863, 0.9588, 0.9436]\n",
      "2024-01-11 03:00:56.850983: Epoch time: 40.28 s\n",
      "2024-01-11 03:00:58.214364: \n",
      "2024-01-11 03:00:58.224365: Epoch 967\n",
      "2024-01-11 03:00:58.230364: Current learning rate: 0.00046\n",
      "2024-01-11 03:01:38.213392: train_loss -0.9576\n",
      "2024-01-11 03:01:38.220394: val_loss -0.8328\n",
      "2024-01-11 03:01:38.228393: Pseudo dice [0.8672, 0.9585, 0.9433]\n",
      "2024-01-11 03:01:38.235394: Epoch time: 40.0 s\n",
      "2024-01-11 03:01:39.576972: \n",
      "2024-01-11 03:01:39.582734: Epoch 968\n",
      "2024-01-11 03:01:39.586731: Current learning rate: 0.00045\n",
      "2024-01-11 03:02:19.587918: train_loss -0.9579\n",
      "2024-01-11 03:02:19.595918: val_loss -0.835\n",
      "2024-01-11 03:02:19.603926: Pseudo dice [0.8704, 0.9587, 0.9431]\n",
      "2024-01-11 03:02:19.611447: Epoch time: 40.01 s\n",
      "2024-01-11 03:02:21.080699: \n",
      "2024-01-11 03:02:21.087641: Epoch 969\n",
      "2024-01-11 03:02:21.091695: Current learning rate: 0.00044\n",
      "2024-01-11 03:03:01.039043: train_loss -0.958\n",
      "2024-01-11 03:03:01.049043: val_loss -0.8328\n",
      "2024-01-11 03:03:01.090673: Pseudo dice [0.8652, 0.9586, 0.9432]\n",
      "2024-01-11 03:03:01.097674: Epoch time: 39.96 s\n",
      "2024-01-11 03:03:02.435377: \n",
      "2024-01-11 03:03:02.441382: Epoch 970\n",
      "2024-01-11 03:03:02.448409: Current learning rate: 0.00043\n",
      "2024-01-11 03:03:42.273030: train_loss -0.9583\n",
      "2024-01-11 03:03:42.309036: val_loss -0.8378\n",
      "2024-01-11 03:03:42.318028: Pseudo dice [0.8705, 0.9594, 0.9451]\n",
      "2024-01-11 03:03:42.326032: Epoch time: 39.84 s\n",
      "2024-01-11 03:03:43.688262: \n",
      "2024-01-11 03:03:43.696265: Epoch 971\n",
      "2024-01-11 03:03:43.702327: Current learning rate: 0.00041\n",
      "2024-01-11 03:04:23.829594: train_loss -0.958\n",
      "2024-01-11 03:04:23.868115: val_loss -0.8374\n",
      "2024-01-11 03:04:23.879119: Pseudo dice [0.8723, 0.9591, 0.9444]\n",
      "2024-01-11 03:04:23.885121: Epoch time: 40.14 s\n",
      "2024-01-11 03:04:25.343910: \n",
      "2024-01-11 03:04:25.349910: Epoch 972\n",
      "2024-01-11 03:04:25.354924: Current learning rate: 0.0004\n",
      "2024-01-11 03:05:05.388628: train_loss -0.958\n",
      "2024-01-11 03:05:05.395627: val_loss -0.8342\n",
      "2024-01-11 03:05:05.402627: Pseudo dice [0.8713, 0.9585, 0.9429]\n",
      "2024-01-11 03:05:05.410626: Epoch time: 40.05 s\n",
      "2024-01-11 03:05:06.842001: \n",
      "2024-01-11 03:05:06.848763: Epoch 973\n",
      "2024-01-11 03:05:06.855762: Current learning rate: 0.00039\n",
      "2024-01-11 03:05:46.735330: train_loss -0.9579\n",
      "2024-01-11 03:05:46.743334: val_loss -0.8355\n",
      "2024-01-11 03:05:46.751331: Pseudo dice [0.874, 0.9586, 0.9438]\n",
      "2024-01-11 03:05:46.757332: Epoch time: 39.89 s\n",
      "2024-01-11 03:05:48.125591: \n",
      "2024-01-11 03:05:48.134675: Epoch 974\n",
      "2024-01-11 03:05:48.145675: Current learning rate: 0.00037\n",
      "2024-01-11 03:06:28.100775: train_loss -0.9579\n",
      "2024-01-11 03:06:28.109775: val_loss -0.8359\n",
      "2024-01-11 03:06:28.116288: Pseudo dice [0.8712, 0.9592, 0.9436]\n",
      "2024-01-11 03:06:28.123288: Epoch time: 39.98 s\n",
      "2024-01-11 03:06:29.440393: \n",
      "2024-01-11 03:06:29.449252: Epoch 975\n",
      "2024-01-11 03:06:29.454222: Current learning rate: 0.00036\n",
      "2024-01-11 03:07:09.300305: train_loss -0.9587\n",
      "2024-01-11 03:07:09.307306: val_loss -0.8333\n",
      "2024-01-11 03:07:09.314306: Pseudo dice [0.8705, 0.9584, 0.9427]\n",
      "2024-01-11 03:07:09.321317: Epoch time: 39.86 s\n",
      "2024-01-11 03:07:10.890228: \n",
      "2024-01-11 03:07:10.895859: Epoch 976\n",
      "2024-01-11 03:07:10.903647: Current learning rate: 0.00035\n",
      "2024-01-11 03:07:50.944850: train_loss -0.9576\n",
      "2024-01-11 03:07:50.954367: val_loss -0.8325\n",
      "2024-01-11 03:07:50.962372: Pseudo dice [0.8669, 0.9588, 0.9436]\n",
      "2024-01-11 03:07:50.968369: Epoch time: 40.06 s\n",
      "2024-01-11 03:07:52.343394: \n",
      "2024-01-11 03:07:52.350409: Epoch 977\n",
      "2024-01-11 03:07:52.357712: Current learning rate: 0.00034\n",
      "2024-01-11 03:08:32.477806: train_loss -0.9584\n",
      "2024-01-11 03:08:32.488316: val_loss -0.8368\n",
      "2024-01-11 03:08:32.497316: Pseudo dice [0.8718, 0.9592, 0.9435]\n",
      "2024-01-11 03:08:32.505353: Epoch time: 40.14 s\n",
      "2024-01-11 03:08:33.868860: \n",
      "2024-01-11 03:08:33.876437: Epoch 978\n",
      "2024-01-11 03:08:33.881455: Current learning rate: 0.00032\n",
      "2024-01-11 03:09:13.956998: train_loss -0.9582\n",
      "2024-01-11 03:09:13.968998: val_loss -0.8342\n",
      "2024-01-11 03:09:14.004016: Pseudo dice [0.8675, 0.9592, 0.9444]\n",
      "2024-01-11 03:09:14.013014: Epoch time: 40.09 s\n",
      "2024-01-11 03:09:15.394486: \n",
      "2024-01-11 03:09:15.403560: Epoch 979\n",
      "2024-01-11 03:09:15.409501: Current learning rate: 0.00031\n",
      "2024-01-11 03:09:55.384788: train_loss -0.9583\n",
      "2024-01-11 03:09:55.393789: val_loss -0.8358\n",
      "2024-01-11 03:09:55.399795: Pseudo dice [0.871, 0.9593, 0.9433]\n",
      "2024-01-11 03:09:55.404794: Epoch time: 39.99 s\n",
      "2024-01-11 03:09:56.834804: \n",
      "2024-01-11 03:09:56.840936: Epoch 980\n",
      "2024-01-11 03:09:56.845903: Current learning rate: 0.0003\n",
      "2024-01-11 03:10:36.791312: train_loss -0.9586\n",
      "2024-01-11 03:10:36.799314: val_loss -0.8356\n",
      "2024-01-11 03:10:36.807312: Pseudo dice [0.8684, 0.9591, 0.9435]\n",
      "2024-01-11 03:10:36.814312: Epoch time: 39.96 s\n",
      "2024-01-11 03:10:38.230126: \n",
      "2024-01-11 03:10:38.238445: Epoch 981\n",
      "2024-01-11 03:10:38.243522: Current learning rate: 0.00028\n",
      "2024-01-11 03:11:18.284928: train_loss -0.959\n",
      "2024-01-11 03:11:18.292928: val_loss -0.8364\n",
      "2024-01-11 03:11:18.299925: Pseudo dice [0.8704, 0.9593, 0.945]\n",
      "2024-01-11 03:11:18.306924: Epoch time: 40.06 s\n",
      "2024-01-11 03:11:19.679884: \n",
      "2024-01-11 03:11:19.693893: Epoch 982\n",
      "2024-01-11 03:11:19.700578: Current learning rate: 0.00027\n",
      "2024-01-11 03:11:59.821668: train_loss -0.9585\n",
      "2024-01-11 03:11:59.830670: val_loss -0.8374\n",
      "2024-01-11 03:11:59.836668: Pseudo dice [0.8715, 0.9594, 0.944]\n",
      "2024-01-11 03:11:59.864018: Epoch time: 40.14 s\n",
      "2024-01-11 03:12:01.246715: \n",
      "2024-01-11 03:12:01.253719: Epoch 983\n",
      "2024-01-11 03:12:01.260388: Current learning rate: 0.00026\n",
      "2024-01-11 03:12:41.231318: train_loss -0.9583\n",
      "2024-01-11 03:12:41.241609: val_loss -0.8355\n",
      "2024-01-11 03:12:41.251607: Pseudo dice [0.8703, 0.9593, 0.944]\n",
      "2024-01-11 03:12:41.258609: Epoch time: 39.99 s\n",
      "2024-01-11 03:12:42.625392: \n",
      "2024-01-11 03:12:42.636454: Epoch 984\n",
      "2024-01-11 03:12:42.641983: Current learning rate: 0.00024\n",
      "2024-01-11 03:13:22.403759: train_loss -0.9587\n",
      "2024-01-11 03:13:22.436853: val_loss -0.8325\n",
      "2024-01-11 03:13:22.446850: Pseudo dice [0.8677, 0.9583, 0.9427]\n",
      "2024-01-11 03:13:22.456851: Epoch time: 39.78 s\n",
      "2024-01-11 03:13:23.861661: \n",
      "2024-01-11 03:13:23.872325: Epoch 985\n",
      "2024-01-11 03:13:23.878324: Current learning rate: 0.00023\n",
      "2024-01-11 03:14:03.856366: train_loss -0.9583\n",
      "2024-01-11 03:14:03.883366: val_loss -0.8358\n",
      "2024-01-11 03:14:03.892369: Pseudo dice [0.8708, 0.9592, 0.9441]\n",
      "2024-01-11 03:14:03.898372: Epoch time: 40.0 s\n",
      "2024-01-11 03:14:05.339630: \n",
      "2024-01-11 03:14:05.345066: Epoch 986\n",
      "2024-01-11 03:14:05.350115: Current learning rate: 0.00021\n",
      "2024-01-11 03:14:45.366421: train_loss -0.9588\n",
      "2024-01-11 03:14:45.376422: val_loss -0.8331\n",
      "2024-01-11 03:14:45.382400: Pseudo dice [0.8648, 0.9588, 0.944]\n",
      "2024-01-11 03:14:45.388401: Epoch time: 40.03 s\n",
      "2024-01-11 03:14:46.883865: \n",
      "2024-01-11 03:14:46.889867: Epoch 987\n",
      "2024-01-11 03:14:46.894807: Current learning rate: 0.0002\n",
      "2024-01-11 03:15:26.796999: train_loss -0.9587\n",
      "2024-01-11 03:15:26.804999: val_loss -0.8323\n",
      "2024-01-11 03:15:26.840000: Pseudo dice [0.8657, 0.9588, 0.9434]\n",
      "2024-01-11 03:15:26.847000: Epoch time: 39.91 s\n",
      "2024-01-11 03:15:28.339364: \n",
      "2024-01-11 03:15:28.350378: Epoch 988\n",
      "2024-01-11 03:15:28.355302: Current learning rate: 0.00019\n",
      "2024-01-11 03:16:08.340240: train_loss -0.9586\n",
      "2024-01-11 03:16:08.349243: val_loss -0.8348\n",
      "2024-01-11 03:16:08.356240: Pseudo dice [0.8685, 0.9588, 0.9432]\n",
      "2024-01-11 03:16:08.362241: Epoch time: 40.0 s\n",
      "2024-01-11 03:16:09.738444: \n",
      "2024-01-11 03:16:09.747440: Epoch 989\n",
      "2024-01-11 03:16:09.751442: Current learning rate: 0.00017\n",
      "2024-01-11 03:16:49.622831: train_loss -0.9586\n",
      "2024-01-11 03:16:49.630838: val_loss -0.8334\n",
      "2024-01-11 03:16:49.658831: Pseudo dice [0.8676, 0.9587, 0.9433]\n",
      "2024-01-11 03:16:49.667831: Epoch time: 39.89 s\n",
      "2024-01-11 03:16:51.060224: \n",
      "2024-01-11 03:16:51.066630: Epoch 990\n",
      "2024-01-11 03:16:51.072173: Current learning rate: 0.00016\n",
      "2024-01-11 03:17:31.003826: train_loss -0.9585\n",
      "2024-01-11 03:17:31.012826: val_loss -0.8365\n",
      "2024-01-11 03:17:31.019839: Pseudo dice [0.8708, 0.9592, 0.9444]\n",
      "2024-01-11 03:17:31.026838: Epoch time: 39.94 s\n",
      "2024-01-11 03:17:32.402310: \n",
      "2024-01-11 03:17:32.407813: Epoch 991\n",
      "2024-01-11 03:17:32.414869: Current learning rate: 0.00014\n",
      "2024-01-11 03:18:12.610428: train_loss -0.9591\n",
      "2024-01-11 03:18:12.618430: val_loss -0.8317\n",
      "2024-01-11 03:18:12.626430: Pseudo dice [0.8695, 0.9584, 0.9431]\n",
      "2024-01-11 03:18:12.633431: Epoch time: 40.21 s\n",
      "2024-01-11 03:18:14.019915: \n",
      "2024-01-11 03:18:14.031017: Epoch 992\n",
      "2024-01-11 03:18:14.038101: Current learning rate: 0.00013\n",
      "2024-01-11 03:18:54.095936: train_loss -0.959\n",
      "2024-01-11 03:18:54.103938: val_loss -0.8344\n",
      "2024-01-11 03:18:54.111937: Pseudo dice [0.8637, 0.9592, 0.9441]\n",
      "2024-01-11 03:18:54.135936: Epoch time: 40.08 s\n",
      "2024-01-11 03:18:55.497357: \n",
      "2024-01-11 03:18:55.506295: Epoch 993\n",
      "2024-01-11 03:18:55.510360: Current learning rate: 0.00011\n",
      "2024-01-11 03:19:35.509341: train_loss -0.9585\n",
      "2024-01-11 03:19:35.519348: val_loss -0.8374\n",
      "2024-01-11 03:19:35.526348: Pseudo dice [0.8738, 0.9595, 0.9443]\n",
      "2024-01-11 03:19:35.532351: Epoch time: 40.01 s\n",
      "2024-01-11 03:19:36.891796: \n",
      "2024-01-11 03:19:36.897870: Epoch 994\n",
      "2024-01-11 03:19:36.902800: Current learning rate: 0.0001\n",
      "2024-01-11 03:20:16.880337: train_loss -0.9584\n",
      "2024-01-11 03:20:16.887340: val_loss -0.8388\n",
      "2024-01-11 03:20:16.912856: Pseudo dice [0.8751, 0.9593, 0.9443]\n",
      "2024-01-11 03:20:16.922856: Epoch time: 39.99 s\n",
      "2024-01-11 03:20:18.332244: \n",
      "2024-01-11 03:20:18.338318: Epoch 995\n",
      "2024-01-11 03:20:18.342312: Current learning rate: 8e-05\n",
      "2024-01-11 03:20:58.348796: train_loss -0.9591\n",
      "2024-01-11 03:20:58.357791: val_loss -0.8342\n",
      "2024-01-11 03:20:58.366792: Pseudo dice [0.8701, 0.9588, 0.943]\n",
      "2024-01-11 03:20:58.374796: Epoch time: 40.02 s\n",
      "2024-01-11 03:20:59.780335: \n",
      "2024-01-11 03:20:59.787343: Epoch 996\n",
      "2024-01-11 03:20:59.793360: Current learning rate: 7e-05\n",
      "2024-01-11 03:21:39.919275: train_loss -0.9593\n",
      "2024-01-11 03:21:39.926277: val_loss -0.8371\n",
      "2024-01-11 03:21:39.933277: Pseudo dice [0.8718, 0.9594, 0.9445]\n",
      "2024-01-11 03:21:39.940277: Epoch time: 40.14 s\n",
      "2024-01-11 03:21:41.354674: \n",
      "2024-01-11 03:21:41.364344: Epoch 997\n",
      "2024-01-11 03:21:41.368420: Current learning rate: 5e-05\n",
      "2024-01-11 03:22:21.241817: train_loss -0.9587\n",
      "2024-01-11 03:22:21.251820: val_loss -0.8323\n",
      "2024-01-11 03:22:21.258817: Pseudo dice [0.8662, 0.9585, 0.9425]\n",
      "2024-01-11 03:22:21.265330: Epoch time: 39.89 s\n",
      "2024-01-11 03:22:22.682396: \n",
      "2024-01-11 03:22:22.696411: Epoch 998\n",
      "2024-01-11 03:22:22.701500: Current learning rate: 4e-05\n",
      "2024-01-11 03:23:02.653702: train_loss -0.9597\n",
      "2024-01-11 03:23:02.663700: val_loss -0.8368\n",
      "2024-01-11 03:23:02.670699: Pseudo dice [0.8696, 0.9593, 0.9442]\n",
      "2024-01-11 03:23:02.678699: Epoch time: 39.97 s\n",
      "2024-01-11 03:23:04.066734: \n",
      "2024-01-11 03:23:04.072891: Epoch 999\n",
      "2024-01-11 03:23:04.079957: Current learning rate: 2e-05\n",
      "2024-01-11 03:23:44.040565: train_loss -0.9595\n",
      "2024-01-11 03:23:44.046566: val_loss -0.8338\n",
      "2024-01-11 03:23:44.052567: Pseudo dice [0.869, 0.9588, 0.9436]\n",
      "2024-01-11 03:23:44.059568: Epoch time: 39.97 s\n",
      "2024-01-11 03:23:45.820754: Training done.\n",
      "2024-01-11 03:23:45.857421: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-11 03:23:45.870241: The split file contains 5 splits.\n",
      "2024-01-11 03:23:45.877240: Desired fold for training: 3\n",
      "2024-01-11 03:23:45.883246: This split has 8 training and 2 validation cases.\n",
      "2024-01-11 03:23:45.888244: predicting case_1\n",
      "2024-01-11 03:23:48.669169: predicting case_8\n",
      "2024-01-11 03:24:00.171118: Validation complete\n",
      "2024-01-11 03:24:00.177111: Mean Validation Dice:  0.9282964985341468\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "WARNING: Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 45, 'patch_size': [160, 160], 'median_image_size_in_voxels': [139.0, 144.5], 'spacing': [0.9375, 0.9375], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_dataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.5, 0.9375, 0.9375], 'original_median_shape_after_transp': [115, 138, 142], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 151.0, 'mean': 67.01873016357422, 'median': 67.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 118.0, 'std': 23.369123458862305}}} \n",
      "\n",
      "2024-01-11 03:24:00.748935: unpacking dataset...\n",
      "2024-01-11 03:24:01.008414: unpacking done...\n",
      "2024-01-11 03:24:01.013413: do_dummy_2d_data_aug: False\n",
      "2024-01-11 03:24:01.017747: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-11 03:24:01.021217: The split file contains 5 splits.\n",
      "2024-01-11 03:24:01.024274: Desired fold for training: 4\n",
      "2024-01-11 03:24:01.027275: This split has 8 training and 2 validation cases.\n",
      "2024-01-11 03:24:01.058553: Unable to plot network architecture:\n",
      "2024-01-11 03:24:01.062558: No module named 'hiddenlayer'\n",
      "2024-01-11 03:24:01.085670: \n",
      "2024-01-11 03:24:01.089669: Epoch 0\n",
      "2024-01-11 03:24:01.093660: Current learning rate: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-01-11 03:24:49.491543: train_loss -0.3569\n",
      "2024-01-11 03:24:49.499535: val_loss -0.6806\n",
      "2024-01-11 03:24:49.507534: Pseudo dice [0.7916, 0.904, 0.8868]\n",
      "2024-01-11 03:24:49.517536: Epoch time: 48.41 s\n",
      "2024-01-11 03:24:49.523536: Yayy! New best EMA pseudo Dice: 0.8608\n",
      "2024-01-11 03:24:51.115499: \n",
      "2024-01-11 03:24:51.122096: Epoch 1\n",
      "2024-01-11 03:24:51.128513: Current learning rate: 0.00999\n",
      "2024-01-11 03:25:31.759013: train_loss -0.7171\n",
      "2024-01-11 03:25:31.767012: val_loss -0.7582\n",
      "2024-01-11 03:25:31.774013: Pseudo dice [0.8467, 0.9204, 0.9013]\n",
      "2024-01-11 03:25:31.780017: Epoch time: 40.64 s\n",
      "2024-01-11 03:25:31.787018: Yayy! New best EMA pseudo Dice: 0.8637\n",
      "2024-01-11 03:25:33.406838: \n",
      "2024-01-11 03:25:33.418914: Epoch 2\n",
      "2024-01-11 03:25:33.422900: Current learning rate: 0.00998\n",
      "2024-01-11 03:26:13.901521: train_loss -0.7747\n",
      "2024-01-11 03:26:13.909518: val_loss -0.781\n",
      "2024-01-11 03:26:13.914520: Pseudo dice [0.8659, 0.9248, 0.905]\n",
      "2024-01-11 03:26:13.921035: Epoch time: 40.5 s\n",
      "2024-01-11 03:26:13.927033: Yayy! New best EMA pseudo Dice: 0.8672\n",
      "2024-01-11 03:26:15.627711: \n",
      "2024-01-11 03:26:15.635636: Epoch 3\n",
      "2024-01-11 03:26:15.644724: Current learning rate: 0.00997\n",
      "2024-01-11 03:26:56.157232: train_loss -0.7984\n",
      "2024-01-11 03:26:56.164233: val_loss -0.7938\n",
      "2024-01-11 03:26:56.191232: Pseudo dice [0.8669, 0.9294, 0.9123]\n",
      "2024-01-11 03:26:56.198233: Epoch time: 40.53 s\n",
      "2024-01-11 03:26:56.204234: Yayy! New best EMA pseudo Dice: 0.8707\n",
      "2024-01-11 03:26:57.872822: \n",
      "2024-01-11 03:26:57.880860: Epoch 4\n",
      "2024-01-11 03:26:57.888924: Current learning rate: 0.00996\n",
      "2024-01-11 03:27:38.486156: train_loss -0.8149\n",
      "2024-01-11 03:27:38.514155: val_loss -0.8064\n",
      "2024-01-11 03:27:38.522156: Pseudo dice [0.8771, 0.9323, 0.9182]\n",
      "2024-01-11 03:27:38.528157: Epoch time: 40.61 s\n",
      "2024-01-11 03:27:38.535158: Yayy! New best EMA pseudo Dice: 0.8746\n",
      "2024-01-11 03:27:40.252007: \n",
      "2024-01-11 03:27:40.258173: Epoch 5\n",
      "2024-01-11 03:27:40.263247: Current learning rate: 0.00995\n",
      "2024-01-11 03:28:20.822236: train_loss -0.8259\n",
      "2024-01-11 03:28:20.830238: val_loss -0.8102\n",
      "2024-01-11 03:28:20.835237: Pseudo dice [0.8812, 0.9337, 0.9179]\n",
      "2024-01-11 03:28:20.839237: Epoch time: 40.57 s\n",
      "2024-01-11 03:28:20.843237: Yayy! New best EMA pseudo Dice: 0.8782\n",
      "2024-01-11 03:28:22.617358: \n",
      "2024-01-11 03:28:22.623639: Epoch 6\n",
      "2024-01-11 03:28:22.628718: Current learning rate: 0.00995\n",
      "2024-01-11 03:29:03.064988: train_loss -0.8328\n",
      "2024-01-11 03:29:03.072987: val_loss -0.8103\n",
      "2024-01-11 03:29:03.077986: Pseudo dice [0.877, 0.9344, 0.919]\n",
      "2024-01-11 03:29:03.083987: Epoch time: 40.45 s\n",
      "2024-01-11 03:29:03.091502: Yayy! New best EMA pseudo Dice: 0.8814\n",
      "2024-01-11 03:29:04.709642: \n",
      "2024-01-11 03:29:04.715082: Epoch 7\n",
      "2024-01-11 03:29:04.720086: Current learning rate: 0.00994\n",
      "2024-01-11 03:29:45.211888: train_loss -0.8381\n",
      "2024-01-11 03:29:45.217888: val_loss -0.8183\n",
      "2024-01-11 03:29:45.223889: Pseudo dice [0.8788, 0.9368, 0.9232]\n",
      "2024-01-11 03:29:45.230404: Epoch time: 40.5 s\n",
      "2024-01-11 03:29:45.235405: Yayy! New best EMA pseudo Dice: 0.8846\n",
      "2024-01-11 03:29:46.837132: \n",
      "2024-01-11 03:29:46.845130: Epoch 8\n",
      "2024-01-11 03:29:46.851127: Current learning rate: 0.00993\n",
      "2024-01-11 03:30:27.279820: train_loss -0.8434\n",
      "2024-01-11 03:30:27.285822: val_loss -0.8195\n",
      "2024-01-11 03:30:27.291810: Pseudo dice [0.8824, 0.9384, 0.9243]\n",
      "2024-01-11 03:30:27.296810: Epoch time: 40.44 s\n",
      "2024-01-11 03:30:27.301811: Yayy! New best EMA pseudo Dice: 0.8876\n",
      "2024-01-11 03:30:29.074621: \n",
      "2024-01-11 03:30:29.080574: Epoch 9\n",
      "2024-01-11 03:30:29.087012: Current learning rate: 0.00992\n",
      "2024-01-11 03:31:09.688865: train_loss -0.8475\n",
      "2024-01-11 03:31:09.694865: val_loss -0.8226\n",
      "2024-01-11 03:31:09.701866: Pseudo dice [0.8779, 0.9387, 0.9259]\n",
      "2024-01-11 03:31:09.707866: Epoch time: 40.62 s\n",
      "2024-01-11 03:31:09.714865: Yayy! New best EMA pseudo Dice: 0.8903\n",
      "2024-01-11 03:31:11.314526: \n",
      "2024-01-11 03:31:11.320526: Epoch 10\n",
      "2024-01-11 03:31:11.324526: Current learning rate: 0.00991\n",
      "2024-01-11 03:31:51.927476: train_loss -0.8503\n",
      "2024-01-11 03:31:51.957476: val_loss -0.8234\n",
      "2024-01-11 03:31:51.964477: Pseudo dice [0.8837, 0.9387, 0.9255]\n",
      "2024-01-11 03:31:51.969477: Epoch time: 40.61 s\n",
      "2024-01-11 03:31:51.975476: Yayy! New best EMA pseudo Dice: 0.8928\n",
      "2024-01-11 03:31:53.750023: \n",
      "2024-01-11 03:31:53.757455: Epoch 11\n",
      "2024-01-11 03:31:53.764456: Current learning rate: 0.0099\n",
      "2024-01-11 03:32:34.330150: train_loss -0.8536\n",
      "2024-01-11 03:32:34.336149: val_loss -0.8252\n",
      "2024-01-11 03:32:34.343148: Pseudo dice [0.8809, 0.9397, 0.9268]\n",
      "2024-01-11 03:32:34.348153: Epoch time: 40.58 s\n",
      "2024-01-11 03:32:34.353149: Yayy! New best EMA pseudo Dice: 0.8951\n",
      "2024-01-11 03:32:35.981349: \n",
      "2024-01-11 03:32:35.986344: Epoch 12\n",
      "2024-01-11 03:32:35.991344: Current learning rate: 0.00989\n",
      "2024-01-11 03:33:16.494515: train_loss -0.8556\n",
      "2024-01-11 03:33:16.499517: val_loss -0.8259\n",
      "2024-01-11 03:33:16.504517: Pseudo dice [0.8815, 0.9405, 0.9267]\n",
      "2024-01-11 03:33:16.513519: Epoch time: 40.51 s\n",
      "2024-01-11 03:33:16.541520: Yayy! New best EMA pseudo Dice: 0.8972\n",
      "2024-01-11 03:33:18.177277: \n",
      "2024-01-11 03:33:18.182277: Epoch 13\n",
      "2024-01-11 03:33:18.187268: Current learning rate: 0.00988\n",
      "2024-01-11 03:33:58.623465: train_loss -0.8593\n",
      "2024-01-11 03:33:58.630468: val_loss -0.8309\n",
      "2024-01-11 03:33:58.656466: Pseudo dice [0.8892, 0.9412, 0.9291]\n",
      "2024-01-11 03:33:58.665468: Epoch time: 40.45 s\n",
      "2024-01-11 03:33:58.669477: Yayy! New best EMA pseudo Dice: 0.8995\n",
      "2024-01-11 03:34:00.316949: \n",
      "2024-01-11 03:34:00.326009: Epoch 14\n",
      "2024-01-11 03:34:00.331007: Current learning rate: 0.00987\n",
      "2024-01-11 03:34:40.539669: train_loss -0.8613\n",
      "2024-01-11 03:34:40.546665: val_loss -0.8274\n",
      "2024-01-11 03:34:40.552665: Pseudo dice [0.8829, 0.9409, 0.9284]\n",
      "2024-01-11 03:34:40.556666: Epoch time: 40.22 s\n",
      "2024-01-11 03:34:40.561666: Yayy! New best EMA pseudo Dice: 0.9013\n",
      "2024-01-11 03:34:42.273366: \n",
      "2024-01-11 03:34:42.279369: Epoch 15\n",
      "2024-01-11 03:34:42.283366: Current learning rate: 0.00986\n",
      "2024-01-11 03:35:22.754901: train_loss -0.8637\n",
      "2024-01-11 03:35:22.762904: val_loss -0.8304\n",
      "2024-01-11 03:35:22.767909: Pseudo dice [0.8818, 0.9428, 0.93]\n",
      "2024-01-11 03:35:22.777909: Epoch time: 40.48 s\n",
      "2024-01-11 03:35:22.784909: Yayy! New best EMA pseudo Dice: 0.903\n",
      "2024-01-11 03:35:24.459045: \n",
      "2024-01-11 03:35:24.465044: Epoch 16\n",
      "2024-01-11 03:35:24.469044: Current learning rate: 0.00986\n",
      "2024-01-11 03:36:04.773333: train_loss -0.8651\n",
      "2024-01-11 03:36:04.782330: val_loss -0.8313\n",
      "2024-01-11 03:36:04.788331: Pseudo dice [0.8841, 0.9426, 0.93]\n",
      "2024-01-11 03:36:04.793335: Epoch time: 40.32 s\n",
      "2024-01-11 03:36:04.797334: Yayy! New best EMA pseudo Dice: 0.9046\n",
      "2024-01-11 03:36:06.558850: \n",
      "2024-01-11 03:36:06.563058: Epoch 17\n",
      "2024-01-11 03:36:06.570064: Current learning rate: 0.00985\n",
      "2024-01-11 03:36:47.007156: train_loss -0.8669\n",
      "2024-01-11 03:36:47.014157: val_loss -0.8336\n",
      "2024-01-11 03:36:47.047154: Pseudo dice [0.8886, 0.9426, 0.9307]\n",
      "2024-01-11 03:36:47.055153: Epoch time: 40.45 s\n",
      "2024-01-11 03:36:47.061166: Yayy! New best EMA pseudo Dice: 0.9062\n",
      "2024-01-11 03:36:48.647841: \n",
      "2024-01-11 03:36:48.653495: Epoch 18\n",
      "2024-01-11 03:36:48.661465: Current learning rate: 0.00984\n",
      "2024-01-11 03:37:28.901689: train_loss -0.8695\n",
      "2024-01-11 03:37:28.920219: val_loss -0.832\n",
      "2024-01-11 03:37:28.928423: Pseudo dice [0.8862, 0.9427, 0.9291]\n",
      "2024-01-11 03:37:28.933940: Epoch time: 40.25 s\n",
      "2024-01-11 03:37:28.938941: Yayy! New best EMA pseudo Dice: 0.9075\n",
      "2024-01-11 03:37:30.626812: \n",
      "2024-01-11 03:37:30.633322: Epoch 19\n",
      "2024-01-11 03:37:30.638365: Current learning rate: 0.00983\n",
      "2024-01-11 03:38:10.939033: train_loss -0.8709\n",
      "2024-01-11 03:38:10.946035: val_loss -0.8323\n",
      "2024-01-11 03:38:10.953547: Pseudo dice [0.8823, 0.9442, 0.9326]\n",
      "2024-01-11 03:38:10.971547: Epoch time: 40.31 s\n",
      "2024-01-11 03:38:10.986548: Yayy! New best EMA pseudo Dice: 0.9087\n",
      "2024-01-11 03:38:12.700308: \n",
      "2024-01-11 03:38:12.706366: Epoch 20\n",
      "2024-01-11 03:38:12.710377: Current learning rate: 0.00982\n",
      "2024-01-11 03:38:53.019241: train_loss -0.8731\n",
      "2024-01-11 03:38:53.025245: val_loss -0.8326\n",
      "2024-01-11 03:38:53.030242: Pseudo dice [0.8838, 0.9441, 0.9303]\n",
      "2024-01-11 03:38:53.037242: Epoch time: 40.32 s\n",
      "2024-01-11 03:38:53.043242: Yayy! New best EMA pseudo Dice: 0.9098\n",
      "2024-01-11 03:38:54.684132: \n",
      "2024-01-11 03:38:54.692612: Epoch 21\n",
      "2024-01-11 03:38:54.699252: Current learning rate: 0.00981\n",
      "2024-01-11 03:39:35.150403: train_loss -0.8736\n",
      "2024-01-11 03:39:35.160404: val_loss -0.8321\n",
      "2024-01-11 03:39:35.166403: Pseudo dice [0.8839, 0.9438, 0.9305]\n",
      "2024-01-11 03:39:35.198925: Epoch time: 40.47 s\n",
      "2024-01-11 03:39:35.206927: Yayy! New best EMA pseudo Dice: 0.9108\n",
      "2024-01-11 03:39:36.763988: \n",
      "2024-01-11 03:39:36.769764: Epoch 22\n",
      "2024-01-11 03:39:36.775355: Current learning rate: 0.0098\n",
      "2024-01-11 03:40:16.843766: train_loss -0.8754\n",
      "2024-01-11 03:40:16.850766: val_loss -0.8343\n",
      "2024-01-11 03:40:16.858764: Pseudo dice [0.8868, 0.9443, 0.933]\n",
      "2024-01-11 03:40:16.885310: Epoch time: 40.08 s\n",
      "2024-01-11 03:40:16.893306: Yayy! New best EMA pseudo Dice: 0.9118\n",
      "2024-01-11 03:40:18.441648: \n",
      "2024-01-11 03:40:18.450652: Epoch 23\n",
      "2024-01-11 03:40:18.455648: Current learning rate: 0.00979\n",
      "2024-01-11 03:40:58.616815: train_loss -0.8766\n",
      "2024-01-11 03:40:58.624817: val_loss -0.8309\n",
      "2024-01-11 03:40:58.631820: Pseudo dice [0.8866, 0.9429, 0.9308]\n",
      "2024-01-11 03:40:58.664840: Epoch time: 40.18 s\n",
      "2024-01-11 03:40:58.671845: Yayy! New best EMA pseudo Dice: 0.9126\n",
      "2024-01-11 03:41:00.321643: \n",
      "2024-01-11 03:41:00.331714: Epoch 24\n",
      "2024-01-11 03:41:00.336718: Current learning rate: 0.00978\n",
      "2024-01-11 03:41:40.539301: train_loss -0.8782\n",
      "2024-01-11 03:41:40.545303: val_loss -0.8377\n",
      "2024-01-11 03:41:40.554373: Pseudo dice [0.8914, 0.9439, 0.9327]\n",
      "2024-01-11 03:41:40.560373: Epoch time: 40.22 s\n",
      "2024-01-11 03:41:40.587883: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2024-01-11 03:41:42.367696: \n",
      "2024-01-11 03:41:42.375318: Epoch 25\n",
      "2024-01-11 03:41:42.379314: Current learning rate: 0.00977\n",
      "2024-01-11 03:42:22.590441: train_loss -0.8797\n",
      "2024-01-11 03:42:22.599953: val_loss -0.8374\n",
      "2024-01-11 03:42:22.605953: Pseudo dice [0.8906, 0.9449, 0.9329]\n",
      "2024-01-11 03:42:22.610950: Epoch time: 40.22 s\n",
      "2024-01-11 03:42:22.615951: Yayy! New best EMA pseudo Dice: 0.9146\n",
      "2024-01-11 03:42:24.267516: \n",
      "2024-01-11 03:42:24.274517: Epoch 26\n",
      "2024-01-11 03:42:24.278523: Current learning rate: 0.00977\n",
      "2024-01-11 03:43:04.527535: train_loss -0.8817\n",
      "2024-01-11 03:43:04.534536: val_loss -0.8395\n",
      "2024-01-11 03:43:04.542534: Pseudo dice [0.89, 0.946, 0.9348]\n",
      "2024-01-11 03:43:04.549535: Epoch time: 40.26 s\n",
      "2024-01-11 03:43:04.555534: Yayy! New best EMA pseudo Dice: 0.9155\n",
      "2024-01-11 03:43:06.261020: \n",
      "2024-01-11 03:43:06.266141: Epoch 27\n",
      "2024-01-11 03:43:06.270142: Current learning rate: 0.00976\n",
      "2024-01-11 03:43:46.544528: train_loss -0.8836\n",
      "2024-01-11 03:43:46.551528: val_loss -0.84\n",
      "2024-01-11 03:43:46.577528: Pseudo dice [0.8878, 0.9461, 0.9368]\n",
      "2024-01-11 03:43:46.583530: Epoch time: 40.28 s\n",
      "2024-01-11 03:43:46.591527: Yayy! New best EMA pseudo Dice: 0.9163\n",
      "2024-01-11 03:43:48.239955: \n",
      "2024-01-11 03:43:48.248950: Epoch 28\n",
      "2024-01-11 03:43:48.255036: Current learning rate: 0.00975\n",
      "2024-01-11 03:44:28.560891: train_loss -0.8842\n",
      "2024-01-11 03:44:28.568914: val_loss -0.8396\n",
      "2024-01-11 03:44:28.575919: Pseudo dice [0.8903, 0.946, 0.9345]\n",
      "2024-01-11 03:44:28.584920: Epoch time: 40.32 s\n",
      "2024-01-11 03:44:28.589920: Yayy! New best EMA pseudo Dice: 0.917\n",
      "2024-01-11 03:44:30.228846: \n",
      "2024-01-11 03:44:30.236083: Epoch 29\n",
      "2024-01-11 03:44:30.241077: Current learning rate: 0.00974\n",
      "2024-01-11 03:45:10.283468: train_loss -0.8856\n",
      "2024-01-11 03:45:10.292470: val_loss -0.8372\n",
      "2024-01-11 03:45:10.299632: Pseudo dice [0.8908, 0.9444, 0.9343]\n",
      "2024-01-11 03:45:10.307677: Epoch time: 40.06 s\n",
      "2024-01-11 03:45:10.313681: Yayy! New best EMA pseudo Dice: 0.9176\n",
      "2024-01-11 03:45:12.013734: \n",
      "2024-01-11 03:45:12.020804: Epoch 30\n",
      "2024-01-11 03:45:12.024865: Current learning rate: 0.00973\n",
      "2024-01-11 03:45:52.123917: train_loss -0.886\n",
      "2024-01-11 03:45:52.129918: val_loss -0.8398\n",
      "2024-01-11 03:45:52.135920: Pseudo dice [0.8916, 0.946, 0.9359]\n",
      "2024-01-11 03:45:52.142919: Epoch time: 40.11 s\n",
      "2024-01-11 03:45:52.149916: Yayy! New best EMA pseudo Dice: 0.9183\n",
      "2024-01-11 03:45:53.723622: \n",
      "2024-01-11 03:45:53.729633: Epoch 31\n",
      "2024-01-11 03:45:53.738699: Current learning rate: 0.00972\n",
      "2024-01-11 03:46:33.776352: train_loss -0.8858\n",
      "2024-01-11 03:46:33.783352: val_loss -0.8399\n",
      "2024-01-11 03:46:33.790352: Pseudo dice [0.8892, 0.9464, 0.9356]\n",
      "2024-01-11 03:46:33.796353: Epoch time: 40.05 s\n",
      "2024-01-11 03:46:33.801353: Yayy! New best EMA pseudo Dice: 0.9188\n",
      "2024-01-11 03:46:35.400624: \n",
      "2024-01-11 03:46:35.406618: Epoch 32\n",
      "2024-01-11 03:46:35.410617: Current learning rate: 0.00971\n",
      "2024-01-11 03:47:15.456056: train_loss -0.889\n",
      "2024-01-11 03:47:15.463569: val_loss -0.8383\n",
      "2024-01-11 03:47:15.469569: Pseudo dice [0.8877, 0.9466, 0.9355]\n",
      "2024-01-11 03:47:15.477569: Epoch time: 40.06 s\n",
      "2024-01-11 03:47:15.483569: Yayy! New best EMA pseudo Dice: 0.9193\n",
      "2024-01-11 03:47:17.122473: \n",
      "2024-01-11 03:47:17.127474: Epoch 33\n",
      "2024-01-11 03:47:17.134653: Current learning rate: 0.0097\n",
      "2024-01-11 03:47:57.349665: train_loss -0.8887\n",
      "2024-01-11 03:47:57.356178: val_loss -0.8406\n",
      "2024-01-11 03:47:57.394205: Pseudo dice [0.8909, 0.9465, 0.9358]\n",
      "2024-01-11 03:47:57.403202: Epoch time: 40.23 s\n",
      "2024-01-11 03:47:57.409205: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2024-01-11 03:47:59.142966: \n",
      "2024-01-11 03:47:59.152340: Epoch 34\n",
      "2024-01-11 03:47:59.157243: Current learning rate: 0.00969\n",
      "2024-01-11 03:48:39.525293: train_loss -0.8894\n",
      "2024-01-11 03:48:39.532294: val_loss -0.8396\n",
      "2024-01-11 03:48:39.538294: Pseudo dice [0.891, 0.9464, 0.9364]\n",
      "2024-01-11 03:48:39.544294: Epoch time: 40.39 s\n",
      "2024-01-11 03:48:39.552293: Yayy! New best EMA pseudo Dice: 0.9203\n",
      "2024-01-11 03:48:41.340349: \n",
      "2024-01-11 03:48:41.345349: Epoch 35\n",
      "2024-01-11 03:48:41.348357: Current learning rate: 0.00968\n",
      "2024-01-11 03:49:21.667237: train_loss -0.8901\n",
      "2024-01-11 03:49:21.693237: val_loss -0.8422\n",
      "2024-01-11 03:49:21.703763: Pseudo dice [0.8944, 0.9468, 0.9356]\n",
      "2024-01-11 03:49:21.710765: Epoch time: 40.33 s\n",
      "2024-01-11 03:49:21.716763: Yayy! New best EMA pseudo Dice: 0.9208\n",
      "2024-01-11 03:49:23.410911: \n",
      "2024-01-11 03:49:23.415974: Epoch 36\n",
      "2024-01-11 03:49:23.423045: Current learning rate: 0.00968\n",
      "2024-01-11 03:50:03.631336: train_loss -0.8921\n",
      "2024-01-11 03:50:03.639344: val_loss -0.841\n",
      "2024-01-11 03:50:03.644344: Pseudo dice [0.8928, 0.9472, 0.9352]\n",
      "2024-01-11 03:50:03.651858: Epoch time: 40.22 s\n",
      "2024-01-11 03:50:03.661069: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2024-01-11 03:50:05.353104: \n",
      "2024-01-11 03:50:05.359102: Epoch 37\n",
      "2024-01-11 03:50:05.363170: Current learning rate: 0.00967\n",
      "2024-01-11 03:50:45.465586: train_loss -0.8916\n",
      "2024-01-11 03:50:45.472589: val_loss -0.8405\n",
      "2024-01-11 03:50:45.493099: Pseudo dice [0.8897, 0.9466, 0.9365]\n",
      "2024-01-11 03:50:45.500100: Epoch time: 40.11 s\n",
      "2024-01-11 03:50:45.505101: Yayy! New best EMA pseudo Dice: 0.9215\n",
      "2024-01-11 03:50:47.026896: \n",
      "2024-01-11 03:50:47.031883: Epoch 38\n",
      "2024-01-11 03:50:47.040828: Current learning rate: 0.00966\n",
      "2024-01-11 03:51:27.217896: train_loss -0.8928\n",
      "2024-01-11 03:51:27.226896: val_loss -0.8374\n",
      "2024-01-11 03:51:27.233895: Pseudo dice [0.8869, 0.9459, 0.9359]\n",
      "2024-01-11 03:51:27.239899: Epoch time: 40.19 s\n",
      "2024-01-11 03:51:27.244900: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2024-01-11 03:51:28.997129: \n",
      "2024-01-11 03:51:29.003627: Epoch 39\n",
      "2024-01-11 03:51:29.007704: Current learning rate: 0.00965\n",
      "2024-01-11 03:52:09.151553: train_loss -0.894\n",
      "2024-01-11 03:52:09.161553: val_loss -0.8389\n",
      "2024-01-11 03:52:09.170581: Pseudo dice [0.8928, 0.9455, 0.9345]\n",
      "2024-01-11 03:52:09.198571: Epoch time: 40.16 s\n",
      "2024-01-11 03:52:09.206571: Yayy! New best EMA pseudo Dice: 0.9219\n",
      "2024-01-11 03:52:11.094450: \n",
      "2024-01-11 03:52:11.100400: Epoch 40\n",
      "2024-01-11 03:52:11.104424: Current learning rate: 0.00964\n",
      "2024-01-11 03:52:51.347758: train_loss -0.8955\n",
      "2024-01-11 03:52:51.356266: val_loss -0.8407\n",
      "2024-01-11 03:52:51.363647: Pseudo dice [0.8899, 0.9478, 0.9369]\n",
      "2024-01-11 03:52:51.393655: Epoch time: 40.25 s\n",
      "2024-01-11 03:52:51.401654: Yayy! New best EMA pseudo Dice: 0.9222\n",
      "2024-01-11 03:52:52.985284: \n",
      "2024-01-11 03:52:52.993626: Epoch 41\n",
      "2024-01-11 03:52:53.002710: Current learning rate: 0.00963\n",
      "2024-01-11 03:53:33.234791: train_loss -0.8953\n",
      "2024-01-11 03:53:33.240792: val_loss -0.8411\n",
      "2024-01-11 03:53:33.247791: Pseudo dice [0.8931, 0.9469, 0.9367]\n",
      "2024-01-11 03:53:33.254792: Epoch time: 40.25 s\n",
      "2024-01-11 03:53:33.259801: Yayy! New best EMA pseudo Dice: 0.9226\n",
      "2024-01-11 03:53:34.915830: \n",
      "2024-01-11 03:53:34.921990: Epoch 42\n",
      "2024-01-11 03:53:34.926289: Current learning rate: 0.00962\n",
      "2024-01-11 03:54:14.999640: train_loss -0.896\n",
      "2024-01-11 03:54:15.006642: val_loss -0.8436\n",
      "2024-01-11 03:54:15.013639: Pseudo dice [0.8945, 0.9481, 0.9367]\n",
      "2024-01-11 03:54:15.020639: Epoch time: 40.08 s\n",
      "2024-01-11 03:54:15.029642: Yayy! New best EMA pseudo Dice: 0.923\n",
      "2024-01-11 03:54:16.682565: \n",
      "2024-01-11 03:54:16.689574: Epoch 43\n",
      "2024-01-11 03:54:16.697209: Current learning rate: 0.00961\n",
      "2024-01-11 03:54:56.821441: train_loss -0.8964\n",
      "2024-01-11 03:54:56.829442: val_loss -0.8424\n",
      "2024-01-11 03:54:56.834444: Pseudo dice [0.8941, 0.9477, 0.9356]\n",
      "2024-01-11 03:54:56.840443: Epoch time: 40.14 s\n",
      "2024-01-11 03:54:56.845442: Yayy! New best EMA pseudo Dice: 0.9232\n",
      "2024-01-11 03:54:58.479337: \n",
      "2024-01-11 03:54:58.484403: Epoch 44\n",
      "2024-01-11 03:54:58.489340: Current learning rate: 0.0096\n",
      "2024-01-11 03:55:38.721279: train_loss -0.8978\n",
      "2024-01-11 03:55:38.728794: val_loss -0.8427\n",
      "2024-01-11 03:55:38.735792: Pseudo dice [0.8975, 0.9469, 0.9356]\n",
      "2024-01-11 03:55:38.740793: Epoch time: 40.24 s\n",
      "2024-01-11 03:55:38.744792: Yayy! New best EMA pseudo Dice: 0.9236\n",
      "2024-01-11 03:55:40.341636: \n",
      "2024-01-11 03:55:40.347630: Epoch 45\n",
      "2024-01-11 03:55:40.351635: Current learning rate: 0.00959\n",
      "2024-01-11 03:56:20.345067: train_loss -0.8987\n",
      "2024-01-11 03:56:20.351067: val_loss -0.8425\n",
      "2024-01-11 03:56:20.357068: Pseudo dice [0.8954, 0.9471, 0.9367]\n",
      "2024-01-11 03:56:20.361070: Epoch time: 40.0 s\n",
      "2024-01-11 03:56:20.369067: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2024-01-11 03:56:21.939827: \n",
      "2024-01-11 03:56:21.947766: Epoch 46\n",
      "2024-01-11 03:56:21.953766: Current learning rate: 0.00959\n",
      "2024-01-11 03:57:02.164423: train_loss -0.8982\n",
      "2024-01-11 03:57:02.169936: val_loss -0.8425\n",
      "2024-01-11 03:57:02.174936: Pseudo dice [0.8985, 0.9466, 0.9366]\n",
      "2024-01-11 03:57:02.179936: Epoch time: 40.23 s\n",
      "2024-01-11 03:57:02.184935: Yayy! New best EMA pseudo Dice: 0.9242\n",
      "2024-01-11 03:57:03.791762: \n",
      "2024-01-11 03:57:03.796767: Epoch 47\n",
      "2024-01-11 03:57:03.804004: Current learning rate: 0.00958\n",
      "2024-01-11 03:57:43.872093: train_loss -0.8991\n",
      "2024-01-11 03:57:43.879094: val_loss -0.8424\n",
      "2024-01-11 03:57:43.885616: Pseudo dice [0.8927, 0.948, 0.9373]\n",
      "2024-01-11 03:57:43.890612: Epoch time: 40.08 s\n",
      "2024-01-11 03:57:43.895611: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2024-01-11 03:57:45.485117: \n",
      "2024-01-11 03:57:45.490304: Epoch 48\n",
      "2024-01-11 03:57:45.494319: Current learning rate: 0.00957\n",
      "2024-01-11 03:58:25.550144: train_loss -0.8989\n",
      "2024-01-11 03:58:25.556147: val_loss -0.8365\n",
      "2024-01-11 03:58:25.562145: Pseudo dice [0.8877, 0.9468, 0.9355]\n",
      "2024-01-11 03:58:25.570146: Epoch time: 40.07 s\n",
      "2024-01-11 03:58:27.231364: \n",
      "2024-01-11 03:58:27.237708: Epoch 49\n",
      "2024-01-11 03:58:27.242780: Current learning rate: 0.00956\n",
      "2024-01-11 03:59:07.267818: train_loss -0.8998\n",
      "2024-01-11 03:59:07.274819: val_loss -0.8413\n",
      "2024-01-11 03:59:07.282818: Pseudo dice [0.8934, 0.9471, 0.9369]\n",
      "2024-01-11 03:59:07.287818: Epoch time: 40.04 s\n",
      "2024-01-11 03:59:07.597363: Yayy! New best EMA pseudo Dice: 0.9244\n",
      "2024-01-11 03:59:09.034632: \n",
      "2024-01-11 03:59:09.044341: Epoch 50\n",
      "2024-01-11 03:59:09.053347: Current learning rate: 0.00955\n",
      "2024-01-11 03:59:49.183199: train_loss -0.9006\n",
      "2024-01-11 03:59:49.190200: val_loss -0.8413\n",
      "2024-01-11 03:59:49.195199: Pseudo dice [0.8934, 0.9474, 0.9364]\n",
      "2024-01-11 03:59:49.199199: Epoch time: 40.15 s\n",
      "2024-01-11 03:59:49.203201: Yayy! New best EMA pseudo Dice: 0.9246\n",
      "2024-01-11 03:59:50.886830: \n",
      "2024-01-11 03:59:50.899960: Epoch 51\n",
      "2024-01-11 03:59:50.905061: Current learning rate: 0.00954\n",
      "2024-01-11 04:00:31.024611: train_loss -0.9016\n",
      "2024-01-11 04:00:31.033612: val_loss -0.8398\n",
      "2024-01-11 04:00:31.064636: Pseudo dice [0.8946, 0.9469, 0.9353]\n",
      "2024-01-11 04:00:31.069639: Epoch time: 40.14 s\n",
      "2024-01-11 04:00:31.074639: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2024-01-11 04:00:32.623254: \n",
      "2024-01-11 04:00:32.629475: Epoch 52\n",
      "2024-01-11 04:00:32.633497: Current learning rate: 0.00953\n",
      "2024-01-11 04:01:12.775520: train_loss -0.901\n",
      "2024-01-11 04:01:12.782033: val_loss -0.8425\n",
      "2024-01-11 04:01:12.789033: Pseudo dice [0.892, 0.9479, 0.9368]\n",
      "2024-01-11 04:01:12.795034: Epoch time: 40.15 s\n",
      "2024-01-11 04:01:12.800033: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2024-01-11 04:01:14.332859: \n",
      "2024-01-11 04:01:14.337850: Epoch 53\n",
      "2024-01-11 04:01:14.342742: Current learning rate: 0.00952\n",
      "2024-01-11 04:01:54.413350: train_loss -0.9016\n",
      "2024-01-11 04:01:54.431350: val_loss -0.8387\n",
      "2024-01-11 04:01:54.448348: Pseudo dice [0.8948, 0.9462, 0.9337]\n",
      "2024-01-11 04:01:54.456349: Epoch time: 40.08 s\n",
      "2024-01-11 04:01:54.465349: Yayy! New best EMA pseudo Dice: 0.9248\n",
      "2024-01-11 04:01:56.223214: \n",
      "2024-01-11 04:01:56.232207: Epoch 54\n",
      "2024-01-11 04:01:56.237206: Current learning rate: 0.00951\n",
      "2024-01-11 04:02:36.511662: train_loss -0.9027\n",
      "2024-01-11 04:02:36.517673: val_loss -0.8422\n",
      "2024-01-11 04:02:36.546182: Pseudo dice [0.8936, 0.9482, 0.9377]\n",
      "2024-01-11 04:02:36.555180: Epoch time: 40.29 s\n",
      "2024-01-11 04:02:36.563180: Yayy! New best EMA pseudo Dice: 0.9249\n",
      "2024-01-11 04:02:38.138373: \n",
      "2024-01-11 04:02:38.145629: Epoch 55\n",
      "2024-01-11 04:02:38.149502: Current learning rate: 0.0095\n",
      "2024-01-11 04:03:18.127160: train_loss -0.9026\n",
      "2024-01-11 04:03:18.154673: val_loss -0.8422\n",
      "2024-01-11 04:03:18.161672: Pseudo dice [0.8927, 0.9483, 0.9371]\n",
      "2024-01-11 04:03:18.168673: Epoch time: 39.99 s\n",
      "2024-01-11 04:03:18.174675: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2024-01-11 04:03:19.779562: \n",
      "2024-01-11 04:03:19.784837: Epoch 56\n",
      "2024-01-11 04:03:19.791917: Current learning rate: 0.00949\n",
      "2024-01-11 04:03:59.905312: train_loss -0.9018\n",
      "2024-01-11 04:03:59.913314: val_loss -0.8439\n",
      "2024-01-11 04:03:59.923314: Pseudo dice [0.8955, 0.9477, 0.9373]\n",
      "2024-01-11 04:03:59.930311: Epoch time: 40.13 s\n",
      "2024-01-11 04:03:59.958323: Yayy! New best EMA pseudo Dice: 0.9252\n",
      "2024-01-11 04:04:01.597527: \n",
      "2024-01-11 04:04:01.605538: Epoch 57\n",
      "2024-01-11 04:04:01.610541: Current learning rate: 0.00949\n",
      "2024-01-11 04:04:41.721829: train_loss -0.9033\n",
      "2024-01-11 04:04:41.729829: val_loss -0.8398\n",
      "2024-01-11 04:04:41.735828: Pseudo dice [0.8942, 0.9474, 0.935]\n",
      "2024-01-11 04:04:41.742830: Epoch time: 40.13 s\n",
      "2024-01-11 04:04:41.747830: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-11 04:04:43.380160: \n",
      "2024-01-11 04:04:43.385249: Epoch 58\n",
      "2024-01-11 04:04:43.389856: Current learning rate: 0.00948\n",
      "2024-01-11 04:05:23.558553: train_loss -0.9036\n",
      "2024-01-11 04:05:23.567561: val_loss -0.839\n",
      "2024-01-11 04:05:23.574555: Pseudo dice [0.8866, 0.9481, 0.9366]\n",
      "2024-01-11 04:05:23.580553: Epoch time: 40.18 s\n",
      "2024-01-11 04:05:25.125600: \n",
      "2024-01-11 04:05:25.131186: Epoch 59\n",
      "2024-01-11 04:05:25.136190: Current learning rate: 0.00947\n",
      "2024-01-11 04:06:05.086190: train_loss -0.9043\n",
      "2024-01-11 04:06:05.094190: val_loss -0.8415\n",
      "2024-01-11 04:06:05.104192: Pseudo dice [0.8947, 0.9476, 0.9361]\n",
      "2024-01-11 04:06:05.139708: Epoch time: 39.96 s\n",
      "2024-01-11 04:06:06.505179: \n",
      "2024-01-11 04:06:06.510424: Epoch 60\n",
      "2024-01-11 04:06:06.518499: Current learning rate: 0.00946\n",
      "2024-01-11 04:06:46.469389: train_loss -0.9044\n",
      "2024-01-11 04:06:46.478395: val_loss -0.8426\n",
      "2024-01-11 04:06:46.484393: Pseudo dice [0.8926, 0.9482, 0.9365]\n",
      "2024-01-11 04:06:46.491393: Epoch time: 39.97 s\n",
      "2024-01-11 04:06:46.498390: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2024-01-11 04:06:48.179271: \n",
      "2024-01-11 04:06:48.184265: Epoch 61\n",
      "2024-01-11 04:06:48.189256: Current learning rate: 0.00945\n",
      "2024-01-11 04:07:28.349550: train_loss -0.9052\n",
      "2024-01-11 04:07:28.358065: val_loss -0.8399\n",
      "2024-01-11 04:07:28.365064: Pseudo dice [0.897, 0.9469, 0.9359]\n",
      "2024-01-11 04:07:28.398079: Epoch time: 40.17 s\n",
      "2024-01-11 04:07:28.406068: Yayy! New best EMA pseudo Dice: 0.9254\n",
      "2024-01-11 04:07:30.006955: \n",
      "2024-01-11 04:07:30.012573: Epoch 62\n",
      "2024-01-11 04:07:30.015970: Current learning rate: 0.00944\n",
      "2024-01-11 04:08:10.083966: train_loss -0.9061\n",
      "2024-01-11 04:08:10.090971: val_loss -0.8416\n",
      "2024-01-11 04:08:10.095973: Pseudo dice [0.8934, 0.9478, 0.9379]\n",
      "2024-01-11 04:08:10.099972: Epoch time: 40.08 s\n",
      "2024-01-11 04:08:10.105973: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2024-01-11 04:08:11.872309: \n",
      "2024-01-11 04:08:11.878382: Epoch 63\n",
      "2024-01-11 04:08:11.885838: Current learning rate: 0.00943\n",
      "2024-01-11 04:08:51.977169: train_loss -0.9057\n",
      "2024-01-11 04:08:51.984170: val_loss -0.8445\n",
      "2024-01-11 04:08:51.990175: Pseudo dice [0.8964, 0.9482, 0.9372]\n",
      "2024-01-11 04:08:51.997176: Epoch time: 40.11 s\n",
      "2024-01-11 04:08:52.003687: Yayy! New best EMA pseudo Dice: 0.9257\n",
      "2024-01-11 04:08:53.695126: \n",
      "2024-01-11 04:08:53.701313: Epoch 64\n",
      "2024-01-11 04:08:53.705488: Current learning rate: 0.00942\n",
      "2024-01-11 04:09:33.791297: train_loss -0.9066\n",
      "2024-01-11 04:09:33.797301: val_loss -0.8388\n",
      "2024-01-11 04:09:33.803297: Pseudo dice [0.8908, 0.9468, 0.9362]\n",
      "2024-01-11 04:09:33.810308: Epoch time: 40.1 s\n",
      "2024-01-11 04:09:35.281246: \n",
      "2024-01-11 04:09:35.287252: Epoch 65\n",
      "2024-01-11 04:09:35.291254: Current learning rate: 0.00941\n",
      "2024-01-11 04:10:15.221588: train_loss -0.9068\n",
      "2024-01-11 04:10:15.229616: val_loss -0.8428\n",
      "2024-01-11 04:10:15.236615: Pseudo dice [0.8965, 0.9479, 0.9376]\n",
      "2024-01-11 04:10:15.259250: Epoch time: 39.94 s\n",
      "2024-01-11 04:10:15.268251: Yayy! New best EMA pseudo Dice: 0.9257\n",
      "2024-01-11 04:10:16.943640: \n",
      "2024-01-11 04:10:16.949528: Epoch 66\n",
      "2024-01-11 04:10:16.953621: Current learning rate: 0.0094\n",
      "2024-01-11 04:10:57.004316: train_loss -0.9067\n",
      "2024-01-11 04:10:57.035313: val_loss -0.8401\n",
      "2024-01-11 04:10:57.048294: Pseudo dice [0.8941, 0.9475, 0.9364]\n",
      "2024-01-11 04:10:57.055296: Epoch time: 40.06 s\n",
      "2024-01-11 04:10:57.062802: Yayy! New best EMA pseudo Dice: 0.9258\n",
      "2024-01-11 04:10:58.756486: \n",
      "2024-01-11 04:10:58.761487: Epoch 67\n",
      "2024-01-11 04:10:58.764989: Current learning rate: 0.00939\n",
      "2024-01-11 04:11:38.880581: train_loss -0.9072\n",
      "2024-01-11 04:11:38.888584: val_loss -0.8394\n",
      "2024-01-11 04:11:38.896705: Pseudo dice [0.8945, 0.9466, 0.9352]\n",
      "2024-01-11 04:11:38.902705: Epoch time: 40.13 s\n",
      "2024-01-11 04:11:40.303245: \n",
      "2024-01-11 04:11:40.308247: Epoch 68\n",
      "2024-01-11 04:11:40.313402: Current learning rate: 0.00939\n",
      "2024-01-11 04:12:20.308915: train_loss -0.9078\n",
      "2024-01-11 04:12:20.315424: val_loss -0.8414\n",
      "2024-01-11 04:12:20.340977: Pseudo dice [0.8963, 0.9472, 0.9372]\n",
      "2024-01-11 04:12:20.347982: Epoch time: 40.01 s\n",
      "2024-01-11 04:12:20.356489: Yayy! New best EMA pseudo Dice: 0.9259\n",
      "2024-01-11 04:12:22.228525: \n",
      "2024-01-11 04:12:22.236026: Epoch 69\n",
      "2024-01-11 04:12:22.242521: Current learning rate: 0.00938\n",
      "2024-01-11 04:13:02.413756: train_loss -0.9085\n",
      "2024-01-11 04:13:02.421757: val_loss -0.8447\n",
      "2024-01-11 04:13:02.429757: Pseudo dice [0.8939, 0.9487, 0.939]\n",
      "2024-01-11 04:13:02.435756: Epoch time: 40.19 s\n",
      "2024-01-11 04:13:02.443757: Yayy! New best EMA pseudo Dice: 0.926\n",
      "2024-01-11 04:13:04.006238: \n",
      "2024-01-11 04:13:04.015249: Epoch 70\n",
      "2024-01-11 04:13:04.020245: Current learning rate: 0.00937\n",
      "2024-01-11 04:13:44.151802: train_loss -0.9087\n",
      "2024-01-11 04:13:44.159318: val_loss -0.8407\n",
      "2024-01-11 04:13:44.188835: Pseudo dice [0.8938, 0.9474, 0.9367]\n",
      "2024-01-11 04:13:44.196345: Epoch time: 40.15 s\n",
      "2024-01-11 04:13:45.670012: \n",
      "2024-01-11 04:13:45.679068: Epoch 71\n",
      "2024-01-11 04:13:45.683838: Current learning rate: 0.00936\n",
      "2024-01-11 04:14:25.751958: train_loss -0.9087\n",
      "2024-01-11 04:14:25.761958: val_loss -0.842\n",
      "2024-01-11 04:14:25.768958: Pseudo dice [0.8992, 0.9472, 0.9361]\n",
      "2024-01-11 04:14:25.776959: Epoch time: 40.08 s\n",
      "2024-01-11 04:14:25.783957: Yayy! New best EMA pseudo Dice: 0.9261\n",
      "2024-01-11 04:14:27.493672: \n",
      "2024-01-11 04:14:27.501809: Epoch 72\n",
      "2024-01-11 04:14:27.505810: Current learning rate: 0.00935\n",
      "2024-01-11 04:15:07.746860: train_loss -0.9091\n",
      "2024-01-11 04:15:07.753860: val_loss -0.839\n",
      "2024-01-11 04:15:07.760863: Pseudo dice [0.8954, 0.9478, 0.9351]\n",
      "2024-01-11 04:15:07.767393: Epoch time: 40.25 s\n",
      "2024-01-11 04:15:09.170424: \n",
      "2024-01-11 04:15:09.179427: Epoch 73\n",
      "2024-01-11 04:15:09.186497: Current learning rate: 0.00934\n",
      "2024-01-11 04:15:49.250191: train_loss -0.9097\n",
      "2024-01-11 04:15:49.281193: val_loss -0.8412\n",
      "2024-01-11 04:15:49.288193: Pseudo dice [0.8963, 0.9474, 0.9361]\n",
      "2024-01-11 04:15:49.293194: Epoch time: 40.08 s\n",
      "2024-01-11 04:15:49.299699: Yayy! New best EMA pseudo Dice: 0.9262\n",
      "2024-01-11 04:15:51.019619: \n",
      "2024-01-11 04:15:51.028967: Epoch 74\n",
      "2024-01-11 04:15:51.034981: Current learning rate: 0.00933\n",
      "2024-01-11 04:16:31.161513: train_loss -0.9092\n",
      "2024-01-11 04:16:31.168034: val_loss -0.8425\n",
      "2024-01-11 04:16:31.175030: Pseudo dice [0.8942, 0.9481, 0.937]\n",
      "2024-01-11 04:16:31.181031: Epoch time: 40.14 s\n",
      "2024-01-11 04:16:31.186030: Yayy! New best EMA pseudo Dice: 0.9262\n",
      "2024-01-11 04:16:32.787620: \n",
      "2024-01-11 04:16:32.796713: Epoch 75\n",
      "2024-01-11 04:16:32.800847: Current learning rate: 0.00932\n",
      "2024-01-11 04:17:12.997173: train_loss -0.9101\n",
      "2024-01-11 04:17:13.004173: val_loss -0.8446\n",
      "2024-01-11 04:17:13.040174: Pseudo dice [0.8965, 0.9487, 0.9373]\n",
      "2024-01-11 04:17:13.048175: Epoch time: 40.21 s\n",
      "2024-01-11 04:17:13.055174: Yayy! New best EMA pseudo Dice: 0.9263\n",
      "2024-01-11 04:17:14.641512: \n",
      "2024-01-11 04:17:14.650500: Epoch 76\n",
      "2024-01-11 04:17:14.660493: Current learning rate: 0.00931\n",
      "2024-01-11 04:17:54.751863: train_loss -0.9094\n",
      "2024-01-11 04:17:54.778864: val_loss -0.8432\n",
      "2024-01-11 04:17:54.785863: Pseudo dice [0.8921, 0.9488, 0.9386]\n",
      "2024-01-11 04:17:54.792869: Epoch time: 40.11 s\n",
      "2024-01-11 04:17:54.799881: Yayy! New best EMA pseudo Dice: 0.9263\n",
      "2024-01-11 04:17:56.461765: \n",
      "2024-01-11 04:17:56.471835: Epoch 77\n",
      "2024-01-11 04:17:56.475837: Current learning rate: 0.0093\n",
      "2024-01-11 04:18:36.492413: train_loss -0.9093\n",
      "2024-01-11 04:18:36.500414: val_loss -0.8433\n",
      "2024-01-11 04:18:36.508415: Pseudo dice [0.8961, 0.9483, 0.9379]\n",
      "2024-01-11 04:18:36.518421: Epoch time: 40.03 s\n",
      "2024-01-11 04:18:36.524426: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2024-01-11 04:18:38.335400: \n",
      "2024-01-11 04:18:38.340332: Epoch 78\n",
      "2024-01-11 04:18:38.344776: Current learning rate: 0.0093\n",
      "2024-01-11 04:19:18.292228: train_loss -0.9097\n",
      "2024-01-11 04:19:18.302239: val_loss -0.8431\n",
      "2024-01-11 04:19:18.311231: Pseudo dice [0.895, 0.9483, 0.937]\n",
      "2024-01-11 04:19:18.342227: Epoch time: 39.96 s\n",
      "2024-01-11 04:19:18.351235: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2024-01-11 04:19:20.045680: \n",
      "2024-01-11 04:19:20.058055: Epoch 79\n",
      "2024-01-11 04:19:20.062709: Current learning rate: 0.00929\n",
      "2024-01-11 04:20:00.104164: train_loss -0.9111\n",
      "2024-01-11 04:20:00.114165: val_loss -0.8422\n",
      "2024-01-11 04:20:00.123165: Pseudo dice [0.8946, 0.9478, 0.9369]\n",
      "2024-01-11 04:20:00.159163: Epoch time: 40.06 s\n",
      "2024-01-11 04:20:01.623559: \n",
      "2024-01-11 04:20:01.632571: Epoch 80\n",
      "2024-01-11 04:20:01.643878: Current learning rate: 0.00928\n",
      "2024-01-11 04:20:41.805265: train_loss -0.9115\n",
      "2024-01-11 04:20:41.815265: val_loss -0.8417\n",
      "2024-01-11 04:20:41.824258: Pseudo dice [0.898, 0.9483, 0.9369]\n",
      "2024-01-11 04:20:41.858274: Epoch time: 40.18 s\n",
      "2024-01-11 04:20:41.867344: Yayy! New best EMA pseudo Dice: 0.9266\n",
      "2024-01-11 04:20:43.534848: \n",
      "2024-01-11 04:20:43.540715: Epoch 81\n",
      "2024-01-11 04:20:43.546781: Current learning rate: 0.00927\n",
      "2024-01-11 04:21:23.611670: train_loss -0.9128\n",
      "2024-01-11 04:21:23.619673: val_loss -0.8423\n",
      "2024-01-11 04:21:23.629185: Pseudo dice [0.8952, 0.9479, 0.9366]\n",
      "2024-01-11 04:21:23.639030: Epoch time: 40.08 s\n",
      "2024-01-11 04:21:25.155607: \n",
      "2024-01-11 04:21:25.161634: Epoch 82\n",
      "2024-01-11 04:21:25.166675: Current learning rate: 0.00926\n",
      "2024-01-11 04:22:05.363823: train_loss -0.9119\n",
      "2024-01-11 04:22:05.371824: val_loss -0.846\n",
      "2024-01-11 04:22:05.392825: Pseudo dice [0.8987, 0.9489, 0.9382]\n",
      "2024-01-11 04:22:05.400825: Epoch time: 40.21 s\n",
      "2024-01-11 04:22:05.406826: Yayy! New best EMA pseudo Dice: 0.9268\n",
      "2024-01-11 04:22:07.040925: \n",
      "2024-01-11 04:22:07.049693: Epoch 83\n",
      "2024-01-11 04:22:07.054694: Current learning rate: 0.00925\n",
      "2024-01-11 04:22:47.204987: train_loss -0.9116\n",
      "2024-01-11 04:22:47.211986: val_loss -0.8406\n",
      "2024-01-11 04:22:47.219986: Pseudo dice [0.8939, 0.9478, 0.9371]\n",
      "2024-01-11 04:22:47.224987: Epoch time: 40.17 s\n",
      "2024-01-11 04:22:48.617400: \n",
      "2024-01-11 04:22:48.628945: Epoch 84\n",
      "2024-01-11 04:22:48.633875: Current learning rate: 0.00924\n",
      "2024-01-11 04:23:28.651440: train_loss -0.9126\n",
      "2024-01-11 04:23:28.657438: val_loss -0.8419\n",
      "2024-01-11 04:23:28.687027: Pseudo dice [0.8952, 0.948, 0.9376]\n",
      "2024-01-11 04:23:28.696028: Epoch time: 40.04 s\n",
      "2024-01-11 04:23:30.029471: \n",
      "2024-01-11 04:23:30.037721: Epoch 85\n",
      "2024-01-11 04:23:30.041781: Current learning rate: 0.00923\n",
      "2024-01-11 04:24:10.075917: train_loss -0.9127\n",
      "2024-01-11 04:24:10.083917: val_loss -0.8462\n",
      "2024-01-11 04:24:10.089918: Pseudo dice [0.8958, 0.9492, 0.9391]\n",
      "2024-01-11 04:24:10.095919: Epoch time: 40.05 s\n",
      "2024-01-11 04:24:10.100451: Yayy! New best EMA pseudo Dice: 0.9269\n",
      "2024-01-11 04:24:11.680399: \n",
      "2024-01-11 04:24:11.686400: Epoch 86\n",
      "2024-01-11 04:24:11.692444: Current learning rate: 0.00922\n",
      "2024-01-11 04:24:51.676846: train_loss -0.913\n",
      "2024-01-11 04:24:51.683848: val_loss -0.8429\n",
      "2024-01-11 04:24:51.692845: Pseudo dice [0.896, 0.9477, 0.9361]\n",
      "2024-01-11 04:24:51.701848: Epoch time: 40.0 s\n",
      "2024-01-11 04:24:53.331942: \n",
      "2024-01-11 04:24:53.340005: Epoch 87\n",
      "2024-01-11 04:24:53.345010: Current learning rate: 0.00921\n",
      "2024-01-11 04:25:33.352341: train_loss -0.9126\n",
      "2024-01-11 04:25:33.360344: val_loss -0.8413\n",
      "2024-01-11 04:25:33.369353: Pseudo dice [0.8938, 0.9482, 0.9364]\n",
      "2024-01-11 04:25:33.375351: Epoch time: 40.02 s\n",
      "2024-01-11 04:25:34.810362: \n",
      "2024-01-11 04:25:34.816361: Epoch 88\n",
      "2024-01-11 04:25:34.820361: Current learning rate: 0.0092\n",
      "2024-01-11 04:26:14.733833: train_loss -0.9132\n",
      "2024-01-11 04:26:14.739833: val_loss -0.8436\n",
      "2024-01-11 04:26:14.744833: Pseudo dice [0.8985, 0.9482, 0.938]\n",
      "2024-01-11 04:26:14.751833: Epoch time: 39.92 s\n",
      "2024-01-11 04:26:14.758836: Yayy! New best EMA pseudo Dice: 0.9269\n",
      "2024-01-11 04:26:16.416196: \n",
      "2024-01-11 04:26:16.426193: Epoch 89\n",
      "2024-01-11 04:26:16.431215: Current learning rate: 0.0092\n",
      "2024-01-11 04:26:56.390798: train_loss -0.9138\n",
      "2024-01-11 04:26:56.402805: val_loss -0.8438\n",
      "2024-01-11 04:26:56.409804: Pseudo dice [0.8922, 0.9492, 0.9385]\n",
      "2024-01-11 04:26:56.415804: Epoch time: 39.98 s\n",
      "2024-01-11 04:26:57.824696: \n",
      "2024-01-11 04:26:57.830693: Epoch 90\n",
      "2024-01-11 04:26:57.834705: Current learning rate: 0.00919\n",
      "2024-01-11 04:27:37.951821: train_loss -0.9147\n",
      "2024-01-11 04:27:37.957823: val_loss -0.8403\n",
      "2024-01-11 04:27:37.965823: Pseudo dice [0.8951, 0.9477, 0.9357]\n",
      "2024-01-11 04:27:37.992822: Epoch time: 40.13 s\n",
      "2024-01-11 04:27:39.359781: \n",
      "2024-01-11 04:27:39.367360: Epoch 91\n",
      "2024-01-11 04:27:39.373376: Current learning rate: 0.00918\n",
      "2024-01-11 04:28:19.801148: train_loss -0.9144\n",
      "2024-01-11 04:28:19.810149: val_loss -0.8415\n",
      "2024-01-11 04:28:19.816665: Pseudo dice [0.8938, 0.9473, 0.9361]\n",
      "2024-01-11 04:28:19.822175: Epoch time: 40.44 s\n",
      "2024-01-11 04:28:21.339891: \n",
      "2024-01-11 04:28:21.347903: Epoch 92\n",
      "2024-01-11 04:28:21.351883: Current learning rate: 0.00917\n",
      "2024-01-11 04:29:01.315211: train_loss -0.9143\n",
      "2024-01-11 04:29:01.323211: val_loss -0.8414\n",
      "2024-01-11 04:29:01.357210: Pseudo dice [0.8957, 0.948, 0.936]\n",
      "2024-01-11 04:29:01.367210: Epoch time: 39.98 s\n",
      "2024-01-11 04:29:02.681620: \n",
      "2024-01-11 04:29:02.693082: Epoch 93\n",
      "2024-01-11 04:29:02.698137: Current learning rate: 0.00916\n",
      "2024-01-11 04:29:42.798758: train_loss -0.915\n",
      "2024-01-11 04:29:42.825264: val_loss -0.8384\n",
      "2024-01-11 04:29:42.832263: Pseudo dice [0.8942, 0.9479, 0.9365]\n",
      "2024-01-11 04:29:42.838263: Epoch time: 40.12 s\n",
      "2024-01-11 04:29:44.167248: \n",
      "2024-01-11 04:29:44.174241: Epoch 94\n",
      "2024-01-11 04:29:44.180251: Current learning rate: 0.00915\n",
      "2024-01-11 04:30:24.320034: train_loss -0.9152\n",
      "2024-01-11 04:30:24.329038: val_loss -0.8414\n",
      "2024-01-11 04:30:24.336033: Pseudo dice [0.8938, 0.948, 0.9373]\n",
      "2024-01-11 04:30:24.362552: Epoch time: 40.15 s\n",
      "2024-01-11 04:30:25.706593: \n",
      "2024-01-11 04:30:25.713207: Epoch 95\n",
      "2024-01-11 04:30:25.717207: Current learning rate: 0.00914\n",
      "2024-01-11 04:31:05.826091: train_loss -0.9153\n",
      "2024-01-11 04:31:05.836090: val_loss -0.8365\n",
      "2024-01-11 04:31:05.843091: Pseudo dice [0.8918, 0.9469, 0.9352]\n",
      "2024-01-11 04:31:05.849091: Epoch time: 40.12 s\n",
      "2024-01-11 04:31:07.226938: \n",
      "2024-01-11 04:31:07.233914: Epoch 96\n",
      "2024-01-11 04:31:07.241912: Current learning rate: 0.00913\n",
      "2024-01-11 04:31:47.277525: train_loss -0.9149\n",
      "2024-01-11 04:31:47.286526: val_loss -0.8415\n",
      "2024-01-11 04:31:47.297535: Pseudo dice [0.8963, 0.9478, 0.9362]\n",
      "2024-01-11 04:31:47.336044: Epoch time: 40.05 s\n",
      "2024-01-11 04:31:48.881129: \n",
      "2024-01-11 04:31:48.887967: Epoch 97\n",
      "2024-01-11 04:31:48.892098: Current learning rate: 0.00912\n",
      "2024-01-11 04:32:28.886942: train_loss -0.9162\n",
      "2024-01-11 04:32:28.894943: val_loss -0.8368\n",
      "2024-01-11 04:32:28.899944: Pseudo dice [0.891, 0.9468, 0.9352]\n",
      "2024-01-11 04:32:28.903943: Epoch time: 40.01 s\n",
      "2024-01-11 04:32:30.363599: \n",
      "2024-01-11 04:32:30.367599: Epoch 98\n",
      "2024-01-11 04:32:30.371600: Current learning rate: 0.00911\n",
      "2024-01-11 04:33:10.367102: train_loss -0.9157\n",
      "2024-01-11 04:33:10.376101: val_loss -0.8413\n",
      "2024-01-11 04:33:10.383102: Pseudo dice [0.8935, 0.9482, 0.9367]\n",
      "2024-01-11 04:33:10.389102: Epoch time: 40.0 s\n",
      "2024-01-11 04:33:11.933478: \n",
      "2024-01-11 04:33:11.938274: Epoch 99\n",
      "2024-01-11 04:33:11.944019: Current learning rate: 0.0091\n",
      "2024-01-11 04:33:52.026574: train_loss -0.9163\n",
      "2024-01-11 04:33:52.033576: val_loss -0.8411\n",
      "2024-01-11 04:33:52.040576: Pseudo dice [0.8926, 0.9485, 0.9374]\n",
      "2024-01-11 04:33:52.047579: Epoch time: 40.09 s\n",
      "2024-01-11 04:33:53.683817: \n",
      "2024-01-11 04:33:53.689045: Epoch 100\n",
      "2024-01-11 04:33:53.694058: Current learning rate: 0.0091\n",
      "2024-01-11 04:34:33.678552: train_loss -0.9163\n",
      "2024-01-11 04:34:33.690068: val_loss -0.8417\n",
      "2024-01-11 04:34:33.722066: Pseudo dice [0.8981, 0.9481, 0.9357]\n",
      "2024-01-11 04:34:33.732068: Epoch time: 40.0 s\n",
      "2024-01-11 04:34:35.120465: \n",
      "2024-01-11 04:34:35.131388: Epoch 101\n",
      "2024-01-11 04:34:35.143255: Current learning rate: 0.00909\n",
      "2024-01-11 04:35:15.238663: train_loss -0.9164\n",
      "2024-01-11 04:35:15.244665: val_loss -0.8381\n",
      "2024-01-11 04:35:15.252672: Pseudo dice [0.887, 0.9475, 0.9358]\n",
      "2024-01-11 04:35:15.260671: Epoch time: 40.12 s\n",
      "2024-01-11 04:35:16.895532: \n",
      "2024-01-11 04:35:16.899970: Epoch 102\n",
      "2024-01-11 04:35:16.905615: Current learning rate: 0.00908\n",
      "2024-01-11 04:35:56.804402: train_loss -0.9169\n",
      "2024-01-11 04:35:56.811411: val_loss -0.8458\n",
      "2024-01-11 04:35:56.817413: Pseudo dice [0.8968, 0.9496, 0.9383]\n",
      "2024-01-11 04:35:56.824426: Epoch time: 39.91 s\n",
      "2024-01-11 04:35:58.205667: \n",
      "2024-01-11 04:35:58.213661: Epoch 103\n",
      "2024-01-11 04:35:58.218601: Current learning rate: 0.00907\n",
      "2024-01-11 04:36:38.266475: train_loss -0.9169\n",
      "2024-01-11 04:36:38.300479: val_loss -0.8427\n",
      "2024-01-11 04:36:38.309481: Pseudo dice [0.8961, 0.9489, 0.9381]\n",
      "2024-01-11 04:36:38.316478: Epoch time: 40.06 s\n",
      "2024-01-11 04:36:39.616885: \n",
      "2024-01-11 04:36:39.621583: Epoch 104\n",
      "2024-01-11 04:36:39.626586: Current learning rate: 0.00906\n",
      "2024-01-11 04:37:19.716049: train_loss -0.9171\n",
      "2024-01-11 04:37:19.722042: val_loss -0.8436\n",
      "2024-01-11 04:37:19.727043: Pseudo dice [0.8977, 0.948, 0.9349]\n",
      "2024-01-11 04:37:19.732040: Epoch time: 40.1 s\n",
      "2024-01-11 04:37:21.188990: \n",
      "2024-01-11 04:37:21.193988: Epoch 105\n",
      "2024-01-11 04:37:21.201981: Current learning rate: 0.00905\n",
      "2024-01-11 04:38:01.163593: train_loss -0.9173\n",
      "2024-01-11 04:38:01.173599: val_loss -0.8429\n",
      "2024-01-11 04:38:01.178599: Pseudo dice [0.8968, 0.9483, 0.938]\n",
      "2024-01-11 04:38:01.205112: Epoch time: 39.98 s\n",
      "2024-01-11 04:38:02.793528: \n",
      "2024-01-11 04:38:02.799227: Epoch 106\n",
      "2024-01-11 04:38:02.803778: Current learning rate: 0.00904\n",
      "2024-01-11 04:38:42.787565: train_loss -0.9165\n",
      "2024-01-11 04:38:42.793567: val_loss -0.8431\n",
      "2024-01-11 04:38:42.800568: Pseudo dice [0.897, 0.9492, 0.9371]\n",
      "2024-01-11 04:38:42.806568: Epoch time: 39.99 s\n",
      "2024-01-11 04:38:44.191437: \n",
      "2024-01-11 04:38:44.197505: Epoch 107\n",
      "2024-01-11 04:38:44.203438: Current learning rate: 0.00903\n",
      "2024-01-11 04:39:24.541288: train_loss -0.9172\n",
      "2024-01-11 04:39:24.547798: val_loss -0.8412\n",
      "2024-01-11 04:39:24.554799: Pseudo dice [0.8946, 0.9484, 0.9367]\n",
      "2024-01-11 04:39:24.559800: Epoch time: 40.35 s\n",
      "2024-01-11 04:39:26.097731: \n",
      "2024-01-11 04:39:26.104054: Epoch 108\n",
      "2024-01-11 04:39:26.109077: Current learning rate: 0.00902\n",
      "2024-01-11 04:40:06.041096: train_loss -0.9177\n",
      "2024-01-11 04:40:06.047096: val_loss -0.8398\n",
      "2024-01-11 04:40:06.054106: Pseudo dice [0.893, 0.9474, 0.9363]\n",
      "2024-01-11 04:40:06.058105: Epoch time: 39.94 s\n",
      "2024-01-11 04:40:07.447261: \n",
      "2024-01-11 04:40:07.452206: Epoch 109\n",
      "2024-01-11 04:40:07.456794: Current learning rate: 0.00901\n",
      "2024-01-11 04:40:47.353700: train_loss -0.9184\n",
      "2024-01-11 04:40:47.360701: val_loss -0.8417\n",
      "2024-01-11 04:40:47.366699: Pseudo dice [0.8949, 0.9482, 0.9367]\n",
      "2024-01-11 04:40:47.391228: Epoch time: 39.91 s\n",
      "2024-01-11 04:40:48.749388: \n",
      "2024-01-11 04:40:48.755349: Epoch 110\n",
      "2024-01-11 04:40:48.760350: Current learning rate: 0.009\n",
      "2024-01-11 04:41:28.896365: train_loss -0.919\n",
      "2024-01-11 04:41:28.904370: val_loss -0.8449\n",
      "2024-01-11 04:41:28.914883: Pseudo dice [0.897, 0.9496, 0.9389]\n",
      "2024-01-11 04:41:28.920883: Epoch time: 40.15 s\n",
      "2024-01-11 04:41:30.395382: \n",
      "2024-01-11 04:41:30.400517: Epoch 111\n",
      "2024-01-11 04:41:30.404525: Current learning rate: 0.009\n",
      "2024-01-11 04:42:10.322654: train_loss -0.9181\n",
      "2024-01-11 04:42:10.331165: val_loss -0.8404\n",
      "2024-01-11 04:42:10.353164: Pseudo dice [0.8935, 0.9488, 0.9368]\n",
      "2024-01-11 04:42:10.365165: Epoch time: 39.93 s\n",
      "2024-01-11 04:42:11.755633: \n",
      "2024-01-11 04:42:11.763962: Epoch 112\n",
      "2024-01-11 04:42:11.771899: Current learning rate: 0.00899\n",
      "2024-01-11 04:42:51.798689: train_loss -0.9189\n",
      "2024-01-11 04:42:51.805690: val_loss -0.8478\n",
      "2024-01-11 04:42:51.812690: Pseudo dice [0.8975, 0.9511, 0.9408]\n",
      "2024-01-11 04:42:51.817690: Epoch time: 40.04 s\n",
      "2024-01-11 04:42:51.822689: Yayy! New best EMA pseudo Dice: 0.927\n",
      "2024-01-11 04:42:53.440423: \n",
      "2024-01-11 04:42:53.444932: Epoch 113\n",
      "2024-01-11 04:42:53.448929: Current learning rate: 0.00898\n",
      "2024-01-11 04:43:33.589378: train_loss -0.9196\n",
      "2024-01-11 04:43:33.598038: val_loss -0.8348\n",
      "2024-01-11 04:43:33.606068: Pseudo dice [0.8928, 0.9463, 0.9336]\n",
      "2024-01-11 04:43:33.613070: Epoch time: 40.15 s\n",
      "2024-01-11 04:43:35.000658: \n",
      "2024-01-11 04:43:35.006479: Epoch 114\n",
      "2024-01-11 04:43:35.011503: Current learning rate: 0.00897\n",
      "2024-01-11 04:44:14.891986: train_loss -0.9198\n",
      "2024-01-11 04:44:14.899991: val_loss -0.837\n",
      "2024-01-11 04:44:14.907988: Pseudo dice [0.892, 0.9475, 0.9357]\n",
      "2024-01-11 04:44:14.913982: Epoch time: 39.89 s\n",
      "2024-01-11 04:44:16.427275: \n",
      "2024-01-11 04:44:16.436350: Epoch 115\n",
      "2024-01-11 04:44:16.440337: Current learning rate: 0.00896\n",
      "2024-01-11 04:44:56.581933: train_loss -0.9192\n",
      "2024-01-11 04:44:56.588941: val_loss -0.8377\n",
      "2024-01-11 04:44:56.595941: Pseudo dice [0.8926, 0.9475, 0.9349]\n",
      "2024-01-11 04:44:56.600942: Epoch time: 40.15 s\n",
      "2024-01-11 04:44:58.071332: \n",
      "2024-01-11 04:44:58.079898: Epoch 116\n",
      "2024-01-11 04:44:58.083958: Current learning rate: 0.00895\n",
      "2024-01-11 04:45:38.286165: train_loss -0.9193\n",
      "2024-01-11 04:45:38.295168: val_loss -0.838\n",
      "2024-01-11 04:45:38.305189: Pseudo dice [0.8934, 0.9475, 0.9366]\n",
      "2024-01-11 04:45:38.331709: Epoch time: 40.22 s\n",
      "2024-01-11 04:45:39.886444: \n",
      "2024-01-11 04:45:39.891026: Epoch 117\n",
      "2024-01-11 04:45:39.897050: Current learning rate: 0.00894\n",
      "2024-01-11 04:46:19.838673: train_loss -0.9203\n",
      "2024-01-11 04:46:19.850195: val_loss -0.8333\n",
      "2024-01-11 04:46:19.855190: Pseudo dice [0.8915, 0.9468, 0.9346]\n",
      "2024-01-11 04:46:19.862904: Epoch time: 39.95 s\n",
      "2024-01-11 04:46:21.252142: \n",
      "2024-01-11 04:46:21.258890: Epoch 118\n",
      "2024-01-11 04:46:21.263548: Current learning rate: 0.00893\n",
      "2024-01-11 04:47:01.171034: train_loss -0.9194\n",
      "2024-01-11 04:47:01.177033: val_loss -0.8439\n",
      "2024-01-11 04:47:01.182036: Pseudo dice [0.8954, 0.9495, 0.9373]\n",
      "2024-01-11 04:47:01.187039: Epoch time: 39.92 s\n",
      "2024-01-11 04:47:02.671170: \n",
      "2024-01-11 04:47:02.676259: Epoch 119\n",
      "2024-01-11 04:47:02.682261: Current learning rate: 0.00892\n",
      "2024-01-11 04:47:42.612772: train_loss -0.9189\n",
      "2024-01-11 04:47:42.618780: val_loss -0.8419\n",
      "2024-01-11 04:47:42.624764: Pseudo dice [0.8971, 0.9485, 0.9376]\n",
      "2024-01-11 04:47:42.631766: Epoch time: 39.94 s\n",
      "2024-01-11 04:47:44.063828: \n",
      "2024-01-11 04:47:44.071137: Epoch 120\n",
      "2024-01-11 04:47:44.075193: Current learning rate: 0.00891\n",
      "2024-01-11 04:48:24.077566: train_loss -0.9205\n",
      "2024-01-11 04:48:24.086573: val_loss -0.8394\n",
      "2024-01-11 04:48:24.095572: Pseudo dice [0.8926, 0.9491, 0.9367]\n",
      "2024-01-11 04:48:24.119079: Epoch time: 40.02 s\n",
      "2024-01-11 04:48:25.511075: \n",
      "2024-01-11 04:48:25.518162: Epoch 121\n",
      "2024-01-11 04:48:25.522666: Current learning rate: 0.0089\n",
      "2024-01-11 04:49:05.686305: train_loss -0.9203\n",
      "2024-01-11 04:49:05.694304: val_loss -0.8446\n",
      "2024-01-11 04:49:05.703306: Pseudo dice [0.8936, 0.9498, 0.9376]\n",
      "2024-01-11 04:49:05.709311: Epoch time: 40.18 s\n",
      "2024-01-11 04:49:07.335199: \n",
      "2024-01-11 04:49:07.342288: Epoch 122\n",
      "2024-01-11 04:49:07.347198: Current learning rate: 0.00889\n",
      "2024-01-11 04:49:47.484438: train_loss -0.92\n",
      "2024-01-11 04:49:47.520358: val_loss -0.8357\n",
      "2024-01-11 04:49:47.528359: Pseudo dice [0.8885, 0.9472, 0.9353]\n",
      "2024-01-11 04:49:47.534364: Epoch time: 40.15 s\n",
      "2024-01-11 04:49:48.834489: \n",
      "2024-01-11 04:49:48.841152: Epoch 123\n",
      "2024-01-11 04:49:48.848850: Current learning rate: 0.00889\n",
      "2024-01-11 04:50:28.864311: train_loss -0.9205\n",
      "2024-01-11 04:50:28.872353: val_loss -0.8468\n",
      "2024-01-11 04:50:28.877366: Pseudo dice [0.8995, 0.9494, 0.938]\n",
      "2024-01-11 04:50:28.882364: Epoch time: 40.03 s\n",
      "2024-01-11 04:50:30.320336: \n",
      "2024-01-11 04:50:30.330354: Epoch 124\n",
      "2024-01-11 04:50:30.334341: Current learning rate: 0.00888\n",
      "2024-01-11 04:51:10.228971: train_loss -0.9202\n",
      "2024-01-11 04:51:10.234974: val_loss -0.8339\n",
      "2024-01-11 04:51:10.240972: Pseudo dice [0.889, 0.9462, 0.9346]\n",
      "2024-01-11 04:51:10.275612: Epoch time: 39.91 s\n",
      "2024-01-11 04:51:11.650600: \n",
      "2024-01-11 04:51:11.659603: Epoch 125\n",
      "2024-01-11 04:51:11.663673: Current learning rate: 0.00887\n",
      "2024-01-11 04:51:51.583978: train_loss -0.921\n",
      "2024-01-11 04:51:51.615494: val_loss -0.8388\n",
      "2024-01-11 04:51:51.622495: Pseudo dice [0.8899, 0.9477, 0.9361]\n",
      "2024-01-11 04:51:51.629495: Epoch time: 39.93 s\n",
      "2024-01-11 04:51:53.001066: \n",
      "2024-01-11 04:51:53.007507: Epoch 126\n",
      "2024-01-11 04:51:53.014139: Current learning rate: 0.00886\n",
      "2024-01-11 04:52:33.374850: train_loss -0.9212\n",
      "2024-01-11 04:52:33.383849: val_loss -0.8403\n",
      "2024-01-11 04:52:33.389850: Pseudo dice [0.8935, 0.9485, 0.9372]\n",
      "2024-01-11 04:52:33.416527: Epoch time: 40.37 s\n",
      "2024-01-11 04:52:34.667653: \n",
      "2024-01-11 04:52:34.678369: Epoch 127\n",
      "2024-01-11 04:52:34.682416: Current learning rate: 0.00885\n",
      "2024-01-11 04:53:14.723253: train_loss -0.9212\n",
      "2024-01-11 04:53:14.734771: val_loss -0.8398\n",
      "2024-01-11 04:53:14.742767: Pseudo dice [0.8885, 0.9492, 0.9378]\n",
      "2024-01-11 04:53:14.747767: Epoch time: 40.06 s\n",
      "2024-01-11 04:53:16.278163: \n",
      "2024-01-11 04:53:16.284162: Epoch 128\n",
      "2024-01-11 04:53:16.288164: Current learning rate: 0.00884\n",
      "2024-01-11 04:53:56.302499: train_loss -0.9216\n",
      "2024-01-11 04:53:56.309501: val_loss -0.8426\n",
      "2024-01-11 04:53:56.317499: Pseudo dice [0.8912, 0.9494, 0.9382]\n",
      "2024-01-11 04:53:56.322499: Epoch time: 40.03 s\n",
      "2024-01-11 04:53:57.672304: \n",
      "2024-01-11 04:53:57.684727: Epoch 129\n",
      "2024-01-11 04:53:57.688719: Current learning rate: 0.00883\n",
      "2024-01-11 04:54:37.676106: train_loss -0.9215\n",
      "2024-01-11 04:54:37.684107: val_loss -0.8415\n",
      "2024-01-11 04:54:37.690621: Pseudo dice [0.8918, 0.9483, 0.9372]\n",
      "2024-01-11 04:54:37.698619: Epoch time: 40.01 s\n",
      "2024-01-11 04:54:39.167792: \n",
      "2024-01-11 04:54:39.176885: Epoch 130\n",
      "2024-01-11 04:54:39.181880: Current learning rate: 0.00882\n",
      "2024-01-11 04:55:19.189835: train_loss -0.9219\n",
      "2024-01-11 04:55:19.201916: val_loss -0.8409\n",
      "2024-01-11 04:55:19.209917: Pseudo dice [0.8959, 0.9489, 0.9374]\n",
      "2024-01-11 04:55:19.216924: Epoch time: 40.02 s\n",
      "2024-01-11 04:55:20.632830: \n",
      "2024-01-11 04:55:20.639825: Epoch 131\n",
      "2024-01-11 04:55:20.646346: Current learning rate: 0.00881\n",
      "2024-01-11 04:56:00.785394: train_loss -0.9227\n",
      "2024-01-11 04:56:00.793397: val_loss -0.8417\n",
      "2024-01-11 04:56:00.800394: Pseudo dice [0.8943, 0.9493, 0.9371]\n",
      "2024-01-11 04:56:00.807394: Epoch time: 40.15 s\n",
      "2024-01-11 04:56:02.220030: \n",
      "2024-01-11 04:56:02.226101: Epoch 132\n",
      "2024-01-11 04:56:02.231173: Current learning rate: 0.0088\n",
      "2024-01-11 04:56:42.349799: train_loss -0.9221\n",
      "2024-01-11 04:56:42.377309: val_loss -0.8429\n",
      "2024-01-11 04:56:42.384308: Pseudo dice [0.8945, 0.9494, 0.9385]\n",
      "2024-01-11 04:56:42.393308: Epoch time: 40.13 s\n",
      "2024-01-11 04:56:43.834445: \n",
      "2024-01-11 04:56:43.840230: Epoch 133\n",
      "2024-01-11 04:56:43.848381: Current learning rate: 0.00879\n",
      "2024-01-11 04:57:23.846039: train_loss -0.9226\n",
      "2024-01-11 04:57:23.856040: val_loss -0.8393\n",
      "2024-01-11 04:57:23.864054: Pseudo dice [0.896, 0.9482, 0.9371]\n",
      "2024-01-11 04:57:23.872551: Epoch time: 40.01 s\n",
      "2024-01-11 04:57:25.354289: \n",
      "2024-01-11 04:57:25.361431: Epoch 134\n",
      "2024-01-11 04:57:25.366416: Current learning rate: 0.00879\n",
      "2024-01-11 04:58:05.188151: train_loss -0.9221\n",
      "2024-01-11 04:58:05.196153: val_loss -0.8434\n",
      "2024-01-11 04:58:05.203149: Pseudo dice [0.8967, 0.9495, 0.9381]\n",
      "2024-01-11 04:58:05.223149: Epoch time: 39.83 s\n",
      "2024-01-11 04:58:06.654310: \n",
      "2024-01-11 04:58:06.660398: Epoch 135\n",
      "2024-01-11 04:58:06.665382: Current learning rate: 0.00878\n",
      "2024-01-11 04:58:46.788129: train_loss -0.9226\n",
      "2024-01-11 04:58:46.795128: val_loss -0.8395\n",
      "2024-01-11 04:58:46.801639: Pseudo dice [0.8886, 0.9493, 0.9381]\n",
      "2024-01-11 04:58:46.806638: Epoch time: 40.14 s\n",
      "2024-01-11 04:58:48.330310: \n",
      "2024-01-11 04:58:48.339310: Epoch 136\n",
      "2024-01-11 04:58:48.346388: Current learning rate: 0.00877\n",
      "2024-01-11 04:59:28.622772: train_loss -0.9227\n",
      "2024-01-11 04:59:28.649771: val_loss -0.8385\n",
      "2024-01-11 04:59:28.655770: Pseudo dice [0.8901, 0.9471, 0.9358]\n",
      "2024-01-11 04:59:28.661772: Epoch time: 40.29 s\n",
      "2024-01-11 04:59:30.105807: \n",
      "2024-01-11 04:59:30.111887: Epoch 137\n",
      "2024-01-11 04:59:30.119413: Current learning rate: 0.00876\n",
      "2024-01-11 05:00:10.043053: train_loss -0.9231\n",
      "2024-01-11 05:00:10.049053: val_loss -0.8417\n",
      "2024-01-11 05:00:10.054053: Pseudo dice [0.8927, 0.9486, 0.9373]\n",
      "2024-01-11 05:00:10.059052: Epoch time: 39.94 s\n",
      "2024-01-11 05:00:11.535944: \n",
      "2024-01-11 05:00:11.541378: Epoch 138\n",
      "2024-01-11 05:00:11.551375: Current learning rate: 0.00875\n",
      "2024-01-11 05:00:51.615613: train_loss -0.9227\n",
      "2024-01-11 05:00:51.622623: val_loss -0.8396\n",
      "2024-01-11 05:00:51.630618: Pseudo dice [0.895, 0.9481, 0.9372]\n",
      "2024-01-11 05:00:51.635618: Epoch time: 40.08 s\n",
      "2024-01-11 05:00:53.077524: \n",
      "2024-01-11 05:00:53.083316: Epoch 139\n",
      "2024-01-11 05:00:53.091971: Current learning rate: 0.00874\n",
      "2024-01-11 05:01:33.150165: train_loss -0.9231\n",
      "2024-01-11 05:01:33.157159: val_loss -0.8399\n",
      "2024-01-11 05:01:33.165673: Pseudo dice [0.8915, 0.9486, 0.9374]\n",
      "2024-01-11 05:01:33.172672: Epoch time: 40.07 s\n",
      "2024-01-11 05:01:34.652800: \n",
      "2024-01-11 05:01:34.658904: Epoch 140\n",
      "2024-01-11 05:01:34.663060: Current learning rate: 0.00873\n",
      "2024-01-11 05:02:14.759296: train_loss -0.9234\n",
      "2024-01-11 05:02:14.767303: val_loss -0.841\n",
      "2024-01-11 05:02:14.793811: Pseudo dice [0.8947, 0.9489, 0.9368]\n",
      "2024-01-11 05:02:14.801813: Epoch time: 40.11 s\n",
      "2024-01-11 05:02:16.274107: \n",
      "2024-01-11 05:02:16.280377: Epoch 141\n",
      "2024-01-11 05:02:16.284434: Current learning rate: 0.00872\n",
      "2024-01-11 05:02:56.210642: train_loss -0.9236\n",
      "2024-01-11 05:02:56.219646: val_loss -0.8388\n",
      "2024-01-11 05:02:56.251641: Pseudo dice [0.8888, 0.949, 0.9373]\n",
      "2024-01-11 05:02:56.259643: Epoch time: 39.94 s\n",
      "2024-01-11 05:02:57.673388: \n",
      "2024-01-11 05:02:57.682710: Epoch 142\n",
      "2024-01-11 05:02:57.688781: Current learning rate: 0.00871\n",
      "2024-01-11 05:03:37.891884: train_loss -0.924\n",
      "2024-01-11 05:03:37.924909: val_loss -0.8327\n",
      "2024-01-11 05:03:37.931908: Pseudo dice [0.8892, 0.9469, 0.9346]\n",
      "2024-01-11 05:03:37.937908: Epoch time: 40.22 s\n",
      "2024-01-11 05:03:39.374088: \n",
      "2024-01-11 05:03:39.379148: Epoch 143\n",
      "2024-01-11 05:03:39.384129: Current learning rate: 0.0087\n",
      "2024-01-11 05:04:19.320571: train_loss -0.9237\n",
      "2024-01-11 05:04:19.350084: val_loss -0.841\n",
      "2024-01-11 05:04:19.358085: Pseudo dice [0.8977, 0.948, 0.9373]\n",
      "2024-01-11 05:04:19.363086: Epoch time: 39.95 s\n",
      "2024-01-11 05:04:20.715009: \n",
      "2024-01-11 05:04:20.722718: Epoch 144\n",
      "2024-01-11 05:04:20.728901: Current learning rate: 0.00869\n",
      "2024-01-11 05:05:00.584075: train_loss -0.9241\n",
      "2024-01-11 05:05:00.594076: val_loss -0.8378\n",
      "2024-01-11 05:05:00.600083: Pseudo dice [0.8925, 0.9481, 0.9363]\n",
      "2024-01-11 05:05:00.608074: Epoch time: 39.87 s\n",
      "2024-01-11 05:05:02.057124: \n",
      "2024-01-11 05:05:02.061124: Epoch 145\n",
      "2024-01-11 05:05:02.066124: Current learning rate: 0.00868\n",
      "2024-01-11 05:05:42.214046: train_loss -0.9239\n",
      "2024-01-11 05:05:42.222045: val_loss -0.8415\n",
      "2024-01-11 05:05:42.228045: Pseudo dice [0.894, 0.9482, 0.9367]\n",
      "2024-01-11 05:05:42.233046: Epoch time: 40.16 s\n",
      "2024-01-11 05:05:43.888993: \n",
      "2024-01-11 05:05:43.894893: Epoch 146\n",
      "2024-01-11 05:05:43.900959: Current learning rate: 0.00868\n",
      "2024-01-11 05:06:24.034900: train_loss -0.9244\n",
      "2024-01-11 05:06:24.040902: val_loss -0.8367\n",
      "2024-01-11 05:06:24.045901: Pseudo dice [0.8933, 0.948, 0.936]\n",
      "2024-01-11 05:06:24.051901: Epoch time: 40.15 s\n",
      "2024-01-11 05:06:25.442085: \n",
      "2024-01-11 05:06:25.447906: Epoch 147\n",
      "2024-01-11 05:06:25.451965: Current learning rate: 0.00867\n",
      "2024-01-11 05:07:05.446452: train_loss -0.9242\n",
      "2024-01-11 05:07:05.453451: val_loss -0.8388\n",
      "2024-01-11 05:07:05.460453: Pseudo dice [0.891, 0.9486, 0.9377]\n",
      "2024-01-11 05:07:05.465451: Epoch time: 40.01 s\n",
      "2024-01-11 05:07:06.943550: \n",
      "2024-01-11 05:07:06.948610: Epoch 148\n",
      "2024-01-11 05:07:06.953450: Current learning rate: 0.00866\n",
      "2024-01-11 05:07:46.977642: train_loss -0.9245\n",
      "2024-01-11 05:07:47.006648: val_loss -0.8422\n",
      "2024-01-11 05:07:47.014650: Pseudo dice [0.8943, 0.9483, 0.9377]\n",
      "2024-01-11 05:07:47.022169: Epoch time: 40.04 s\n",
      "2024-01-11 05:07:48.443696: \n",
      "2024-01-11 05:07:48.449665: Epoch 149\n",
      "2024-01-11 05:07:48.453668: Current learning rate: 0.00865\n",
      "2024-01-11 05:08:28.381492: train_loss -0.925\n",
      "2024-01-11 05:08:28.387507: val_loss -0.8382\n",
      "2024-01-11 05:08:28.395504: Pseudo dice [0.8898, 0.948, 0.9379]\n",
      "2024-01-11 05:08:28.403504: Epoch time: 39.94 s\n",
      "2024-01-11 05:08:30.104135: \n",
      "2024-01-11 05:08:30.110062: Epoch 150\n",
      "2024-01-11 05:08:30.114069: Current learning rate: 0.00864\n",
      "2024-01-11 05:09:10.254010: train_loss -0.9247\n",
      "2024-01-11 05:09:10.263015: val_loss -0.8363\n",
      "2024-01-11 05:09:10.272020: Pseudo dice [0.8911, 0.9478, 0.936]\n",
      "2024-01-11 05:09:10.279016: Epoch time: 40.15 s\n",
      "2024-01-11 05:09:11.814368: \n",
      "2024-01-11 05:09:11.826126: Epoch 151\n",
      "2024-01-11 05:09:11.833219: Current learning rate: 0.00863\n",
      "2024-01-11 05:09:51.975945: train_loss -0.9251\n",
      "2024-01-11 05:09:51.987464: val_loss -0.8331\n",
      "2024-01-11 05:09:51.997466: Pseudo dice [0.8899, 0.947, 0.9353]\n",
      "2024-01-11 05:09:52.005468: Epoch time: 40.16 s\n",
      "2024-01-11 05:09:53.440838: \n",
      "2024-01-11 05:09:53.448836: Epoch 152\n",
      "2024-01-11 05:09:53.453591: Current learning rate: 0.00862\n",
      "2024-01-11 05:10:33.365860: train_loss -0.9254\n",
      "2024-01-11 05:10:33.377866: val_loss -0.8427\n",
      "2024-01-11 05:10:33.386372: Pseudo dice [0.8932, 0.9494, 0.9384]\n",
      "2024-01-11 05:10:33.393378: Epoch time: 39.93 s\n",
      "2024-01-11 05:10:34.895231: \n",
      "2024-01-11 05:10:34.901177: Epoch 153\n",
      "2024-01-11 05:10:34.910255: Current learning rate: 0.00861\n",
      "2024-01-11 05:11:15.043077: train_loss -0.924\n",
      "2024-01-11 05:11:15.053077: val_loss -0.8388\n",
      "2024-01-11 05:11:15.061591: Pseudo dice [0.891, 0.9487, 0.9367]\n",
      "2024-01-11 05:11:15.068592: Epoch time: 40.15 s\n",
      "2024-01-11 05:11:16.857689: \n",
      "2024-01-11 05:11:16.864084: Epoch 154\n",
      "2024-01-11 05:11:16.874163: Current learning rate: 0.0086\n",
      "2024-01-11 05:11:57.033658: train_loss -0.9254\n",
      "2024-01-11 05:11:57.063214: val_loss -0.8384\n",
      "2024-01-11 05:11:57.071206: Pseudo dice [0.8957, 0.9479, 0.9363]\n",
      "2024-01-11 05:11:57.078208: Epoch time: 40.18 s\n",
      "2024-01-11 05:11:58.602344: \n",
      "2024-01-11 05:11:58.610366: Epoch 155\n",
      "2024-01-11 05:11:58.615422: Current learning rate: 0.00859\n",
      "2024-01-11 05:12:38.882311: train_loss -0.9248\n",
      "2024-01-11 05:12:38.888311: val_loss -0.8398\n",
      "2024-01-11 05:12:38.895311: Pseudo dice [0.8938, 0.9485, 0.9367]\n",
      "2024-01-11 05:12:38.902311: Epoch time: 40.28 s\n",
      "2024-01-11 05:12:40.348496: \n",
      "2024-01-11 05:12:40.360709: Epoch 156\n",
      "2024-01-11 05:12:40.364755: Current learning rate: 0.00858\n",
      "2024-01-11 05:13:20.328446: train_loss -0.9255\n",
      "2024-01-11 05:13:20.336443: val_loss -0.8456\n",
      "2024-01-11 05:13:20.344444: Pseudo dice [0.9001, 0.9497, 0.9391]\n",
      "2024-01-11 05:13:20.375451: Epoch time: 39.98 s\n",
      "2024-01-11 05:13:21.896015: \n",
      "2024-01-11 05:13:21.902015: Epoch 157\n",
      "2024-01-11 05:13:21.907017: Current learning rate: 0.00858\n",
      "2024-01-11 05:14:01.897114: train_loss -0.9252\n",
      "2024-01-11 05:14:01.904115: val_loss -0.8381\n",
      "2024-01-11 05:14:01.911631: Pseudo dice [0.8947, 0.9487, 0.9381]\n",
      "2024-01-11 05:14:01.918631: Epoch time: 40.0 s\n",
      "2024-01-11 05:14:03.267907: \n",
      "2024-01-11 05:14:03.273611: Epoch 158\n",
      "2024-01-11 05:14:03.280872: Current learning rate: 0.00857\n",
      "2024-01-11 05:14:43.334788: train_loss -0.9255\n",
      "2024-01-11 05:14:43.345792: val_loss -0.8411\n",
      "2024-01-11 05:14:43.352788: Pseudo dice [0.8978, 0.9481, 0.937]\n",
      "2024-01-11 05:14:43.382787: Epoch time: 40.07 s\n",
      "2024-01-11 05:14:44.833638: \n",
      "2024-01-11 05:14:44.838922: Epoch 159\n",
      "2024-01-11 05:14:44.845004: Current learning rate: 0.00856\n",
      "2024-01-11 05:15:24.940751: train_loss -0.9258\n",
      "2024-01-11 05:15:24.946266: val_loss -0.839\n",
      "2024-01-11 05:15:24.981889: Pseudo dice [0.8934, 0.9484, 0.9367]\n",
      "2024-01-11 05:15:24.990887: Epoch time: 40.11 s\n",
      "2024-01-11 05:15:26.698503: \n",
      "2024-01-11 05:15:26.703814: Epoch 160\n",
      "2024-01-11 05:15:26.707894: Current learning rate: 0.00855\n",
      "2024-01-11 05:16:06.761758: train_loss -0.9261\n",
      "2024-01-11 05:16:06.771755: val_loss -0.8418\n",
      "2024-01-11 05:16:06.779762: Pseudo dice [0.8968, 0.9494, 0.9383]\n",
      "2024-01-11 05:16:06.786772: Epoch time: 40.06 s\n",
      "2024-01-11 05:16:08.222584: \n",
      "2024-01-11 05:16:08.228585: Epoch 161\n",
      "2024-01-11 05:16:08.233966: Current learning rate: 0.00854\n",
      "2024-01-11 05:16:48.194618: train_loss -0.9257\n",
      "2024-01-11 05:16:48.222134: val_loss -0.8489\n",
      "2024-01-11 05:16:48.230127: Pseudo dice [0.9004, 0.9512, 0.9399]\n",
      "2024-01-11 05:16:48.237126: Epoch time: 39.97 s\n",
      "2024-01-11 05:16:48.243126: Yayy! New best EMA pseudo Dice: 0.927\n",
      "2024-01-11 05:16:49.823689: \n",
      "2024-01-11 05:16:49.828652: Epoch 162\n",
      "2024-01-11 05:16:49.833611: Current learning rate: 0.00853\n",
      "2024-01-11 05:17:29.768565: train_loss -0.9266\n",
      "2024-01-11 05:17:29.776566: val_loss -0.8446\n",
      "2024-01-11 05:17:29.782565: Pseudo dice [0.8964, 0.9503, 0.9381]\n",
      "2024-01-11 05:17:29.789568: Epoch time: 39.95 s\n",
      "2024-01-11 05:17:29.794572: Yayy! New best EMA pseudo Dice: 0.9272\n",
      "2024-01-11 05:17:31.491097: \n",
      "2024-01-11 05:17:31.496789: Epoch 163\n",
      "2024-01-11 05:17:31.500865: Current learning rate: 0.00852\n",
      "2024-01-11 05:18:11.596991: train_loss -0.9257\n",
      "2024-01-11 05:18:11.603991: val_loss -0.8403\n",
      "2024-01-11 05:18:11.635993: Pseudo dice [0.8919, 0.9496, 0.9386]\n",
      "2024-01-11 05:18:11.644508: Epoch time: 40.11 s\n",
      "2024-01-11 05:18:13.196265: \n",
      "2024-01-11 05:18:13.203265: Epoch 164\n",
      "2024-01-11 05:18:13.209265: Current learning rate: 0.00851\n",
      "2024-01-11 05:18:53.303897: train_loss -0.9267\n",
      "2024-01-11 05:18:53.312901: val_loss -0.8361\n",
      "2024-01-11 05:18:53.321895: Pseudo dice [0.8865, 0.948, 0.9365]\n",
      "2024-01-11 05:18:53.329895: Epoch time: 40.11 s\n",
      "2024-01-11 05:18:54.909732: \n",
      "2024-01-11 05:18:54.921002: Epoch 165\n",
      "2024-01-11 05:18:54.929073: Current learning rate: 0.0085\n",
      "2024-01-11 05:19:35.028645: train_loss -0.9265\n",
      "2024-01-11 05:19:35.035646: val_loss -0.84\n",
      "2024-01-11 05:19:35.041646: Pseudo dice [0.8926, 0.9481, 0.9373]\n",
      "2024-01-11 05:19:35.046646: Epoch time: 40.12 s\n",
      "2024-01-11 05:19:36.479111: \n",
      "2024-01-11 05:19:36.485261: Epoch 166\n",
      "2024-01-11 05:19:36.491315: Current learning rate: 0.00849\n",
      "2024-01-11 05:20:16.454754: train_loss -0.9271\n",
      "2024-01-11 05:20:16.461754: val_loss -0.8397\n",
      "2024-01-11 05:20:16.468755: Pseudo dice [0.8902, 0.9488, 0.9375]\n",
      "2024-01-11 05:20:16.475768: Epoch time: 39.98 s\n",
      "2024-01-11 05:20:17.970439: \n",
      "2024-01-11 05:20:17.981290: Epoch 167\n",
      "2024-01-11 05:20:17.985358: Current learning rate: 0.00848\n",
      "2024-01-11 05:20:57.870549: train_loss -0.9264\n",
      "2024-01-11 05:20:57.877557: val_loss -0.8436\n",
      "2024-01-11 05:20:57.885549: Pseudo dice [0.8942, 0.9499, 0.9396]\n",
      "2024-01-11 05:20:57.891562: Epoch time: 39.9 s\n",
      "2024-01-11 05:20:59.445382: \n",
      "2024-01-11 05:20:59.450953: Epoch 168\n",
      "2024-01-11 05:20:59.458023: Current learning rate: 0.00847\n",
      "2024-01-11 05:21:39.435311: train_loss -0.927\n",
      "2024-01-11 05:21:39.443315: val_loss -0.8387\n",
      "2024-01-11 05:21:39.449316: Pseudo dice [0.894, 0.9479, 0.9354]\n",
      "2024-01-11 05:21:39.456311: Epoch time: 39.99 s\n",
      "2024-01-11 05:21:40.904642: \n",
      "2024-01-11 05:21:40.910702: Epoch 169\n",
      "2024-01-11 05:21:40.914773: Current learning rate: 0.00847\n",
      "2024-01-11 05:22:21.056774: train_loss -0.9268\n",
      "2024-01-11 05:22:21.065295: val_loss -0.8329\n",
      "2024-01-11 05:22:21.072303: Pseudo dice [0.8876, 0.9479, 0.9362]\n",
      "2024-01-11 05:22:21.080300: Epoch time: 40.15 s\n",
      "2024-01-11 05:22:22.658146: \n",
      "2024-01-11 05:22:22.664122: Epoch 170\n",
      "2024-01-11 05:22:22.671863: Current learning rate: 0.00846\n",
      "2024-01-11 05:23:02.692117: train_loss -0.9276\n",
      "2024-01-11 05:23:02.699119: val_loss -0.8377\n",
      "2024-01-11 05:23:02.706119: Pseudo dice [0.8893, 0.9485, 0.9366]\n",
      "2024-01-11 05:23:02.735118: Epoch time: 40.03 s\n",
      "2024-01-11 05:23:04.208949: \n",
      "2024-01-11 05:23:04.216021: Epoch 171\n",
      "2024-01-11 05:23:04.219947: Current learning rate: 0.00845\n",
      "2024-01-11 05:23:44.136765: train_loss -0.9275\n",
      "2024-01-11 05:23:44.144756: val_loss -0.8416\n",
      "2024-01-11 05:23:44.151757: Pseudo dice [0.8907, 0.949, 0.938]\n",
      "2024-01-11 05:23:44.157765: Epoch time: 39.93 s\n",
      "2024-01-11 05:23:45.618713: \n",
      "2024-01-11 05:23:45.626286: Epoch 172\n",
      "2024-01-11 05:23:45.637285: Current learning rate: 0.00844\n",
      "2024-01-11 05:24:25.745404: train_loss -0.9272\n",
      "2024-01-11 05:24:25.753404: val_loss -0.8358\n",
      "2024-01-11 05:24:25.759910: Pseudo dice [0.8863, 0.9488, 0.9371]\n",
      "2024-01-11 05:24:25.765916: Epoch time: 40.13 s\n",
      "2024-01-11 05:24:27.246575: \n",
      "2024-01-11 05:24:27.252970: Epoch 173\n",
      "2024-01-11 05:24:27.258998: Current learning rate: 0.00843\n",
      "2024-01-11 05:25:07.331437: train_loss -0.9282\n",
      "2024-01-11 05:25:07.343957: val_loss -0.844\n",
      "2024-01-11 05:25:07.350955: Pseudo dice [0.8963, 0.9503, 0.9387]\n",
      "2024-01-11 05:25:07.384471: Epoch time: 40.09 s\n",
      "2024-01-11 05:25:08.914371: \n",
      "2024-01-11 05:25:08.923834: Epoch 174\n",
      "2024-01-11 05:25:08.931827: Current learning rate: 0.00842\n",
      "2024-01-11 05:25:48.946371: train_loss -0.9278\n",
      "2024-01-11 05:25:48.957372: val_loss -0.8406\n",
      "2024-01-11 05:25:48.989897: Pseudo dice [0.8944, 0.9486, 0.9367]\n",
      "2024-01-11 05:25:48.996897: Epoch time: 40.03 s\n",
      "2024-01-11 05:25:50.454876: \n",
      "2024-01-11 05:25:50.460628: Epoch 175\n",
      "2024-01-11 05:25:50.468771: Current learning rate: 0.00841\n",
      "2024-01-11 05:26:30.691381: train_loss -0.9279\n",
      "2024-01-11 05:26:30.697379: val_loss -0.8368\n",
      "2024-01-11 05:26:30.703912: Pseudo dice [0.895, 0.9482, 0.9369]\n",
      "2024-01-11 05:26:30.733453: Epoch time: 40.24 s\n",
      "2024-01-11 05:26:32.169114: \n",
      "2024-01-11 05:26:32.176185: Epoch 176\n",
      "2024-01-11 05:26:32.184179: Current learning rate: 0.0084\n",
      "2024-01-11 05:27:12.275589: train_loss -0.9286\n",
      "2024-01-11 05:27:12.283586: val_loss -0.8405\n",
      "2024-01-11 05:27:12.290589: Pseudo dice [0.8907, 0.9483, 0.9369]\n",
      "2024-01-11 05:27:12.320112: Epoch time: 40.11 s\n",
      "2024-01-11 05:27:13.696884: \n",
      "2024-01-11 05:27:13.704681: Epoch 177\n",
      "2024-01-11 05:27:13.708724: Current learning rate: 0.00839\n",
      "2024-01-11 05:27:53.782029: train_loss -0.9274\n",
      "2024-01-11 05:27:53.792031: val_loss -0.8387\n",
      "2024-01-11 05:27:53.799029: Pseudo dice [0.8931, 0.9482, 0.9368]\n",
      "2024-01-11 05:27:53.804029: Epoch time: 40.09 s\n",
      "2024-01-11 05:27:55.220496: \n",
      "2024-01-11 05:27:55.231496: Epoch 178\n",
      "2024-01-11 05:27:55.237498: Current learning rate: 0.00838\n",
      "2024-01-11 05:28:35.130426: train_loss -0.9277\n",
      "2024-01-11 05:28:35.137428: val_loss -0.8404\n",
      "2024-01-11 05:28:35.145427: Pseudo dice [0.8948, 0.9478, 0.9376]\n",
      "2024-01-11 05:28:35.152430: Epoch time: 39.91 s\n",
      "2024-01-11 05:28:36.846470: \n",
      "2024-01-11 05:28:36.851663: Epoch 179\n",
      "2024-01-11 05:28:36.858740: Current learning rate: 0.00837\n",
      "2024-01-11 05:29:17.111034: train_loss -0.9287\n",
      "2024-01-11 05:29:17.119045: val_loss -0.8391\n",
      "2024-01-11 05:29:17.126032: Pseudo dice [0.8915, 0.9481, 0.937]\n",
      "2024-01-11 05:29:17.132033: Epoch time: 40.27 s\n",
      "2024-01-11 05:29:18.552904: \n",
      "2024-01-11 05:29:18.557983: Epoch 180\n",
      "2024-01-11 05:29:18.566058: Current learning rate: 0.00836\n",
      "2024-01-11 05:29:58.632648: train_loss -0.9281\n",
      "2024-01-11 05:29:58.662156: val_loss -0.8345\n",
      "2024-01-11 05:29:58.669156: Pseudo dice [0.8931, 0.9478, 0.9353]\n",
      "2024-01-11 05:29:58.674159: Epoch time: 40.08 s\n",
      "2024-01-11 05:30:00.199856: \n",
      "2024-01-11 05:30:00.206925: Epoch 181\n",
      "2024-01-11 05:30:00.210917: Current learning rate: 0.00836\n",
      "2024-01-11 05:30:40.310098: train_loss -0.9287\n",
      "2024-01-11 05:30:40.352127: val_loss -0.8362\n",
      "2024-01-11 05:30:40.363127: Pseudo dice [0.8922, 0.9476, 0.9358]\n",
      "2024-01-11 05:30:40.372129: Epoch time: 40.11 s\n",
      "2024-01-11 05:30:41.735358: \n",
      "2024-01-11 05:30:41.744197: Epoch 182\n",
      "2024-01-11 05:30:41.751214: Current learning rate: 0.00835\n",
      "2024-01-11 05:31:21.886447: train_loss -0.9292\n",
      "2024-01-11 05:31:21.894449: val_loss -0.8372\n",
      "2024-01-11 05:31:21.902447: Pseudo dice [0.8932, 0.9477, 0.9355]\n",
      "2024-01-11 05:31:21.908450: Epoch time: 40.15 s\n",
      "2024-01-11 05:31:23.430734: \n",
      "2024-01-11 05:31:23.438325: Epoch 183\n",
      "2024-01-11 05:31:23.446355: Current learning rate: 0.00834\n",
      "2024-01-11 05:32:03.516393: train_loss -0.9289\n",
      "2024-01-11 05:32:03.549399: val_loss -0.8352\n",
      "2024-01-11 05:32:03.557396: Pseudo dice [0.8924, 0.9478, 0.936]\n",
      "2024-01-11 05:32:03.564396: Epoch time: 40.09 s\n",
      "2024-01-11 05:32:05.128929: \n",
      "2024-01-11 05:32:05.138325: Epoch 184\n",
      "2024-01-11 05:32:05.143222: Current learning rate: 0.00833\n",
      "2024-01-11 05:32:45.239067: train_loss -0.9304\n",
      "2024-01-11 05:32:45.246068: val_loss -0.8349\n",
      "2024-01-11 05:32:45.254067: Pseudo dice [0.8927, 0.947, 0.9363]\n",
      "2024-01-11 05:32:45.261068: Epoch time: 40.11 s\n",
      "2024-01-11 05:32:46.681048: \n",
      "2024-01-11 05:32:46.687078: Epoch 185\n",
      "2024-01-11 05:32:46.692108: Current learning rate: 0.00832\n",
      "2024-01-11 05:33:26.767880: train_loss -0.9292\n",
      "2024-01-11 05:33:26.773882: val_loss -0.839\n",
      "2024-01-11 05:33:26.778881: Pseudo dice [0.8909, 0.9487, 0.9373]\n",
      "2024-01-11 05:33:26.783882: Epoch time: 40.09 s\n",
      "2024-01-11 05:33:28.284411: \n",
      "2024-01-11 05:33:28.289411: Epoch 186\n",
      "2024-01-11 05:33:28.293411: Current learning rate: 0.00831\n",
      "2024-01-11 05:34:08.263138: train_loss -0.9295\n",
      "2024-01-11 05:34:08.271147: val_loss -0.8391\n",
      "2024-01-11 05:34:08.299679: Pseudo dice [0.8915, 0.9491, 0.9362]\n",
      "2024-01-11 05:34:08.308679: Epoch time: 39.98 s\n",
      "2024-01-11 05:34:09.780213: \n",
      "2024-01-11 05:34:09.786412: Epoch 187\n",
      "2024-01-11 05:34:09.791070: Current learning rate: 0.0083\n",
      "2024-01-11 05:34:49.806495: train_loss -0.9283\n",
      "2024-01-11 05:34:49.833496: val_loss -0.8425\n",
      "2024-01-11 05:34:49.841497: Pseudo dice [0.8936, 0.9495, 0.9375]\n",
      "2024-01-11 05:34:49.849498: Epoch time: 40.03 s\n",
      "2024-01-11 05:34:51.329321: \n",
      "2024-01-11 05:34:51.335244: Epoch 188\n",
      "2024-01-11 05:34:51.338654: Current learning rate: 0.00829\n",
      "2024-01-11 05:35:31.322545: train_loss -0.9287\n",
      "2024-01-11 05:35:31.330544: val_loss -0.8409\n",
      "2024-01-11 05:35:31.336545: Pseudo dice [0.8945, 0.949, 0.9373]\n",
      "2024-01-11 05:35:31.341546: Epoch time: 39.99 s\n",
      "2024-01-11 05:35:33.000232: \n",
      "2024-01-11 05:35:33.010098: Epoch 189\n",
      "2024-01-11 05:35:33.020707: Current learning rate: 0.00828\n",
      "2024-01-11 05:36:12.890618: train_loss -0.9294\n",
      "2024-01-11 05:36:12.897621: val_loss -0.8374\n",
      "2024-01-11 05:36:12.903618: Pseudo dice [0.8897, 0.9494, 0.9371]\n",
      "2024-01-11 05:36:12.912625: Epoch time: 39.89 s\n",
      "2024-01-11 05:36:14.436969: \n",
      "2024-01-11 05:36:14.444039: Epoch 190\n",
      "2024-01-11 05:36:14.447679: Current learning rate: 0.00827\n",
      "2024-01-11 05:36:54.349294: train_loss -0.93\n",
      "2024-01-11 05:36:54.357300: val_loss -0.8382\n",
      "2024-01-11 05:36:54.363300: Pseudo dice [0.8946, 0.9484, 0.936]\n",
      "2024-01-11 05:36:54.368811: Epoch time: 39.91 s\n",
      "2024-01-11 05:36:55.862842: \n",
      "2024-01-11 05:36:55.867949: Epoch 191\n",
      "2024-01-11 05:36:55.876040: Current learning rate: 0.00826\n",
      "2024-01-11 05:37:35.867555: train_loss -0.93\n",
      "2024-01-11 05:37:35.874553: val_loss -0.8358\n",
      "2024-01-11 05:37:35.880555: Pseudo dice [0.8869, 0.9485, 0.9378]\n",
      "2024-01-11 05:37:35.884563: Epoch time: 40.0 s\n",
      "2024-01-11 05:37:37.358065: \n",
      "2024-01-11 05:37:37.365841: Epoch 192\n",
      "2024-01-11 05:37:37.370851: Current learning rate: 0.00825\n",
      "2024-01-11 05:38:17.345638: train_loss -0.93\n",
      "2024-01-11 05:38:17.354637: val_loss -0.8372\n",
      "2024-01-11 05:38:17.360642: Pseudo dice [0.893, 0.9477, 0.9364]\n",
      "2024-01-11 05:38:17.366637: Epoch time: 39.99 s\n",
      "2024-01-11 05:38:18.804947: \n",
      "2024-01-11 05:38:18.812970: Epoch 193\n",
      "2024-01-11 05:38:18.816952: Current learning rate: 0.00824\n",
      "2024-01-11 05:38:58.861036: train_loss -0.9302\n",
      "2024-01-11 05:38:58.868038: val_loss -0.8413\n",
      "2024-01-11 05:38:58.875038: Pseudo dice [0.8956, 0.9488, 0.9376]\n",
      "2024-01-11 05:38:58.881040: Epoch time: 40.06 s\n",
      "2024-01-11 05:39:00.585472: \n",
      "2024-01-11 05:39:00.591644: Epoch 194\n",
      "2024-01-11 05:39:00.595712: Current learning rate: 0.00824\n",
      "2024-01-11 05:39:40.462707: train_loss -0.9298\n",
      "2024-01-11 05:39:40.470245: val_loss -0.8397\n",
      "2024-01-11 05:39:40.478243: Pseudo dice [0.8934, 0.949, 0.938]\n",
      "2024-01-11 05:39:40.484245: Epoch time: 39.88 s\n",
      "2024-01-11 05:39:41.970700: \n",
      "2024-01-11 05:39:41.976321: Epoch 195\n",
      "2024-01-11 05:39:41.980392: Current learning rate: 0.00823\n",
      "2024-01-11 05:40:21.896890: train_loss -0.93\n",
      "2024-01-11 05:40:21.902890: val_loss -0.8417\n",
      "2024-01-11 05:40:21.908889: Pseudo dice [0.8944, 0.9497, 0.9382]\n",
      "2024-01-11 05:40:21.913888: Epoch time: 39.93 s\n",
      "2024-01-11 05:40:23.459132: \n",
      "2024-01-11 05:40:23.464728: Epoch 196\n",
      "2024-01-11 05:40:23.474790: Current learning rate: 0.00822\n",
      "2024-01-11 05:41:03.410742: train_loss -0.93\n",
      "2024-01-11 05:41:03.420741: val_loss -0.8315\n",
      "2024-01-11 05:41:03.427122: Pseudo dice [0.8887, 0.946, 0.9332]\n",
      "2024-01-11 05:41:03.434125: Epoch time: 39.95 s\n",
      "2024-01-11 05:41:05.132279: \n",
      "2024-01-11 05:41:05.137444: Epoch 197\n",
      "2024-01-11 05:41:05.142171: Current learning rate: 0.00821\n",
      "2024-01-11 05:41:45.165168: train_loss -0.9307\n",
      "2024-01-11 05:41:45.171166: val_loss -0.8376\n",
      "2024-01-11 05:41:45.180165: Pseudo dice [0.8908, 0.9476, 0.9351]\n",
      "2024-01-11 05:41:45.187169: Epoch time: 40.03 s\n",
      "2024-01-11 05:41:46.618393: \n",
      "2024-01-11 05:41:46.624107: Epoch 198\n",
      "2024-01-11 05:41:46.628180: Current learning rate: 0.0082\n",
      "2024-01-11 05:42:26.661644: train_loss -0.9305\n",
      "2024-01-11 05:42:26.669649: val_loss -0.8395\n",
      "2024-01-11 05:42:26.674652: Pseudo dice [0.8935, 0.9486, 0.9362]\n",
      "2024-01-11 05:42:26.681181: Epoch time: 40.04 s\n",
      "2024-01-11 05:42:28.332994: \n",
      "2024-01-11 05:42:28.343061: Epoch 199\n",
      "2024-01-11 05:42:28.349060: Current learning rate: 0.00819\n",
      "2024-01-11 05:43:08.452775: train_loss -0.9301\n",
      "2024-01-11 05:43:08.460780: val_loss -0.8374\n",
      "2024-01-11 05:43:08.469778: Pseudo dice [0.8941, 0.9482, 0.9371]\n",
      "2024-01-11 05:43:08.476778: Epoch time: 40.12 s\n",
      "2024-01-11 05:43:10.123015: \n",
      "2024-01-11 05:43:10.128367: Epoch 200\n",
      "2024-01-11 05:43:10.139449: Current learning rate: 0.00818\n",
      "2024-01-11 05:43:50.196707: train_loss -0.9304\n",
      "2024-01-11 05:43:50.204708: val_loss -0.8358\n",
      "2024-01-11 05:43:50.213710: Pseudo dice [0.8927, 0.9488, 0.937]\n",
      "2024-01-11 05:43:50.218704: Epoch time: 40.07 s\n",
      "2024-01-11 05:43:51.692564: \n",
      "2024-01-11 05:43:51.698469: Epoch 201\n",
      "2024-01-11 05:43:51.706011: Current learning rate: 0.00817\n",
      "2024-01-11 05:44:31.753685: train_loss -0.9304\n",
      "2024-01-11 05:44:31.760192: val_loss -0.8322\n",
      "2024-01-11 05:44:31.767199: Pseudo dice [0.8901, 0.9478, 0.9349]\n",
      "2024-01-11 05:44:31.771199: Epoch time: 40.06 s\n",
      "2024-01-11 05:44:33.225229: \n",
      "2024-01-11 05:44:33.233225: Epoch 202\n",
      "2024-01-11 05:44:33.238216: Current learning rate: 0.00816\n",
      "2024-01-11 05:45:13.083896: train_loss -0.9303\n",
      "2024-01-11 05:45:13.094070: val_loss -0.8431\n",
      "2024-01-11 05:45:13.122084: Pseudo dice [0.8954, 0.9488, 0.9374]\n",
      "2024-01-11 05:45:13.130075: Epoch time: 39.86 s\n",
      "2024-01-11 05:45:14.617990: \n",
      "2024-01-11 05:45:14.623990: Epoch 203\n",
      "2024-01-11 05:45:14.628332: Current learning rate: 0.00815\n",
      "2024-01-11 05:45:54.841343: train_loss -0.9315\n",
      "2024-01-11 05:45:54.847335: val_loss -0.8328\n",
      "2024-01-11 05:45:54.854332: Pseudo dice [0.8923, 0.9473, 0.9334]\n",
      "2024-01-11 05:45:54.862332: Epoch time: 40.22 s\n",
      "2024-01-11 05:45:56.373001: \n",
      "2024-01-11 05:45:56.378520: Epoch 204\n",
      "2024-01-11 05:45:56.382613: Current learning rate: 0.00814\n",
      "2024-01-11 05:46:36.355478: train_loss -0.9303\n",
      "2024-01-11 05:46:36.390595: val_loss -0.8369\n",
      "2024-01-11 05:46:36.399596: Pseudo dice [0.8895, 0.9486, 0.9362]\n",
      "2024-01-11 05:46:36.405596: Epoch time: 39.98 s\n",
      "2024-01-11 05:46:37.835747: \n",
      "2024-01-11 05:46:37.847434: Epoch 205\n",
      "2024-01-11 05:46:37.853971: Current learning rate: 0.00813\n",
      "2024-01-11 05:47:17.717407: train_loss -0.9302\n",
      "2024-01-11 05:47:17.725410: val_loss -0.8369\n",
      "2024-01-11 05:47:17.733407: Pseudo dice [0.8901, 0.9489, 0.9372]\n",
      "2024-01-11 05:47:17.741408: Epoch time: 39.88 s\n",
      "2024-01-11 05:47:19.068763: \n",
      "2024-01-11 05:47:19.074302: Epoch 206\n",
      "2024-01-11 05:47:19.079370: Current learning rate: 0.00813\n",
      "2024-01-11 05:47:59.242589: train_loss -0.9306\n",
      "2024-01-11 05:47:59.252589: val_loss -0.8389\n",
      "2024-01-11 05:47:59.260590: Pseudo dice [0.8935, 0.9483, 0.9359]\n",
      "2024-01-11 05:47:59.267597: Epoch time: 40.17 s\n",
      "2024-01-11 05:48:00.608027: \n",
      "2024-01-11 05:48:00.612927: Epoch 207\n",
      "2024-01-11 05:48:00.617935: Current learning rate: 0.00812\n",
      "2024-01-11 05:48:40.600579: train_loss -0.9312\n",
      "2024-01-11 05:48:40.609579: val_loss -0.8381\n",
      "2024-01-11 05:48:40.616577: Pseudo dice [0.8923, 0.9487, 0.9366]\n",
      "2024-01-11 05:48:40.624577: Epoch time: 39.99 s\n",
      "2024-01-11 05:48:42.204353: \n",
      "2024-01-11 05:48:42.212353: Epoch 208\n",
      "2024-01-11 05:48:42.218446: Current learning rate: 0.00811\n",
      "2024-01-11 05:49:22.223850: train_loss -0.9303\n",
      "2024-01-11 05:49:22.231849: val_loss -0.8358\n",
      "2024-01-11 05:49:22.241851: Pseudo dice [0.8897, 0.9477, 0.935]\n",
      "2024-01-11 05:49:22.268851: Epoch time: 40.02 s\n",
      "2024-01-11 05:49:23.543487: \n",
      "2024-01-11 05:49:23.549162: Epoch 209\n",
      "2024-01-11 05:49:23.554163: Current learning rate: 0.0081\n",
      "2024-01-11 05:50:03.703830: train_loss -0.9309\n",
      "2024-01-11 05:50:03.713840: val_loss -0.8352\n",
      "2024-01-11 05:50:03.721833: Pseudo dice [0.8901, 0.9485, 0.937]\n",
      "2024-01-11 05:50:03.757338: Epoch time: 40.16 s\n",
      "2024-01-11 05:50:05.106200: \n",
      "2024-01-11 05:50:05.111519: Epoch 210\n",
      "2024-01-11 05:50:05.115588: Current learning rate: 0.00809\n",
      "2024-01-11 05:50:45.211801: train_loss -0.9305\n",
      "2024-01-11 05:50:45.218803: val_loss -0.8342\n",
      "2024-01-11 05:50:45.225805: Pseudo dice [0.8908, 0.9476, 0.9354]\n",
      "2024-01-11 05:50:45.232808: Epoch time: 40.11 s\n",
      "2024-01-11 05:50:46.581803: \n",
      "2024-01-11 05:50:46.588861: Epoch 211\n",
      "2024-01-11 05:50:46.592805: Current learning rate: 0.00808\n",
      "2024-01-11 05:51:26.745002: train_loss -0.9312\n",
      "2024-01-11 05:51:26.778528: val_loss -0.8396\n",
      "2024-01-11 05:51:26.786525: Pseudo dice [0.8887, 0.949, 0.938]\n",
      "2024-01-11 05:51:26.791525: Epoch time: 40.16 s\n",
      "2024-01-11 05:51:28.123616: \n",
      "2024-01-11 05:51:28.128380: Epoch 212\n",
      "2024-01-11 05:51:28.135113: Current learning rate: 0.00807\n",
      "2024-01-11 05:52:08.215395: train_loss -0.9322\n",
      "2024-01-11 05:52:08.223405: val_loss -0.8384\n",
      "2024-01-11 05:52:08.233398: Pseudo dice [0.8897, 0.9491, 0.9385]\n",
      "2024-01-11 05:52:08.240397: Epoch time: 40.09 s\n",
      "2024-01-11 05:52:09.600827: \n",
      "2024-01-11 05:52:09.609475: Epoch 213\n",
      "2024-01-11 05:52:09.614420: Current learning rate: 0.00806\n",
      "2024-01-11 05:52:49.869345: train_loss -0.9323\n",
      "2024-01-11 05:52:49.877345: val_loss -0.8374\n",
      "2024-01-11 05:52:49.886346: Pseudo dice [0.8912, 0.948, 0.9365]\n",
      "2024-01-11 05:52:49.891345: Epoch time: 40.27 s\n",
      "2024-01-11 05:52:51.383175: \n",
      "2024-01-11 05:52:51.391178: Epoch 214\n",
      "2024-01-11 05:52:51.396215: Current learning rate: 0.00805\n",
      "2024-01-11 05:53:31.352232: train_loss -0.9316\n",
      "2024-01-11 05:53:31.360237: val_loss -0.8298\n",
      "2024-01-11 05:53:31.368234: Pseudo dice [0.8859, 0.9467, 0.9347]\n",
      "2024-01-11 05:53:31.402236: Epoch time: 39.97 s\n",
      "2024-01-11 05:53:32.670518: \n",
      "2024-01-11 05:53:32.675511: Epoch 215\n",
      "2024-01-11 05:53:32.680466: Current learning rate: 0.00804\n",
      "2024-01-11 05:54:12.550844: train_loss -0.9321\n",
      "2024-01-11 05:54:12.557843: val_loss -0.8375\n",
      "2024-01-11 05:54:12.564844: Pseudo dice [0.8927, 0.9478, 0.936]\n",
      "2024-01-11 05:54:12.571843: Epoch time: 39.88 s\n",
      "2024-01-11 05:54:13.962716: \n",
      "2024-01-11 05:54:13.967460: Epoch 216\n",
      "2024-01-11 05:54:13.975465: Current learning rate: 0.00803\n",
      "2024-01-11 05:54:53.994858: train_loss -0.9323\n",
      "2024-01-11 05:54:54.001860: val_loss -0.837\n",
      "2024-01-11 05:54:54.009859: Pseudo dice [0.8918, 0.9482, 0.935]\n",
      "2024-01-11 05:54:54.017857: Epoch time: 40.03 s\n",
      "2024-01-11 05:54:55.398287: \n",
      "2024-01-11 05:54:55.403344: Epoch 217\n",
      "2024-01-11 05:54:55.408287: Current learning rate: 0.00802\n",
      "2024-01-11 05:55:35.284115: train_loss -0.932\n",
      "2024-01-11 05:55:35.294629: val_loss -0.839\n",
      "2024-01-11 05:55:35.301630: Pseudo dice [0.8902, 0.9489, 0.9379]\n",
      "2024-01-11 05:55:35.308644: Epoch time: 39.89 s\n",
      "2024-01-11 05:55:36.717918: \n",
      "2024-01-11 05:55:36.723310: Epoch 218\n",
      "2024-01-11 05:55:36.732384: Current learning rate: 0.00801\n",
      "2024-01-11 05:56:16.808617: train_loss -0.932\n",
      "2024-01-11 05:56:16.816617: val_loss -0.8343\n",
      "2024-01-11 05:56:16.823618: Pseudo dice [0.8916, 0.947, 0.9335]\n",
      "2024-01-11 05:56:16.831616: Epoch time: 40.09 s\n",
      "2024-01-11 05:56:18.310879: \n",
      "2024-01-11 05:56:18.316878: Epoch 219\n",
      "2024-01-11 05:56:18.320878: Current learning rate: 0.00801\n",
      "2024-01-11 05:56:58.403176: train_loss -0.9315\n",
      "2024-01-11 05:56:58.412176: val_loss -0.8463\n",
      "2024-01-11 05:56:58.419884: Pseudo dice [0.8999, 0.9506, 0.9399]\n",
      "2024-01-11 05:56:58.445900: Epoch time: 40.09 s\n",
      "2024-01-11 05:56:59.719598: \n",
      "2024-01-11 05:56:59.725605: Epoch 220\n",
      "2024-01-11 05:56:59.730607: Current learning rate: 0.008\n",
      "2024-01-11 05:57:39.923559: train_loss -0.9324\n",
      "2024-01-11 05:57:39.955567: val_loss -0.8375\n",
      "2024-01-11 05:57:39.964083: Pseudo dice [0.8889, 0.9487, 0.9369]\n",
      "2024-01-11 05:57:39.973080: Epoch time: 40.2 s\n",
      "2024-01-11 05:57:41.502993: \n",
      "2024-01-11 05:57:41.508927: Epoch 221\n",
      "2024-01-11 05:57:41.512925: Current learning rate: 0.00799\n",
      "2024-01-11 05:58:21.510926: train_loss -0.9318\n",
      "2024-01-11 05:58:21.544925: val_loss -0.8354\n",
      "2024-01-11 05:58:21.551925: Pseudo dice [0.8905, 0.9483, 0.9372]\n",
      "2024-01-11 05:58:21.558933: Epoch time: 40.01 s\n",
      "2024-01-11 05:58:23.006218: \n",
      "2024-01-11 05:58:23.011274: Epoch 222\n",
      "2024-01-11 05:58:23.016289: Current learning rate: 0.00798\n",
      "2024-01-11 05:59:03.065852: train_loss -0.9321\n",
      "2024-01-11 05:59:03.073855: val_loss -0.8413\n",
      "2024-01-11 05:59:03.081850: Pseudo dice [0.8929, 0.9497, 0.938]\n",
      "2024-01-11 05:59:03.086854: Epoch time: 40.06 s\n",
      "2024-01-11 05:59:04.458627: \n",
      "2024-01-11 05:59:04.465221: Epoch 223\n",
      "2024-01-11 05:59:04.470249: Current learning rate: 0.00797\n",
      "2024-01-11 05:59:44.762289: train_loss -0.9321\n",
      "2024-01-11 05:59:44.772300: val_loss -0.8291\n",
      "2024-01-11 05:59:44.777289: Pseudo dice [0.8886, 0.9451, 0.9333]\n",
      "2024-01-11 05:59:44.784289: Epoch time: 40.31 s\n",
      "2024-01-11 05:59:46.147033: \n",
      "2024-01-11 05:59:46.154115: Epoch 224\n",
      "2024-01-11 05:59:46.158100: Current learning rate: 0.00796\n",
      "2024-01-11 06:00:26.208955: train_loss -0.9324\n",
      "2024-01-11 06:00:26.216957: val_loss -0.8362\n",
      "2024-01-11 06:00:26.223957: Pseudo dice [0.888, 0.9491, 0.9378]\n",
      "2024-01-11 06:00:26.229962: Epoch time: 40.06 s\n",
      "2024-01-11 06:00:27.689366: \n",
      "2024-01-11 06:00:27.694369: Epoch 225\n",
      "2024-01-11 06:00:27.698440: Current learning rate: 0.00795\n",
      "2024-01-11 06:01:07.719933: train_loss -0.9328\n",
      "2024-01-11 06:01:07.726932: val_loss -0.8292\n",
      "2024-01-11 06:01:07.733933: Pseudo dice [0.8855, 0.9469, 0.9345]\n",
      "2024-01-11 06:01:07.764222: Epoch time: 40.03 s\n",
      "2024-01-11 06:01:09.192524: \n",
      "2024-01-11 06:01:09.198515: Epoch 226\n",
      "2024-01-11 06:01:09.203525: Current learning rate: 0.00794\n",
      "2024-01-11 06:01:49.284156: train_loss -0.9321\n",
      "2024-01-11 06:01:49.293660: val_loss -0.8406\n",
      "2024-01-11 06:01:49.301666: Pseudo dice [0.8915, 0.9499, 0.9384]\n",
      "2024-01-11 06:01:49.327666: Epoch time: 40.09 s\n",
      "2024-01-11 06:01:50.733125: \n",
      "2024-01-11 06:01:50.738185: Epoch 227\n",
      "2024-01-11 06:01:50.743132: Current learning rate: 0.00793\n",
      "2024-01-11 06:02:30.874389: train_loss -0.9324\n",
      "2024-01-11 06:02:30.881390: val_loss -0.8435\n",
      "2024-01-11 06:02:30.887391: Pseudo dice [0.8931, 0.9511, 0.9397]\n",
      "2024-01-11 06:02:30.892389: Epoch time: 40.14 s\n",
      "2024-01-11 06:02:32.249678: \n",
      "2024-01-11 06:02:32.254674: Epoch 228\n",
      "2024-01-11 06:02:32.258673: Current learning rate: 0.00792\n",
      "2024-01-11 06:03:12.294865: train_loss -0.933\n",
      "2024-01-11 06:03:12.302168: val_loss -0.8298\n",
      "2024-01-11 06:03:12.312554: Pseudo dice [0.888, 0.9462, 0.9337]\n",
      "2024-01-11 06:03:12.318555: Epoch time: 40.05 s\n",
      "2024-01-11 06:03:13.732056: \n",
      "2024-01-11 06:03:13.738704: Epoch 229\n",
      "2024-01-11 06:03:13.743705: Current learning rate: 0.00791\n",
      "2024-01-11 06:03:53.677709: train_loss -0.9331\n",
      "2024-01-11 06:03:53.683709: val_loss -0.8351\n",
      "2024-01-11 06:03:53.689709: Pseudo dice [0.8888, 0.9481, 0.9356]\n",
      "2024-01-11 06:03:53.694712: Epoch time: 39.95 s\n",
      "2024-01-11 06:03:55.162388: \n",
      "2024-01-11 06:03:55.168097: Epoch 230\n",
      "2024-01-11 06:03:55.174153: Current learning rate: 0.0079\n",
      "2024-01-11 06:04:35.058465: train_loss -0.9332\n",
      "2024-01-11 06:04:35.067974: val_loss -0.8352\n",
      "2024-01-11 06:04:35.091976: Pseudo dice [0.8877, 0.9482, 0.9372]\n",
      "2024-01-11 06:04:35.099983: Epoch time: 39.9 s\n",
      "2024-01-11 06:04:36.415209: \n",
      "2024-01-11 06:04:36.419807: Epoch 231\n",
      "2024-01-11 06:04:36.427913: Current learning rate: 0.00789\n",
      "2024-01-11 06:05:16.639209: train_loss -0.9329\n",
      "2024-01-11 06:05:16.667214: val_loss -0.8394\n",
      "2024-01-11 06:05:16.673215: Pseudo dice [0.8922, 0.9491, 0.9373]\n",
      "2024-01-11 06:05:16.680214: Epoch time: 40.22 s\n",
      "2024-01-11 06:05:18.090042: \n",
      "2024-01-11 06:05:18.095521: Epoch 232\n",
      "2024-01-11 06:05:18.100520: Current learning rate: 0.00789\n",
      "2024-01-11 06:05:58.204488: train_loss -0.9338\n",
      "2024-01-11 06:05:58.213489: val_loss -0.8413\n",
      "2024-01-11 06:05:58.220490: Pseudo dice [0.8959, 0.9497, 0.9375]\n",
      "2024-01-11 06:05:58.226495: Epoch time: 40.12 s\n",
      "2024-01-11 06:05:59.555658: \n",
      "2024-01-11 06:05:59.560995: Epoch 233\n",
      "2024-01-11 06:05:59.569477: Current learning rate: 0.00788\n",
      "2024-01-11 06:06:39.604352: train_loss -0.9327\n",
      "2024-01-11 06:06:39.612862: val_loss -0.834\n",
      "2024-01-11 06:06:39.618861: Pseudo dice [0.8881, 0.9477, 0.937]\n",
      "2024-01-11 06:06:39.624860: Epoch time: 40.05 s\n",
      "2024-01-11 06:06:41.034468: \n",
      "2024-01-11 06:06:41.042153: Epoch 234\n",
      "2024-01-11 06:06:41.049150: Current learning rate: 0.00787\n",
      "2024-01-11 06:07:21.284763: train_loss -0.9335\n",
      "2024-01-11 06:07:21.291768: val_loss -0.8309\n",
      "2024-01-11 06:07:21.298760: Pseudo dice [0.8843, 0.9465, 0.9332]\n",
      "2024-01-11 06:07:21.334318: Epoch time: 40.25 s\n",
      "2024-01-11 06:07:22.752844: \n",
      "2024-01-11 06:07:22.759938: Epoch 235\n",
      "2024-01-11 06:07:22.763627: Current learning rate: 0.00786\n",
      "2024-01-11 06:08:02.863304: train_loss -0.9341\n",
      "2024-01-11 06:08:02.895303: val_loss -0.8326\n",
      "2024-01-11 06:08:02.904301: Pseudo dice [0.8864, 0.9461, 0.9332]\n",
      "2024-01-11 06:08:02.910309: Epoch time: 40.11 s\n",
      "2024-01-11 06:08:04.350571: \n",
      "2024-01-11 06:08:04.356650: Epoch 236\n",
      "2024-01-11 06:08:04.362341: Current learning rate: 0.00785\n",
      "2024-01-11 06:08:44.282609: train_loss -0.9345\n",
      "2024-01-11 06:08:44.290609: val_loss -0.836\n",
      "2024-01-11 06:08:44.296610: Pseudo dice [0.8922, 0.9479, 0.9351]\n",
      "2024-01-11 06:08:44.301609: Epoch time: 39.93 s\n",
      "2024-01-11 06:08:45.766526: \n",
      "2024-01-11 06:08:45.771684: Epoch 237\n",
      "2024-01-11 06:08:45.782706: Current learning rate: 0.00784\n",
      "2024-01-11 06:09:25.796137: train_loss -0.9334\n",
      "2024-01-11 06:09:25.804645: val_loss -0.8454\n",
      "2024-01-11 06:09:25.813387: Pseudo dice [0.8943, 0.9512, 0.9399]\n",
      "2024-01-11 06:09:25.818385: Epoch time: 40.03 s\n",
      "2024-01-11 06:09:27.261286: \n",
      "2024-01-11 06:09:27.268285: Epoch 238\n",
      "2024-01-11 06:09:27.273289: Current learning rate: 0.00783\n",
      "2024-01-11 06:10:07.508797: train_loss -0.9326\n",
      "2024-01-11 06:10:07.515799: val_loss -0.8391\n",
      "2024-01-11 06:10:07.522796: Pseudo dice [0.896, 0.9486, 0.9362]\n",
      "2024-01-11 06:10:07.527796: Epoch time: 40.25 s\n",
      "2024-01-11 06:10:08.940574: \n",
      "2024-01-11 06:10:08.946059: Epoch 239\n",
      "2024-01-11 06:10:08.952069: Current learning rate: 0.00782\n",
      "2024-01-11 06:10:49.050271: train_loss -0.9335\n",
      "2024-01-11 06:10:49.089028: val_loss -0.8409\n",
      "2024-01-11 06:10:49.095028: Pseudo dice [0.899, 0.949, 0.938]\n",
      "2024-01-11 06:10:49.100027: Epoch time: 40.11 s\n",
      "2024-01-11 06:10:50.412757: \n",
      "2024-01-11 06:10:50.417879: Epoch 240\n",
      "2024-01-11 06:10:50.422942: Current learning rate: 0.00781\n",
      "2024-01-11 06:11:30.462874: train_loss -0.9335\n",
      "2024-01-11 06:11:30.472987: val_loss -0.8397\n",
      "2024-01-11 06:11:30.481988: Pseudo dice [0.8937, 0.9484, 0.9371]\n",
      "2024-01-11 06:11:30.493023: Epoch time: 40.05 s\n",
      "2024-01-11 06:11:31.902813: \n",
      "2024-01-11 06:11:31.907208: Epoch 241\n",
      "2024-01-11 06:11:31.911206: Current learning rate: 0.0078\n",
      "2024-01-11 06:12:12.061742: train_loss -0.9333\n",
      "2024-01-11 06:12:12.068742: val_loss -0.8363\n",
      "2024-01-11 06:12:12.077750: Pseudo dice [0.8888, 0.9495, 0.9378]\n",
      "2024-01-11 06:12:12.084747: Epoch time: 40.16 s\n",
      "2024-01-11 06:12:13.496337: \n",
      "2024-01-11 06:12:13.501315: Epoch 242\n",
      "2024-01-11 06:12:13.505376: Current learning rate: 0.00779\n",
      "2024-01-11 06:12:53.545589: train_loss -0.9336\n",
      "2024-01-11 06:12:53.553585: val_loss -0.8362\n",
      "2024-01-11 06:12:53.560584: Pseudo dice [0.894, 0.9488, 0.9366]\n",
      "2024-01-11 06:12:53.567585: Epoch time: 40.05 s\n",
      "2024-01-11 06:12:54.954490: \n",
      "2024-01-11 06:12:54.962490: Epoch 243\n",
      "2024-01-11 06:12:54.966164: Current learning rate: 0.00778\n",
      "2024-01-11 06:13:35.198196: train_loss -0.9341\n",
      "2024-01-11 06:13:35.206196: val_loss -0.8382\n",
      "2024-01-11 06:13:35.212196: Pseudo dice [0.8926, 0.9482, 0.9371]\n",
      "2024-01-11 06:13:35.219196: Epoch time: 40.24 s\n",
      "2024-01-11 06:13:36.651335: \n",
      "2024-01-11 06:13:36.657185: Epoch 244\n",
      "2024-01-11 06:13:36.663190: Current learning rate: 0.00777\n",
      "2024-01-11 06:14:16.644992: train_loss -0.9337\n",
      "2024-01-11 06:14:16.653510: val_loss -0.8339\n",
      "2024-01-11 06:14:16.661510: Pseudo dice [0.8937, 0.9473, 0.9353]\n",
      "2024-01-11 06:14:16.687507: Epoch time: 39.99 s\n",
      "2024-01-11 06:14:18.104699: \n",
      "2024-01-11 06:14:18.111109: Epoch 245\n",
      "2024-01-11 06:14:18.121135: Current learning rate: 0.00777\n",
      "2024-01-11 06:14:58.106149: train_loss -0.9347\n",
      "2024-01-11 06:14:58.116150: val_loss -0.8409\n",
      "2024-01-11 06:14:58.123150: Pseudo dice [0.8914, 0.9501, 0.9383]\n",
      "2024-01-11 06:14:58.164163: Epoch time: 40.0 s\n",
      "2024-01-11 06:14:59.603128: \n",
      "2024-01-11 06:14:59.609135: Epoch 246\n",
      "2024-01-11 06:14:59.614069: Current learning rate: 0.00776\n",
      "2024-01-11 06:15:39.744838: train_loss -0.9338\n",
      "2024-01-11 06:15:39.753845: val_loss -0.8317\n",
      "2024-01-11 06:15:39.761842: Pseudo dice [0.8892, 0.9481, 0.9355]\n",
      "2024-01-11 06:15:39.767842: Epoch time: 40.14 s\n",
      "2024-01-11 06:15:41.189166: \n",
      "2024-01-11 06:15:41.194801: Epoch 247\n",
      "2024-01-11 06:15:41.199875: Current learning rate: 0.00775\n",
      "2024-01-11 06:16:21.336311: train_loss -0.9349\n",
      "2024-01-11 06:16:21.365315: val_loss -0.8361\n",
      "2024-01-11 06:16:21.375312: Pseudo dice [0.8919, 0.9481, 0.9361]\n",
      "2024-01-11 06:16:21.382311: Epoch time: 40.15 s\n",
      "2024-01-11 06:16:22.914272: \n",
      "2024-01-11 06:16:22.923650: Epoch 248\n",
      "2024-01-11 06:16:22.928653: Current learning rate: 0.00774\n",
      "2024-01-11 06:17:03.131120: train_loss -0.9339\n",
      "2024-01-11 06:17:03.138645: val_loss -0.8386\n",
      "2024-01-11 06:17:03.146645: Pseudo dice [0.8917, 0.9493, 0.9381]\n",
      "2024-01-11 06:17:03.153645: Epoch time: 40.22 s\n",
      "2024-01-11 06:17:04.550655: \n",
      "2024-01-11 06:17:04.556614: Epoch 249\n",
      "2024-01-11 06:17:04.560664: Current learning rate: 0.00773\n",
      "2024-01-11 06:17:44.481993: train_loss -0.9341\n",
      "2024-01-11 06:17:44.489987: val_loss -0.8379\n",
      "2024-01-11 06:17:44.497990: Pseudo dice [0.8944, 0.9493, 0.9376]\n",
      "2024-01-11 06:17:44.504989: Epoch time: 39.93 s\n",
      "2024-01-11 06:17:46.149786: \n",
      "2024-01-11 06:17:46.158384: Epoch 250\n",
      "2024-01-11 06:17:46.163368: Current learning rate: 0.00772\n",
      "2024-01-11 06:18:26.358620: train_loss -0.9339\n",
      "2024-01-11 06:18:26.367625: val_loss -0.8307\n",
      "2024-01-11 06:18:26.375627: Pseudo dice [0.8894, 0.946, 0.9334]\n",
      "2024-01-11 06:18:26.384143: Epoch time: 40.21 s\n",
      "2024-01-11 06:18:27.859359: \n",
      "2024-01-11 06:18:27.864867: Epoch 251\n",
      "2024-01-11 06:18:27.868897: Current learning rate: 0.00771\n",
      "2024-01-11 06:19:08.034046: train_loss -0.9339\n",
      "2024-01-11 06:19:08.043048: val_loss -0.8419\n",
      "2024-01-11 06:19:08.053047: Pseudo dice [0.8936, 0.9497, 0.9377]\n",
      "2024-01-11 06:19:08.058047: Epoch time: 40.18 s\n",
      "2024-01-11 06:19:09.489895: \n",
      "2024-01-11 06:19:09.497886: Epoch 252\n",
      "2024-01-11 06:19:09.503514: Current learning rate: 0.0077\n",
      "2024-01-11 06:19:49.597805: train_loss -0.9346\n",
      "2024-01-11 06:19:49.604809: val_loss -0.8339\n",
      "2024-01-11 06:19:49.610806: Pseudo dice [0.8894, 0.947, 0.9352]\n",
      "2024-01-11 06:19:49.616813: Epoch time: 40.11 s\n",
      "2024-01-11 06:19:51.124228: \n",
      "2024-01-11 06:19:51.134054: Epoch 253\n",
      "2024-01-11 06:19:51.141125: Current learning rate: 0.00769\n",
      "2024-01-11 06:20:31.404341: train_loss -0.9343\n",
      "2024-01-11 06:20:31.412345: val_loss -0.8413\n",
      "2024-01-11 06:20:31.419344: Pseudo dice [0.8947, 0.9491, 0.9377]\n",
      "2024-01-11 06:20:31.427343: Epoch time: 40.28 s\n",
      "2024-01-11 06:20:32.887926: \n",
      "2024-01-11 06:20:32.892908: Epoch 254\n",
      "2024-01-11 06:20:32.896901: Current learning rate: 0.00768\n",
      "2024-01-11 06:21:12.920018: train_loss -0.9347\n",
      "2024-01-11 06:21:12.945019: val_loss -0.8365\n",
      "2024-01-11 06:21:12.954023: Pseudo dice [0.885, 0.9487, 0.9369]\n",
      "2024-01-11 06:21:12.961027: Epoch time: 40.04 s\n",
      "2024-01-11 06:21:14.457941: \n",
      "2024-01-11 06:21:14.470080: Epoch 255\n",
      "2024-01-11 06:21:14.475061: Current learning rate: 0.00767\n",
      "2024-01-11 06:21:54.562218: train_loss -0.935\n",
      "2024-01-11 06:21:54.571215: val_loss -0.8342\n",
      "2024-01-11 06:21:54.580217: Pseudo dice [0.8894, 0.9476, 0.937]\n",
      "2024-01-11 06:21:54.586221: Epoch time: 40.11 s\n",
      "2024-01-11 06:21:55.991594: \n",
      "2024-01-11 06:21:55.996208: Epoch 256\n",
      "2024-01-11 06:21:56.003948: Current learning rate: 0.00766\n",
      "2024-01-11 06:22:36.228677: train_loss -0.935\n",
      "2024-01-11 06:22:36.235193: val_loss -0.8389\n",
      "2024-01-11 06:22:36.244192: Pseudo dice [0.8924, 0.9491, 0.9384]\n",
      "2024-01-11 06:22:36.249196: Epoch time: 40.24 s\n",
      "2024-01-11 06:22:37.666168: \n",
      "2024-01-11 06:22:37.670553: Epoch 257\n",
      "2024-01-11 06:22:37.675485: Current learning rate: 0.00765\n",
      "2024-01-11 06:23:17.634629: train_loss -0.9344\n",
      "2024-01-11 06:23:17.642633: val_loss -0.8352\n",
      "2024-01-11 06:23:17.650632: Pseudo dice [0.8879, 0.9485, 0.9362]\n",
      "2024-01-11 06:23:17.675171: Epoch time: 39.97 s\n",
      "2024-01-11 06:23:19.055803: \n",
      "2024-01-11 06:23:19.060819: Epoch 258\n",
      "2024-01-11 06:23:19.068013: Current learning rate: 0.00764\n",
      "2024-01-11 06:23:59.159182: train_loss -0.9344\n",
      "2024-01-11 06:23:59.188487: val_loss -0.8375\n",
      "2024-01-11 06:23:59.198005: Pseudo dice [0.8927, 0.948, 0.9366]\n",
      "2024-01-11 06:23:59.204007: Epoch time: 40.1 s\n",
      "2024-01-11 06:24:00.721407: \n",
      "2024-01-11 06:24:00.730370: Epoch 259\n",
      "2024-01-11 06:24:00.740643: Current learning rate: 0.00764\n",
      "2024-01-11 06:24:40.704524: train_loss -0.936\n",
      "2024-01-11 06:24:40.716140: val_loss -0.8363\n",
      "2024-01-11 06:24:40.722140: Pseudo dice [0.8876, 0.9483, 0.9363]\n",
      "2024-01-11 06:24:40.728140: Epoch time: 39.98 s\n",
      "2024-01-11 06:24:42.199394: \n",
      "2024-01-11 06:24:42.207395: Epoch 260\n",
      "2024-01-11 06:24:42.212916: Current learning rate: 0.00763\n",
      "2024-01-11 06:25:22.211415: train_loss -0.935\n",
      "2024-01-11 06:25:22.221416: val_loss -0.8377\n",
      "2024-01-11 06:25:22.232418: Pseudo dice [0.8944, 0.9485, 0.9366]\n",
      "2024-01-11 06:25:22.239930: Epoch time: 40.01 s\n",
      "2024-01-11 06:25:23.725072: \n",
      "2024-01-11 06:25:23.739182: Epoch 261\n",
      "2024-01-11 06:25:23.743746: Current learning rate: 0.00762\n",
      "2024-01-11 06:26:03.978440: train_loss -0.9353\n",
      "2024-01-11 06:26:03.988440: val_loss -0.832\n",
      "2024-01-11 06:26:03.996440: Pseudo dice [0.8926, 0.9463, 0.9344]\n",
      "2024-01-11 06:26:04.002439: Epoch time: 40.25 s\n",
      "2024-01-11 06:26:05.357453: \n",
      "2024-01-11 06:26:05.365486: Epoch 262\n",
      "2024-01-11 06:26:05.369794: Current learning rate: 0.00761\n",
      "2024-01-11 06:26:45.214144: train_loss -0.9349\n",
      "2024-01-11 06:26:45.223847: val_loss -0.8361\n",
      "2024-01-11 06:26:45.255847: Pseudo dice [0.8892, 0.9481, 0.9364]\n",
      "2024-01-11 06:26:45.265371: Epoch time: 39.86 s\n",
      "2024-01-11 06:26:46.874035: \n",
      "2024-01-11 06:26:46.880028: Epoch 263\n",
      "2024-01-11 06:26:46.888034: Current learning rate: 0.0076\n",
      "2024-01-11 06:27:26.908420: train_loss -0.9355\n",
      "2024-01-11 06:27:26.917929: val_loss -0.8363\n",
      "2024-01-11 06:27:26.925928: Pseudo dice [0.8949, 0.9482, 0.936]\n",
      "2024-01-11 06:27:26.932925: Epoch time: 40.04 s\n",
      "2024-01-11 06:27:28.317913: \n",
      "2024-01-11 06:27:28.326148: Epoch 264\n",
      "2024-01-11 06:27:28.333149: Current learning rate: 0.00759\n",
      "2024-01-11 06:28:08.466890: train_loss -0.9352\n",
      "2024-01-11 06:28:08.473891: val_loss -0.8385\n",
      "2024-01-11 06:28:08.479891: Pseudo dice [0.8953, 0.9477, 0.9362]\n",
      "2024-01-11 06:28:08.486891: Epoch time: 40.15 s\n",
      "2024-01-11 06:28:09.872522: \n",
      "2024-01-11 06:28:09.877188: Epoch 265\n",
      "2024-01-11 06:28:09.881265: Current learning rate: 0.00758\n",
      "2024-01-11 06:28:49.754555: train_loss -0.9351\n",
      "2024-01-11 06:28:49.796556: val_loss -0.8327\n",
      "2024-01-11 06:28:49.806557: Pseudo dice [0.8912, 0.9462, 0.9341]\n",
      "2024-01-11 06:28:49.812554: Epoch time: 39.88 s\n",
      "2024-01-11 06:28:51.197422: \n",
      "2024-01-11 06:28:51.205450: Epoch 266\n",
      "2024-01-11 06:28:51.212690: Current learning rate: 0.00757\n",
      "2024-01-11 06:29:31.314907: train_loss -0.9346\n",
      "2024-01-11 06:29:31.324908: val_loss -0.8415\n",
      "2024-01-11 06:29:31.365426: Pseudo dice [0.8931, 0.95, 0.9389]\n",
      "2024-01-11 06:29:31.373426: Epoch time: 40.12 s\n",
      "2024-01-11 06:29:32.767362: \n",
      "2024-01-11 06:29:32.773917: Epoch 267\n",
      "2024-01-11 06:29:32.781965: Current learning rate: 0.00756\n",
      "2024-01-11 06:30:12.726364: train_loss -0.9353\n",
      "2024-01-11 06:30:12.733367: val_loss -0.8419\n",
      "2024-01-11 06:30:12.738367: Pseudo dice [0.8967, 0.9499, 0.939]\n",
      "2024-01-11 06:30:12.745363: Epoch time: 39.96 s\n",
      "2024-01-11 06:30:14.320215: \n",
      "2024-01-11 06:30:14.325214: Epoch 268\n",
      "2024-01-11 06:30:14.329214: Current learning rate: 0.00755\n",
      "2024-01-11 06:30:54.371761: train_loss -0.936\n",
      "2024-01-11 06:30:54.377759: val_loss -0.8378\n",
      "2024-01-11 06:30:54.384760: Pseudo dice [0.8924, 0.9491, 0.937]\n",
      "2024-01-11 06:30:54.390773: Epoch time: 40.05 s\n",
      "2024-01-11 06:30:55.843187: \n",
      "2024-01-11 06:30:55.847694: Epoch 269\n",
      "2024-01-11 06:30:55.851694: Current learning rate: 0.00754\n",
      "2024-01-11 06:31:35.857810: train_loss -0.9353\n",
      "2024-01-11 06:31:35.869799: val_loss -0.8361\n",
      "2024-01-11 06:31:35.876801: Pseudo dice [0.8928, 0.9491, 0.937]\n",
      "2024-01-11 06:31:35.881799: Epoch time: 40.01 s\n",
      "2024-01-11 06:31:37.436068: \n",
      "2024-01-11 06:31:37.441056: Epoch 270\n",
      "2024-01-11 06:31:37.446998: Current learning rate: 0.00753\n",
      "2024-01-11 06:32:17.438939: train_loss -0.935\n",
      "2024-01-11 06:32:17.446940: val_loss -0.8408\n",
      "2024-01-11 06:32:17.455452: Pseudo dice [0.8961, 0.949, 0.9378]\n",
      "2024-01-11 06:32:17.483449: Epoch time: 40.0 s\n",
      "2024-01-11 06:32:18.962486: \n",
      "2024-01-11 06:32:18.967415: Epoch 271\n",
      "2024-01-11 06:32:18.970853: Current learning rate: 0.00752\n",
      "2024-01-11 06:32:58.904581: train_loss -0.9355\n",
      "2024-01-11 06:32:58.940584: val_loss -0.8351\n",
      "2024-01-11 06:32:58.948587: Pseudo dice [0.8905, 0.9481, 0.9362]\n",
      "2024-01-11 06:32:58.954586: Epoch time: 39.94 s\n",
      "2024-01-11 06:33:00.441671: \n",
      "2024-01-11 06:33:00.446302: Epoch 272\n",
      "2024-01-11 06:33:00.451331: Current learning rate: 0.00751\n",
      "2024-01-11 06:33:40.551340: train_loss -0.9356\n",
      "2024-01-11 06:33:40.558343: val_loss -0.8402\n",
      "2024-01-11 06:33:40.588874: Pseudo dice [0.8942, 0.9495, 0.9389]\n",
      "2024-01-11 06:33:40.595884: Epoch time: 40.11 s\n",
      "2024-01-11 06:33:41.965865: \n",
      "2024-01-11 06:33:41.970547: Epoch 273\n",
      "2024-01-11 06:33:41.977630: Current learning rate: 0.00751\n",
      "2024-01-11 06:34:22.002774: train_loss -0.9355\n",
      "2024-01-11 06:34:22.012797: val_loss -0.8348\n",
      "2024-01-11 06:34:22.019796: Pseudo dice [0.8924, 0.9484, 0.936]\n",
      "2024-01-11 06:34:22.026298: Epoch time: 40.04 s\n",
      "2024-01-11 06:34:23.462454: \n",
      "2024-01-11 06:34:23.469561: Epoch 274\n",
      "2024-01-11 06:34:23.475604: Current learning rate: 0.0075\n",
      "2024-01-11 06:35:03.583782: train_loss -0.9353\n",
      "2024-01-11 06:35:03.593780: val_loss -0.8368\n",
      "2024-01-11 06:35:03.633795: Pseudo dice [0.8918, 0.9482, 0.9376]\n",
      "2024-01-11 06:35:03.640797: Epoch time: 40.12 s\n",
      "2024-01-11 06:35:05.077660: \n",
      "2024-01-11 06:35:05.082988: Epoch 275\n",
      "2024-01-11 06:35:05.089051: Current learning rate: 0.00749\n",
      "2024-01-11 06:35:45.174781: train_loss -0.9357\n",
      "2024-01-11 06:35:45.180804: val_loss -0.8312\n",
      "2024-01-11 06:35:45.185796: Pseudo dice [0.8863, 0.9471, 0.9358]\n",
      "2024-01-11 06:35:45.191804: Epoch time: 40.1 s\n",
      "2024-01-11 06:35:46.535915: \n",
      "2024-01-11 06:35:46.547422: Epoch 276\n",
      "2024-01-11 06:35:46.557437: Current learning rate: 0.00748\n",
      "2024-01-11 06:36:26.512837: train_loss -0.9367\n",
      "2024-01-11 06:36:26.521839: val_loss -0.8391\n",
      "2024-01-11 06:36:26.527843: Pseudo dice [0.8927, 0.9482, 0.9364]\n",
      "2024-01-11 06:36:26.534838: Epoch time: 39.98 s\n",
      "2024-01-11 06:36:27.946306: \n",
      "2024-01-11 06:36:27.953297: Epoch 277\n",
      "2024-01-11 06:36:27.958296: Current learning rate: 0.00747\n",
      "2024-01-11 06:37:07.903793: train_loss -0.9346\n",
      "2024-01-11 06:37:07.909790: val_loss -0.838\n",
      "2024-01-11 06:37:07.915789: Pseudo dice [0.8896, 0.9492, 0.9378]\n",
      "2024-01-11 06:37:07.922308: Epoch time: 39.96 s\n",
      "2024-01-11 06:37:09.418594: \n",
      "2024-01-11 06:37:09.426102: Epoch 278\n",
      "2024-01-11 06:37:09.430638: Current learning rate: 0.00746\n",
      "2024-01-11 06:37:49.412751: train_loss -0.9366\n",
      "2024-01-11 06:37:49.421761: val_loss -0.8423\n",
      "2024-01-11 06:37:49.429514: Pseudo dice [0.8974, 0.9503, 0.9388]\n",
      "2024-01-11 06:37:49.435515: Epoch time: 40.0 s\n",
      "2024-01-11 06:37:50.824124: \n",
      "2024-01-11 06:37:50.832217: Epoch 279\n",
      "2024-01-11 06:37:50.836903: Current learning rate: 0.00745\n",
      "2024-01-11 06:38:30.912768: train_loss -0.9363\n",
      "2024-01-11 06:38:30.921766: val_loss -0.8381\n",
      "2024-01-11 06:38:30.929769: Pseudo dice [0.891, 0.949, 0.938]\n",
      "2024-01-11 06:38:30.937766: Epoch time: 40.09 s\n",
      "2024-01-11 06:38:32.399664: \n",
      "2024-01-11 06:38:32.404659: Epoch 280\n",
      "2024-01-11 06:38:32.410664: Current learning rate: 0.00744\n",
      "2024-01-11 06:39:12.483944: train_loss -0.9358\n",
      "2024-01-11 06:39:12.491943: val_loss -0.8379\n",
      "2024-01-11 06:39:12.499941: Pseudo dice [0.8909, 0.9491, 0.9381]\n",
      "2024-01-11 06:39:12.531940: Epoch time: 40.09 s\n",
      "2024-01-11 06:39:13.902320: \n",
      "2024-01-11 06:39:13.911875: Epoch 281\n",
      "2024-01-11 06:39:13.921936: Current learning rate: 0.00743\n",
      "2024-01-11 06:39:53.949290: train_loss -0.9371\n",
      "2024-01-11 06:39:53.956292: val_loss -0.8341\n",
      "2024-01-11 06:39:53.965290: Pseudo dice [0.8894, 0.9488, 0.9368]\n",
      "2024-01-11 06:39:53.973289: Epoch time: 40.05 s\n",
      "2024-01-11 06:39:55.349494: \n",
      "2024-01-11 06:39:55.354582: Epoch 282\n",
      "2024-01-11 06:39:55.359572: Current learning rate: 0.00742\n",
      "2024-01-11 06:40:35.490786: train_loss -0.9361\n",
      "2024-01-11 06:40:35.500784: val_loss -0.8345\n",
      "2024-01-11 06:40:35.507877: Pseudo dice [0.8889, 0.9489, 0.937]\n",
      "2024-01-11 06:40:35.514878: Epoch time: 40.14 s\n",
      "2024-01-11 06:40:36.871465: \n",
      "2024-01-11 06:40:36.877102: Epoch 283\n",
      "2024-01-11 06:40:36.885417: Current learning rate: 0.00741\n",
      "2024-01-11 06:41:17.067786: train_loss -0.9369\n",
      "2024-01-11 06:41:17.074783: val_loss -0.8352\n",
      "2024-01-11 06:41:17.081782: Pseudo dice [0.8929, 0.9486, 0.9366]\n",
      "2024-01-11 06:41:17.088312: Epoch time: 40.2 s\n",
      "2024-01-11 06:41:18.506219: \n",
      "2024-01-11 06:41:18.511144: Epoch 284\n",
      "2024-01-11 06:41:18.517431: Current learning rate: 0.0074\n",
      "2024-01-11 06:41:58.482018: train_loss -0.9361\n",
      "2024-01-11 06:41:58.488018: val_loss -0.83\n",
      "2024-01-11 06:41:58.497018: Pseudo dice [0.8893, 0.9465, 0.9354]\n",
      "2024-01-11 06:41:58.504019: Epoch time: 39.98 s\n",
      "2024-01-11 06:41:59.958935: \n",
      "2024-01-11 06:41:59.964263: Epoch 285\n",
      "2024-01-11 06:41:59.969268: Current learning rate: 0.00739\n",
      "2024-01-11 06:42:40.046746: train_loss -0.937\n",
      "2024-01-11 06:42:40.052747: val_loss -0.8301\n",
      "2024-01-11 06:42:40.059747: Pseudo dice [0.8865, 0.9467, 0.9346]\n",
      "2024-01-11 06:42:40.065747: Epoch time: 40.09 s\n",
      "2024-01-11 06:42:41.479673: \n",
      "2024-01-11 06:42:41.489768: Epoch 286\n",
      "2024-01-11 06:42:41.493828: Current learning rate: 0.00738\n",
      "2024-01-11 06:43:21.443924: train_loss -0.9374\n",
      "2024-01-11 06:43:21.452920: val_loss -0.836\n",
      "2024-01-11 06:43:21.459921: Pseudo dice [0.8947, 0.9478, 0.9349]\n",
      "2024-01-11 06:43:21.476608: Epoch time: 39.97 s\n",
      "2024-01-11 06:43:22.919682: \n",
      "2024-01-11 06:43:22.924265: Epoch 287\n",
      "2024-01-11 06:43:22.930004: Current learning rate: 0.00738\n",
      "2024-01-11 06:44:03.039410: train_loss -0.9372\n",
      "2024-01-11 06:44:03.047410: val_loss -0.8365\n",
      "2024-01-11 06:44:03.052916: Pseudo dice [0.8935, 0.9494, 0.9379]\n",
      "2024-01-11 06:44:03.058923: Epoch time: 40.12 s\n",
      "2024-01-11 06:44:04.556319: \n",
      "2024-01-11 06:44:04.562010: Epoch 288\n",
      "2024-01-11 06:44:04.566081: Current learning rate: 0.00737\n",
      "2024-01-11 06:44:44.630426: train_loss -0.9358\n",
      "2024-01-11 06:44:44.638428: val_loss -0.8412\n",
      "2024-01-11 06:44:44.645623: Pseudo dice [0.8931, 0.95, 0.9367]\n",
      "2024-01-11 06:44:44.650625: Epoch time: 40.08 s\n",
      "2024-01-11 06:44:46.082447: \n",
      "2024-01-11 06:44:46.086992: Epoch 289\n",
      "2024-01-11 06:44:46.093031: Current learning rate: 0.00736\n",
      "2024-01-11 06:45:26.062625: train_loss -0.9365\n",
      "2024-01-11 06:45:26.070629: val_loss -0.8342\n",
      "2024-01-11 06:45:26.079650: Pseudo dice [0.8864, 0.9489, 0.9379]\n",
      "2024-01-11 06:45:26.087645: Epoch time: 39.98 s\n",
      "2024-01-11 06:45:27.565511: \n",
      "2024-01-11 06:45:27.569511: Epoch 290\n",
      "2024-01-11 06:45:27.573512: Current learning rate: 0.00735\n",
      "2024-01-11 06:46:07.758831: train_loss -0.9372\n",
      "2024-01-11 06:46:07.765832: val_loss -0.835\n",
      "2024-01-11 06:46:07.770832: Pseudo dice [0.8878, 0.949, 0.9369]\n",
      "2024-01-11 06:46:07.824514: Epoch time: 40.2 s\n",
      "2024-01-11 06:46:09.170648: \n",
      "2024-01-11 06:46:09.176636: Epoch 291\n",
      "2024-01-11 06:46:09.180633: Current learning rate: 0.00734\n",
      "2024-01-11 06:46:49.255092: train_loss -0.9364\n",
      "2024-01-11 06:46:49.263093: val_loss -0.8349\n",
      "2024-01-11 06:46:49.273090: Pseudo dice [0.8914, 0.9482, 0.9366]\n",
      "2024-01-11 06:46:49.304092: Epoch time: 40.09 s\n",
      "2024-01-11 06:46:50.687865: \n",
      "2024-01-11 06:46:50.698609: Epoch 292\n",
      "2024-01-11 06:46:50.710640: Current learning rate: 0.00733\n",
      "2024-01-11 06:47:30.700602: train_loss -0.937\n",
      "2024-01-11 06:47:30.707601: val_loss -0.8464\n",
      "2024-01-11 06:47:30.715992: Pseudo dice [0.897, 0.9513, 0.9407]\n",
      "2024-01-11 06:47:30.742683: Epoch time: 40.02 s\n",
      "2024-01-11 06:47:32.341199: \n",
      "2024-01-11 06:47:32.346199: Epoch 293\n",
      "2024-01-11 06:47:32.350194: Current learning rate: 0.00732\n",
      "2024-01-11 06:48:12.305295: train_loss -0.9369\n",
      "2024-01-11 06:48:12.313296: val_loss -0.8399\n",
      "2024-01-11 06:48:12.320295: Pseudo dice [0.8962, 0.9492, 0.9387]\n",
      "2024-01-11 06:48:12.326295: Epoch time: 39.97 s\n",
      "2024-01-11 06:48:13.768690: \n",
      "2024-01-11 06:48:13.775715: Epoch 294\n",
      "2024-01-11 06:48:13.783857: Current learning rate: 0.00731\n",
      "2024-01-11 06:48:53.716050: train_loss -0.9375\n",
      "2024-01-11 06:48:53.724052: val_loss -0.8385\n",
      "2024-01-11 06:48:53.733050: Pseudo dice [0.8906, 0.949, 0.9383]\n",
      "2024-01-11 06:48:53.739052: Epoch time: 39.95 s\n",
      "2024-01-11 06:48:55.202730: \n",
      "2024-01-11 06:48:55.210730: Epoch 295\n",
      "2024-01-11 06:48:55.215236: Current learning rate: 0.0073\n",
      "2024-01-11 06:49:35.279523: train_loss -0.9369\n",
      "2024-01-11 06:49:35.287523: val_loss -0.8383\n",
      "2024-01-11 06:49:35.294522: Pseudo dice [0.8969, 0.9487, 0.9371]\n",
      "2024-01-11 06:49:35.299525: Epoch time: 40.08 s\n",
      "2024-01-11 06:49:36.693002: \n",
      "2024-01-11 06:49:36.697595: Epoch 296\n",
      "2024-01-11 06:49:36.701587: Current learning rate: 0.00729\n",
      "2024-01-11 06:50:16.743849: train_loss -0.9366\n",
      "2024-01-11 06:50:16.749851: val_loss -0.8414\n",
      "2024-01-11 06:50:16.754854: Pseudo dice [0.8941, 0.95, 0.9391]\n",
      "2024-01-11 06:50:16.762373: Epoch time: 40.05 s\n",
      "2024-01-11 06:50:18.195839: \n",
      "2024-01-11 06:50:18.201819: Epoch 297\n",
      "2024-01-11 06:50:18.206763: Current learning rate: 0.00728\n",
      "2024-01-11 06:50:58.196205: train_loss -0.9373\n",
      "2024-01-11 06:50:58.207158: val_loss -0.8379\n",
      "2024-01-11 06:50:58.215157: Pseudo dice [0.8953, 0.9488, 0.9368]\n",
      "2024-01-11 06:50:58.248158: Epoch time: 40.0 s\n",
      "2024-01-11 06:50:59.799045: \n",
      "2024-01-11 06:50:59.805289: Epoch 298\n",
      "2024-01-11 06:50:59.811360: Current learning rate: 0.00727\n",
      "2024-01-11 06:51:39.766566: train_loss -0.9374\n",
      "2024-01-11 06:51:39.775570: val_loss -0.8339\n",
      "2024-01-11 06:51:39.783569: Pseudo dice [0.8912, 0.9483, 0.9364]\n",
      "2024-01-11 06:51:39.790087: Epoch time: 39.97 s\n",
      "2024-01-11 06:51:41.263579: \n",
      "2024-01-11 06:51:41.270655: Epoch 299\n",
      "2024-01-11 06:51:41.274577: Current learning rate: 0.00726\n",
      "2024-01-11 06:52:21.205211: train_loss -0.9374\n",
      "2024-01-11 06:52:21.216213: val_loss -0.8395\n",
      "2024-01-11 06:52:21.222207: Pseudo dice [0.8918, 0.9497, 0.9379]\n",
      "2024-01-11 06:52:21.247263: Epoch time: 39.94 s\n",
      "2024-01-11 06:52:22.920427: \n",
      "2024-01-11 06:52:22.927435: Epoch 300\n",
      "2024-01-11 06:52:22.931950: Current learning rate: 0.00725\n",
      "2024-01-11 06:53:03.066318: train_loss -0.9376\n",
      "2024-01-11 06:53:03.076831: val_loss -0.8306\n",
      "2024-01-11 06:53:03.086828: Pseudo dice [0.8946, 0.9472, 0.9339]\n",
      "2024-01-11 06:53:03.094831: Epoch time: 40.15 s\n",
      "2024-01-11 06:53:04.566808: \n",
      "2024-01-11 06:53:04.575466: Epoch 301\n",
      "2024-01-11 06:53:04.579988: Current learning rate: 0.00724\n",
      "2024-01-11 06:53:44.479656: train_loss -0.9376\n",
      "2024-01-11 06:53:44.488660: val_loss -0.8388\n",
      "2024-01-11 06:53:44.496159: Pseudo dice [0.8955, 0.9492, 0.9367]\n",
      "2024-01-11 06:53:44.502164: Epoch time: 39.91 s\n",
      "2024-01-11 06:53:45.903230: \n",
      "2024-01-11 06:53:45.909862: Epoch 302\n",
      "2024-01-11 06:53:45.919814: Current learning rate: 0.00724\n",
      "2024-01-11 06:54:25.916800: train_loss -0.9377\n",
      "2024-01-11 06:54:25.945798: val_loss -0.8304\n",
      "2024-01-11 06:54:25.953799: Pseudo dice [0.8885, 0.9473, 0.9353]\n",
      "2024-01-11 06:54:25.961799: Epoch time: 40.01 s\n",
      "2024-01-11 06:54:27.508890: \n",
      "2024-01-11 06:54:27.519157: Epoch 303\n",
      "2024-01-11 06:54:27.527153: Current learning rate: 0.00723\n",
      "2024-01-11 06:55:07.672137: train_loss -0.9383\n",
      "2024-01-11 06:55:07.679923: val_loss -0.8352\n",
      "2024-01-11 06:55:07.686929: Pseudo dice [0.8926, 0.9497, 0.9372]\n",
      "2024-01-11 06:55:07.693929: Epoch time: 40.16 s\n",
      "2024-01-11 06:55:09.105431: \n",
      "2024-01-11 06:55:09.110920: Epoch 304\n",
      "2024-01-11 06:55:09.115921: Current learning rate: 0.00722\n",
      "2024-01-11 06:55:48.918950: train_loss -0.9379\n",
      "2024-01-11 06:55:48.952963: val_loss -0.8385\n",
      "2024-01-11 06:55:48.963479: Pseudo dice [0.8913, 0.9498, 0.9391]\n",
      "2024-01-11 06:55:48.968476: Epoch time: 39.81 s\n",
      "2024-01-11 06:55:50.475431: \n",
      "2024-01-11 06:55:50.481321: Epoch 305\n",
      "2024-01-11 06:55:50.489328: Current learning rate: 0.00721\n",
      "2024-01-11 06:56:30.479946: train_loss -0.9372\n",
      "2024-01-11 06:56:30.487953: val_loss -0.8312\n",
      "2024-01-11 06:56:30.492961: Pseudo dice [0.8863, 0.9475, 0.9363]\n",
      "2024-01-11 06:56:30.499953: Epoch time: 40.01 s\n",
      "2024-01-11 06:56:31.890832: \n",
      "2024-01-11 06:56:31.897909: Epoch 306\n",
      "2024-01-11 06:56:31.903498: Current learning rate: 0.0072\n",
      "2024-01-11 06:57:11.763787: train_loss -0.9371\n",
      "2024-01-11 06:57:11.795788: val_loss -0.8367\n",
      "2024-01-11 06:57:11.802787: Pseudo dice [0.8914, 0.949, 0.9373]\n",
      "2024-01-11 06:57:11.808797: Epoch time: 39.87 s\n",
      "2024-01-11 06:57:13.244022: \n",
      "2024-01-11 06:57:13.250019: Epoch 307\n",
      "2024-01-11 06:57:13.255018: Current learning rate: 0.00719\n",
      "2024-01-11 06:57:53.186645: train_loss -0.9379\n",
      "2024-01-11 06:57:53.194645: val_loss -0.8356\n",
      "2024-01-11 06:57:53.201645: Pseudo dice [0.8895, 0.9488, 0.9366]\n",
      "2024-01-11 06:57:53.206645: Epoch time: 39.94 s\n",
      "2024-01-11 06:57:54.794310: \n",
      "2024-01-11 06:57:54.800472: Epoch 308\n",
      "2024-01-11 06:57:54.804545: Current learning rate: 0.00718\n",
      "2024-01-11 06:58:34.898069: train_loss -0.9373\n",
      "2024-01-11 06:58:34.906070: val_loss -0.8335\n",
      "2024-01-11 06:58:34.914914: Pseudo dice [0.8918, 0.9477, 0.9345]\n",
      "2024-01-11 06:58:34.936428: Epoch time: 40.11 s\n",
      "2024-01-11 06:58:36.354572: \n",
      "2024-01-11 06:58:36.359642: Epoch 309\n",
      "2024-01-11 06:58:36.367314: Current learning rate: 0.00717\n",
      "2024-01-11 06:59:16.433578: train_loss -0.9389\n",
      "2024-01-11 06:59:16.442578: val_loss -0.8311\n",
      "2024-01-11 06:59:16.452085: Pseudo dice [0.8917, 0.9468, 0.9337]\n",
      "2024-01-11 06:59:16.459095: Epoch time: 40.08 s\n",
      "2024-01-11 06:59:17.896902: \n",
      "2024-01-11 06:59:17.901441: Epoch 310\n",
      "2024-01-11 06:59:17.905441: Current learning rate: 0.00716\n",
      "2024-01-11 06:59:57.895987: train_loss -0.9386\n",
      "2024-01-11 06:59:57.903466: val_loss -0.8364\n",
      "2024-01-11 06:59:57.910486: Pseudo dice [0.8935, 0.9495, 0.9362]\n",
      "2024-01-11 06:59:57.918484: Epoch time: 40.0 s\n",
      "2024-01-11 06:59:59.332128: \n",
      "2024-01-11 06:59:59.341129: Epoch 311\n",
      "2024-01-11 06:59:59.345129: Current learning rate: 0.00715\n",
      "2024-01-11 07:00:39.307090: train_loss -0.9384\n",
      "2024-01-11 07:00:39.316618: val_loss -0.8375\n",
      "2024-01-11 07:00:39.323128: Pseudo dice [0.8929, 0.9485, 0.9362]\n",
      "2024-01-11 07:00:39.353181: Epoch time: 39.98 s\n",
      "2024-01-11 07:00:40.833846: \n",
      "2024-01-11 07:00:40.839741: Epoch 312\n",
      "2024-01-11 07:00:40.846802: Current learning rate: 0.00714\n",
      "2024-01-11 07:01:20.796553: train_loss -0.9381\n",
      "2024-01-11 07:01:20.807565: val_loss -0.8296\n",
      "2024-01-11 07:01:20.814558: Pseudo dice [0.8888, 0.9458, 0.9339]\n",
      "2024-01-11 07:01:20.824557: Epoch time: 39.96 s\n",
      "2024-01-11 07:01:22.398364: \n",
      "2024-01-11 07:01:22.404335: Epoch 313\n",
      "2024-01-11 07:01:22.409302: Current learning rate: 0.00713\n",
      "2024-01-11 07:02:02.464775: train_loss -0.939\n",
      "2024-01-11 07:02:02.496282: val_loss -0.8417\n",
      "2024-01-11 07:02:02.503283: Pseudo dice [0.8964, 0.9496, 0.9381]\n",
      "2024-01-11 07:02:02.507282: Epoch time: 40.07 s\n",
      "2024-01-11 07:02:03.980899: \n",
      "2024-01-11 07:02:03.988574: Epoch 314\n",
      "2024-01-11 07:02:03.995562: Current learning rate: 0.00712\n",
      "2024-01-11 07:02:44.109736: train_loss -0.938\n",
      "2024-01-11 07:02:44.115737: val_loss -0.8367\n",
      "2024-01-11 07:02:44.124737: Pseudo dice [0.8897, 0.9492, 0.9371]\n",
      "2024-01-11 07:02:44.131738: Epoch time: 40.13 s\n",
      "2024-01-11 07:02:45.548339: \n",
      "2024-01-11 07:02:45.553403: Epoch 315\n",
      "2024-01-11 07:02:45.558339: Current learning rate: 0.00711\n",
      "2024-01-11 07:03:25.473906: train_loss -0.9383\n",
      "2024-01-11 07:03:25.480911: val_loss -0.8375\n",
      "2024-01-11 07:03:25.485909: Pseudo dice [0.8921, 0.9485, 0.9361]\n",
      "2024-01-11 07:03:25.494904: Epoch time: 39.93 s\n",
      "2024-01-11 07:03:26.932821: \n",
      "2024-01-11 07:03:26.938671: Epoch 316\n",
      "2024-01-11 07:03:26.946757: Current learning rate: 0.0071\n",
      "2024-01-11 07:04:06.829848: train_loss -0.938\n",
      "2024-01-11 07:04:06.841360: val_loss -0.834\n",
      "2024-01-11 07:04:06.849361: Pseudo dice [0.8909, 0.9487, 0.9366]\n",
      "2024-01-11 07:04:06.856361: Epoch time: 39.9 s\n",
      "2024-01-11 07:04:08.347349: \n",
      "2024-01-11 07:04:08.355342: Epoch 317\n",
      "2024-01-11 07:04:08.359343: Current learning rate: 0.0071\n",
      "2024-01-11 07:04:48.270936: train_loss -0.9367\n",
      "2024-01-11 07:04:48.278933: val_loss -0.8364\n",
      "2024-01-11 07:04:48.305933: Pseudo dice [0.8948, 0.948, 0.937]\n",
      "2024-01-11 07:04:48.322441: Epoch time: 39.92 s\n",
      "2024-01-11 07:04:49.862551: \n",
      "2024-01-11 07:04:49.867715: Epoch 318\n",
      "2024-01-11 07:04:49.871777: Current learning rate: 0.00709\n",
      "2024-01-11 07:05:30.111533: train_loss -0.9371\n",
      "2024-01-11 07:05:30.119534: val_loss -0.8334\n",
      "2024-01-11 07:05:30.126534: Pseudo dice [0.8912, 0.9474, 0.9357]\n",
      "2024-01-11 07:05:30.132534: Epoch time: 40.25 s\n",
      "2024-01-11 07:05:31.575339: \n",
      "2024-01-11 07:05:31.580406: Epoch 319\n",
      "2024-01-11 07:05:31.584412: Current learning rate: 0.00708\n",
      "2024-01-11 07:06:11.703959: train_loss -0.937\n",
      "2024-01-11 07:06:11.715960: val_loss -0.8321\n",
      "2024-01-11 07:06:11.721470: Pseudo dice [0.8925, 0.9478, 0.9353]\n",
      "2024-01-11 07:06:11.726474: Epoch time: 40.13 s\n",
      "2024-01-11 07:06:13.164503: \n",
      "2024-01-11 07:06:13.171448: Epoch 320\n",
      "2024-01-11 07:06:13.176513: Current learning rate: 0.00707\n",
      "2024-01-11 07:06:53.142348: train_loss -0.939\n",
      "2024-01-11 07:06:53.150349: val_loss -0.8333\n",
      "2024-01-11 07:06:53.182347: Pseudo dice [0.8939, 0.9478, 0.935]\n",
      "2024-01-11 07:06:53.191346: Epoch time: 39.98 s\n",
      "2024-01-11 07:06:54.589628: \n",
      "2024-01-11 07:06:54.596703: Epoch 321\n",
      "2024-01-11 07:06:54.605632: Current learning rate: 0.00706\n",
      "2024-01-11 07:07:34.548895: train_loss -0.9371\n",
      "2024-01-11 07:07:34.555899: val_loss -0.8264\n",
      "2024-01-11 07:07:34.589757: Pseudo dice [0.8888, 0.946, 0.9326]\n",
      "2024-01-11 07:07:34.643335: Epoch time: 39.96 s\n",
      "2024-01-11 07:07:35.986506: \n",
      "2024-01-11 07:07:35.991693: Epoch 322\n",
      "2024-01-11 07:07:35.995687: Current learning rate: 0.00705\n",
      "2024-01-11 07:08:15.956885: train_loss -0.9373\n",
      "2024-01-11 07:08:15.966886: val_loss -0.832\n",
      "2024-01-11 07:08:15.976406: Pseudo dice [0.8901, 0.9476, 0.9363]\n",
      "2024-01-11 07:08:15.984402: Epoch time: 39.97 s\n",
      "2024-01-11 07:08:17.464880: \n",
      "2024-01-11 07:08:17.471360: Epoch 323\n",
      "2024-01-11 07:08:17.479105: Current learning rate: 0.00704\n",
      "2024-01-11 07:08:57.446213: train_loss -0.9382\n",
      "2024-01-11 07:08:57.455214: val_loss -0.8382\n",
      "2024-01-11 07:08:57.462214: Pseudo dice [0.8959, 0.9497, 0.9375]\n",
      "2024-01-11 07:08:57.467218: Epoch time: 39.98 s\n",
      "2024-01-11 07:08:58.983682: \n",
      "2024-01-11 07:08:58.989456: Epoch 324\n",
      "2024-01-11 07:08:58.994458: Current learning rate: 0.00703\n",
      "2024-01-11 07:09:38.946223: train_loss -0.939\n",
      "2024-01-11 07:09:38.955739: val_loss -0.8342\n",
      "2024-01-11 07:09:38.964740: Pseudo dice [0.8946, 0.9474, 0.9354]\n",
      "2024-01-11 07:09:38.993182: Epoch time: 39.96 s\n",
      "2024-01-11 07:09:40.432905: \n",
      "2024-01-11 07:09:40.438930: Epoch 325\n",
      "2024-01-11 07:09:40.443930: Current learning rate: 0.00702\n",
      "2024-01-11 07:10:20.471627: train_loss -0.9387\n",
      "2024-01-11 07:10:20.481628: val_loss -0.8293\n",
      "2024-01-11 07:10:20.488640: Pseudo dice [0.89, 0.9463, 0.9336]\n",
      "2024-01-11 07:10:20.495627: Epoch time: 40.04 s\n",
      "2024-01-11 07:10:21.922302: \n",
      "2024-01-11 07:10:21.929003: Epoch 326\n",
      "2024-01-11 07:10:21.934554: Current learning rate: 0.00701\n",
      "2024-01-11 07:11:02.093523: train_loss -0.9391\n",
      "2024-01-11 07:11:02.104517: val_loss -0.8322\n",
      "2024-01-11 07:11:02.111520: Pseudo dice [0.8884, 0.9478, 0.9364]\n",
      "2024-01-11 07:11:02.117515: Epoch time: 40.17 s\n",
      "2024-01-11 07:11:03.587539: \n",
      "2024-01-11 07:11:03.598480: Epoch 327\n",
      "2024-01-11 07:11:03.606536: Current learning rate: 0.007\n",
      "2024-01-11 07:11:43.851407: train_loss -0.9391\n",
      "2024-01-11 07:11:43.862411: val_loss -0.8323\n",
      "2024-01-11 07:11:43.868412: Pseudo dice [0.8913, 0.9481, 0.9364]\n",
      "2024-01-11 07:11:43.876932: Epoch time: 40.27 s\n",
      "2024-01-11 07:11:45.413461: \n",
      "2024-01-11 07:11:45.420471: Epoch 328\n",
      "2024-01-11 07:11:45.425450: Current learning rate: 0.00699\n",
      "2024-01-11 07:12:25.468433: train_loss -0.9384\n",
      "2024-01-11 07:12:25.502951: val_loss -0.833\n",
      "2024-01-11 07:12:25.510949: Pseudo dice [0.8901, 0.9476, 0.9359]\n",
      "2024-01-11 07:12:25.518948: Epoch time: 40.06 s\n",
      "2024-01-11 07:12:27.038364: \n",
      "2024-01-11 07:12:27.044453: Epoch 329\n",
      "2024-01-11 07:12:27.052386: Current learning rate: 0.00698\n",
      "2024-01-11 07:13:07.148340: train_loss -0.9398\n",
      "2024-01-11 07:13:07.155342: val_loss -0.836\n",
      "2024-01-11 07:13:07.162341: Pseudo dice [0.8943, 0.9486, 0.9361]\n",
      "2024-01-11 07:13:07.169340: Epoch time: 40.11 s\n",
      "2024-01-11 07:13:08.609381: \n",
      "2024-01-11 07:13:08.616049: Epoch 330\n",
      "2024-01-11 07:13:08.623190: Current learning rate: 0.00697\n",
      "2024-01-11 07:13:48.423435: train_loss -0.9399\n",
      "2024-01-11 07:13:48.431435: val_loss -0.8367\n",
      "2024-01-11 07:13:48.438435: Pseudo dice [0.8914, 0.949, 0.9381]\n",
      "2024-01-11 07:13:48.445441: Epoch time: 39.82 s\n",
      "2024-01-11 07:13:49.859336: \n",
      "2024-01-11 07:13:49.865354: Epoch 331\n",
      "2024-01-11 07:13:49.874363: Current learning rate: 0.00696\n",
      "2024-01-11 07:14:29.880264: train_loss -0.9389\n",
      "2024-01-11 07:14:29.888264: val_loss -0.8294\n",
      "2024-01-11 07:14:29.894262: Pseudo dice [0.896, 0.9473, 0.934]\n",
      "2024-01-11 07:14:29.902262: Epoch time: 40.02 s\n",
      "2024-01-11 07:14:31.281055: \n",
      "2024-01-11 07:14:31.285999: Epoch 332\n",
      "2024-01-11 07:14:31.291078: Current learning rate: 0.00696\n",
      "2024-01-11 07:15:11.518248: train_loss -0.9399\n",
      "2024-01-11 07:15:11.525249: val_loss -0.8347\n",
      "2024-01-11 07:15:11.530251: Pseudo dice [0.8922, 0.9481, 0.9361]\n",
      "2024-01-11 07:15:11.536249: Epoch time: 40.24 s\n",
      "2024-01-11 07:15:13.150804: \n",
      "2024-01-11 07:15:13.158751: Epoch 333\n",
      "2024-01-11 07:15:13.163752: Current learning rate: 0.00695\n",
      "2024-01-11 07:15:53.103691: train_loss -0.9391\n",
      "2024-01-11 07:15:53.114693: val_loss -0.8363\n",
      "2024-01-11 07:15:53.142206: Pseudo dice [0.8933, 0.9482, 0.9356]\n",
      "2024-01-11 07:15:53.152206: Epoch time: 39.95 s\n",
      "2024-01-11 07:15:54.588930: \n",
      "2024-01-11 07:15:54.593990: Epoch 334\n",
      "2024-01-11 07:15:54.598068: Current learning rate: 0.00694\n",
      "2024-01-11 07:16:34.688586: train_loss -0.9389\n",
      "2024-01-11 07:16:34.696585: val_loss -0.8397\n",
      "2024-01-11 07:16:34.701586: Pseudo dice [0.8919, 0.9495, 0.9377]\n",
      "2024-01-11 07:16:34.707584: Epoch time: 40.1 s\n",
      "2024-01-11 07:16:36.268968: \n",
      "2024-01-11 07:16:36.273969: Epoch 335\n",
      "2024-01-11 07:16:36.278968: Current learning rate: 0.00693\n",
      "2024-01-11 07:17:16.248332: train_loss -0.9391\n",
      "2024-01-11 07:17:16.257838: val_loss -0.8367\n",
      "2024-01-11 07:17:16.266347: Pseudo dice [0.8914, 0.9489, 0.937]\n",
      "2024-01-11 07:17:16.272349: Epoch time: 39.98 s\n",
      "2024-01-11 07:17:17.663544: \n",
      "2024-01-11 07:17:17.670390: Epoch 336\n",
      "2024-01-11 07:17:17.677478: Current learning rate: 0.00692\n",
      "2024-01-11 07:17:57.779948: train_loss -0.9393\n",
      "2024-01-11 07:17:57.785953: val_loss -0.8394\n",
      "2024-01-11 07:17:57.793952: Pseudo dice [0.8941, 0.9495, 0.9371]\n",
      "2024-01-11 07:17:57.801464: Epoch time: 40.12 s\n",
      "2024-01-11 07:17:59.322258: \n",
      "2024-01-11 07:17:59.329203: Epoch 337\n",
      "2024-01-11 07:17:59.333265: Current learning rate: 0.00691\n",
      "2024-01-11 07:18:39.423136: train_loss -0.9385\n",
      "2024-01-11 07:18:39.455140: val_loss -0.8318\n",
      "2024-01-11 07:18:39.464138: Pseudo dice [0.8877, 0.947, 0.9346]\n",
      "2024-01-11 07:18:39.471138: Epoch time: 40.1 s\n",
      "2024-01-11 07:18:41.038960: \n",
      "2024-01-11 07:18:41.044450: Epoch 338\n",
      "2024-01-11 07:18:41.050512: Current learning rate: 0.0069\n",
      "2024-01-11 07:19:21.099477: train_loss -0.938\n",
      "2024-01-11 07:19:21.106478: val_loss -0.8356\n",
      "2024-01-11 07:19:21.114476: Pseudo dice [0.8927, 0.9479, 0.9349]\n",
      "2024-01-11 07:19:21.120480: Epoch time: 40.06 s\n",
      "2024-01-11 07:19:22.570945: \n",
      "2024-01-11 07:19:22.576491: Epoch 339\n",
      "2024-01-11 07:19:22.582555: Current learning rate: 0.00689\n",
      "2024-01-11 07:20:02.601636: train_loss -0.9399\n",
      "2024-01-11 07:20:02.609632: val_loss -0.839\n",
      "2024-01-11 07:20:02.617633: Pseudo dice [0.8981, 0.9493, 0.9379]\n",
      "2024-01-11 07:20:02.624633: Epoch time: 40.03 s\n",
      "2024-01-11 07:20:04.104158: \n",
      "2024-01-11 07:20:04.110221: Epoch 340\n",
      "2024-01-11 07:20:04.115158: Current learning rate: 0.00688\n",
      "2024-01-11 07:20:44.190030: train_loss -0.9387\n",
      "2024-01-11 07:20:44.198030: val_loss -0.8352\n",
      "2024-01-11 07:20:44.205030: Pseudo dice [0.8943, 0.9479, 0.9365]\n",
      "2024-01-11 07:20:44.211030: Epoch time: 40.09 s\n",
      "2024-01-11 07:20:45.710474: \n",
      "2024-01-11 07:20:45.721565: Epoch 341\n",
      "2024-01-11 07:20:45.726547: Current learning rate: 0.00687\n",
      "2024-01-11 07:21:25.679328: train_loss -0.9387\n",
      "2024-01-11 07:21:25.687333: val_loss -0.8411\n",
      "2024-01-11 07:21:25.695330: Pseudo dice [0.8952, 0.9495, 0.9383]\n",
      "2024-01-11 07:21:25.700331: Epoch time: 39.97 s\n",
      "2024-01-11 07:21:27.162790: \n",
      "2024-01-11 07:21:27.172789: Epoch 342\n",
      "2024-01-11 07:21:27.179790: Current learning rate: 0.00686\n",
      "2024-01-11 07:22:07.217203: train_loss -0.9399\n",
      "2024-01-11 07:22:07.224205: val_loss -0.835\n",
      "2024-01-11 07:22:07.234204: Pseudo dice [0.8897, 0.9488, 0.9355]\n",
      "2024-01-11 07:22:07.257727: Epoch time: 40.06 s\n",
      "2024-01-11 07:22:08.806341: \n",
      "2024-01-11 07:22:08.814323: Epoch 343\n",
      "2024-01-11 07:22:08.819332: Current learning rate: 0.00685\n",
      "2024-01-11 07:22:48.760809: train_loss -0.9406\n",
      "2024-01-11 07:22:48.769811: val_loss -0.8377\n",
      "2024-01-11 07:22:48.775326: Pseudo dice [0.8935, 0.9487, 0.9369]\n",
      "2024-01-11 07:22:48.780333: Epoch time: 39.96 s\n",
      "2024-01-11 07:22:50.196817: \n",
      "2024-01-11 07:22:50.202820: Epoch 344\n",
      "2024-01-11 07:22:50.206816: Current learning rate: 0.00684\n",
      "2024-01-11 07:23:30.208789: train_loss -0.9389\n",
      "2024-01-11 07:23:30.217789: val_loss -0.8321\n",
      "2024-01-11 07:23:30.225790: Pseudo dice [0.8935, 0.9473, 0.9331]\n",
      "2024-01-11 07:23:30.233798: Epoch time: 40.01 s\n",
      "2024-01-11 07:23:31.677156: \n",
      "2024-01-11 07:23:31.683252: Epoch 345\n",
      "2024-01-11 07:23:31.688359: Current learning rate: 0.00683\n",
      "2024-01-11 07:24:11.796023: train_loss -0.9405\n",
      "2024-01-11 07:24:11.807019: val_loss -0.8397\n",
      "2024-01-11 07:24:11.813056: Pseudo dice [0.896, 0.949, 0.9381]\n",
      "2024-01-11 07:24:11.818031: Epoch time: 40.12 s\n",
      "2024-01-11 07:24:13.232893: \n",
      "2024-01-11 07:24:13.238912: Epoch 346\n",
      "2024-01-11 07:24:13.243891: Current learning rate: 0.00682\n",
      "2024-01-11 07:24:53.269793: train_loss -0.9392\n",
      "2024-01-11 07:24:53.277793: val_loss -0.8326\n",
      "2024-01-11 07:24:53.284793: Pseudo dice [0.8904, 0.9468, 0.9345]\n",
      "2024-01-11 07:24:53.289796: Epoch time: 40.04 s\n",
      "2024-01-11 07:24:54.784961: \n",
      "2024-01-11 07:24:54.790962: Epoch 347\n",
      "2024-01-11 07:24:54.795966: Current learning rate: 0.00681\n",
      "2024-01-11 07:25:35.156187: train_loss -0.9401\n",
      "2024-01-11 07:25:35.166698: val_loss -0.8329\n",
      "2024-01-11 07:25:35.196700: Pseudo dice [0.8876, 0.9482, 0.9354]\n",
      "2024-01-11 07:25:35.202701: Epoch time: 40.37 s\n",
      "2024-01-11 07:25:36.568120: \n",
      "2024-01-11 07:25:36.573922: Epoch 348\n",
      "2024-01-11 07:25:36.577924: Current learning rate: 0.0068\n",
      "2024-01-11 07:26:16.783041: train_loss -0.9405\n",
      "2024-01-11 07:26:16.794042: val_loss -0.833\n",
      "2024-01-11 07:26:16.799042: Pseudo dice [0.8911, 0.9482, 0.9357]\n",
      "2024-01-11 07:26:16.804041: Epoch time: 40.22 s\n",
      "2024-01-11 07:26:18.251734: \n",
      "2024-01-11 07:26:18.258015: Epoch 349\n",
      "2024-01-11 07:26:18.263003: Current learning rate: 0.0068\n",
      "2024-01-11 07:26:58.458581: train_loss -0.941\n",
      "2024-01-11 07:26:58.470577: val_loss -0.8302\n",
      "2024-01-11 07:26:58.478576: Pseudo dice [0.8892, 0.9474, 0.9348]\n",
      "2024-01-11 07:26:58.484576: Epoch time: 40.21 s\n",
      "2024-01-11 07:27:00.050838: \n",
      "2024-01-11 07:27:00.056303: Epoch 350\n",
      "2024-01-11 07:27:00.060322: Current learning rate: 0.00679\n",
      "2024-01-11 07:27:40.137050: train_loss -0.9398\n",
      "2024-01-11 07:27:40.145559: val_loss -0.8331\n",
      "2024-01-11 07:27:40.153558: Pseudo dice [0.8918, 0.9474, 0.9348]\n",
      "2024-01-11 07:27:40.160558: Epoch time: 40.09 s\n",
      "2024-01-11 07:27:41.653380: \n",
      "2024-01-11 07:27:41.662379: Epoch 351\n",
      "2024-01-11 07:27:41.666704: Current learning rate: 0.00678\n",
      "2024-01-11 07:28:21.700942: train_loss -0.9411\n",
      "2024-01-11 07:28:21.728941: val_loss -0.8307\n",
      "2024-01-11 07:28:21.735941: Pseudo dice [0.8891, 0.9473, 0.9357]\n",
      "2024-01-11 07:28:21.742942: Epoch time: 40.05 s\n",
      "2024-01-11 07:28:23.327320: \n",
      "2024-01-11 07:28:23.335321: Epoch 352\n",
      "2024-01-11 07:28:23.339321: Current learning rate: 0.00677\n",
      "2024-01-11 07:29:03.457208: train_loss -0.9403\n",
      "2024-01-11 07:29:03.466213: val_loss -0.8287\n",
      "2024-01-11 07:29:03.474211: Pseudo dice [0.891, 0.9479, 0.9347]\n",
      "2024-01-11 07:29:03.480208: Epoch time: 40.13 s\n",
      "2024-01-11 07:29:04.945607: \n",
      "2024-01-11 07:29:04.950183: Epoch 353\n",
      "2024-01-11 07:29:04.955195: Current learning rate: 0.00676\n",
      "2024-01-11 07:29:44.980117: train_loss -0.9416\n",
      "2024-01-11 07:29:44.989118: val_loss -0.8386\n",
      "2024-01-11 07:29:44.996120: Pseudo dice [0.8942, 0.9489, 0.9377]\n",
      "2024-01-11 07:29:45.003117: Epoch time: 40.04 s\n",
      "2024-01-11 07:29:46.429160: \n",
      "2024-01-11 07:29:46.434630: Epoch 354\n",
      "2024-01-11 07:29:46.442698: Current learning rate: 0.00675\n",
      "2024-01-11 07:30:26.582973: train_loss -0.9396\n",
      "2024-01-11 07:30:26.592973: val_loss -0.8294\n",
      "2024-01-11 07:30:26.597972: Pseudo dice [0.8869, 0.9486, 0.9361]\n",
      "2024-01-11 07:30:26.602973: Epoch time: 40.15 s\n",
      "2024-01-11 07:30:28.079106: \n",
      "2024-01-11 07:30:28.085059: Epoch 355\n",
      "2024-01-11 07:30:28.091059: Current learning rate: 0.00674\n",
      "2024-01-11 07:31:08.286091: train_loss -0.9394\n",
      "2024-01-11 07:31:08.296091: val_loss -0.8362\n",
      "2024-01-11 07:31:08.302096: Pseudo dice [0.8914, 0.9488, 0.9372]\n",
      "2024-01-11 07:31:08.307098: Epoch time: 40.21 s\n",
      "2024-01-11 07:31:09.808926: \n",
      "2024-01-11 07:31:09.814474: Epoch 356\n",
      "2024-01-11 07:31:09.821479: Current learning rate: 0.00673\n",
      "2024-01-11 07:31:49.905218: train_loss -0.9398\n",
      "2024-01-11 07:31:49.913229: val_loss -0.8356\n",
      "2024-01-11 07:31:49.919227: Pseudo dice [0.8944, 0.9487, 0.9373]\n",
      "2024-01-11 07:31:49.928219: Epoch time: 40.1 s\n",
      "2024-01-11 07:31:51.521965: \n",
      "2024-01-11 07:31:51.529041: Epoch 357\n",
      "2024-01-11 07:31:51.533108: Current learning rate: 0.00672\n",
      "2024-01-11 07:32:31.597628: train_loss -0.9401\n",
      "2024-01-11 07:32:31.605635: val_loss -0.8354\n",
      "2024-01-11 07:32:31.610640: Pseudo dice [0.8967, 0.9479, 0.9355]\n",
      "2024-01-11 07:32:31.615145: Epoch time: 40.08 s\n",
      "2024-01-11 07:32:33.107022: \n",
      "2024-01-11 07:32:33.112372: Epoch 358\n",
      "2024-01-11 07:32:33.115440: Current learning rate: 0.00671\n",
      "2024-01-11 07:33:13.083687: train_loss -0.9402\n",
      "2024-01-11 07:33:13.089688: val_loss -0.8319\n",
      "2024-01-11 07:33:13.096689: Pseudo dice [0.8887, 0.9479, 0.9357]\n",
      "2024-01-11 07:33:13.104685: Epoch time: 39.98 s\n",
      "2024-01-11 07:33:14.553339: \n",
      "2024-01-11 07:33:14.559306: Epoch 359\n",
      "2024-01-11 07:33:14.563364: Current learning rate: 0.0067\n",
      "2024-01-11 07:33:54.629890: train_loss -0.9391\n",
      "2024-01-11 07:33:54.638891: val_loss -0.8325\n",
      "2024-01-11 07:33:54.671440: Pseudo dice [0.891, 0.9483, 0.9347]\n",
      "2024-01-11 07:33:54.679431: Epoch time: 40.08 s\n",
      "2024-01-11 07:33:56.105794: \n",
      "2024-01-11 07:33:56.113954: Epoch 360\n",
      "2024-01-11 07:33:56.118994: Current learning rate: 0.00669\n",
      "2024-01-11 07:34:36.167400: train_loss -0.9394\n",
      "2024-01-11 07:34:36.174388: val_loss -0.8349\n",
      "2024-01-11 07:34:36.180388: Pseudo dice [0.893, 0.9487, 0.9366]\n",
      "2024-01-11 07:34:36.189398: Epoch time: 40.06 s\n",
      "2024-01-11 07:34:37.627109: \n",
      "2024-01-11 07:34:37.632159: Epoch 361\n",
      "2024-01-11 07:34:37.640111: Current learning rate: 0.00668\n",
      "2024-01-11 07:35:17.727978: train_loss -0.9402\n",
      "2024-01-11 07:35:17.738978: val_loss -0.8389\n",
      "2024-01-11 07:35:17.745976: Pseudo dice [0.8926, 0.9499, 0.9367]\n",
      "2024-01-11 07:35:17.777977: Epoch time: 40.1 s\n",
      "2024-01-11 07:35:19.348421: \n",
      "2024-01-11 07:35:19.356482: Epoch 362\n",
      "2024-01-11 07:35:19.360493: Current learning rate: 0.00667\n",
      "2024-01-11 07:35:59.328364: train_loss -0.94\n",
      "2024-01-11 07:35:59.351882: val_loss -0.8313\n",
      "2024-01-11 07:35:59.357884: Pseudo dice [0.8887, 0.9467, 0.9356]\n",
      "2024-01-11 07:35:59.364884: Epoch time: 39.98 s\n",
      "2024-01-11 07:36:00.830278: \n",
      "2024-01-11 07:36:00.838663: Epoch 363\n",
      "2024-01-11 07:36:00.842727: Current learning rate: 0.00666\n",
      "2024-01-11 07:36:40.915039: train_loss -0.9406\n",
      "2024-01-11 07:36:40.924038: val_loss -0.8296\n",
      "2024-01-11 07:36:40.929042: Pseudo dice [0.8817, 0.9479, 0.9348]\n",
      "2024-01-11 07:36:40.935040: Epoch time: 40.09 s\n",
      "2024-01-11 07:36:42.413893: \n",
      "2024-01-11 07:36:42.420958: Epoch 364\n",
      "2024-01-11 07:36:42.424953: Current learning rate: 0.00665\n",
      "2024-01-11 07:37:22.692909: train_loss -0.9403\n",
      "2024-01-11 07:37:22.700911: val_loss -0.8365\n",
      "2024-01-11 07:37:22.707910: Pseudo dice [0.8952, 0.9484, 0.937]\n",
      "2024-01-11 07:37:22.714911: Epoch time: 40.28 s\n",
      "2024-01-11 07:37:24.093427: \n",
      "2024-01-11 07:37:24.100411: Epoch 365\n",
      "2024-01-11 07:37:24.104571: Current learning rate: 0.00665\n",
      "2024-01-11 07:38:04.310259: train_loss -0.9408\n",
      "2024-01-11 07:38:04.319261: val_loss -0.8277\n",
      "2024-01-11 07:38:04.326257: Pseudo dice [0.883, 0.9467, 0.9359]\n",
      "2024-01-11 07:38:04.331257: Epoch time: 40.22 s\n",
      "2024-01-11 07:38:05.748397: \n",
      "2024-01-11 07:38:05.754430: Epoch 366\n",
      "2024-01-11 07:38:05.762502: Current learning rate: 0.00664\n",
      "2024-01-11 07:38:45.884743: train_loss -0.9388\n",
      "2024-01-11 07:38:45.894741: val_loss -0.831\n",
      "2024-01-11 07:38:45.902743: Pseudo dice [0.8887, 0.9479, 0.9361]\n",
      "2024-01-11 07:38:45.907748: Epoch time: 40.14 s\n",
      "2024-01-11 07:38:47.445691: \n",
      "2024-01-11 07:38:47.451850: Epoch 367\n",
      "2024-01-11 07:38:47.455908: Current learning rate: 0.00663\n",
      "2024-01-11 07:39:27.597214: train_loss -0.9389\n",
      "2024-01-11 07:39:27.607216: val_loss -0.8365\n",
      "2024-01-11 07:39:27.613214: Pseudo dice [0.8893, 0.9489, 0.937]\n",
      "2024-01-11 07:39:27.620213: Epoch time: 40.15 s\n",
      "2024-01-11 07:39:29.259864: \n",
      "2024-01-11 07:39:29.264872: Epoch 368\n",
      "2024-01-11 07:39:29.269872: Current learning rate: 0.00662\n",
      "2024-01-11 07:40:09.382519: train_loss -0.9399\n",
      "2024-01-11 07:40:09.389739: val_loss -0.8281\n",
      "2024-01-11 07:40:09.397748: Pseudo dice [0.8907, 0.9472, 0.9327]\n",
      "2024-01-11 07:40:09.404746: Epoch time: 40.12 s\n",
      "2024-01-11 07:40:10.830537: \n",
      "2024-01-11 07:40:10.836626: Epoch 369\n",
      "2024-01-11 07:40:10.844648: Current learning rate: 0.00661\n",
      "2024-01-11 07:40:51.000927: train_loss -0.9401\n",
      "2024-01-11 07:40:51.009439: val_loss -0.8377\n",
      "2024-01-11 07:40:51.018444: Pseudo dice [0.8948, 0.9488, 0.9364]\n",
      "2024-01-11 07:40:51.023440: Epoch time: 40.17 s\n",
      "2024-01-11 07:40:52.574785: \n",
      "2024-01-11 07:40:52.579782: Epoch 370\n",
      "2024-01-11 07:40:52.589857: Current learning rate: 0.0066\n",
      "2024-01-11 07:41:32.714080: train_loss -0.9397\n",
      "2024-01-11 07:41:32.723602: val_loss -0.8364\n",
      "2024-01-11 07:41:32.729599: Pseudo dice [0.8948, 0.9485, 0.9365]\n",
      "2024-01-11 07:41:32.736599: Epoch time: 40.14 s\n",
      "2024-01-11 07:41:34.109864: \n",
      "2024-01-11 07:41:34.115861: Epoch 371\n",
      "2024-01-11 07:41:34.122932: Current learning rate: 0.00659\n",
      "2024-01-11 07:42:14.163872: train_loss -0.9406\n",
      "2024-01-11 07:42:14.172870: val_loss -0.8301\n",
      "2024-01-11 07:42:14.179872: Pseudo dice [0.8879, 0.9485, 0.9364]\n",
      "2024-01-11 07:42:14.188870: Epoch time: 40.06 s\n",
      "2024-01-11 07:42:15.812284: \n",
      "2024-01-11 07:42:15.817960: Epoch 372\n",
      "2024-01-11 07:42:15.822977: Current learning rate: 0.00658\n",
      "2024-01-11 07:42:55.945520: train_loss -0.9405\n",
      "2024-01-11 07:42:55.954538: val_loss -0.8357\n",
      "2024-01-11 07:42:55.985062: Pseudo dice [0.8882, 0.9484, 0.9365]\n",
      "2024-01-11 07:42:55.995054: Epoch time: 40.13 s\n",
      "2024-01-11 07:42:57.385003: \n",
      "2024-01-11 07:42:57.392313: Epoch 373\n",
      "2024-01-11 07:42:57.397253: Current learning rate: 0.00657\n",
      "2024-01-11 07:43:37.416421: train_loss -0.9396\n",
      "2024-01-11 07:43:37.428053: val_loss -0.8365\n",
      "2024-01-11 07:43:37.435070: Pseudo dice [0.8928, 0.9495, 0.9382]\n",
      "2024-01-11 07:43:37.465061: Epoch time: 40.03 s\n",
      "2024-01-11 07:43:38.984073: \n",
      "2024-01-11 07:43:38.989026: Epoch 374\n",
      "2024-01-11 07:43:38.993093: Current learning rate: 0.00656\n",
      "2024-01-11 07:44:19.065507: train_loss -0.9405\n",
      "2024-01-11 07:44:19.074509: val_loss -0.8328\n",
      "2024-01-11 07:44:19.098505: Pseudo dice [0.8902, 0.9487, 0.9368]\n",
      "2024-01-11 07:44:19.104515: Epoch time: 40.08 s\n",
      "2024-01-11 07:44:20.561776: \n",
      "2024-01-11 07:44:20.570912: Epoch 375\n",
      "2024-01-11 07:44:20.577902: Current learning rate: 0.00655\n",
      "2024-01-11 07:45:00.965733: train_loss -0.941\n",
      "2024-01-11 07:45:00.972733: val_loss -0.8365\n",
      "2024-01-11 07:45:00.978734: Pseudo dice [0.8935, 0.9492, 0.9373]\n",
      "2024-01-11 07:45:00.985735: Epoch time: 40.4 s\n",
      "2024-01-11 07:45:02.413797: \n",
      "2024-01-11 07:45:02.419110: Epoch 376\n",
      "2024-01-11 07:45:02.423900: Current learning rate: 0.00654\n",
      "2024-01-11 07:45:42.558285: train_loss -0.9403\n",
      "2024-01-11 07:45:42.565802: val_loss -0.8359\n",
      "2024-01-11 07:45:42.571811: Pseudo dice [0.8926, 0.9489, 0.9368]\n",
      "2024-01-11 07:45:42.580801: Epoch time: 40.15 s\n",
      "2024-01-11 07:45:43.977906: \n",
      "2024-01-11 07:45:43.986576: Epoch 377\n",
      "2024-01-11 07:45:43.990646: Current learning rate: 0.00653\n",
      "2024-01-11 07:46:24.054521: train_loss -0.9415\n",
      "2024-01-11 07:46:24.063535: val_loss -0.8343\n",
      "2024-01-11 07:46:24.070532: Pseudo dice [0.893, 0.9479, 0.9358]\n",
      "2024-01-11 07:46:24.098045: Epoch time: 40.08 s\n",
      "2024-01-11 07:46:25.642696: \n",
      "2024-01-11 07:46:25.647809: Epoch 378\n",
      "2024-01-11 07:46:25.657885: Current learning rate: 0.00652\n",
      "2024-01-11 07:47:05.889799: train_loss -0.9415\n",
      "2024-01-11 07:47:05.897316: val_loss -0.8339\n",
      "2024-01-11 07:47:05.903316: Pseudo dice [0.8916, 0.9493, 0.9361]\n",
      "2024-01-11 07:47:05.909316: Epoch time: 40.25 s\n",
      "2024-01-11 07:47:07.386300: \n",
      "2024-01-11 07:47:07.395726: Epoch 379\n",
      "2024-01-11 07:47:07.403671: Current learning rate: 0.00651\n",
      "2024-01-11 07:47:47.421618: train_loss -0.9419\n",
      "2024-01-11 07:47:47.430619: val_loss -0.8285\n",
      "2024-01-11 07:47:47.438619: Pseudo dice [0.8876, 0.9478, 0.936]\n",
      "2024-01-11 07:47:47.444619: Epoch time: 40.04 s\n",
      "2024-01-11 07:47:48.813497: \n",
      "2024-01-11 07:47:48.819901: Epoch 380\n",
      "2024-01-11 07:47:48.824961: Current learning rate: 0.0065\n",
      "2024-01-11 07:48:28.871629: train_loss -0.9413\n",
      "2024-01-11 07:48:28.882629: val_loss -0.8355\n",
      "2024-01-11 07:48:28.916031: Pseudo dice [0.8926, 0.9478, 0.9351]\n",
      "2024-01-11 07:48:28.922544: Epoch time: 40.06 s\n",
      "2024-01-11 07:48:30.357874: \n",
      "2024-01-11 07:48:30.362525: Epoch 381\n",
      "2024-01-11 07:48:30.372544: Current learning rate: 0.00649\n",
      "2024-01-11 07:49:10.605775: train_loss -0.9418\n",
      "2024-01-11 07:49:10.614775: val_loss -0.8332\n",
      "2024-01-11 07:49:10.623782: Pseudo dice [0.8898, 0.9472, 0.9358]\n",
      "2024-01-11 07:49:10.629783: Epoch time: 40.25 s\n",
      "2024-01-11 07:49:12.074151: \n",
      "2024-01-11 07:49:12.079248: Epoch 382\n",
      "2024-01-11 07:49:12.083294: Current learning rate: 0.00648\n",
      "2024-01-11 07:49:52.082543: train_loss -0.9415\n",
      "2024-01-11 07:49:52.090544: val_loss -0.8295\n",
      "2024-01-11 07:49:52.122542: Pseudo dice [0.8868, 0.9484, 0.9351]\n",
      "2024-01-11 07:49:52.129543: Epoch time: 40.01 s\n",
      "2024-01-11 07:49:53.582303: \n",
      "2024-01-11 07:49:53.587390: Epoch 383\n",
      "2024-01-11 07:49:53.591461: Current learning rate: 0.00648\n",
      "2024-01-11 07:50:33.682410: train_loss -0.9413\n",
      "2024-01-11 07:50:33.691411: val_loss -0.8333\n",
      "2024-01-11 07:50:33.699410: Pseudo dice [0.8931, 0.9482, 0.9356]\n",
      "2024-01-11 07:50:33.705420: Epoch time: 40.1 s\n",
      "2024-01-11 07:50:35.277701: \n",
      "2024-01-11 07:50:35.282763: Epoch 384\n",
      "2024-01-11 07:50:35.286770: Current learning rate: 0.00647\n",
      "2024-01-11 07:51:15.419136: train_loss -0.9418\n",
      "2024-01-11 07:51:15.426136: val_loss -0.8322\n",
      "2024-01-11 07:51:15.431136: Pseudo dice [0.8887, 0.9483, 0.9357]\n",
      "2024-01-11 07:51:15.436136: Epoch time: 40.14 s\n",
      "2024-01-11 07:51:16.946340: \n",
      "2024-01-11 07:51:16.950813: Epoch 385\n",
      "2024-01-11 07:51:16.960873: Current learning rate: 0.00646\n",
      "2024-01-11 07:51:56.946718: train_loss -0.9419\n",
      "2024-01-11 07:51:56.954719: val_loss -0.8344\n",
      "2024-01-11 07:51:56.961725: Pseudo dice [0.8902, 0.9486, 0.937]\n",
      "2024-01-11 07:51:56.967720: Epoch time: 40.0 s\n",
      "2024-01-11 07:51:58.568206: \n",
      "2024-01-11 07:51:58.574067: Epoch 386\n",
      "2024-01-11 07:51:58.584229: Current learning rate: 0.00645\n",
      "2024-01-11 07:52:38.598986: train_loss -0.9418\n",
      "2024-01-11 07:52:38.633528: val_loss -0.8369\n",
      "2024-01-11 07:52:38.640526: Pseudo dice [0.8919, 0.949, 0.936]\n",
      "2024-01-11 07:52:38.646527: Epoch time: 40.03 s\n",
      "2024-01-11 07:52:40.068395: \n",
      "2024-01-11 07:52:40.074709: Epoch 387\n",
      "2024-01-11 07:52:40.081768: Current learning rate: 0.00644\n",
      "2024-01-11 07:53:20.114378: train_loss -0.9423\n",
      "2024-01-11 07:53:20.122377: val_loss -0.8385\n",
      "2024-01-11 07:53:20.129383: Pseudo dice [0.8945, 0.9489, 0.9381]\n",
      "2024-01-11 07:53:20.163893: Epoch time: 40.05 s\n",
      "2024-01-11 07:53:21.556858: \n",
      "2024-01-11 07:53:21.562883: Epoch 388\n",
      "2024-01-11 07:53:21.566880: Current learning rate: 0.00643\n",
      "2024-01-11 07:54:01.818658: train_loss -0.9417\n",
      "2024-01-11 07:54:01.828658: val_loss -0.8341\n",
      "2024-01-11 07:54:01.836662: Pseudo dice [0.8897, 0.9483, 0.9352]\n",
      "2024-01-11 07:54:01.841659: Epoch time: 40.26 s\n",
      "2024-01-11 07:54:03.339934: \n",
      "2024-01-11 07:54:03.346996: Epoch 389\n",
      "2024-01-11 07:54:03.352067: Current learning rate: 0.00642\n",
      "2024-01-11 07:54:43.612711: train_loss -0.9419\n",
      "2024-01-11 07:54:43.622712: val_loss -0.8365\n",
      "2024-01-11 07:54:43.629712: Pseudo dice [0.8944, 0.9489, 0.9372]\n",
      "2024-01-11 07:54:43.636712: Epoch time: 40.27 s\n",
      "2024-01-11 07:54:45.078879: \n",
      "2024-01-11 07:54:45.084885: Epoch 390\n",
      "2024-01-11 07:54:45.092984: Current learning rate: 0.00641\n",
      "2024-01-11 07:55:25.036122: train_loss -0.9419\n",
      "2024-01-11 07:55:25.041642: val_loss -0.8335\n",
      "2024-01-11 07:55:25.071171: Pseudo dice [0.8906, 0.9478, 0.9349]\n",
      "2024-01-11 07:55:25.078175: Epoch time: 39.96 s\n",
      "2024-01-11 07:55:26.728795: \n",
      "2024-01-11 07:55:26.734397: Epoch 391\n",
      "2024-01-11 07:55:26.745928: Current learning rate: 0.0064\n",
      "2024-01-11 07:56:06.884151: train_loss -0.9418\n",
      "2024-01-11 07:56:06.892151: val_loss -0.833\n",
      "2024-01-11 07:56:06.901151: Pseudo dice [0.8923, 0.9481, 0.9352]\n",
      "2024-01-11 07:56:06.908152: Epoch time: 40.16 s\n",
      "2024-01-11 07:56:08.436828: \n",
      "2024-01-11 07:56:08.446907: Epoch 392\n",
      "2024-01-11 07:56:08.454992: Current learning rate: 0.00639\n",
      "2024-01-11 07:56:48.326318: train_loss -0.942\n",
      "2024-01-11 07:56:48.333316: val_loss -0.8258\n",
      "2024-01-11 07:56:48.339316: Pseudo dice [0.8879, 0.9471, 0.9334]\n",
      "2024-01-11 07:56:48.344316: Epoch time: 39.89 s\n",
      "2024-01-11 07:56:49.819079: \n",
      "2024-01-11 07:56:49.823495: Epoch 393\n",
      "2024-01-11 07:56:49.828562: Current learning rate: 0.00638\n",
      "2024-01-11 07:57:29.763836: train_loss -0.9415\n",
      "2024-01-11 07:57:29.772837: val_loss -0.8297\n",
      "2024-01-11 07:57:29.780838: Pseudo dice [0.8868, 0.9471, 0.9336]\n",
      "2024-01-11 07:57:29.787836: Epoch time: 39.95 s\n",
      "2024-01-11 07:57:31.326783: \n",
      "2024-01-11 07:57:31.332850: Epoch 394\n",
      "2024-01-11 07:57:31.337847: Current learning rate: 0.00637\n",
      "2024-01-11 07:58:11.247993: train_loss -0.9421\n",
      "2024-01-11 07:58:11.255992: val_loss -0.8352\n",
      "2024-01-11 07:58:11.262993: Pseudo dice [0.8901, 0.9473, 0.9354]\n",
      "2024-01-11 07:58:11.267994: Epoch time: 39.92 s\n",
      "2024-01-11 07:58:12.783628: \n",
      "2024-01-11 07:58:12.790077: Epoch 395\n",
      "2024-01-11 07:58:12.794074: Current learning rate: 0.00636\n",
      "2024-01-11 07:58:53.008224: train_loss -0.9415\n",
      "2024-01-11 07:58:53.015228: val_loss -0.835\n",
      "2024-01-11 07:58:53.022216: Pseudo dice [0.8952, 0.9482, 0.9357]\n",
      "2024-01-11 07:58:53.031219: Epoch time: 40.22 s\n",
      "2024-01-11 07:58:54.686101: \n",
      "2024-01-11 07:58:54.692027: Epoch 396\n",
      "2024-01-11 07:58:54.697028: Current learning rate: 0.00635\n",
      "2024-01-11 07:59:35.030293: train_loss -0.9418\n",
      "2024-01-11 07:59:35.039292: val_loss -0.8357\n",
      "2024-01-11 07:59:35.045305: Pseudo dice [0.8928, 0.9488, 0.937]\n",
      "2024-01-11 07:59:35.049305: Epoch time: 40.35 s\n",
      "2024-01-11 07:59:36.438481: \n",
      "2024-01-11 07:59:36.443624: Epoch 397\n",
      "2024-01-11 07:59:36.448565: Current learning rate: 0.00634\n",
      "2024-01-11 08:00:16.563766: train_loss -0.9409\n",
      "2024-01-11 08:00:16.572766: val_loss -0.8298\n",
      "2024-01-11 08:00:16.599277: Pseudo dice [0.8877, 0.9477, 0.9366]\n",
      "2024-01-11 08:00:16.608277: Epoch time: 40.13 s\n",
      "2024-01-11 08:00:18.092149: \n",
      "2024-01-11 08:00:18.097654: Epoch 398\n",
      "2024-01-11 08:00:18.102644: Current learning rate: 0.00633\n",
      "2024-01-11 08:00:58.394815: train_loss -0.9415\n",
      "2024-01-11 08:00:58.405329: val_loss -0.8333\n",
      "2024-01-11 08:00:58.414330: Pseudo dice [0.8882, 0.9477, 0.9361]\n",
      "2024-01-11 08:00:58.451328: Epoch time: 40.3 s\n",
      "2024-01-11 08:00:59.932927: \n",
      "2024-01-11 08:00:59.944204: Epoch 399\n",
      "2024-01-11 08:00:59.948858: Current learning rate: 0.00632\n",
      "2024-01-11 08:01:40.008706: train_loss -0.9417\n",
      "2024-01-11 08:01:40.016700: val_loss -0.8349\n",
      "2024-01-11 08:01:40.025215: Pseudo dice [0.8903, 0.9495, 0.9386]\n",
      "2024-01-11 08:01:40.032214: Epoch time: 40.08 s\n",
      "2024-01-11 08:01:41.903770: \n",
      "2024-01-11 08:01:41.908843: Epoch 400\n",
      "2024-01-11 08:01:41.913186: Current learning rate: 0.00631\n",
      "2024-01-11 08:02:22.120983: train_loss -0.9419\n",
      "2024-01-11 08:02:22.127975: val_loss -0.8352\n",
      "2024-01-11 08:02:22.135973: Pseudo dice [0.8878, 0.9493, 0.9368]\n",
      "2024-01-11 08:02:22.140973: Epoch time: 40.22 s\n",
      "2024-01-11 08:02:23.707999: \n",
      "2024-01-11 08:02:23.714008: Epoch 401\n",
      "2024-01-11 08:02:23.721071: Current learning rate: 0.0063\n",
      "2024-01-11 08:03:03.774160: train_loss -0.9414\n",
      "2024-01-11 08:03:03.802691: val_loss -0.8355\n",
      "2024-01-11 08:03:03.810693: Pseudo dice [0.8911, 0.9483, 0.936]\n",
      "2024-01-11 08:03:03.816689: Epoch time: 40.07 s\n",
      "2024-01-11 08:03:05.279734: \n",
      "2024-01-11 08:03:05.285384: Epoch 402\n",
      "2024-01-11 08:03:05.290370: Current learning rate: 0.0063\n",
      "2024-01-11 08:03:45.361074: train_loss -0.9417\n",
      "2024-01-11 08:03:45.368076: val_loss -0.8385\n",
      "2024-01-11 08:03:45.375075: Pseudo dice [0.894, 0.9494, 0.9369]\n",
      "2024-01-11 08:03:45.381077: Epoch time: 40.08 s\n",
      "2024-01-11 08:03:46.821681: \n",
      "2024-01-11 08:03:46.827765: Epoch 403\n",
      "2024-01-11 08:03:46.836863: Current learning rate: 0.00629\n",
      "2024-01-11 08:04:26.937122: train_loss -0.943\n",
      "2024-01-11 08:04:26.945117: val_loss -0.8264\n",
      "2024-01-11 08:04:26.952117: Pseudo dice [0.8898, 0.9463, 0.9336]\n",
      "2024-01-11 08:04:26.957117: Epoch time: 40.12 s\n",
      "2024-01-11 08:04:28.447023: \n",
      "2024-01-11 08:04:28.451680: Epoch 404\n",
      "2024-01-11 08:04:28.455683: Current learning rate: 0.00628\n",
      "2024-01-11 08:05:08.391907: train_loss -0.9421\n",
      "2024-01-11 08:05:08.399908: val_loss -0.8381\n",
      "2024-01-11 08:05:08.408921: Pseudo dice [0.8919, 0.9495, 0.9386]\n",
      "2024-01-11 08:05:08.417916: Epoch time: 39.95 s\n",
      "2024-01-11 08:05:10.049483: \n",
      "2024-01-11 08:05:10.059144: Epoch 405\n",
      "2024-01-11 08:05:10.065150: Current learning rate: 0.00627\n",
      "2024-01-11 08:05:50.039317: train_loss -0.9427\n",
      "2024-01-11 08:05:50.067321: val_loss -0.8328\n",
      "2024-01-11 08:05:50.074319: Pseudo dice [0.8928, 0.9481, 0.9361]\n",
      "2024-01-11 08:05:50.080319: Epoch time: 39.99 s\n",
      "2024-01-11 08:05:51.669318: \n",
      "2024-01-11 08:05:51.674249: Epoch 406\n",
      "2024-01-11 08:05:51.679060: Current learning rate: 0.00626\n",
      "2024-01-11 08:06:31.795977: train_loss -0.9423\n",
      "2024-01-11 08:06:31.802977: val_loss -0.8327\n",
      "2024-01-11 08:06:31.811977: Pseudo dice [0.8889, 0.9478, 0.9364]\n",
      "2024-01-11 08:06:31.818978: Epoch time: 40.13 s\n",
      "2024-01-11 08:06:33.360731: \n",
      "2024-01-11 08:06:33.366730: Epoch 407\n",
      "2024-01-11 08:06:33.370740: Current learning rate: 0.00625\n",
      "2024-01-11 08:07:13.706551: train_loss -0.942\n",
      "2024-01-11 08:07:13.718619: val_loss -0.8358\n",
      "2024-01-11 08:07:13.749901: Pseudo dice [0.8903, 0.9489, 0.937]\n",
      "2024-01-11 08:07:13.758911: Epoch time: 40.35 s\n",
      "2024-01-11 08:07:15.302331: \n",
      "2024-01-11 08:07:15.307391: Epoch 408\n",
      "2024-01-11 08:07:15.312397: Current learning rate: 0.00624\n",
      "2024-01-11 08:07:55.289109: train_loss -0.9426\n",
      "2024-01-11 08:07:55.298109: val_loss -0.83\n",
      "2024-01-11 08:07:55.305107: Pseudo dice [0.8922, 0.9478, 0.9342]\n",
      "2024-01-11 08:07:55.311107: Epoch time: 39.99 s\n",
      "2024-01-11 08:07:56.832774: \n",
      "2024-01-11 08:07:56.838714: Epoch 409\n",
      "2024-01-11 08:07:56.846305: Current learning rate: 0.00623\n",
      "2024-01-11 08:08:36.968453: train_loss -0.943\n",
      "2024-01-11 08:08:36.977452: val_loss -0.8305\n",
      "2024-01-11 08:08:36.986454: Pseudo dice [0.8879, 0.9486, 0.9371]\n",
      "2024-01-11 08:08:36.992458: Epoch time: 40.14 s\n",
      "2024-01-11 08:08:38.511738: \n",
      "2024-01-11 08:08:38.517098: Epoch 410\n",
      "2024-01-11 08:08:38.525100: Current learning rate: 0.00622\n",
      "2024-01-11 08:09:18.412769: train_loss -0.9437\n",
      "2024-01-11 08:09:18.421274: val_loss -0.8329\n",
      "2024-01-11 08:09:18.428281: Pseudo dice [0.8906, 0.9481, 0.9356]\n",
      "2024-01-11 08:09:18.435282: Epoch time: 39.9 s\n",
      "2024-01-11 08:09:19.899855: \n",
      "2024-01-11 08:09:19.906268: Epoch 411\n",
      "2024-01-11 08:09:19.910254: Current learning rate: 0.00621\n",
      "2024-01-11 08:10:00.025930: train_loss -0.9437\n",
      "2024-01-11 08:10:00.033939: val_loss -0.8406\n",
      "2024-01-11 08:10:00.041938: Pseudo dice [0.8982, 0.9494, 0.9382]\n",
      "2024-01-11 08:10:00.047445: Epoch time: 40.13 s\n",
      "2024-01-11 08:10:01.404085: \n",
      "2024-01-11 08:10:01.412153: Epoch 412\n",
      "2024-01-11 08:10:01.416169: Current learning rate: 0.0062\n",
      "2024-01-11 08:10:41.436568: train_loss -0.9425\n",
      "2024-01-11 08:10:41.444569: val_loss -0.829\n",
      "2024-01-11 08:10:41.454574: Pseudo dice [0.8876, 0.9482, 0.9361]\n",
      "2024-01-11 08:10:41.485083: Epoch time: 40.03 s\n",
      "2024-01-11 08:10:42.861440: \n",
      "2024-01-11 08:10:42.867517: Epoch 413\n",
      "2024-01-11 08:10:42.871900: Current learning rate: 0.00619\n",
      "2024-01-11 08:11:22.902304: train_loss -0.9433\n",
      "2024-01-11 08:11:22.909305: val_loss -0.838\n",
      "2024-01-11 08:11:22.917307: Pseudo dice [0.8959, 0.9491, 0.9381]\n",
      "2024-01-11 08:11:22.925305: Epoch time: 40.04 s\n",
      "2024-01-11 08:11:24.406502: \n",
      "2024-01-11 08:11:24.413330: Epoch 414\n",
      "2024-01-11 08:11:24.422417: Current learning rate: 0.00618\n",
      "2024-01-11 08:12:04.605363: train_loss -0.9432\n",
      "2024-01-11 08:12:04.612597: val_loss -0.8274\n",
      "2024-01-11 08:12:04.620595: Pseudo dice [0.8881, 0.947, 0.9348]\n",
      "2024-01-11 08:12:04.645595: Epoch time: 40.2 s\n",
      "2024-01-11 08:12:06.015440: \n",
      "2024-01-11 08:12:06.020624: Epoch 415\n",
      "2024-01-11 08:12:06.025623: Current learning rate: 0.00617\n",
      "2024-01-11 08:12:46.128058: train_loss -0.9424\n",
      "2024-01-11 08:12:46.138061: val_loss -0.837\n",
      "2024-01-11 08:12:46.146060: Pseudo dice [0.8934, 0.9492, 0.9379]\n",
      "2024-01-11 08:12:46.153059: Epoch time: 40.11 s\n",
      "2024-01-11 08:12:47.584219: \n",
      "2024-01-11 08:12:47.590155: Epoch 416\n",
      "2024-01-11 08:12:47.597154: Current learning rate: 0.00616\n",
      "2024-01-11 08:13:27.683549: train_loss -0.9416\n",
      "2024-01-11 08:13:27.690548: val_loss -0.8265\n",
      "2024-01-11 08:13:27.698551: Pseudo dice [0.8826, 0.9461, 0.9328]\n",
      "2024-01-11 08:13:27.706552: Epoch time: 40.1 s\n",
      "2024-01-11 08:13:29.104932: \n",
      "2024-01-11 08:13:29.111084: Epoch 417\n",
      "2024-01-11 08:13:29.117159: Current learning rate: 0.00615\n",
      "2024-01-11 08:14:09.090764: train_loss -0.9425\n",
      "2024-01-11 08:14:09.097763: val_loss -0.8346\n",
      "2024-01-11 08:14:09.104771: Pseudo dice [0.886, 0.9485, 0.9378]\n",
      "2024-01-11 08:14:09.113763: Epoch time: 39.99 s\n",
      "2024-01-11 08:14:10.504572: \n",
      "2024-01-11 08:14:10.515868: Epoch 418\n",
      "2024-01-11 08:14:10.525870: Current learning rate: 0.00614\n",
      "2024-01-11 08:14:50.548340: train_loss -0.9436\n",
      "2024-01-11 08:14:50.556336: val_loss -0.8286\n",
      "2024-01-11 08:14:50.588855: Pseudo dice [0.8856, 0.948, 0.9359]\n",
      "2024-01-11 08:14:50.596854: Epoch time: 40.05 s\n",
      "2024-01-11 08:14:52.061891: \n",
      "2024-01-11 08:14:52.067140: Epoch 419\n",
      "2024-01-11 08:14:52.072274: Current learning rate: 0.00613\n",
      "2024-01-11 08:15:32.050192: train_loss -0.9433\n",
      "2024-01-11 08:15:32.059193: val_loss -0.8286\n",
      "2024-01-11 08:15:32.064193: Pseudo dice [0.8862, 0.948, 0.935]\n",
      "2024-01-11 08:15:32.071195: Epoch time: 39.99 s\n",
      "2024-01-11 08:15:33.505669: \n",
      "2024-01-11 08:15:33.512253: Epoch 420\n",
      "2024-01-11 08:15:33.520317: Current learning rate: 0.00612\n",
      "2024-01-11 08:16:13.384899: train_loss -0.9424\n",
      "2024-01-11 08:16:13.390898: val_loss -0.8306\n",
      "2024-01-11 08:16:13.397901: Pseudo dice [0.8906, 0.9471, 0.9351]\n",
      "2024-01-11 08:16:13.402906: Epoch time: 39.88 s\n",
      "2024-01-11 08:16:14.867159: \n",
      "2024-01-11 08:16:14.874146: Epoch 421\n",
      "2024-01-11 08:16:14.881160: Current learning rate: 0.00612\n",
      "2024-01-11 08:16:54.910635: train_loss -0.9428\n",
      "2024-01-11 08:16:54.921622: val_loss -0.8358\n",
      "2024-01-11 08:16:54.929630: Pseudo dice [0.8901, 0.949, 0.9367]\n",
      "2024-01-11 08:16:54.934628: Epoch time: 40.04 s\n",
      "2024-01-11 08:16:56.308175: \n",
      "2024-01-11 08:16:56.314476: Epoch 422\n",
      "2024-01-11 08:16:56.321544: Current learning rate: 0.00611\n",
      "2024-01-11 08:17:36.310214: train_loss -0.9424\n",
      "2024-01-11 08:17:36.319214: val_loss -0.8338\n",
      "2024-01-11 08:17:36.327214: Pseudo dice [0.8909, 0.9479, 0.9356]\n",
      "2024-01-11 08:17:36.359230: Epoch time: 40.0 s\n",
      "2024-01-11 08:17:37.820500: \n",
      "2024-01-11 08:17:37.825535: Epoch 423\n",
      "2024-01-11 08:17:37.829385: Current learning rate: 0.0061\n",
      "2024-01-11 08:18:17.969098: train_loss -0.942\n",
      "2024-01-11 08:18:17.978104: val_loss -0.8359\n",
      "2024-01-11 08:18:17.985615: Pseudo dice [0.893, 0.9486, 0.9364]\n",
      "2024-01-11 08:18:17.991620: Epoch time: 40.15 s\n",
      "2024-01-11 08:18:19.392562: \n",
      "2024-01-11 08:18:19.399566: Epoch 424\n",
      "2024-01-11 08:18:19.403855: Current learning rate: 0.00609\n",
      "2024-01-11 08:18:59.574709: train_loss -0.9421\n",
      "2024-01-11 08:18:59.580710: val_loss -0.8333\n",
      "2024-01-11 08:18:59.588713: Pseudo dice [0.8937, 0.9479, 0.9362]\n",
      "2024-01-11 08:18:59.595714: Epoch time: 40.18 s\n",
      "2024-01-11 08:19:00.928828: \n",
      "2024-01-11 08:19:00.934461: Epoch 425\n",
      "2024-01-11 08:19:00.941532: Current learning rate: 0.00608\n",
      "2024-01-11 08:19:41.032809: train_loss -0.9429\n",
      "2024-01-11 08:19:41.039815: val_loss -0.8326\n",
      "2024-01-11 08:19:41.045814: Pseudo dice [0.8877, 0.9484, 0.9361]\n",
      "2024-01-11 08:19:41.052812: Epoch time: 40.11 s\n",
      "2024-01-11 08:19:42.448475: \n",
      "2024-01-11 08:19:42.453472: Epoch 426\n",
      "2024-01-11 08:19:42.457999: Current learning rate: 0.00607\n",
      "2024-01-11 08:20:22.402263: train_loss -0.9411\n",
      "2024-01-11 08:20:22.410263: val_loss -0.8333\n",
      "2024-01-11 08:20:22.417870: Pseudo dice [0.8888, 0.9491, 0.9362]\n",
      "2024-01-11 08:20:22.423873: Epoch time: 39.95 s\n",
      "2024-01-11 08:20:23.885208: \n",
      "2024-01-11 08:20:23.890717: Epoch 427\n",
      "2024-01-11 08:20:23.896785: Current learning rate: 0.00606\n",
      "2024-01-11 08:21:03.943037: train_loss -0.9419\n",
      "2024-01-11 08:21:03.955031: val_loss -0.8393\n",
      "2024-01-11 08:21:03.961040: Pseudo dice [0.8952, 0.9489, 0.9378]\n",
      "2024-01-11 08:21:03.969069: Epoch time: 40.06 s\n",
      "2024-01-11 08:21:05.324396: \n",
      "2024-01-11 08:21:05.329991: Epoch 428\n",
      "2024-01-11 08:21:05.333987: Current learning rate: 0.00605\n",
      "2024-01-11 08:21:45.413966: train_loss -0.9417\n",
      "2024-01-11 08:21:45.423968: val_loss -0.8334\n",
      "2024-01-11 08:21:45.459968: Pseudo dice [0.891, 0.9485, 0.9376]\n",
      "2024-01-11 08:21:45.467968: Epoch time: 40.09 s\n",
      "2024-01-11 08:21:46.823672: \n",
      "2024-01-11 08:21:46.832794: Epoch 429\n",
      "2024-01-11 08:21:46.836858: Current learning rate: 0.00604\n",
      "2024-01-11 08:22:27.033674: train_loss -0.9434\n",
      "2024-01-11 08:22:27.042671: val_loss -0.831\n",
      "2024-01-11 08:22:27.048671: Pseudo dice [0.887, 0.948, 0.936]\n",
      "2024-01-11 08:22:27.054672: Epoch time: 40.21 s\n",
      "2024-01-11 08:22:28.473499: \n",
      "2024-01-11 08:22:28.478558: Epoch 430\n",
      "2024-01-11 08:22:28.488575: Current learning rate: 0.00603\n",
      "2024-01-11 08:23:08.443208: train_loss -0.9431\n",
      "2024-01-11 08:23:08.451208: val_loss -0.8319\n",
      "2024-01-11 08:23:08.457209: Pseudo dice [0.8926, 0.948, 0.9355]\n",
      "2024-01-11 08:23:08.462210: Epoch time: 39.97 s\n",
      "2024-01-11 08:23:09.872803: \n",
      "2024-01-11 08:23:09.879731: Epoch 431\n",
      "2024-01-11 08:23:09.883811: Current learning rate: 0.00602\n",
      "2024-01-11 08:23:49.876220: train_loss -0.9436\n",
      "2024-01-11 08:23:49.887222: val_loss -0.8302\n",
      "2024-01-11 08:23:49.893221: Pseudo dice [0.8873, 0.9478, 0.9362]\n",
      "2024-01-11 08:23:49.921220: Epoch time: 40.01 s\n",
      "2024-01-11 08:23:51.214456: \n",
      "2024-01-11 08:23:51.223534: Epoch 432\n",
      "2024-01-11 08:23:51.232557: Current learning rate: 0.00601\n",
      "2024-01-11 08:24:31.262957: train_loss -0.9433\n",
      "2024-01-11 08:24:31.270958: val_loss -0.8335\n",
      "2024-01-11 08:24:31.277986: Pseudo dice [0.8923, 0.947, 0.935]\n",
      "2024-01-11 08:24:31.283981: Epoch time: 40.05 s\n",
      "2024-01-11 08:24:32.687003: \n",
      "2024-01-11 08:24:32.693748: Epoch 433\n",
      "2024-01-11 08:24:32.697815: Current learning rate: 0.006\n",
      "2024-01-11 08:25:12.664337: train_loss -0.9429\n",
      "2024-01-11 08:25:12.670339: val_loss -0.8343\n",
      "2024-01-11 08:25:12.676339: Pseudo dice [0.8879, 0.9491, 0.9375]\n",
      "2024-01-11 08:25:12.681340: Epoch time: 39.98 s\n",
      "2024-01-11 08:25:14.231308: \n",
      "2024-01-11 08:25:14.239306: Epoch 434\n",
      "2024-01-11 08:25:14.244981: Current learning rate: 0.00599\n",
      "2024-01-11 08:25:54.371338: train_loss -0.9437\n",
      "2024-01-11 08:25:54.377338: val_loss -0.832\n",
      "2024-01-11 08:25:54.385339: Pseudo dice [0.8888, 0.948, 0.9367]\n",
      "2024-01-11 08:25:54.391423: Epoch time: 40.14 s\n",
      "2024-01-11 08:25:55.801457: \n",
      "2024-01-11 08:25:55.807220: Epoch 435\n",
      "2024-01-11 08:25:55.811281: Current learning rate: 0.00598\n",
      "2024-01-11 08:26:35.764117: train_loss -0.9432\n",
      "2024-01-11 08:26:35.773117: val_loss -0.8356\n",
      "2024-01-11 08:26:35.780118: Pseudo dice [0.8926, 0.9484, 0.9361]\n",
      "2024-01-11 08:26:35.806117: Epoch time: 39.96 s\n",
      "2024-01-11 08:26:37.245799: \n",
      "2024-01-11 08:26:37.252857: Epoch 436\n",
      "2024-01-11 08:26:37.258800: Current learning rate: 0.00597\n",
      "2024-01-11 08:27:17.138812: train_loss -0.9435\n",
      "2024-01-11 08:27:17.146813: val_loss -0.8363\n",
      "2024-01-11 08:27:17.154813: Pseudo dice [0.8919, 0.9488, 0.9366]\n",
      "2024-01-11 08:27:17.160811: Epoch time: 39.89 s\n",
      "2024-01-11 08:27:18.619030: \n",
      "2024-01-11 08:27:18.628491: Epoch 437\n",
      "2024-01-11 08:27:18.635478: Current learning rate: 0.00596\n",
      "2024-01-11 08:27:58.753523: train_loss -0.9428\n",
      "2024-01-11 08:27:58.762523: val_loss -0.8265\n",
      "2024-01-11 08:27:58.768523: Pseudo dice [0.885, 0.9465, 0.9328]\n",
      "2024-01-11 08:27:58.786294: Epoch time: 40.14 s\n",
      "2024-01-11 08:28:00.208609: \n",
      "2024-01-11 08:28:00.215533: Epoch 438\n",
      "2024-01-11 08:28:00.220531: Current learning rate: 0.00595\n",
      "2024-01-11 08:28:40.178528: train_loss -0.9434\n",
      "2024-01-11 08:28:40.187530: val_loss -0.8339\n",
      "2024-01-11 08:28:40.195530: Pseudo dice [0.8912, 0.9497, 0.9376]\n",
      "2024-01-11 08:28:40.202531: Epoch time: 39.97 s\n",
      "2024-01-11 08:28:41.540408: \n",
      "2024-01-11 08:28:41.545726: Epoch 439\n",
      "2024-01-11 08:28:41.553785: Current learning rate: 0.00594\n",
      "2024-01-11 08:29:21.949927: train_loss -0.9416\n",
      "2024-01-11 08:29:21.957929: val_loss -0.8344\n",
      "2024-01-11 08:29:21.985928: Pseudo dice [0.8942, 0.9484, 0.9367]\n",
      "2024-01-11 08:29:21.994932: Epoch time: 40.41 s\n",
      "2024-01-11 08:29:23.356444: \n",
      "2024-01-11 08:29:23.364502: Epoch 440\n",
      "2024-01-11 08:29:23.368773: Current learning rate: 0.00593\n",
      "2024-01-11 08:30:03.355337: train_loss -0.9433\n",
      "2024-01-11 08:30:03.363070: val_loss -0.8303\n",
      "2024-01-11 08:30:03.374889: Pseudo dice [0.8897, 0.9477, 0.9344]\n",
      "2024-01-11 08:30:03.405898: Epoch time: 40.0 s\n",
      "2024-01-11 08:30:04.742856: \n",
      "2024-01-11 08:30:04.749435: Epoch 441\n",
      "2024-01-11 08:30:04.753510: Current learning rate: 0.00592\n",
      "2024-01-11 08:30:44.656242: train_loss -0.9439\n",
      "2024-01-11 08:30:44.664256: val_loss -0.827\n",
      "2024-01-11 08:30:44.671249: Pseudo dice [0.8871, 0.9476, 0.9358]\n",
      "2024-01-11 08:30:44.698783: Epoch time: 39.91 s\n",
      "2024-01-11 08:30:46.038399: \n",
      "2024-01-11 08:30:46.044533: Epoch 442\n",
      "2024-01-11 08:30:46.050595: Current learning rate: 0.00592\n",
      "2024-01-11 08:31:26.100554: train_loss -0.9431\n",
      "2024-01-11 08:31:26.106554: val_loss -0.8262\n",
      "2024-01-11 08:31:26.113554: Pseudo dice [0.8927, 0.9469, 0.9354]\n",
      "2024-01-11 08:31:26.120554: Epoch time: 40.06 s\n",
      "2024-01-11 08:31:27.524047: \n",
      "2024-01-11 08:31:27.536139: Epoch 443\n",
      "2024-01-11 08:31:27.546109: Current learning rate: 0.00591\n",
      "2024-01-11 08:32:07.471855: train_loss -0.9427\n",
      "2024-01-11 08:32:07.481854: val_loss -0.8283\n",
      "2024-01-11 08:32:07.486856: Pseudo dice [0.8849, 0.9486, 0.9363]\n",
      "2024-01-11 08:32:07.494857: Epoch time: 39.95 s\n",
      "2024-01-11 08:32:09.007857: \n",
      "2024-01-11 08:32:09.014754: Epoch 444\n",
      "2024-01-11 08:32:09.022761: Current learning rate: 0.0059\n",
      "2024-01-11 08:32:48.991589: train_loss -0.9429\n",
      "2024-01-11 08:32:48.997590: val_loss -0.8322\n",
      "2024-01-11 08:32:49.004593: Pseudo dice [0.8884, 0.9478, 0.935]\n",
      "2024-01-11 08:32:49.013592: Epoch time: 39.98 s\n",
      "2024-01-11 08:32:50.370337: \n",
      "2024-01-11 08:32:50.378409: Epoch 445\n",
      "2024-01-11 08:32:50.383373: Current learning rate: 0.00589\n",
      "2024-01-11 08:33:30.433162: train_loss -0.9431\n",
      "2024-01-11 08:33:30.441168: val_loss -0.8401\n",
      "2024-01-11 08:33:30.447169: Pseudo dice [0.8966, 0.95, 0.9375]\n",
      "2024-01-11 08:33:30.452681: Epoch time: 40.06 s\n",
      "2024-01-11 08:33:31.784120: \n",
      "2024-01-11 08:33:31.790122: Epoch 446\n",
      "2024-01-11 08:33:31.797191: Current learning rate: 0.00588\n",
      "2024-01-11 08:34:11.893825: train_loss -0.9436\n",
      "2024-01-11 08:34:11.901825: val_loss -0.8239\n",
      "2024-01-11 08:34:11.907825: Pseudo dice [0.8796, 0.9454, 0.9336]\n",
      "2024-01-11 08:34:11.913826: Epoch time: 40.11 s\n",
      "2024-01-11 08:34:13.250326: \n",
      "2024-01-11 08:34:13.257667: Epoch 447\n",
      "2024-01-11 08:34:13.264146: Current learning rate: 0.00587\n",
      "2024-01-11 08:34:53.538844: train_loss -0.9431\n",
      "2024-01-11 08:34:53.546836: val_loss -0.8307\n",
      "2024-01-11 08:34:53.555846: Pseudo dice [0.889, 0.9477, 0.9357]\n",
      "2024-01-11 08:34:53.562837: Epoch time: 40.29 s\n",
      "2024-01-11 08:34:54.927972: \n",
      "2024-01-11 08:34:54.933717: Epoch 448\n",
      "2024-01-11 08:34:54.938776: Current learning rate: 0.00586\n",
      "2024-01-11 08:35:34.989255: train_loss -0.9435\n",
      "2024-01-11 08:35:34.998252: val_loss -0.8314\n",
      "2024-01-11 08:35:35.029767: Pseudo dice [0.8892, 0.9481, 0.9353]\n",
      "2024-01-11 08:35:35.039767: Epoch time: 40.06 s\n",
      "2024-01-11 08:35:36.405091: \n",
      "2024-01-11 08:35:36.412240: Epoch 449\n",
      "2024-01-11 08:35:36.416234: Current learning rate: 0.00585\n",
      "2024-01-11 08:36:16.679005: train_loss -0.9428\n",
      "2024-01-11 08:36:16.688005: val_loss -0.8353\n",
      "2024-01-11 08:36:16.695008: Pseudo dice [0.8913, 0.948, 0.9367]\n",
      "2024-01-11 08:36:16.701010: Epoch time: 40.27 s\n",
      "2024-01-11 08:36:18.227755: \n",
      "2024-01-11 08:36:18.234809: Epoch 450\n",
      "2024-01-11 08:36:18.240753: Current learning rate: 0.00584\n",
      "2024-01-11 08:36:58.503707: train_loss -0.9442\n",
      "2024-01-11 08:36:58.510707: val_loss -0.8362\n",
      "2024-01-11 08:36:58.515706: Pseudo dice [0.8933, 0.9495, 0.9376]\n",
      "2024-01-11 08:36:58.520707: Epoch time: 40.28 s\n",
      "2024-01-11 08:36:59.878159: \n",
      "2024-01-11 08:36:59.886156: Epoch 451\n",
      "2024-01-11 08:36:59.891094: Current learning rate: 0.00583\n",
      "2024-01-11 08:37:40.016773: train_loss -0.9443\n",
      "2024-01-11 08:37:40.023774: val_loss -0.8324\n",
      "2024-01-11 08:37:40.031775: Pseudo dice [0.8902, 0.9475, 0.9361]\n",
      "2024-01-11 08:37:40.062294: Epoch time: 40.14 s\n",
      "2024-01-11 08:37:41.459076: \n",
      "2024-01-11 08:37:41.467708: Epoch 452\n",
      "2024-01-11 08:37:41.478724: Current learning rate: 0.00582\n",
      "2024-01-11 08:38:21.368289: train_loss -0.9434\n",
      "2024-01-11 08:38:21.375811: val_loss -0.8418\n",
      "2024-01-11 08:38:21.401807: Pseudo dice [0.8976, 0.9504, 0.9395]\n",
      "2024-01-11 08:38:21.408804: Epoch time: 39.91 s\n",
      "2024-01-11 08:38:22.785164: \n",
      "2024-01-11 08:38:22.792216: Epoch 453\n",
      "2024-01-11 08:38:22.797285: Current learning rate: 0.00581\n",
      "2024-01-11 08:39:02.926705: train_loss -0.9437\n",
      "2024-01-11 08:39:02.934706: val_loss -0.8415\n",
      "2024-01-11 08:39:02.939714: Pseudo dice [0.8963, 0.9505, 0.9389]\n",
      "2024-01-11 08:39:02.965711: Epoch time: 40.14 s\n",
      "2024-01-11 08:39:04.278889: \n",
      "2024-01-11 08:39:04.283943: Epoch 454\n",
      "2024-01-11 08:39:04.289018: Current learning rate: 0.0058\n",
      "2024-01-11 08:39:44.640023: train_loss -0.9442\n",
      "2024-01-11 08:39:44.650018: val_loss -0.8321\n",
      "2024-01-11 08:39:44.656020: Pseudo dice [0.8883, 0.9476, 0.9352]\n",
      "2024-01-11 08:39:44.661020: Epoch time: 40.36 s\n",
      "2024-01-11 08:39:45.963986: \n",
      "2024-01-11 08:39:45.969679: Epoch 455\n",
      "2024-01-11 08:39:45.977689: Current learning rate: 0.00579\n",
      "2024-01-11 08:40:26.006568: train_loss -0.9446\n",
      "2024-01-11 08:40:26.014575: val_loss -0.8401\n",
      "2024-01-11 08:40:26.021572: Pseudo dice [0.8992, 0.9487, 0.9377]\n",
      "2024-01-11 08:40:26.046088: Epoch time: 40.04 s\n",
      "2024-01-11 08:40:27.356260: \n",
      "2024-01-11 08:40:27.361486: Epoch 456\n",
      "2024-01-11 08:40:27.366062: Current learning rate: 0.00578\n",
      "2024-01-11 08:41:07.229504: train_loss -0.9435\n",
      "2024-01-11 08:41:07.236504: val_loss -0.8321\n",
      "2024-01-11 08:41:07.242507: Pseudo dice [0.8903, 0.9478, 0.9349]\n",
      "2024-01-11 08:41:07.246508: Epoch time: 39.87 s\n",
      "2024-01-11 08:41:08.679131: \n",
      "2024-01-11 08:41:08.686122: Epoch 457\n",
      "2024-01-11 08:41:08.690471: Current learning rate: 0.00577\n",
      "2024-01-11 08:41:48.853833: train_loss -0.943\n",
      "2024-01-11 08:41:48.880948: val_loss -0.8324\n",
      "2024-01-11 08:41:48.888943: Pseudo dice [0.892, 0.9475, 0.9352]\n",
      "2024-01-11 08:41:48.896286: Epoch time: 40.18 s\n",
      "2024-01-11 08:41:50.343181: \n",
      "2024-01-11 08:41:50.349211: Epoch 458\n",
      "2024-01-11 08:41:50.353207: Current learning rate: 0.00576\n",
      "2024-01-11 08:42:30.469777: train_loss -0.9441\n",
      "2024-01-11 08:42:30.477778: val_loss -0.8314\n",
      "2024-01-11 08:42:30.484792: Pseudo dice [0.89, 0.9486, 0.9358]\n",
      "2024-01-11 08:42:30.507293: Epoch time: 40.13 s\n",
      "2024-01-11 08:42:31.892707: \n",
      "2024-01-11 08:42:31.900428: Epoch 459\n",
      "2024-01-11 08:42:31.908490: Current learning rate: 0.00575\n",
      "2024-01-11 08:43:12.029614: train_loss -0.9431\n",
      "2024-01-11 08:43:12.037611: val_loss -0.8356\n",
      "2024-01-11 08:43:12.044611: Pseudo dice [0.8868, 0.949, 0.9364]\n",
      "2024-01-11 08:43:12.069611: Epoch time: 40.14 s\n",
      "2024-01-11 08:43:13.480004: \n",
      "2024-01-11 08:43:13.485381: Epoch 460\n",
      "2024-01-11 08:43:13.492935: Current learning rate: 0.00574\n",
      "2024-01-11 08:43:53.437093: train_loss -0.9442\n",
      "2024-01-11 08:43:53.443100: val_loss -0.8336\n",
      "2024-01-11 08:43:53.452099: Pseudo dice [0.8912, 0.9482, 0.9361]\n",
      "2024-01-11 08:43:53.457608: Epoch time: 39.96 s\n",
      "2024-01-11 08:43:54.770285: \n",
      "2024-01-11 08:43:54.776232: Epoch 461\n",
      "2024-01-11 08:43:54.783262: Current learning rate: 0.00573\n",
      "2024-01-11 08:44:34.739461: train_loss -0.9443\n",
      "2024-01-11 08:44:34.746461: val_loss -0.8338\n",
      "2024-01-11 08:44:34.755462: Pseudo dice [0.8929, 0.9478, 0.935]\n",
      "2024-01-11 08:44:34.760462: Epoch time: 39.97 s\n",
      "2024-01-11 08:44:36.129632: \n",
      "2024-01-11 08:44:36.134701: Epoch 462\n",
      "2024-01-11 08:44:36.138701: Current learning rate: 0.00572\n",
      "2024-01-11 08:45:16.180371: train_loss -0.9438\n",
      "2024-01-11 08:45:16.187364: val_loss -0.8337\n",
      "2024-01-11 08:45:16.195362: Pseudo dice [0.8937, 0.9473, 0.9351]\n",
      "2024-01-11 08:45:16.203363: Epoch time: 40.05 s\n",
      "2024-01-11 08:45:17.710025: \n",
      "2024-01-11 08:45:17.714578: Epoch 463\n",
      "2024-01-11 08:45:17.726658: Current learning rate: 0.00571\n",
      "2024-01-11 08:45:57.857446: train_loss -0.9437\n",
      "2024-01-11 08:45:57.867446: val_loss -0.8328\n",
      "2024-01-11 08:45:57.875446: Pseudo dice [0.8914, 0.9478, 0.9361]\n",
      "2024-01-11 08:45:57.906448: Epoch time: 40.15 s\n",
      "2024-01-11 08:45:59.348200: \n",
      "2024-01-11 08:45:59.354204: Epoch 464\n",
      "2024-01-11 08:45:59.359204: Current learning rate: 0.0057\n",
      "2024-01-11 08:46:39.537076: train_loss -0.944\n",
      "2024-01-11 08:46:39.547078: val_loss -0.8288\n",
      "2024-01-11 08:46:39.555085: Pseudo dice [0.8869, 0.9483, 0.9352]\n",
      "2024-01-11 08:46:39.583076: Epoch time: 40.19 s\n",
      "2024-01-11 08:46:40.902492: \n",
      "2024-01-11 08:46:40.910345: Epoch 465\n",
      "2024-01-11 08:46:40.915419: Current learning rate: 0.0057\n",
      "2024-01-11 08:47:20.946929: train_loss -0.9442\n",
      "2024-01-11 08:47:20.955928: val_loss -0.8334\n",
      "2024-01-11 08:47:20.963928: Pseudo dice [0.8925, 0.9483, 0.9357]\n",
      "2024-01-11 08:47:20.992452: Epoch time: 40.05 s\n",
      "2024-01-11 08:47:22.349757: \n",
      "2024-01-11 08:47:22.356793: Epoch 466\n",
      "2024-01-11 08:47:22.361074: Current learning rate: 0.00569\n",
      "2024-01-11 08:48:02.304713: train_loss -0.9445\n",
      "2024-01-11 08:48:02.311229: val_loss -0.8327\n",
      "2024-01-11 08:48:02.318229: Pseudo dice [0.8951, 0.949, 0.9368]\n",
      "2024-01-11 08:48:02.323735: Epoch time: 39.96 s\n",
      "2024-01-11 08:48:03.654613: \n",
      "2024-01-11 08:48:03.660185: Epoch 467\n",
      "2024-01-11 08:48:03.668265: Current learning rate: 0.00568\n",
      "2024-01-11 08:48:43.565411: train_loss -0.9443\n",
      "2024-01-11 08:48:43.573409: val_loss -0.8382\n",
      "2024-01-11 08:48:43.580408: Pseudo dice [0.8966, 0.9486, 0.9369]\n",
      "2024-01-11 08:48:43.587407: Epoch time: 39.91 s\n",
      "2024-01-11 08:48:45.061915: \n",
      "2024-01-11 08:48:45.069915: Epoch 468\n",
      "2024-01-11 08:48:45.077147: Current learning rate: 0.00567\n",
      "2024-01-11 08:49:25.115331: train_loss -0.944\n",
      "2024-01-11 08:49:25.123331: val_loss -0.832\n",
      "2024-01-11 08:49:25.129333: Pseudo dice [0.892, 0.9473, 0.936]\n",
      "2024-01-11 08:49:25.134332: Epoch time: 40.06 s\n",
      "2024-01-11 08:49:26.621916: \n",
      "2024-01-11 08:49:26.632953: Epoch 469\n",
      "2024-01-11 08:49:26.637934: Current learning rate: 0.00566\n",
      "2024-01-11 08:50:06.626337: train_loss -0.9444\n",
      "2024-01-11 08:50:06.634339: val_loss -0.8326\n",
      "2024-01-11 08:50:06.643337: Pseudo dice [0.8906, 0.9484, 0.936]\n",
      "2024-01-11 08:50:06.650337: Epoch time: 40.01 s\n",
      "2024-01-11 08:50:07.980831: \n",
      "2024-01-11 08:50:07.986550: Epoch 470\n",
      "2024-01-11 08:50:07.994553: Current learning rate: 0.00565\n",
      "2024-01-11 08:50:48.242992: train_loss -0.9451\n",
      "2024-01-11 08:50:48.250993: val_loss -0.8365\n",
      "2024-01-11 08:50:48.257992: Pseudo dice [0.8941, 0.9498, 0.9379]\n",
      "2024-01-11 08:50:48.262992: Epoch time: 40.26 s\n",
      "2024-01-11 08:50:49.636892: \n",
      "2024-01-11 08:50:49.647534: Epoch 471\n",
      "2024-01-11 08:50:49.654596: Current learning rate: 0.00564\n",
      "2024-01-11 08:51:29.687384: train_loss -0.9442\n",
      "2024-01-11 08:51:29.694390: val_loss -0.8336\n",
      "2024-01-11 08:51:29.702392: Pseudo dice [0.8887, 0.9488, 0.9368]\n",
      "2024-01-11 08:51:29.707912: Epoch time: 40.05 s\n",
      "2024-01-11 08:51:31.106405: \n",
      "2024-01-11 08:51:31.112445: Epoch 472\n",
      "2024-01-11 08:51:31.120142: Current learning rate: 0.00563\n",
      "2024-01-11 08:52:11.182903: train_loss -0.9438\n",
      "2024-01-11 08:52:11.189903: val_loss -0.8283\n",
      "2024-01-11 08:52:11.198903: Pseudo dice [0.8885, 0.9472, 0.9357]\n",
      "2024-01-11 08:52:11.206910: Epoch time: 40.08 s\n",
      "2024-01-11 08:52:12.525856: \n",
      "2024-01-11 08:52:12.532507: Epoch 473\n",
      "2024-01-11 08:52:12.537519: Current learning rate: 0.00562\n",
      "2024-01-11 08:52:52.618870: train_loss -0.9449\n",
      "2024-01-11 08:52:52.646404: val_loss -0.8363\n",
      "2024-01-11 08:52:52.655384: Pseudo dice [0.8863, 0.9498, 0.9385]\n",
      "2024-01-11 08:52:52.664909: Epoch time: 40.09 s\n",
      "2024-01-11 08:52:54.052711: \n",
      "2024-01-11 08:52:54.060318: Epoch 474\n",
      "2024-01-11 08:52:54.065387: Current learning rate: 0.00561\n",
      "2024-01-11 08:53:34.144334: train_loss -0.9447\n",
      "2024-01-11 08:53:34.153341: val_loss -0.8395\n",
      "2024-01-11 08:53:34.162585: Pseudo dice [0.8948, 0.95, 0.9376]\n",
      "2024-01-11 08:53:34.200104: Epoch time: 40.09 s\n",
      "2024-01-11 08:53:35.531438: \n",
      "2024-01-11 08:53:35.537538: Epoch 475\n",
      "2024-01-11 08:53:35.546991: Current learning rate: 0.0056\n",
      "2024-01-11 08:54:15.549273: train_loss -0.9447\n",
      "2024-01-11 08:54:15.555277: val_loss -0.8366\n",
      "2024-01-11 08:54:15.562274: Pseudo dice [0.8926, 0.9483, 0.9361]\n",
      "2024-01-11 08:54:15.568283: Epoch time: 40.02 s\n",
      "2024-01-11 08:54:16.894832: \n",
      "2024-01-11 08:54:16.902832: Epoch 476\n",
      "2024-01-11 08:54:16.906838: Current learning rate: 0.00559\n",
      "2024-01-11 08:54:56.990604: train_loss -0.9451\n",
      "2024-01-11 08:54:57.000128: val_loss -0.8353\n",
      "2024-01-11 08:54:57.008128: Pseudo dice [0.8939, 0.9487, 0.9374]\n",
      "2024-01-11 08:54:57.015126: Epoch time: 40.1 s\n",
      "2024-01-11 08:54:58.345818: \n",
      "2024-01-11 08:54:58.350806: Epoch 477\n",
      "2024-01-11 08:54:58.356791: Current learning rate: 0.00558\n",
      "2024-01-11 08:55:38.362629: train_loss -0.9445\n",
      "2024-01-11 08:55:38.407633: val_loss -0.8287\n",
      "2024-01-11 08:55:38.416146: Pseudo dice [0.8847, 0.9476, 0.935]\n",
      "2024-01-11 08:55:38.423145: Epoch time: 40.02 s\n",
      "2024-01-11 08:55:39.832614: \n",
      "2024-01-11 08:55:39.838830: Epoch 478\n",
      "2024-01-11 08:55:39.844831: Current learning rate: 0.00557\n",
      "2024-01-11 08:56:19.807231: train_loss -0.9457\n",
      "2024-01-11 08:56:19.816238: val_loss -0.83\n",
      "2024-01-11 08:56:19.823238: Pseudo dice [0.8887, 0.9478, 0.9361]\n",
      "2024-01-11 08:56:19.830754: Epoch time: 39.98 s\n",
      "2024-01-11 08:56:21.206772: \n",
      "2024-01-11 08:56:21.214844: Epoch 479\n",
      "2024-01-11 08:56:21.219788: Current learning rate: 0.00556\n",
      "2024-01-11 08:57:01.535487: train_loss -0.9443\n",
      "2024-01-11 08:57:01.565995: val_loss -0.8315\n",
      "2024-01-11 08:57:01.574000: Pseudo dice [0.8888, 0.9484, 0.9362]\n",
      "2024-01-11 08:57:01.580996: Epoch time: 40.33 s\n",
      "2024-01-11 08:57:02.982266: \n",
      "2024-01-11 08:57:02.987266: Epoch 480\n",
      "2024-01-11 08:57:02.992266: Current learning rate: 0.00555\n",
      "2024-01-11 08:57:43.149403: train_loss -0.9446\n",
      "2024-01-11 08:57:43.182916: val_loss -0.8373\n",
      "2024-01-11 08:57:43.190917: Pseudo dice [0.893, 0.9494, 0.9375]\n",
      "2024-01-11 08:57:43.196918: Epoch time: 40.17 s\n",
      "2024-01-11 08:57:44.638201: \n",
      "2024-01-11 08:57:44.646182: Epoch 481\n",
      "2024-01-11 08:57:44.650246: Current learning rate: 0.00554\n",
      "2024-01-11 08:58:24.747268: train_loss -0.945\n",
      "2024-01-11 08:58:24.754269: val_loss -0.8328\n",
      "2024-01-11 08:58:24.762278: Pseudo dice [0.8953, 0.9487, 0.9364]\n",
      "2024-01-11 08:58:24.767278: Epoch time: 40.11 s\n",
      "2024-01-11 08:58:26.238117: \n",
      "2024-01-11 08:58:26.244117: Epoch 482\n",
      "2024-01-11 08:58:26.250108: Current learning rate: 0.00553\n",
      "2024-01-11 08:59:06.281995: train_loss -0.9452\n",
      "2024-01-11 08:59:06.290995: val_loss -0.8336\n",
      "2024-01-11 08:59:06.296996: Pseudo dice [0.8941, 0.9479, 0.9359]\n",
      "2024-01-11 08:59:06.304014: Epoch time: 40.05 s\n",
      "2024-01-11 08:59:07.711610: \n",
      "2024-01-11 08:59:07.723146: Epoch 483\n",
      "2024-01-11 08:59:07.727248: Current learning rate: 0.00552\n",
      "2024-01-11 08:59:47.870777: train_loss -0.9453\n",
      "2024-01-11 08:59:47.880777: val_loss -0.8336\n",
      "2024-01-11 08:59:47.887778: Pseudo dice [0.8922, 0.9487, 0.9365]\n",
      "2024-01-11 08:59:47.893783: Epoch time: 40.16 s\n",
      "2024-01-11 08:59:49.240254: \n",
      "2024-01-11 08:59:49.245259: Epoch 484\n",
      "2024-01-11 08:59:49.250267: Current learning rate: 0.00551\n",
      "2024-01-11 09:00:29.862925: train_loss -0.9456\n",
      "2024-01-11 09:00:29.871927: val_loss -0.8358\n",
      "2024-01-11 09:00:29.902924: Pseudo dice [0.8929, 0.9487, 0.9372]\n",
      "2024-01-11 09:00:29.909923: Epoch time: 40.62 s\n",
      "2024-01-11 09:00:31.255903: \n",
      "2024-01-11 09:00:31.264511: Epoch 485\n",
      "2024-01-11 09:00:31.270506: Current learning rate: 0.0055\n",
      "2024-01-11 09:01:11.287314: train_loss -0.9452\n",
      "2024-01-11 09:01:11.294314: val_loss -0.8377\n",
      "2024-01-11 09:01:11.301314: Pseudo dice [0.8944, 0.949, 0.9378]\n",
      "2024-01-11 09:01:11.308315: Epoch time: 40.03 s\n",
      "2024-01-11 09:01:12.695357: \n",
      "2024-01-11 09:01:12.703357: Epoch 486\n",
      "2024-01-11 09:01:12.707357: Current learning rate: 0.00549\n",
      "2024-01-11 09:01:52.658330: train_loss -0.9445\n",
      "2024-01-11 09:01:52.667334: val_loss -0.8345\n",
      "2024-01-11 09:01:52.688346: Pseudo dice [0.8924, 0.9485, 0.9369]\n",
      "2024-01-11 09:01:52.695377: Epoch time: 39.96 s\n",
      "2024-01-11 09:01:54.119607: \n",
      "2024-01-11 09:01:54.125855: Epoch 487\n",
      "2024-01-11 09:01:54.133917: Current learning rate: 0.00548\n",
      "2024-01-11 09:02:34.125514: train_loss -0.9448\n",
      "2024-01-11 09:02:34.133515: val_loss -0.8301\n",
      "2024-01-11 09:02:34.160512: Pseudo dice [0.888, 0.9493, 0.9362]\n",
      "2024-01-11 09:02:34.168514: Epoch time: 40.01 s\n",
      "2024-01-11 09:02:35.634832: \n",
      "2024-01-11 09:02:35.640362: Epoch 488\n",
      "2024-01-11 09:02:35.647363: Current learning rate: 0.00547\n",
      "2024-01-11 09:03:15.658155: train_loss -0.9459\n",
      "2024-01-11 09:03:15.666164: val_loss -0.8332\n",
      "2024-01-11 09:03:15.674163: Pseudo dice [0.8876, 0.9483, 0.9373]\n",
      "2024-01-11 09:03:15.683160: Epoch time: 40.03 s\n",
      "2024-01-11 09:03:17.059514: \n",
      "2024-01-11 09:03:17.064518: Epoch 489\n",
      "2024-01-11 09:03:17.072587: Current learning rate: 0.00546\n",
      "2024-01-11 09:03:57.255875: train_loss -0.9447\n",
      "2024-01-11 09:03:57.264391: val_loss -0.8369\n",
      "2024-01-11 09:03:57.272391: Pseudo dice [0.8974, 0.9491, 0.9374]\n",
      "2024-01-11 09:03:57.280391: Epoch time: 40.2 s\n",
      "2024-01-11 09:03:58.657382: \n",
      "2024-01-11 09:03:58.663347: Epoch 490\n",
      "2024-01-11 09:03:58.673272: Current learning rate: 0.00546\n",
      "2024-01-11 09:04:38.639608: train_loss -0.9454\n",
      "2024-01-11 09:04:38.650753: val_loss -0.8351\n",
      "2024-01-11 09:04:38.658751: Pseudo dice [0.8919, 0.9482, 0.9358]\n",
      "2024-01-11 09:04:38.665753: Epoch time: 39.98 s\n",
      "2024-01-11 09:04:40.100796: \n",
      "2024-01-11 09:04:40.105771: Epoch 491\n",
      "2024-01-11 09:04:40.113829: Current learning rate: 0.00545\n",
      "2024-01-11 09:05:20.151932: train_loss -0.9448\n",
      "2024-01-11 09:05:20.158933: val_loss -0.8358\n",
      "2024-01-11 09:05:20.166934: Pseudo dice [0.8962, 0.9496, 0.9382]\n",
      "2024-01-11 09:05:20.175933: Epoch time: 40.05 s\n",
      "2024-01-11 09:05:21.598809: \n",
      "2024-01-11 09:05:21.605603: Epoch 492\n",
      "2024-01-11 09:05:21.614609: Current learning rate: 0.00544\n",
      "2024-01-11 09:06:01.650409: train_loss -0.9453\n",
      "2024-01-11 09:06:01.659410: val_loss -0.8355\n",
      "2024-01-11 09:06:01.664411: Pseudo dice [0.8935, 0.9486, 0.9367]\n",
      "2024-01-11 09:06:01.669413: Epoch time: 40.05 s\n",
      "2024-01-11 09:06:03.099421: \n",
      "2024-01-11 09:06:03.107461: Epoch 493\n",
      "2024-01-11 09:06:03.112548: Current learning rate: 0.00543\n",
      "2024-01-11 09:06:43.141342: train_loss -0.9457\n",
      "2024-01-11 09:06:43.166860: val_loss -0.8381\n",
      "2024-01-11 09:06:43.176860: Pseudo dice [0.8932, 0.9495, 0.9388]\n",
      "2024-01-11 09:06:43.184861: Epoch time: 40.04 s\n",
      "2024-01-11 09:06:44.520999: \n",
      "2024-01-11 09:06:44.530046: Epoch 494\n",
      "2024-01-11 09:06:44.535065: Current learning rate: 0.00542\n",
      "2024-01-11 09:07:25.023644: train_loss -0.9458\n",
      "2024-01-11 09:07:25.033843: val_loss -0.824\n",
      "2024-01-11 09:07:25.041844: Pseudo dice [0.8859, 0.9462, 0.9337]\n",
      "2024-01-11 09:07:25.049844: Epoch time: 40.51 s\n",
      "2024-01-11 09:07:26.443768: \n",
      "2024-01-11 09:07:26.449829: Epoch 495\n",
      "2024-01-11 09:07:26.454777: Current learning rate: 0.00541\n",
      "2024-01-11 09:08:06.514552: train_loss -0.9456\n",
      "2024-01-11 09:08:06.523552: val_loss -0.8324\n",
      "2024-01-11 09:08:06.530556: Pseudo dice [0.8894, 0.9486, 0.9359]\n",
      "2024-01-11 09:08:06.536553: Epoch time: 40.07 s\n",
      "2024-01-11 09:08:07.983938: \n",
      "2024-01-11 09:08:07.990458: Epoch 496\n",
      "2024-01-11 09:08:07.995517: Current learning rate: 0.0054\n",
      "2024-01-11 09:08:48.044704: train_loss -0.9455\n",
      "2024-01-11 09:08:48.080215: val_loss -0.8345\n",
      "2024-01-11 09:08:48.092104: Pseudo dice [0.894, 0.9491, 0.9365]\n",
      "2024-01-11 09:08:48.103103: Epoch time: 40.06 s\n",
      "2024-01-11 09:08:49.524581: \n",
      "2024-01-11 09:08:49.532223: Epoch 497\n",
      "2024-01-11 09:08:49.539978: Current learning rate: 0.00539\n",
      "2024-01-11 09:09:29.754679: train_loss -0.9452\n",
      "2024-01-11 09:09:29.763685: val_loss -0.8258\n",
      "2024-01-11 09:09:29.771689: Pseudo dice [0.8865, 0.9471, 0.935]\n",
      "2024-01-11 09:09:29.779690: Epoch time: 40.23 s\n",
      "2024-01-11 09:09:31.159652: \n",
      "2024-01-11 09:09:31.166654: Epoch 498\n",
      "2024-01-11 09:09:31.175665: Current learning rate: 0.00538\n",
      "2024-01-11 09:10:11.126738: train_loss -0.9448\n",
      "2024-01-11 09:10:11.138742: val_loss -0.8318\n",
      "2024-01-11 09:10:11.146741: Pseudo dice [0.8889, 0.9483, 0.937]\n",
      "2024-01-11 09:10:11.153743: Epoch time: 39.97 s\n",
      "2024-01-11 09:10:12.586515: \n",
      "2024-01-11 09:10:12.594930: Epoch 499\n",
      "2024-01-11 09:10:12.602997: Current learning rate: 0.00537\n",
      "2024-01-11 09:10:52.668356: train_loss -0.9454\n",
      "2024-01-11 09:10:52.676357: val_loss -0.8322\n",
      "2024-01-11 09:10:52.714886: Pseudo dice [0.8888, 0.9481, 0.9354]\n",
      "2024-01-11 09:10:52.724882: Epoch time: 40.08 s\n",
      "2024-01-11 09:10:54.396489: \n",
      "2024-01-11 09:10:54.402811: Epoch 500\n",
      "2024-01-11 09:10:54.406889: Current learning rate: 0.00536\n",
      "2024-01-11 09:11:34.706295: train_loss -0.9453\n",
      "2024-01-11 09:11:34.712810: val_loss -0.8252\n",
      "2024-01-11 09:11:34.721815: Pseudo dice [0.8826, 0.9467, 0.934]\n",
      "2024-01-11 09:11:34.727816: Epoch time: 40.31 s\n",
      "2024-01-11 09:11:36.184983: \n",
      "2024-01-11 09:11:36.190978: Epoch 501\n",
      "2024-01-11 09:11:36.194968: Current learning rate: 0.00535\n",
      "2024-01-11 09:12:16.090924: train_loss -0.9452\n",
      "2024-01-11 09:12:16.097925: val_loss -0.8282\n",
      "2024-01-11 09:12:16.104926: Pseudo dice [0.8864, 0.9475, 0.9347]\n",
      "2024-01-11 09:12:16.109925: Epoch time: 39.91 s\n",
      "2024-01-11 09:12:17.473933: \n",
      "2024-01-11 09:12:17.485226: Epoch 502\n",
      "2024-01-11 09:12:17.490283: Current learning rate: 0.00534\n",
      "2024-01-11 09:12:57.471595: train_loss -0.9454\n",
      "2024-01-11 09:12:57.480111: val_loss -0.8321\n",
      "2024-01-11 09:12:57.508107: Pseudo dice [0.8897, 0.9487, 0.937]\n",
      "2024-01-11 09:12:57.517109: Epoch time: 40.0 s\n",
      "2024-01-11 09:12:58.929092: \n",
      "2024-01-11 09:12:58.935286: Epoch 503\n",
      "2024-01-11 09:12:58.938904: Current learning rate: 0.00533\n",
      "2024-01-11 09:13:39.011477: train_loss -0.9454\n",
      "2024-01-11 09:13:39.019477: val_loss -0.8332\n",
      "2024-01-11 09:13:39.028015: Pseudo dice [0.8955, 0.9478, 0.9354]\n",
      "2024-01-11 09:13:39.035016: Epoch time: 40.08 s\n",
      "2024-01-11 09:13:40.521257: \n",
      "2024-01-11 09:13:40.527204: Epoch 504\n",
      "2024-01-11 09:13:40.531137: Current learning rate: 0.00532\n",
      "2024-01-11 09:14:20.477136: train_loss -0.946\n",
      "2024-01-11 09:14:20.485143: val_loss -0.833\n",
      "2024-01-11 09:14:20.495136: Pseudo dice [0.8887, 0.9492, 0.9361]\n",
      "2024-01-11 09:14:20.500139: Epoch time: 39.96 s\n",
      "2024-01-11 09:14:21.943065: \n",
      "2024-01-11 09:14:21.947669: Epoch 505\n",
      "2024-01-11 09:14:21.955728: Current learning rate: 0.00531\n",
      "2024-01-11 09:15:02.253528: train_loss -0.9449\n",
      "2024-01-11 09:15:02.261528: val_loss -0.8368\n",
      "2024-01-11 09:15:02.267528: Pseudo dice [0.8908, 0.9502, 0.9395]\n",
      "2024-01-11 09:15:02.300530: Epoch time: 40.31 s\n",
      "2024-01-11 09:15:03.677701: \n",
      "2024-01-11 09:15:03.682762: Epoch 506\n",
      "2024-01-11 09:15:03.690778: Current learning rate: 0.0053\n",
      "2024-01-11 09:15:43.573294: train_loss -0.9449\n",
      "2024-01-11 09:15:43.581294: val_loss -0.838\n",
      "2024-01-11 09:15:43.588295: Pseudo dice [0.8898, 0.9496, 0.9387]\n",
      "2024-01-11 09:15:43.596294: Epoch time: 39.9 s\n",
      "2024-01-11 09:15:45.089498: \n",
      "2024-01-11 09:15:45.098544: Epoch 507\n",
      "2024-01-11 09:15:45.102620: Current learning rate: 0.00529\n",
      "2024-01-11 09:16:25.171528: train_loss -0.9461\n",
      "2024-01-11 09:16:25.180533: val_loss -0.8312\n",
      "2024-01-11 09:16:25.187533: Pseudo dice [0.8891, 0.948, 0.9362]\n",
      "2024-01-11 09:16:25.214533: Epoch time: 40.08 s\n",
      "2024-01-11 09:16:26.584654: \n",
      "2024-01-11 09:16:26.590512: Epoch 508\n",
      "2024-01-11 09:16:26.594514: Current learning rate: 0.00528\n",
      "2024-01-11 09:17:06.543931: train_loss -0.9452\n",
      "2024-01-11 09:17:06.569929: val_loss -0.8269\n",
      "2024-01-11 09:17:06.575930: Pseudo dice [0.8864, 0.9467, 0.9351]\n",
      "2024-01-11 09:17:06.580936: Epoch time: 39.96 s\n",
      "2024-01-11 09:17:08.027081: \n",
      "2024-01-11 09:17:08.033138: Epoch 509\n",
      "2024-01-11 09:17:08.041107: Current learning rate: 0.00527\n",
      "2024-01-11 09:17:48.186340: train_loss -0.945\n",
      "2024-01-11 09:17:48.194340: val_loss -0.8308\n",
      "2024-01-11 09:17:48.200346: Pseudo dice [0.8888, 0.948, 0.9352]\n",
      "2024-01-11 09:17:48.229860: Epoch time: 40.16 s\n",
      "2024-01-11 09:17:49.546960: \n",
      "2024-01-11 09:17:49.556596: Epoch 510\n",
      "2024-01-11 09:17:49.560663: Current learning rate: 0.00526\n",
      "2024-01-11 09:18:29.519772: train_loss -0.9441\n",
      "2024-01-11 09:18:29.526778: val_loss -0.8362\n",
      "2024-01-11 09:18:29.535794: Pseudo dice [0.8911, 0.9495, 0.9372]\n",
      "2024-01-11 09:18:29.562307: Epoch time: 39.97 s\n",
      "2024-01-11 09:18:31.039424: \n",
      "2024-01-11 09:18:31.044817: Epoch 511\n",
      "2024-01-11 09:18:31.049893: Current learning rate: 0.00525\n",
      "2024-01-11 09:19:11.094742: train_loss -0.9446\n",
      "2024-01-11 09:19:11.103743: val_loss -0.8247\n",
      "2024-01-11 09:19:11.113744: Pseudo dice [0.8837, 0.9479, 0.9342]\n",
      "2024-01-11 09:19:11.141814: Epoch time: 40.06 s\n",
      "2024-01-11 09:19:12.527765: \n",
      "2024-01-11 09:19:12.533880: Epoch 512\n",
      "2024-01-11 09:19:12.538949: Current learning rate: 0.00524\n",
      "2024-01-11 09:19:52.715675: train_loss -0.9457\n",
      "2024-01-11 09:19:52.724673: val_loss -0.8325\n",
      "2024-01-11 09:19:52.731676: Pseudo dice [0.891, 0.9498, 0.9389]\n",
      "2024-01-11 09:19:52.736675: Epoch time: 40.19 s\n",
      "2024-01-11 09:19:54.155872: \n",
      "2024-01-11 09:19:54.161900: Epoch 513\n",
      "2024-01-11 09:19:54.165867: Current learning rate: 0.00523\n",
      "2024-01-11 09:20:34.272328: train_loss -0.946\n",
      "2024-01-11 09:20:34.282328: val_loss -0.8302\n",
      "2024-01-11 09:20:34.291327: Pseudo dice [0.8899, 0.9477, 0.9353]\n",
      "2024-01-11 09:20:34.297337: Epoch time: 40.12 s\n",
      "2024-01-11 09:20:35.710846: \n",
      "2024-01-11 09:20:35.718931: Epoch 514\n",
      "2024-01-11 09:20:35.724200: Current learning rate: 0.00522\n",
      "2024-01-11 09:21:16.066105: train_loss -0.9457\n",
      "2024-01-11 09:21:16.074621: val_loss -0.8335\n",
      "2024-01-11 09:21:16.106617: Pseudo dice [0.8861, 0.9495, 0.9366]\n",
      "2024-01-11 09:21:16.113618: Epoch time: 40.36 s\n",
      "2024-01-11 09:21:17.630483: \n",
      "2024-01-11 09:21:17.638021: Epoch 515\n",
      "2024-01-11 09:21:17.644020: Current learning rate: 0.00521\n",
      "2024-01-11 09:21:58.033207: train_loss -0.9453\n",
      "2024-01-11 09:21:58.041208: val_loss -0.8306\n",
      "2024-01-11 09:21:58.047206: Pseudo dice [0.8868, 0.9486, 0.9371]\n",
      "2024-01-11 09:21:58.054212: Epoch time: 40.4 s\n",
      "2024-01-11 09:21:59.624186: \n",
      "2024-01-11 09:21:59.630185: Epoch 516\n",
      "2024-01-11 09:21:59.635177: Current learning rate: 0.0052\n",
      "2024-01-11 09:22:40.233126: train_loss -0.9459\n",
      "2024-01-11 09:22:40.241128: val_loss -0.8362\n",
      "2024-01-11 09:22:40.251132: Pseudo dice [0.8916, 0.9494, 0.938]\n",
      "2024-01-11 09:22:40.260127: Epoch time: 40.61 s\n",
      "2024-01-11 09:22:41.838566: \n",
      "2024-01-11 09:22:41.845515: Epoch 517\n",
      "2024-01-11 09:22:41.850516: Current learning rate: 0.00519\n",
      "2024-01-11 09:23:22.209503: train_loss -0.9461\n",
      "2024-01-11 09:23:22.218182: val_loss -0.829\n",
      "2024-01-11 09:23:22.225196: Pseudo dice [0.889, 0.9479, 0.9348]\n",
      "2024-01-11 09:23:22.231182: Epoch time: 40.37 s\n",
      "2024-01-11 09:23:23.782803: \n",
      "2024-01-11 09:23:23.789806: Epoch 518\n",
      "2024-01-11 09:23:23.795803: Current learning rate: 0.00518\n",
      "2024-01-11 09:24:04.017334: train_loss -0.9458\n",
      "2024-01-11 09:24:04.026345: val_loss -0.8285\n",
      "2024-01-11 09:24:04.032344: Pseudo dice [0.8838, 0.9481, 0.9361]\n",
      "2024-01-11 09:24:04.038364: Epoch time: 40.24 s\n",
      "2024-01-11 09:24:05.461388: \n",
      "2024-01-11 09:24:05.474423: Epoch 519\n",
      "2024-01-11 09:24:05.483410: Current learning rate: 0.00518\n",
      "2024-01-11 09:24:45.619367: train_loss -0.9461\n",
      "2024-01-11 09:24:45.625366: val_loss -0.8305\n",
      "2024-01-11 09:24:45.631368: Pseudo dice [0.8904, 0.9474, 0.9352]\n",
      "2024-01-11 09:24:45.637372: Epoch time: 40.16 s\n",
      "2024-01-11 09:24:47.055184: \n",
      "2024-01-11 09:24:47.063201: Epoch 520\n",
      "2024-01-11 09:24:47.067193: Current learning rate: 0.00517\n",
      "2024-01-11 09:25:27.124991: train_loss -0.9455\n",
      "2024-01-11 09:25:27.131988: val_loss -0.8326\n",
      "2024-01-11 09:25:27.140987: Pseudo dice [0.8906, 0.9485, 0.9368]\n",
      "2024-01-11 09:25:27.146989: Epoch time: 40.07 s\n",
      "2024-01-11 09:25:28.556907: \n",
      "2024-01-11 09:25:28.566040: Epoch 521\n",
      "2024-01-11 09:25:28.570110: Current learning rate: 0.00516\n",
      "2024-01-11 09:26:08.497053: train_loss -0.9452\n",
      "2024-01-11 09:26:08.505054: val_loss -0.8393\n",
      "2024-01-11 09:26:08.510053: Pseudo dice [0.8913, 0.9508, 0.9396]\n",
      "2024-01-11 09:26:08.515053: Epoch time: 39.94 s\n",
      "2024-01-11 09:26:09.919623: \n",
      "2024-01-11 09:26:09.929470: Epoch 522\n",
      "2024-01-11 09:26:09.936454: Current learning rate: 0.00515\n",
      "2024-01-11 09:26:49.914694: train_loss -0.9453\n",
      "2024-01-11 09:26:49.920694: val_loss -0.8378\n",
      "2024-01-11 09:26:49.926695: Pseudo dice [0.8902, 0.949, 0.937]\n",
      "2024-01-11 09:26:49.931694: Epoch time: 40.0 s\n",
      "2024-01-11 09:26:51.452638: \n",
      "2024-01-11 09:26:51.459137: Epoch 523\n",
      "2024-01-11 09:26:51.466211: Current learning rate: 0.00514\n",
      "2024-01-11 09:27:31.282835: train_loss -0.9459\n",
      "2024-01-11 09:27:31.289837: val_loss -0.8351\n",
      "2024-01-11 09:27:31.298836: Pseudo dice [0.8899, 0.95, 0.939]\n",
      "2024-01-11 09:27:31.304845: Epoch time: 39.83 s\n",
      "2024-01-11 09:27:32.859875: \n",
      "2024-01-11 09:27:32.864670: Epoch 524\n",
      "2024-01-11 09:27:32.872736: Current learning rate: 0.00513\n",
      "2024-01-11 09:28:12.870417: train_loss -0.9459\n",
      "2024-01-11 09:28:12.879418: val_loss -0.8335\n",
      "2024-01-11 09:28:12.886415: Pseudo dice [0.8941, 0.9478, 0.9363]\n",
      "2024-01-11 09:28:12.892416: Epoch time: 40.01 s\n",
      "2024-01-11 09:28:14.330988: \n",
      "2024-01-11 09:28:14.335625: Epoch 525\n",
      "2024-01-11 09:28:14.340564: Current learning rate: 0.00512\n",
      "2024-01-11 09:28:54.278938: train_loss -0.9468\n",
      "2024-01-11 09:28:54.285938: val_loss -0.8296\n",
      "2024-01-11 09:28:54.292939: Pseudo dice [0.8885, 0.9474, 0.935]\n",
      "2024-01-11 09:28:54.299939: Epoch time: 39.95 s\n",
      "2024-01-11 09:28:55.718476: \n",
      "2024-01-11 09:28:55.725912: Epoch 526\n",
      "2024-01-11 09:28:55.734975: Current learning rate: 0.00511\n",
      "2024-01-11 09:29:35.762131: train_loss -0.9456\n",
      "2024-01-11 09:29:35.769643: val_loss -0.8328\n",
      "2024-01-11 09:29:35.776642: Pseudo dice [0.8921, 0.9491, 0.9376]\n",
      "2024-01-11 09:29:35.785151: Epoch time: 40.04 s\n",
      "2024-01-11 09:29:37.203108: \n",
      "2024-01-11 09:29:37.211039: Epoch 527\n",
      "2024-01-11 09:29:37.216039: Current learning rate: 0.0051\n",
      "2024-01-11 09:30:17.239086: train_loss -0.9468\n",
      "2024-01-11 09:30:17.247349: val_loss -0.8356\n",
      "2024-01-11 09:30:17.254364: Pseudo dice [0.8905, 0.9498, 0.9377]\n",
      "2024-01-11 09:30:17.260363: Epoch time: 40.04 s\n",
      "2024-01-11 09:30:18.651113: \n",
      "2024-01-11 09:30:18.660793: Epoch 528\n",
      "2024-01-11 09:30:18.667794: Current learning rate: 0.00509\n",
      "2024-01-11 09:30:58.520789: train_loss -0.9463\n",
      "2024-01-11 09:30:58.528789: val_loss -0.839\n",
      "2024-01-11 09:30:58.556792: Pseudo dice [0.8953, 0.9503, 0.938]\n",
      "2024-01-11 09:30:58.564790: Epoch time: 39.87 s\n",
      "2024-01-11 09:30:59.920870: \n",
      "2024-01-11 09:30:59.926545: Epoch 529\n",
      "2024-01-11 09:30:59.930608: Current learning rate: 0.00508\n",
      "2024-01-11 09:31:40.071205: train_loss -0.9464\n",
      "2024-01-11 09:31:40.078205: val_loss -0.8314\n",
      "2024-01-11 09:31:40.085207: Pseudo dice [0.8869, 0.9488, 0.9366]\n",
      "2024-01-11 09:31:40.090205: Epoch time: 40.15 s\n",
      "2024-01-11 09:31:41.586145: \n",
      "2024-01-11 09:31:41.593214: Epoch 530\n",
      "2024-01-11 09:31:41.597228: Current learning rate: 0.00507\n",
      "2024-01-11 09:32:21.608125: train_loss -0.9462\n",
      "2024-01-11 09:32:21.617131: val_loss -0.8398\n",
      "2024-01-11 09:32:21.624132: Pseudo dice [0.898, 0.9496, 0.9377]\n",
      "2024-01-11 09:32:21.628646: Epoch time: 40.02 s\n",
      "2024-01-11 09:32:23.038915: \n",
      "2024-01-11 09:32:23.048500: Epoch 531\n",
      "2024-01-11 09:32:23.052603: Current learning rate: 0.00506\n",
      "2024-01-11 09:33:03.164540: train_loss -0.9466\n",
      "2024-01-11 09:33:03.170540: val_loss -0.8337\n",
      "2024-01-11 09:33:03.175541: Pseudo dice [0.8914, 0.9485, 0.9375]\n",
      "2024-01-11 09:33:03.180540: Epoch time: 40.13 s\n",
      "2024-01-11 09:33:04.594864: \n",
      "2024-01-11 09:33:04.600003: Epoch 532\n",
      "2024-01-11 09:33:04.607078: Current learning rate: 0.00505\n",
      "2024-01-11 09:33:44.550167: train_loss -0.9465\n",
      "2024-01-11 09:33:44.558169: val_loss -0.8313\n",
      "2024-01-11 09:33:44.566684: Pseudo dice [0.889, 0.9487, 0.9362]\n",
      "2024-01-11 09:33:44.574687: Epoch time: 39.96 s\n",
      "2024-01-11 09:33:46.065343: \n",
      "2024-01-11 09:33:46.070515: Epoch 533\n",
      "2024-01-11 09:33:46.074595: Current learning rate: 0.00504\n",
      "2024-01-11 09:34:26.073350: train_loss -0.9459\n",
      "2024-01-11 09:34:26.079857: val_loss -0.8322\n",
      "2024-01-11 09:34:26.108862: Pseudo dice [0.8922, 0.9472, 0.9353]\n",
      "2024-01-11 09:34:26.117861: Epoch time: 40.01 s\n",
      "2024-01-11 09:34:27.492814: \n",
      "2024-01-11 09:34:27.501884: Epoch 534\n",
      "2024-01-11 09:34:27.506890: Current learning rate: 0.00503\n",
      "2024-01-11 09:35:07.733954: train_loss -0.9469\n",
      "2024-01-11 09:35:07.740955: val_loss -0.8283\n",
      "2024-01-11 09:35:07.748951: Pseudo dice [0.8959, 0.9467, 0.9348]\n",
      "2024-01-11 09:35:07.753952: Epoch time: 40.24 s\n",
      "2024-01-11 09:35:09.281259: \n",
      "2024-01-11 09:35:09.287261: Epoch 535\n",
      "2024-01-11 09:35:09.291257: Current learning rate: 0.00502\n",
      "2024-01-11 09:35:49.377535: train_loss -0.9464\n",
      "2024-01-11 09:35:49.386536: val_loss -0.8332\n",
      "2024-01-11 09:35:49.393535: Pseudo dice [0.8938, 0.9483, 0.9359]\n",
      "2024-01-11 09:35:49.420540: Epoch time: 40.1 s\n",
      "2024-01-11 09:35:50.749312: \n",
      "2024-01-11 09:35:50.756460: Epoch 536\n",
      "2024-01-11 09:35:50.761463: Current learning rate: 0.00501\n",
      "2024-01-11 09:36:30.709400: train_loss -0.9465\n",
      "2024-01-11 09:36:30.716401: val_loss -0.8329\n",
      "2024-01-11 09:36:30.721401: Pseudo dice [0.8898, 0.948, 0.9357]\n",
      "2024-01-11 09:36:30.727407: Epoch time: 39.96 s\n",
      "2024-01-11 09:36:32.156457: \n",
      "2024-01-11 09:36:32.165048: Epoch 537\n",
      "2024-01-11 09:36:32.170075: Current learning rate: 0.005\n",
      "2024-01-11 09:37:12.145622: train_loss -0.9455\n",
      "2024-01-11 09:37:12.152623: val_loss -0.8328\n",
      "2024-01-11 09:37:12.160633: Pseudo dice [0.8896, 0.9488, 0.9365]\n",
      "2024-01-11 09:37:12.167630: Epoch time: 39.99 s\n",
      "2024-01-11 09:37:13.556079: \n",
      "2024-01-11 09:37:13.561146: Epoch 538\n",
      "2024-01-11 09:37:13.567219: Current learning rate: 0.00499\n",
      "2024-01-11 09:37:53.619026: train_loss -0.9462\n",
      "2024-01-11 09:37:53.628027: val_loss -0.8336\n",
      "2024-01-11 09:37:53.635027: Pseudo dice [0.887, 0.949, 0.9358]\n",
      "2024-01-11 09:37:53.640027: Epoch time: 40.06 s\n",
      "2024-01-11 09:37:55.141288: \n",
      "2024-01-11 09:37:55.149963: Epoch 539\n",
      "2024-01-11 09:37:55.157054: Current learning rate: 0.00498\n",
      "2024-01-11 09:38:35.385052: train_loss -0.9473\n",
      "2024-01-11 09:38:35.392053: val_loss -0.8276\n",
      "2024-01-11 09:38:35.415083: Pseudo dice [0.8877, 0.9474, 0.9344]\n",
      "2024-01-11 09:38:35.423084: Epoch time: 40.24 s\n",
      "2024-01-11 09:38:36.777868: \n",
      "2024-01-11 09:38:36.785935: Epoch 540\n",
      "2024-01-11 09:38:36.790947: Current learning rate: 0.00497\n",
      "2024-01-11 09:39:16.898062: train_loss -0.9468\n",
      "2024-01-11 09:39:16.908063: val_loss -0.8369\n",
      "2024-01-11 09:39:16.937581: Pseudo dice [0.895, 0.9493, 0.937]\n",
      "2024-01-11 09:39:16.945582: Epoch time: 40.12 s\n",
      "2024-01-11 09:39:18.310527: \n",
      "2024-01-11 09:39:18.316542: Epoch 541\n",
      "2024-01-11 09:39:18.320541: Current learning rate: 0.00496\n",
      "2024-01-11 09:39:58.228297: train_loss -0.9461\n",
      "2024-01-11 09:39:58.237303: val_loss -0.8329\n",
      "2024-01-11 09:39:58.244813: Pseudo dice [0.8895, 0.9485, 0.9374]\n",
      "2024-01-11 09:39:58.249815: Epoch time: 39.92 s\n",
      "2024-01-11 09:39:59.642214: \n",
      "2024-01-11 09:39:59.648603: Epoch 542\n",
      "2024-01-11 09:39:59.655598: Current learning rate: 0.00495\n",
      "2024-01-11 09:40:39.750793: train_loss -0.9471\n",
      "2024-01-11 09:40:39.758794: val_loss -0.832\n",
      "2024-01-11 09:40:39.768146: Pseudo dice [0.8883, 0.9484, 0.937]\n",
      "2024-01-11 09:40:39.773144: Epoch time: 40.11 s\n",
      "2024-01-11 09:40:41.274807: \n",
      "2024-01-11 09:40:41.282517: Epoch 543\n",
      "2024-01-11 09:40:41.287531: Current learning rate: 0.00494\n",
      "2024-01-11 09:41:21.197210: train_loss -0.9466\n",
      "2024-01-11 09:41:21.205210: val_loss -0.8304\n",
      "2024-01-11 09:41:21.213209: Pseudo dice [0.889, 0.9476, 0.936]\n",
      "2024-01-11 09:41:21.220210: Epoch time: 39.92 s\n",
      "2024-01-11 09:41:22.624214: \n",
      "2024-01-11 09:41:22.630226: Epoch 544\n",
      "2024-01-11 09:41:22.634308: Current learning rate: 0.00493\n",
      "2024-01-11 09:42:02.807341: train_loss -0.9471\n",
      "2024-01-11 09:42:02.815853: val_loss -0.8291\n",
      "2024-01-11 09:42:02.820853: Pseudo dice [0.8928, 0.9475, 0.9348]\n",
      "2024-01-11 09:42:02.827853: Epoch time: 40.19 s\n",
      "2024-01-11 09:42:04.242067: \n",
      "2024-01-11 09:42:04.247420: Epoch 545\n",
      "2024-01-11 09:42:04.252417: Current learning rate: 0.00492\n",
      "2024-01-11 09:42:44.288539: train_loss -0.9461\n",
      "2024-01-11 09:42:44.295539: val_loss -0.833\n",
      "2024-01-11 09:42:44.302545: Pseudo dice [0.8889, 0.948, 0.9359]\n",
      "2024-01-11 09:42:44.308541: Epoch time: 40.05 s\n",
      "2024-01-11 09:42:45.640782: \n",
      "2024-01-11 09:42:45.646065: Epoch 546\n",
      "2024-01-11 09:42:45.650143: Current learning rate: 0.00491\n",
      "2024-01-11 09:43:25.570753: train_loss -0.9472\n",
      "2024-01-11 09:43:25.578761: val_loss -0.8349\n",
      "2024-01-11 09:43:25.584754: Pseudo dice [0.8881, 0.9494, 0.9381]\n",
      "2024-01-11 09:43:25.592295: Epoch time: 39.93 s\n",
      "2024-01-11 09:43:26.968135: \n",
      "2024-01-11 09:43:26.976190: Epoch 547\n",
      "2024-01-11 09:43:26.981200: Current learning rate: 0.0049\n",
      "2024-01-11 09:44:06.946502: train_loss -0.9463\n",
      "2024-01-11 09:44:06.954973: val_loss -0.8348\n",
      "2024-01-11 09:44:06.964974: Pseudo dice [0.893, 0.9494, 0.9381]\n",
      "2024-01-11 09:44:06.970975: Epoch time: 39.98 s\n",
      "2024-01-11 09:44:08.429737: \n",
      "2024-01-11 09:44:08.440723: Epoch 548\n",
      "2024-01-11 09:44:08.445726: Current learning rate: 0.00489\n",
      "2024-01-11 09:44:48.386585: train_loss -0.9474\n",
      "2024-01-11 09:44:48.394592: val_loss -0.8333\n",
      "2024-01-11 09:44:48.401592: Pseudo dice [0.893, 0.9491, 0.9378]\n",
      "2024-01-11 09:44:48.408111: Epoch time: 39.96 s\n",
      "2024-01-11 09:44:49.850089: \n",
      "2024-01-11 09:44:49.857099: Epoch 549\n",
      "2024-01-11 09:44:49.862099: Current learning rate: 0.00488\n",
      "2024-01-11 09:45:30.202404: train_loss -0.9469\n",
      "2024-01-11 09:45:30.209412: val_loss -0.8309\n",
      "2024-01-11 09:45:30.214411: Pseudo dice [0.8842, 0.9491, 0.9363]\n",
      "2024-01-11 09:45:30.219411: Epoch time: 40.35 s\n",
      "2024-01-11 09:45:31.836249: \n",
      "2024-01-11 09:45:31.841760: Epoch 550\n",
      "2024-01-11 09:45:31.849827: Current learning rate: 0.00487\n",
      "2024-01-11 09:46:11.729872: train_loss -0.9467\n",
      "2024-01-11 09:46:11.738383: val_loss -0.8381\n",
      "2024-01-11 09:46:11.760382: Pseudo dice [0.8925, 0.9501, 0.9396]\n",
      "2024-01-11 09:46:11.768382: Epoch time: 39.89 s\n",
      "2024-01-11 09:46:13.177998: \n",
      "2024-01-11 09:46:13.189041: Epoch 551\n",
      "2024-01-11 09:46:13.193999: Current learning rate: 0.00486\n",
      "2024-01-11 09:46:53.308443: train_loss -0.9462\n",
      "2024-01-11 09:46:53.319444: val_loss -0.8323\n",
      "2024-01-11 09:46:53.327444: Pseudo dice [0.8898, 0.9489, 0.9367]\n",
      "2024-01-11 09:46:53.334456: Epoch time: 40.13 s\n",
      "2024-01-11 09:46:54.728971: \n",
      "2024-01-11 09:46:54.737912: Epoch 552\n",
      "2024-01-11 09:46:54.743002: Current learning rate: 0.00485\n",
      "2024-01-11 09:47:34.706027: train_loss -0.9465\n",
      "2024-01-11 09:47:34.715029: val_loss -0.833\n",
      "2024-01-11 09:47:34.724028: Pseudo dice [0.8927, 0.9482, 0.9362]\n",
      "2024-01-11 09:47:34.731542: Epoch time: 39.98 s\n",
      "2024-01-11 09:47:36.136258: \n",
      "2024-01-11 09:47:36.141296: Epoch 553\n",
      "2024-01-11 09:47:36.146372: Current learning rate: 0.00484\n",
      "2024-01-11 09:48:16.165491: train_loss -0.9468\n",
      "2024-01-11 09:48:16.175498: val_loss -0.8336\n",
      "2024-01-11 09:48:16.183002: Pseudo dice [0.8926, 0.9487, 0.9366]\n",
      "2024-01-11 09:48:16.190009: Epoch time: 40.03 s\n",
      "2024-01-11 09:48:17.718830: \n",
      "2024-01-11 09:48:17.724828: Epoch 554\n",
      "2024-01-11 09:48:17.728845: Current learning rate: 0.00484\n",
      "2024-01-11 09:48:57.856328: train_loss -0.946\n",
      "2024-01-11 09:48:57.865328: val_loss -0.8259\n",
      "2024-01-11 09:48:57.872325: Pseudo dice [0.8869, 0.947, 0.9347]\n",
      "2024-01-11 09:48:57.877330: Epoch time: 40.14 s\n",
      "2024-01-11 09:48:59.285980: \n",
      "2024-01-11 09:48:59.290983: Epoch 555\n",
      "2024-01-11 09:48:59.295983: Current learning rate: 0.00483\n",
      "2024-01-11 09:49:39.238961: train_loss -0.9465\n",
      "2024-01-11 09:49:39.278960: val_loss -0.8354\n",
      "2024-01-11 09:49:39.295571: Pseudo dice [0.8907, 0.9483, 0.9369]\n",
      "2024-01-11 09:49:39.319580: Epoch time: 39.95 s\n",
      "2024-01-11 09:49:40.679329: \n",
      "2024-01-11 09:49:40.686011: Epoch 556\n",
      "2024-01-11 09:49:40.690013: Current learning rate: 0.00482\n",
      "2024-01-11 09:50:20.883096: train_loss -0.946\n",
      "2024-01-11 09:50:20.890097: val_loss -0.8341\n",
      "2024-01-11 09:50:20.896098: Pseudo dice [0.8936, 0.9485, 0.9375]\n",
      "2024-01-11 09:50:20.902098: Epoch time: 40.2 s\n",
      "2024-01-11 09:50:22.240781: \n",
      "2024-01-11 09:50:22.247839: Epoch 557\n",
      "2024-01-11 09:50:22.252491: Current learning rate: 0.00481\n",
      "2024-01-11 09:51:02.308346: train_loss -0.9452\n",
      "2024-01-11 09:51:02.314347: val_loss -0.8285\n",
      "2024-01-11 09:51:02.321346: Pseudo dice [0.8922, 0.9478, 0.9361]\n",
      "2024-01-11 09:51:02.328345: Epoch time: 40.07 s\n",
      "2024-01-11 09:51:03.721565: \n",
      "2024-01-11 09:51:03.731640: Epoch 558\n",
      "2024-01-11 09:51:03.737569: Current learning rate: 0.0048\n",
      "2024-01-11 09:51:43.692340: train_loss -0.9461\n",
      "2024-01-11 09:51:43.698338: val_loss -0.8309\n",
      "2024-01-11 09:51:43.704338: Pseudo dice [0.8915, 0.9483, 0.9364]\n",
      "2024-01-11 09:51:43.709341: Epoch time: 39.97 s\n",
      "2024-01-11 09:51:45.049820: \n",
      "2024-01-11 09:51:45.056190: Epoch 559\n",
      "2024-01-11 09:51:45.062721: Current learning rate: 0.00479\n",
      "2024-01-11 09:52:25.235606: train_loss -0.947\n",
      "2024-01-11 09:52:25.241607: val_loss -0.8348\n",
      "2024-01-11 09:52:25.250609: Pseudo dice [0.8942, 0.9491, 0.9377]\n",
      "2024-01-11 09:52:25.278608: Epoch time: 40.19 s\n",
      "2024-01-11 09:52:26.650856: \n",
      "2024-01-11 09:52:26.657146: Epoch 560\n",
      "2024-01-11 09:52:26.665145: Current learning rate: 0.00478\n",
      "2024-01-11 09:53:06.754268: train_loss -0.9468\n",
      "2024-01-11 09:53:06.762270: val_loss -0.8304\n",
      "2024-01-11 09:53:06.770268: Pseudo dice [0.8896, 0.9479, 0.936]\n",
      "2024-01-11 09:53:06.798276: Epoch time: 40.1 s\n",
      "2024-01-11 09:53:08.103829: \n",
      "2024-01-11 09:53:08.111871: Epoch 561\n",
      "2024-01-11 09:53:08.121607: Current learning rate: 0.00477\n",
      "2024-01-11 09:53:48.092228: train_loss -0.9471\n",
      "2024-01-11 09:53:48.098229: val_loss -0.835\n",
      "2024-01-11 09:53:48.106228: Pseudo dice [0.8952, 0.9491, 0.937]\n",
      "2024-01-11 09:53:48.111229: Epoch time: 39.99 s\n",
      "2024-01-11 09:53:49.505883: \n",
      "2024-01-11 09:53:49.513424: Epoch 562\n",
      "2024-01-11 09:53:49.518523: Current learning rate: 0.00476\n",
      "2024-01-11 09:54:29.621084: train_loss -0.9465\n",
      "2024-01-11 09:54:29.628085: val_loss -0.834\n",
      "2024-01-11 09:54:29.635086: Pseudo dice [0.8926, 0.9477, 0.9359]\n",
      "2024-01-11 09:54:29.640089: Epoch time: 40.12 s\n",
      "2024-01-11 09:54:31.094080: \n",
      "2024-01-11 09:54:31.103437: Epoch 563\n",
      "2024-01-11 09:54:31.107497: Current learning rate: 0.00475\n",
      "2024-01-11 09:55:11.240573: train_loss -0.9469\n",
      "2024-01-11 09:55:11.249576: val_loss -0.8294\n",
      "2024-01-11 09:55:11.258574: Pseudo dice [0.8912, 0.9482, 0.9355]\n",
      "2024-01-11 09:55:11.265573: Epoch time: 40.15 s\n",
      "2024-01-11 09:55:12.607191: \n",
      "2024-01-11 09:55:12.612192: Epoch 564\n",
      "2024-01-11 09:55:12.619567: Current learning rate: 0.00474\n",
      "2024-01-11 09:55:52.703882: train_loss -0.9476\n",
      "2024-01-11 09:55:52.712884: val_loss -0.8305\n",
      "2024-01-11 09:55:52.719396: Pseudo dice [0.8897, 0.9479, 0.9345]\n",
      "2024-01-11 09:55:52.726400: Epoch time: 40.1 s\n",
      "2024-01-11 09:55:54.103584: \n",
      "2024-01-11 09:55:54.108604: Epoch 565\n",
      "2024-01-11 09:55:54.116608: Current learning rate: 0.00473\n",
      "2024-01-11 09:56:34.140746: train_loss -0.9465\n",
      "2024-01-11 09:56:34.148746: val_loss -0.8319\n",
      "2024-01-11 09:56:34.156745: Pseudo dice [0.8937, 0.9475, 0.935]\n",
      "2024-01-11 09:56:34.164747: Epoch time: 40.04 s\n",
      "2024-01-11 09:56:35.568826: \n",
      "2024-01-11 09:56:35.578815: Epoch 566\n",
      "2024-01-11 09:56:35.585056: Current learning rate: 0.00472\n",
      "2024-01-11 09:57:15.671737: train_loss -0.9473\n",
      "2024-01-11 09:57:15.679738: val_loss -0.8263\n",
      "2024-01-11 09:57:15.704738: Pseudo dice [0.8932, 0.9469, 0.9351]\n",
      "2024-01-11 09:57:15.712738: Epoch time: 40.11 s\n",
      "2024-01-11 09:57:17.141959: \n",
      "2024-01-11 09:57:17.152197: Epoch 567\n",
      "2024-01-11 09:57:17.157267: Current learning rate: 0.00471\n",
      "2024-01-11 09:57:57.332131: train_loss -0.9472\n",
      "2024-01-11 09:57:57.338131: val_loss -0.8263\n",
      "2024-01-11 09:57:57.344131: Pseudo dice [0.8887, 0.9477, 0.9335]\n",
      "2024-01-11 09:57:57.374653: Epoch time: 40.19 s\n",
      "2024-01-11 09:57:58.782544: \n",
      "2024-01-11 09:57:58.787583: Epoch 568\n",
      "2024-01-11 09:57:58.791648: Current learning rate: 0.0047\n",
      "2024-01-11 09:58:38.768827: train_loss -0.9474\n",
      "2024-01-11 09:58:38.777829: val_loss -0.8396\n",
      "2024-01-11 09:58:38.785336: Pseudo dice [0.8973, 0.9498, 0.938]\n",
      "2024-01-11 09:58:38.792336: Epoch time: 39.99 s\n",
      "2024-01-11 09:58:40.306661: \n",
      "2024-01-11 09:58:40.314679: Epoch 569\n",
      "2024-01-11 09:58:40.319665: Current learning rate: 0.00469\n",
      "2024-01-11 09:59:20.311583: train_loss -0.9476\n",
      "2024-01-11 09:59:20.318573: val_loss -0.837\n",
      "2024-01-11 09:59:20.326574: Pseudo dice [0.8936, 0.9496, 0.9378]\n",
      "2024-01-11 09:59:20.350093: Epoch time: 40.01 s\n",
      "2024-01-11 09:59:21.803941: \n",
      "2024-01-11 09:59:21.816427: Epoch 570\n",
      "2024-01-11 09:59:21.821437: Current learning rate: 0.00468\n",
      "2024-01-11 10:00:01.890885: train_loss -0.9481\n",
      "2024-01-11 10:00:01.901884: val_loss -0.8353\n",
      "2024-01-11 10:00:01.911959: Pseudo dice [0.8943, 0.9486, 0.9368]\n",
      "2024-01-11 10:00:01.939477: Epoch time: 40.09 s\n",
      "2024-01-11 10:00:03.463659: \n",
      "2024-01-11 10:00:03.469665: Epoch 571\n",
      "2024-01-11 10:00:03.473601: Current learning rate: 0.00467\n",
      "2024-01-11 10:00:43.683780: train_loss -0.9478\n",
      "2024-01-11 10:00:43.690781: val_loss -0.835\n",
      "2024-01-11 10:00:43.697779: Pseudo dice [0.8905, 0.9494, 0.9377]\n",
      "2024-01-11 10:00:43.704778: Epoch time: 40.22 s\n",
      "2024-01-11 10:00:45.110402: \n",
      "2024-01-11 10:00:45.117391: Epoch 572\n",
      "2024-01-11 10:00:45.122395: Current learning rate: 0.00466\n",
      "2024-01-11 10:01:25.190895: train_loss -0.9476\n",
      "2024-01-11 10:01:25.200897: val_loss -0.832\n",
      "2024-01-11 10:01:25.209895: Pseudo dice [0.8883, 0.9487, 0.9364]\n",
      "2024-01-11 10:01:25.215897: Epoch time: 40.08 s\n",
      "2024-01-11 10:01:26.648112: \n",
      "2024-01-11 10:01:26.654088: Epoch 573\n",
      "2024-01-11 10:01:26.659097: Current learning rate: 0.00465\n",
      "2024-01-11 10:02:06.729075: train_loss -0.9482\n",
      "2024-01-11 10:02:06.738939: val_loss -0.8356\n",
      "2024-01-11 10:02:06.751961: Pseudo dice [0.8925, 0.9488, 0.937]\n",
      "2024-01-11 10:02:06.758975: Epoch time: 40.08 s\n",
      "2024-01-11 10:02:08.145593: \n",
      "2024-01-11 10:02:08.151538: Epoch 574\n",
      "2024-01-11 10:02:08.159538: Current learning rate: 0.00464\n",
      "2024-01-11 10:02:48.372266: train_loss -0.947\n",
      "2024-01-11 10:02:48.379274: val_loss -0.8294\n",
      "2024-01-11 10:02:48.386276: Pseudo dice [0.892, 0.9479, 0.9364]\n",
      "2024-01-11 10:02:48.393780: Epoch time: 40.23 s\n",
      "2024-01-11 10:02:49.822127: \n",
      "2024-01-11 10:02:49.828705: Epoch 575\n",
      "2024-01-11 10:02:49.832766: Current learning rate: 0.00463\n",
      "2024-01-11 10:03:29.991396: train_loss -0.9478\n",
      "2024-01-11 10:03:29.999405: val_loss -0.8292\n",
      "2024-01-11 10:03:30.008406: Pseudo dice [0.8848, 0.9482, 0.9368]\n",
      "2024-01-11 10:03:30.015920: Epoch time: 40.17 s\n",
      "2024-01-11 10:03:31.466300: \n",
      "2024-01-11 10:03:31.472455: Epoch 576\n",
      "2024-01-11 10:03:31.477490: Current learning rate: 0.00462\n",
      "2024-01-11 10:04:11.503814: train_loss -0.9473\n",
      "2024-01-11 10:04:11.512824: val_loss -0.8309\n",
      "2024-01-11 10:04:11.523822: Pseudo dice [0.8875, 0.9475, 0.937]\n",
      "2024-01-11 10:04:11.533337: Epoch time: 40.04 s\n",
      "2024-01-11 10:04:12.942681: \n",
      "2024-01-11 10:04:12.951669: Epoch 577\n",
      "2024-01-11 10:04:12.958607: Current learning rate: 0.00461\n",
      "2024-01-11 10:04:53.032228: train_loss -0.9479\n",
      "2024-01-11 10:04:53.039731: val_loss -0.8273\n",
      "2024-01-11 10:04:53.045737: Pseudo dice [0.8902, 0.9469, 0.9342]\n",
      "2024-01-11 10:04:53.053736: Epoch time: 40.09 s\n",
      "2024-01-11 10:04:54.486692: \n",
      "2024-01-11 10:04:54.494368: Epoch 578\n",
      "2024-01-11 10:04:54.499348: Current learning rate: 0.0046\n",
      "2024-01-11 10:05:34.660080: train_loss -0.948\n",
      "2024-01-11 10:05:34.670608: val_loss -0.8265\n",
      "2024-01-11 10:05:34.678596: Pseudo dice [0.8875, 0.9462, 0.9337]\n",
      "2024-01-11 10:05:34.686596: Epoch time: 40.17 s\n",
      "2024-01-11 10:05:36.315807: \n",
      "2024-01-11 10:05:36.320809: Epoch 579\n",
      "2024-01-11 10:05:36.325804: Current learning rate: 0.00459\n",
      "2024-01-11 10:06:16.306234: train_loss -0.9476\n",
      "2024-01-11 10:06:16.317236: val_loss -0.8299\n",
      "2024-01-11 10:06:16.325235: Pseudo dice [0.8942, 0.948, 0.9361]\n",
      "2024-01-11 10:06:16.356236: Epoch time: 39.99 s\n",
      "2024-01-11 10:06:17.731270: \n",
      "2024-01-11 10:06:17.737275: Epoch 580\n",
      "2024-01-11 10:06:17.745342: Current learning rate: 0.00458\n",
      "2024-01-11 10:06:57.707854: train_loss -0.9477\n",
      "2024-01-11 10:06:57.714857: val_loss -0.8315\n",
      "2024-01-11 10:06:57.721396: Pseudo dice [0.8954, 0.9483, 0.9362]\n",
      "2024-01-11 10:06:57.732396: Epoch time: 39.98 s\n",
      "2024-01-11 10:06:59.167068: \n",
      "2024-01-11 10:06:59.174156: Epoch 581\n",
      "2024-01-11 10:06:59.179136: Current learning rate: 0.00457\n",
      "2024-01-11 10:07:39.299683: train_loss -0.9472\n",
      "2024-01-11 10:07:39.306216: val_loss -0.8315\n",
      "2024-01-11 10:07:39.312207: Pseudo dice [0.8911, 0.9477, 0.9363]\n",
      "2024-01-11 10:07:39.317208: Epoch time: 40.13 s\n",
      "2024-01-11 10:07:40.769109: \n",
      "2024-01-11 10:07:40.775121: Epoch 582\n",
      "2024-01-11 10:07:40.779110: Current learning rate: 0.00456\n",
      "2024-01-11 10:08:21.001278: train_loss -0.9468\n",
      "2024-01-11 10:08:21.013279: val_loss -0.8275\n",
      "2024-01-11 10:08:21.021275: Pseudo dice [0.8903, 0.947, 0.9344]\n",
      "2024-01-11 10:08:21.028275: Epoch time: 40.23 s\n",
      "2024-01-11 10:08:22.476696: \n",
      "2024-01-11 10:08:22.483240: Epoch 583\n",
      "2024-01-11 10:08:22.491242: Current learning rate: 0.00455\n",
      "2024-01-11 10:09:02.674444: train_loss -0.9471\n",
      "2024-01-11 10:09:02.682449: val_loss -0.8324\n",
      "2024-01-11 10:09:02.692449: Pseudo dice [0.8958, 0.9486, 0.9366]\n",
      "2024-01-11 10:09:02.697756: Epoch time: 40.2 s\n",
      "2024-01-11 10:09:04.276502: \n",
      "2024-01-11 10:09:04.282507: Epoch 584\n",
      "2024-01-11 10:09:04.286507: Current learning rate: 0.00454\n",
      "2024-01-11 10:09:44.573858: train_loss -0.9475\n",
      "2024-01-11 10:09:44.585367: val_loss -0.8324\n",
      "2024-01-11 10:09:44.595366: Pseudo dice [0.8905, 0.9485, 0.9365]\n",
      "2024-01-11 10:09:44.602371: Epoch time: 40.3 s\n",
      "2024-01-11 10:09:46.056993: \n",
      "2024-01-11 10:09:46.065032: Epoch 585\n",
      "2024-01-11 10:09:46.069109: Current learning rate: 0.00453\n",
      "2024-01-11 10:10:26.276143: train_loss -0.9476\n",
      "2024-01-11 10:10:26.285141: val_loss -0.8328\n",
      "2024-01-11 10:10:26.293657: Pseudo dice [0.8918, 0.9486, 0.9367]\n",
      "2024-01-11 10:10:26.301656: Epoch time: 40.22 s\n",
      "2024-01-11 10:10:27.718491: \n",
      "2024-01-11 10:10:27.728465: Epoch 586\n",
      "2024-01-11 10:10:27.733432: Current learning rate: 0.00452\n",
      "2024-01-11 10:11:07.660480: train_loss -0.9475\n",
      "2024-01-11 10:11:07.691992: val_loss -0.8272\n",
      "2024-01-11 10:11:07.698992: Pseudo dice [0.8883, 0.9478, 0.9353]\n",
      "2024-01-11 10:11:07.703996: Epoch time: 39.94 s\n",
      "2024-01-11 10:11:09.072487: \n",
      "2024-01-11 10:11:09.081497: Epoch 587\n",
      "2024-01-11 10:11:09.086167: Current learning rate: 0.00451\n",
      "2024-01-11 10:11:49.023943: train_loss -0.9487\n",
      "2024-01-11 10:11:49.031945: val_loss -0.831\n",
      "2024-01-11 10:11:49.039943: Pseudo dice [0.888, 0.9491, 0.9369]\n",
      "2024-01-11 10:11:49.046945: Epoch time: 39.95 s\n",
      "2024-01-11 10:11:50.483323: \n",
      "2024-01-11 10:11:50.489958: Epoch 588\n",
      "2024-01-11 10:11:50.496948: Current learning rate: 0.0045\n",
      "2024-01-11 10:12:30.576706: train_loss -0.9478\n",
      "2024-01-11 10:12:30.584707: val_loss -0.8302\n",
      "2024-01-11 10:12:30.591707: Pseudo dice [0.8913, 0.9485, 0.9366]\n",
      "2024-01-11 10:12:30.596707: Epoch time: 40.09 s\n",
      "2024-01-11 10:12:32.156085: \n",
      "2024-01-11 10:12:32.163085: Epoch 589\n",
      "2024-01-11 10:12:32.170085: Current learning rate: 0.00449\n",
      "2024-01-11 10:13:12.329551: train_loss -0.9483\n",
      "2024-01-11 10:13:12.336553: val_loss -0.8339\n",
      "2024-01-11 10:13:12.347061: Pseudo dice [0.8891, 0.9485, 0.9371]\n",
      "2024-01-11 10:13:12.385061: Epoch time: 40.18 s\n",
      "2024-01-11 10:13:13.780035: \n",
      "2024-01-11 10:13:13.786043: Epoch 590\n",
      "2024-01-11 10:13:13.790019: Current learning rate: 0.00448\n",
      "2024-01-11 10:13:53.771886: train_loss -0.9479\n",
      "2024-01-11 10:13:53.780881: val_loss -0.8337\n",
      "2024-01-11 10:13:53.788881: Pseudo dice [0.8912, 0.9479, 0.9364]\n",
      "2024-01-11 10:13:53.795882: Epoch time: 39.99 s\n",
      "2024-01-11 10:13:55.278124: \n",
      "2024-01-11 10:13:55.284195: Epoch 591\n",
      "2024-01-11 10:13:55.290203: Current learning rate: 0.00447\n",
      "2024-01-11 10:14:35.269289: train_loss -0.9487\n",
      "2024-01-11 10:14:35.278796: val_loss -0.8314\n",
      "2024-01-11 10:14:35.285797: Pseudo dice [0.889, 0.9488, 0.9372]\n",
      "2024-01-11 10:14:35.292797: Epoch time: 39.99 s\n",
      "2024-01-11 10:14:36.735550: \n",
      "2024-01-11 10:14:36.743338: Epoch 592\n",
      "2024-01-11 10:14:36.748376: Current learning rate: 0.00446\n",
      "2024-01-11 10:15:16.981714: train_loss -0.9479\n",
      "2024-01-11 10:15:16.991262: val_loss -0.8319\n",
      "2024-01-11 10:15:16.996259: Pseudo dice [0.8911, 0.9484, 0.9365]\n",
      "2024-01-11 10:15:17.003257: Epoch time: 40.25 s\n",
      "2024-01-11 10:15:18.473858: \n",
      "2024-01-11 10:15:18.479564: Epoch 593\n",
      "2024-01-11 10:15:18.483634: Current learning rate: 0.00445\n",
      "2024-01-11 10:15:58.539075: train_loss -0.9481\n",
      "2024-01-11 10:15:58.567074: val_loss -0.8355\n",
      "2024-01-11 10:15:58.575078: Pseudo dice [0.8927, 0.949, 0.9366]\n",
      "2024-01-11 10:15:58.585074: Epoch time: 40.07 s\n",
      "2024-01-11 10:15:59.969214: \n",
      "2024-01-11 10:15:59.974557: Epoch 594\n",
      "2024-01-11 10:15:59.983635: Current learning rate: 0.00444\n",
      "2024-01-11 10:16:40.391344: train_loss -0.9479\n",
      "2024-01-11 10:16:40.401345: val_loss -0.8315\n",
      "2024-01-11 10:16:40.409346: Pseudo dice [0.8908, 0.9486, 0.936]\n",
      "2024-01-11 10:16:40.419348: Epoch time: 40.42 s\n",
      "2024-01-11 10:16:41.883652: \n",
      "2024-01-11 10:16:41.889649: Epoch 595\n",
      "2024-01-11 10:16:41.894602: Current learning rate: 0.00443\n",
      "2024-01-11 10:17:21.971332: train_loss -0.948\n",
      "2024-01-11 10:17:21.980331: val_loss -0.8357\n",
      "2024-01-11 10:17:21.988332: Pseudo dice [0.8918, 0.949, 0.9371]\n",
      "2024-01-11 10:17:22.021333: Epoch time: 40.09 s\n",
      "2024-01-11 10:17:23.446310: \n",
      "2024-01-11 10:17:23.458068: Epoch 596\n",
      "2024-01-11 10:17:23.463133: Current learning rate: 0.00442\n",
      "2024-01-11 10:18:03.580869: train_loss -0.948\n",
      "2024-01-11 10:18:03.591379: val_loss -0.8383\n",
      "2024-01-11 10:18:03.598380: Pseudo dice [0.8966, 0.9502, 0.9386]\n",
      "2024-01-11 10:18:03.609375: Epoch time: 40.14 s\n",
      "2024-01-11 10:18:05.111700: \n",
      "2024-01-11 10:18:05.122644: Epoch 597\n",
      "2024-01-11 10:18:05.127644: Current learning rate: 0.00441\n",
      "2024-01-11 10:18:45.286463: train_loss -0.9482\n",
      "2024-01-11 10:18:45.295470: val_loss -0.8321\n",
      "2024-01-11 10:18:45.304471: Pseudo dice [0.8944, 0.948, 0.9356]\n",
      "2024-01-11 10:18:45.313991: Epoch time: 40.18 s\n",
      "2024-01-11 10:18:46.776668: \n",
      "2024-01-11 10:18:46.782597: Epoch 598\n",
      "2024-01-11 10:18:46.786621: Current learning rate: 0.0044\n",
      "2024-01-11 10:19:26.789185: train_loss -0.947\n",
      "2024-01-11 10:19:26.801191: val_loss -0.8345\n",
      "2024-01-11 10:19:26.808196: Pseudo dice [0.8911, 0.9495, 0.9371]\n",
      "2024-01-11 10:19:26.816200: Epoch time: 40.01 s\n",
      "2024-01-11 10:19:28.380539: \n",
      "2024-01-11 10:19:28.386606: Epoch 599\n",
      "2024-01-11 10:19:28.391539: Current learning rate: 0.00439\n",
      "2024-01-11 10:20:08.465374: train_loss -0.9467\n",
      "2024-01-11 10:20:08.474374: val_loss -0.8324\n",
      "2024-01-11 10:20:08.480882: Pseudo dice [0.893, 0.9484, 0.9355]\n",
      "2024-01-11 10:20:08.485882: Epoch time: 40.09 s\n",
      "2024-01-11 10:20:10.214901: \n",
      "2024-01-11 10:20:10.222904: Epoch 600\n",
      "2024-01-11 10:20:10.227902: Current learning rate: 0.00438\n",
      "2024-01-11 10:20:50.268738: train_loss -0.9474\n",
      "2024-01-11 10:20:50.277735: val_loss -0.8272\n",
      "2024-01-11 10:20:50.284737: Pseudo dice [0.8875, 0.9458, 0.9334]\n",
      "2024-01-11 10:20:50.289745: Epoch time: 40.06 s\n",
      "2024-01-11 10:20:51.721870: \n",
      "2024-01-11 10:20:51.730930: Epoch 601\n",
      "2024-01-11 10:20:51.740872: Current learning rate: 0.00437\n",
      "2024-01-11 10:21:31.828072: train_loss -0.9482\n",
      "2024-01-11 10:21:31.836075: val_loss -0.8357\n",
      "2024-01-11 10:21:31.846072: Pseudo dice [0.8894, 0.949, 0.9383]\n",
      "2024-01-11 10:21:31.853073: Epoch time: 40.11 s\n",
      "2024-01-11 10:21:33.311552: \n",
      "2024-01-11 10:21:33.319624: Epoch 602\n",
      "2024-01-11 10:21:33.323619: Current learning rate: 0.00436\n",
      "2024-01-11 10:22:13.383575: train_loss -0.9477\n",
      "2024-01-11 10:22:13.392570: val_loss -0.8256\n",
      "2024-01-11 10:22:13.401572: Pseudo dice [0.8845, 0.9473, 0.9352]\n",
      "2024-01-11 10:22:13.409570: Epoch time: 40.07 s\n",
      "2024-01-11 10:22:14.806158: \n",
      "2024-01-11 10:22:14.817436: Epoch 603\n",
      "2024-01-11 10:22:14.822528: Current learning rate: 0.00435\n",
      "2024-01-11 10:22:54.963212: train_loss -0.9472\n",
      "2024-01-11 10:22:54.970222: val_loss -0.8232\n",
      "2024-01-11 10:22:54.975223: Pseudo dice [0.8842, 0.9473, 0.9348]\n",
      "2024-01-11 10:22:54.982412: Epoch time: 40.16 s\n",
      "2024-01-11 10:22:56.580321: \n",
      "2024-01-11 10:22:56.589006: Epoch 604\n",
      "2024-01-11 10:22:56.592993: Current learning rate: 0.00434\n",
      "2024-01-11 10:23:36.503523: train_loss -0.9478\n",
      "2024-01-11 10:23:36.515048: val_loss -0.8315\n",
      "2024-01-11 10:23:36.523048: Pseudo dice [0.8909, 0.9483, 0.937]\n",
      "2024-01-11 10:23:36.556049: Epoch time: 39.92 s\n",
      "2024-01-11 10:23:37.988146: \n",
      "2024-01-11 10:23:37.994145: Epoch 605\n",
      "2024-01-11 10:23:38.001232: Current learning rate: 0.00433\n",
      "2024-01-11 10:24:17.995381: train_loss -0.9479\n",
      "2024-01-11 10:24:18.003382: val_loss -0.8364\n",
      "2024-01-11 10:24:18.010383: Pseudo dice [0.8896, 0.9495, 0.9381]\n",
      "2024-01-11 10:24:18.018381: Epoch time: 40.01 s\n",
      "2024-01-11 10:24:19.440825: \n",
      "2024-01-11 10:24:19.449927: Epoch 606\n",
      "2024-01-11 10:24:19.454912: Current learning rate: 0.00432\n",
      "2024-01-11 10:24:59.388968: train_loss -0.9481\n",
      "2024-01-11 10:24:59.398971: val_loss -0.8305\n",
      "2024-01-11 10:24:59.407968: Pseudo dice [0.888, 0.9481, 0.9357]\n",
      "2024-01-11 10:24:59.416969: Epoch time: 39.95 s\n",
      "2024-01-11 10:25:00.873497: \n",
      "2024-01-11 10:25:00.878799: Epoch 607\n",
      "2024-01-11 10:25:00.886472: Current learning rate: 0.00431\n",
      "2024-01-11 10:25:40.882174: train_loss -0.9475\n",
      "2024-01-11 10:25:40.891181: val_loss -0.8337\n",
      "2024-01-11 10:25:40.896182: Pseudo dice [0.8886, 0.9491, 0.9376]\n",
      "2024-01-11 10:25:40.901180: Epoch time: 40.01 s\n",
      "2024-01-11 10:25:42.402056: \n",
      "2024-01-11 10:25:42.408694: Epoch 608\n",
      "2024-01-11 10:25:42.419638: Current learning rate: 0.0043\n",
      "2024-01-11 10:26:22.642734: train_loss -0.9477\n",
      "2024-01-11 10:26:22.650798: val_loss -0.8316\n",
      "2024-01-11 10:26:22.658799: Pseudo dice [0.8912, 0.9473, 0.9359]\n",
      "2024-01-11 10:26:22.683796: Epoch time: 40.24 s\n",
      "2024-01-11 10:26:24.288529: \n",
      "2024-01-11 10:26:24.295524: Epoch 609\n",
      "2024-01-11 10:26:24.301537: Current learning rate: 0.00429\n",
      "2024-01-11 10:27:04.449408: train_loss -0.9478\n",
      "2024-01-11 10:27:04.457409: val_loss -0.8345\n",
      "2024-01-11 10:27:04.463408: Pseudo dice [0.8889, 0.9491, 0.9371]\n",
      "2024-01-11 10:27:04.470408: Epoch time: 40.16 s\n",
      "2024-01-11 10:27:05.798645: \n",
      "2024-01-11 10:27:05.804933: Epoch 610\n",
      "2024-01-11 10:27:05.810995: Current learning rate: 0.00429\n",
      "2024-01-11 10:27:45.825938: train_loss -0.9475\n",
      "2024-01-11 10:27:45.833939: val_loss -0.8286\n",
      "2024-01-11 10:27:45.842940: Pseudo dice [0.8889, 0.9471, 0.9347]\n",
      "2024-01-11 10:27:45.851961: Epoch time: 40.03 s\n",
      "2024-01-11 10:27:47.279293: \n",
      "2024-01-11 10:27:47.285387: Epoch 611\n",
      "2024-01-11 10:27:47.289358: Current learning rate: 0.00428\n",
      "2024-01-11 10:28:27.297878: train_loss -0.9482\n",
      "2024-01-11 10:28:27.309390: val_loss -0.8283\n",
      "2024-01-11 10:28:27.339389: Pseudo dice [0.8901, 0.9472, 0.9352]\n",
      "2024-01-11 10:28:27.348389: Epoch time: 40.02 s\n",
      "2024-01-11 10:28:28.799352: \n",
      "2024-01-11 10:28:28.806941: Epoch 612\n",
      "2024-01-11 10:28:28.816074: Current learning rate: 0.00427\n",
      "2024-01-11 10:29:08.763227: train_loss -0.9482\n",
      "2024-01-11 10:29:08.791219: val_loss -0.8313\n",
      "2024-01-11 10:29:08.801218: Pseudo dice [0.8894, 0.9484, 0.9366]\n",
      "2024-01-11 10:29:08.811237: Epoch time: 39.96 s\n",
      "2024-01-11 10:29:10.274576: \n",
      "2024-01-11 10:29:10.280656: Epoch 613\n",
      "2024-01-11 10:29:10.285645: Current learning rate: 0.00426\n",
      "2024-01-11 10:29:50.327279: train_loss -0.9485\n",
      "2024-01-11 10:29:50.336790: val_loss -0.8331\n",
      "2024-01-11 10:29:50.347790: Pseudo dice [0.8922, 0.9491, 0.9372]\n",
      "2024-01-11 10:29:50.390793: Epoch time: 40.05 s\n",
      "2024-01-11 10:29:51.946049: \n",
      "2024-01-11 10:29:51.952206: Epoch 614\n",
      "2024-01-11 10:29:51.957833: Current learning rate: 0.00425\n",
      "2024-01-11 10:30:32.138588: train_loss -0.948\n",
      "2024-01-11 10:30:32.144602: val_loss -0.8416\n",
      "2024-01-11 10:30:32.152592: Pseudo dice [0.893, 0.9503, 0.9384]\n",
      "2024-01-11 10:30:32.159103: Epoch time: 40.19 s\n",
      "2024-01-11 10:30:33.592354: \n",
      "2024-01-11 10:30:33.598148: Epoch 615\n",
      "2024-01-11 10:30:33.604221: Current learning rate: 0.00424\n",
      "2024-01-11 10:31:13.624778: train_loss -0.9486\n",
      "2024-01-11 10:31:13.631770: val_loss -0.8299\n",
      "2024-01-11 10:31:13.636775: Pseudo dice [0.891, 0.948, 0.9364]\n",
      "2024-01-11 10:31:13.645770: Epoch time: 40.03 s\n",
      "2024-01-11 10:31:15.029093: \n",
      "2024-01-11 10:31:15.036191: Epoch 616\n",
      "2024-01-11 10:31:15.047224: Current learning rate: 0.00423\n",
      "2024-01-11 10:31:54.987283: train_loss -0.9473\n",
      "2024-01-11 10:31:54.997281: val_loss -0.8271\n",
      "2024-01-11 10:31:55.005793: Pseudo dice [0.8859, 0.9483, 0.9366]\n",
      "2024-01-11 10:31:55.012793: Epoch time: 39.96 s\n",
      "2024-01-11 10:31:56.382155: \n",
      "2024-01-11 10:31:56.389278: Epoch 617\n",
      "2024-01-11 10:31:56.396637: Current learning rate: 0.00422\n",
      "2024-01-11 10:32:36.320483: train_loss -0.9488\n",
      "2024-01-11 10:32:36.328482: val_loss -0.8329\n",
      "2024-01-11 10:32:36.354481: Pseudo dice [0.89, 0.9485, 0.9376]\n",
      "2024-01-11 10:32:36.363483: Epoch time: 39.94 s\n",
      "2024-01-11 10:32:37.695230: \n",
      "2024-01-11 10:32:37.702959: Epoch 618\n",
      "2024-01-11 10:32:37.707097: Current learning rate: 0.00421\n",
      "2024-01-11 10:33:17.695124: train_loss -0.9482\n",
      "2024-01-11 10:33:17.704126: val_loss -0.8256\n",
      "2024-01-11 10:33:17.712124: Pseudo dice [0.8862, 0.9458, 0.9338]\n",
      "2024-01-11 10:33:17.717124: Epoch time: 40.0 s\n",
      "2024-01-11 10:33:19.225533: \n",
      "2024-01-11 10:33:19.231019: Epoch 619\n",
      "2024-01-11 10:33:19.236024: Current learning rate: 0.0042\n",
      "2024-01-11 10:33:59.317669: train_loss -0.9473\n",
      "2024-01-11 10:33:59.325672: val_loss -0.8271\n",
      "2024-01-11 10:33:59.332671: Pseudo dice [0.8875, 0.9471, 0.9348]\n",
      "2024-01-11 10:33:59.338670: Epoch time: 40.09 s\n",
      "2024-01-11 10:34:00.719828: \n",
      "2024-01-11 10:34:00.725586: Epoch 620\n",
      "2024-01-11 10:34:00.731687: Current learning rate: 0.00419\n",
      "2024-01-11 10:34:40.776684: train_loss -0.9483\n",
      "2024-01-11 10:34:40.789682: val_loss -0.8339\n",
      "2024-01-11 10:34:40.826682: Pseudo dice [0.8943, 0.9482, 0.9359]\n",
      "2024-01-11 10:34:40.835683: Epoch time: 40.06 s\n",
      "2024-01-11 10:34:42.230024: \n",
      "2024-01-11 10:34:42.237081: Epoch 621\n",
      "2024-01-11 10:34:42.243041: Current learning rate: 0.00418\n",
      "2024-01-11 10:35:22.477604: train_loss -0.948\n",
      "2024-01-11 10:35:22.486615: val_loss -0.8313\n",
      "2024-01-11 10:35:22.494120: Pseudo dice [0.888, 0.9478, 0.9358]\n",
      "2024-01-11 10:35:22.503124: Epoch time: 40.25 s\n",
      "2024-01-11 10:35:23.946255: \n",
      "2024-01-11 10:35:23.952638: Epoch 622\n",
      "2024-01-11 10:35:23.959717: Current learning rate: 0.00417\n",
      "2024-01-11 10:36:04.164187: train_loss -0.949\n",
      "2024-01-11 10:36:04.174192: val_loss -0.8283\n",
      "2024-01-11 10:36:04.211984: Pseudo dice [0.8874, 0.9481, 0.9352]\n",
      "2024-01-11 10:36:04.219986: Epoch time: 40.22 s\n",
      "2024-01-11 10:36:05.588031: \n",
      "2024-01-11 10:36:05.594820: Epoch 623\n",
      "2024-01-11 10:36:05.602817: Current learning rate: 0.00416\n",
      "2024-01-11 10:36:45.683250: train_loss -0.949\n",
      "2024-01-11 10:36:45.691254: val_loss -0.8345\n",
      "2024-01-11 10:36:45.697258: Pseudo dice [0.8888, 0.9495, 0.9382]\n",
      "2024-01-11 10:36:45.702250: Epoch time: 40.1 s\n",
      "2024-01-11 10:36:47.318749: \n",
      "2024-01-11 10:36:47.324751: Epoch 624\n",
      "2024-01-11 10:36:47.328319: Current learning rate: 0.00415\n",
      "2024-01-11 10:37:27.838951: train_loss -0.9494\n",
      "2024-01-11 10:37:27.845952: val_loss -0.8342\n",
      "2024-01-11 10:37:27.851952: Pseudo dice [0.8866, 0.9492, 0.9374]\n",
      "2024-01-11 10:37:27.857951: Epoch time: 40.52 s\n",
      "2024-01-11 10:37:29.372202: \n",
      "2024-01-11 10:37:29.377264: Epoch 625\n",
      "2024-01-11 10:37:29.381843: Current learning rate: 0.00414\n",
      "2024-01-11 10:38:10.020900: train_loss -0.9481\n",
      "2024-01-11 10:38:10.028899: val_loss -0.83\n",
      "2024-01-11 10:38:10.073600: Pseudo dice [0.8888, 0.9479, 0.9362]\n",
      "2024-01-11 10:38:10.083608: Epoch time: 40.65 s\n",
      "2024-01-11 10:38:11.514319: \n",
      "2024-01-11 10:38:11.520385: Epoch 626\n",
      "2024-01-11 10:38:11.524392: Current learning rate: 0.00413\n",
      "2024-01-11 10:38:51.953149: train_loss -0.9487\n",
      "2024-01-11 10:38:51.963150: val_loss -0.8332\n",
      "2024-01-11 10:38:51.972152: Pseudo dice [0.8942, 0.9481, 0.9358]\n",
      "2024-01-11 10:38:51.980152: Epoch time: 40.44 s\n",
      "2024-01-11 10:38:53.335767: \n",
      "2024-01-11 10:38:53.347070: Epoch 627\n",
      "2024-01-11 10:38:53.351116: Current learning rate: 0.00412\n",
      "2024-01-11 10:39:34.072993: train_loss -0.9491\n",
      "2024-01-11 10:39:34.080296: val_loss -0.8291\n",
      "2024-01-11 10:39:34.090999: Pseudo dice [0.8891, 0.9482, 0.9357]\n",
      "2024-01-11 10:39:34.101002: Epoch time: 40.74 s\n",
      "2024-01-11 10:39:35.572952: \n",
      "2024-01-11 10:39:35.581486: Epoch 628\n",
      "2024-01-11 10:39:35.586748: Current learning rate: 0.00411\n",
      "2024-01-11 10:40:19.049692: train_loss -0.9486\n",
      "2024-01-11 10:40:19.062693: val_loss -0.8362\n",
      "2024-01-11 10:40:19.073795: Pseudo dice [0.8901, 0.9486, 0.9374]\n",
      "2024-01-11 10:40:19.086061: Epoch time: 43.48 s\n",
      "2024-01-11 10:40:20.870721: \n",
      "2024-01-11 10:40:20.878723: Epoch 629\n",
      "2024-01-11 10:40:20.885722: Current learning rate: 0.0041\n",
      "2024-01-11 10:41:02.920489: train_loss -0.9492\n",
      "2024-01-11 10:41:02.930487: val_loss -0.8363\n",
      "2024-01-11 10:41:02.966912: Pseudo dice [0.8947, 0.9491, 0.9365]\n",
      "2024-01-11 10:41:02.977209: Epoch time: 42.05 s\n",
      "2024-01-11 10:41:04.549400: \n",
      "2024-01-11 10:41:04.555604: Epoch 630\n",
      "2024-01-11 10:41:04.560594: Current learning rate: 0.00409\n",
      "2024-01-11 10:41:45.093606: train_loss -0.9488\n",
      "2024-01-11 10:41:45.104605: val_loss -0.8381\n",
      "2024-01-11 10:41:45.113605: Pseudo dice [0.8967, 0.9498, 0.9379]\n",
      "2024-01-11 10:41:45.124605: Epoch time: 40.55 s\n",
      "2024-01-11 10:41:46.349130: \n",
      "2024-01-11 10:41:46.355130: Epoch 631\n",
      "2024-01-11 10:41:46.359130: Current learning rate: 0.00408\n",
      "2024-01-11 10:42:27.831394: train_loss -0.9493\n",
      "2024-01-11 10:42:27.842392: val_loss -0.8244\n",
      "2024-01-11 10:42:27.854419: Pseudo dice [0.8891, 0.9474, 0.9354]\n",
      "2024-01-11 10:42:27.864415: Epoch time: 41.48 s\n",
      "2024-01-11 10:42:29.177195: \n",
      "2024-01-11 10:42:29.186201: Epoch 632\n",
      "2024-01-11 10:42:29.192202: Current learning rate: 0.00407\n",
      "2024-01-11 10:43:10.967437: train_loss -0.9491\n",
      "2024-01-11 10:43:11.007575: val_loss -0.8299\n",
      "2024-01-11 10:43:11.019256: Pseudo dice [0.886, 0.9477, 0.9357]\n",
      "2024-01-11 10:43:11.030060: Epoch time: 41.79 s\n",
      "2024-01-11 10:43:12.597749: \n",
      "2024-01-11 10:43:12.605140: Epoch 633\n",
      "2024-01-11 10:43:12.610274: Current learning rate: 0.00406\n",
      "2024-01-11 10:43:53.338829: train_loss -0.9498\n",
      "2024-01-11 10:43:53.345339: val_loss -0.8291\n",
      "2024-01-11 10:43:53.353343: Pseudo dice [0.8887, 0.9484, 0.9367]\n",
      "2024-01-11 10:43:53.359340: Epoch time: 40.74 s\n",
      "2024-01-11 10:43:54.958261: \n",
      "2024-01-11 10:43:54.964017: Epoch 634\n",
      "2024-01-11 10:43:54.968246: Current learning rate: 0.00405\n",
      "2024-01-11 10:44:35.470813: train_loss -0.9498\n",
      "2024-01-11 10:44:35.478817: val_loss -0.8262\n",
      "2024-01-11 10:44:35.486814: Pseudo dice [0.8899, 0.9471, 0.9354]\n",
      "2024-01-11 10:44:35.492815: Epoch time: 40.51 s\n",
      "2024-01-11 10:44:36.912645: \n",
      "2024-01-11 10:44:36.918404: Epoch 635\n",
      "2024-01-11 10:44:36.922469: Current learning rate: 0.00404\n",
      "2024-01-11 10:45:17.557404: train_loss -0.9488\n",
      "2024-01-11 10:45:17.564404: val_loss -0.8354\n",
      "2024-01-11 10:45:17.570404: Pseudo dice [0.8945, 0.9494, 0.9376]\n",
      "2024-01-11 10:45:17.576409: Epoch time: 40.65 s\n",
      "2024-01-11 10:45:19.053810: \n",
      "2024-01-11 10:45:19.059891: Epoch 636\n",
      "2024-01-11 10:45:19.065886: Current learning rate: 0.00403\n",
      "2024-01-11 10:45:59.533228: train_loss -0.9487\n",
      "2024-01-11 10:45:59.542227: val_loss -0.8334\n",
      "2024-01-11 10:45:59.548229: Pseudo dice [0.8898, 0.9493, 0.9379]\n",
      "2024-01-11 10:45:59.553229: Epoch time: 40.48 s\n",
      "2024-01-11 10:46:01.080205: \n",
      "2024-01-11 10:46:01.086343: Epoch 637\n",
      "2024-01-11 10:46:01.093345: Current learning rate: 0.00402\n",
      "2024-01-11 10:46:42.400084: train_loss -0.9491\n",
      "2024-01-11 10:46:42.446093: val_loss -0.8342\n",
      "2024-01-11 10:46:42.456092: Pseudo dice [0.8942, 0.9489, 0.9368]\n",
      "2024-01-11 10:46:42.466840: Epoch time: 41.32 s\n",
      "2024-01-11 10:46:44.559156: \n",
      "2024-01-11 10:46:44.566162: Epoch 638\n",
      "2024-01-11 10:46:44.573719: Current learning rate: 0.00401\n",
      "2024-01-11 10:47:25.956229: train_loss -0.9493\n",
      "2024-01-11 10:47:25.964229: val_loss -0.8309\n",
      "2024-01-11 10:47:25.970231: Pseudo dice [0.8889, 0.9488, 0.9363]\n",
      "2024-01-11 10:47:25.979230: Epoch time: 41.4 s\n",
      "2024-01-11 10:47:27.714686: \n",
      "2024-01-11 10:47:27.721198: Epoch 639\n",
      "2024-01-11 10:47:27.725264: Current learning rate: 0.004\n",
      "2024-01-11 10:48:09.371651: train_loss -0.9491\n",
      "2024-01-11 10:48:09.379595: val_loss -0.829\n",
      "2024-01-11 10:48:09.386107: Pseudo dice [0.893, 0.9472, 0.9341]\n",
      "2024-01-11 10:48:09.391107: Epoch time: 41.66 s\n",
      "2024-01-11 10:48:10.758584: \n",
      "2024-01-11 10:48:10.764071: Epoch 640\n",
      "2024-01-11 10:48:10.768672: Current learning rate: 0.00399\n",
      "2024-01-11 10:48:52.993684: train_loss -0.9493\n",
      "2024-01-11 10:48:53.002685: val_loss -0.8283\n",
      "2024-01-11 10:48:53.012690: Pseudo dice [0.892, 0.9484, 0.9358]\n",
      "2024-01-11 10:48:53.021683: Epoch time: 42.24 s\n",
      "2024-01-11 10:48:54.923932: \n",
      "2024-01-11 10:48:54.932624: Epoch 641\n",
      "2024-01-11 10:48:54.937596: Current learning rate: 0.00398\n",
      "2024-01-11 10:49:38.157301: train_loss -0.9488\n",
      "2024-01-11 10:49:38.164301: val_loss -0.8308\n",
      "2024-01-11 10:49:38.173305: Pseudo dice [0.8919, 0.9493, 0.9374]\n",
      "2024-01-11 10:49:38.182303: Epoch time: 43.23 s\n",
      "2024-01-11 10:49:39.687552: \n",
      "2024-01-11 10:49:39.695550: Epoch 642\n",
      "2024-01-11 10:49:39.699550: Current learning rate: 0.00397\n",
      "2024-01-11 10:50:21.614433: train_loss -0.9495\n",
      "2024-01-11 10:50:21.625137: val_loss -0.8353\n",
      "2024-01-11 10:50:21.635952: Pseudo dice [0.8951, 0.9486, 0.9369]\n",
      "2024-01-11 10:50:21.684797: Epoch time: 41.93 s\n",
      "2024-01-11 10:50:23.538070: \n",
      "2024-01-11 10:50:23.545148: Epoch 643\n",
      "2024-01-11 10:50:23.551323: Current learning rate: 0.00396\n",
      "2024-01-11 10:51:05.949976: train_loss -0.9498\n",
      "2024-01-11 10:51:05.962591: val_loss -0.8334\n",
      "2024-01-11 10:51:05.970196: Pseudo dice [0.8949, 0.9483, 0.9368]\n",
      "2024-01-11 10:51:05.980451: Epoch time: 42.41 s\n",
      "2024-01-11 10:51:07.883412: \n",
      "2024-01-11 10:51:07.889948: Epoch 644\n",
      "2024-01-11 10:51:07.894948: Current learning rate: 0.00395\n",
      "2024-01-11 10:51:49.830569: train_loss -0.9493\n",
      "2024-01-11 10:51:49.842099: val_loss -0.83\n",
      "2024-01-11 10:51:49.853618: Pseudo dice [0.8929, 0.9485, 0.9356]\n",
      "2024-01-11 10:51:49.887696: Epoch time: 41.95 s\n",
      "2024-01-11 10:51:51.376129: \n",
      "2024-01-11 10:51:51.383142: Epoch 645\n",
      "2024-01-11 10:51:51.388309: Current learning rate: 0.00394\n",
      "2024-01-11 10:52:33.696059: train_loss -0.949\n",
      "2024-01-11 10:52:33.706577: val_loss -0.8311\n",
      "2024-01-11 10:52:33.715097: Pseudo dice [0.8906, 0.9485, 0.9354]\n",
      "2024-01-11 10:52:33.724620: Epoch time: 42.32 s\n",
      "2024-01-11 10:52:35.315504: \n",
      "2024-01-11 10:52:35.322004: Epoch 646\n",
      "2024-01-11 10:52:35.327554: Current learning rate: 0.00393\n",
      "2024-01-11 10:53:17.497415: train_loss -0.9489\n",
      "2024-01-11 10:53:17.539490: val_loss -0.8298\n",
      "2024-01-11 10:53:17.548003: Pseudo dice [0.8905, 0.9489, 0.9368]\n",
      "2024-01-11 10:53:17.555008: Epoch time: 42.18 s\n",
      "2024-01-11 10:53:19.065036: \n",
      "2024-01-11 10:53:19.072221: Epoch 647\n",
      "2024-01-11 10:53:19.076909: Current learning rate: 0.00392\n",
      "2024-01-11 10:54:01.705393: train_loss -0.9488\n",
      "2024-01-11 10:54:01.716516: val_loss -0.8372\n",
      "2024-01-11 10:54:01.727946: Pseudo dice [0.8916, 0.9498, 0.9392]\n",
      "2024-01-11 10:54:01.764040: Epoch time: 42.64 s\n",
      "2024-01-11 10:54:03.289481: \n",
      "2024-01-11 10:54:03.295481: Epoch 648\n",
      "2024-01-11 10:54:03.301021: Current learning rate: 0.00391\n",
      "2024-01-11 10:54:46.025049: train_loss -0.9489\n",
      "2024-01-11 10:54:46.038084: val_loss -0.8326\n",
      "2024-01-11 10:54:46.048608: Pseudo dice [0.8949, 0.9499, 0.9378]\n",
      "2024-01-11 10:54:46.060135: Epoch time: 42.74 s\n",
      "2024-01-11 10:54:47.889505: \n",
      "2024-01-11 10:54:47.894676: Epoch 649\n",
      "2024-01-11 10:54:47.901195: Current learning rate: 0.0039\n",
      "2024-01-11 10:55:30.571246: train_loss -0.9497\n",
      "2024-01-11 10:55:30.582014: val_loss -0.8372\n",
      "2024-01-11 10:55:30.594544: Pseudo dice [0.897, 0.9491, 0.9378]\n",
      "2024-01-11 10:55:30.602577: Epoch time: 42.68 s\n",
      "2024-01-11 10:55:32.417499: \n",
      "2024-01-11 10:55:32.424507: Epoch 650\n",
      "2024-01-11 10:55:32.429806: Current learning rate: 0.00389\n",
      "2024-01-11 10:56:15.111965: train_loss -0.9502\n",
      "2024-01-11 10:56:15.151047: val_loss -0.8329\n",
      "2024-01-11 10:56:15.162567: Pseudo dice [0.895, 0.9489, 0.9368]\n",
      "2024-01-11 10:56:15.174088: Epoch time: 42.7 s\n",
      "2024-01-11 10:56:16.867802: \n",
      "2024-01-11 10:56:16.873793: Epoch 651\n",
      "2024-01-11 10:56:16.879333: Current learning rate: 0.00388\n",
      "2024-01-11 10:56:59.204278: train_loss -0.9498\n",
      "2024-01-11 10:56:59.245527: val_loss -0.8301\n",
      "2024-01-11 10:56:59.258629: Pseudo dice [0.89, 0.9477, 0.9355]\n",
      "2024-01-11 10:56:59.269158: Epoch time: 42.34 s\n",
      "2024-01-11 10:57:00.798783: \n",
      "2024-01-11 10:57:00.806286: Epoch 652\n",
      "2024-01-11 10:57:00.810839: Current learning rate: 0.00387\n",
      "2024-01-11 10:57:42.850863: train_loss -0.9491\n",
      "2024-01-11 10:57:42.862865: val_loss -0.8258\n",
      "2024-01-11 10:57:42.871863: Pseudo dice [0.8874, 0.9471, 0.9351]\n",
      "2024-01-11 10:57:42.892861: Epoch time: 42.05 s\n",
      "2024-01-11 10:57:44.494464: \n",
      "2024-01-11 10:57:44.501472: Epoch 653\n",
      "2024-01-11 10:57:44.505536: Current learning rate: 0.00386\n",
      "2024-01-11 10:58:26.798650: train_loss -0.9503\n",
      "2024-01-11 10:58:26.818043: val_loss -0.825\n",
      "2024-01-11 10:58:26.837049: Pseudo dice [0.8885, 0.9484, 0.9355]\n",
      "2024-01-11 10:58:26.895785: Epoch time: 42.31 s\n",
      "2024-01-11 10:58:29.160004: \n",
      "2024-01-11 10:58:29.167013: Epoch 654\n",
      "2024-01-11 10:58:29.171015: Current learning rate: 0.00385\n",
      "2024-01-11 10:59:11.282742: train_loss -0.9485\n",
      "2024-01-11 10:59:11.293746: val_loss -0.8257\n",
      "2024-01-11 10:59:11.300743: Pseudo dice [0.8854, 0.9482, 0.9364]\n",
      "2024-01-11 10:59:11.306743: Epoch time: 42.12 s\n",
      "2024-01-11 10:59:12.816402: \n",
      "2024-01-11 10:59:12.825484: Epoch 655\n",
      "2024-01-11 10:59:12.830471: Current learning rate: 0.00384\n",
      "2024-01-11 10:59:55.064570: train_loss -0.95\n",
      "2024-01-11 10:59:55.073570: val_loss -0.8319\n",
      "2024-01-11 10:59:55.107090: Pseudo dice [0.8932, 0.9489, 0.936]\n",
      "2024-01-11 10:59:55.116090: Epoch time: 42.25 s\n",
      "2024-01-11 10:59:56.637222: \n",
      "2024-01-11 10:59:56.643314: Epoch 656\n",
      "2024-01-11 10:59:56.649599: Current learning rate: 0.00383\n",
      "2024-01-11 11:00:38.218035: train_loss -0.9501\n",
      "2024-01-11 11:00:38.249034: val_loss -0.8303\n",
      "2024-01-11 11:00:38.259034: Pseudo dice [0.8899, 0.9481, 0.9363]\n",
      "2024-01-11 11:00:38.267035: Epoch time: 41.58 s\n",
      "2024-01-11 11:00:39.701001: \n",
      "2024-01-11 11:00:39.710845: Epoch 657\n",
      "2024-01-11 11:00:39.715898: Current learning rate: 0.00382\n",
      "2024-01-11 11:01:20.320434: train_loss -0.9495\n",
      "2024-01-11 11:01:20.328435: val_loss -0.8278\n",
      "2024-01-11 11:01:20.338447: Pseudo dice [0.8861, 0.9483, 0.9362]\n",
      "2024-01-11 11:01:20.344438: Epoch time: 40.62 s\n",
      "2024-01-11 11:01:21.796399: \n",
      "2024-01-11 11:01:21.801550: Epoch 658\n",
      "2024-01-11 11:01:21.806564: Current learning rate: 0.00381\n",
      "2024-01-11 11:02:02.625400: train_loss -0.9496\n",
      "2024-01-11 11:02:02.660403: val_loss -0.8275\n",
      "2024-01-11 11:02:02.668403: Pseudo dice [0.8841, 0.949, 0.9372]\n",
      "2024-01-11 11:02:02.675915: Epoch time: 40.83 s\n",
      "2024-01-11 11:02:04.277995: \n",
      "2024-01-11 11:02:04.284071: Epoch 659\n",
      "2024-01-11 11:02:04.289053: Current learning rate: 0.0038\n",
      "2024-01-11 11:02:45.197922: train_loss -0.9496\n",
      "2024-01-11 11:02:45.221435: val_loss -0.8296\n",
      "2024-01-11 11:02:45.229873: Pseudo dice [0.8862, 0.9486, 0.9361]\n",
      "2024-01-11 11:02:45.238875: Epoch time: 40.92 s\n",
      "2024-01-11 11:02:46.754623: \n",
      "2024-01-11 11:02:46.760260: Epoch 660\n",
      "2024-01-11 11:02:46.764339: Current learning rate: 0.00379\n",
      "2024-01-11 11:03:27.818516: train_loss -0.95\n",
      "2024-01-11 11:03:27.829005: val_loss -0.8294\n",
      "2024-01-11 11:03:27.866518: Pseudo dice [0.888, 0.9478, 0.9352]\n",
      "2024-01-11 11:03:27.873518: Epoch time: 41.06 s\n",
      "2024-01-11 11:03:29.319905: \n",
      "2024-01-11 11:03:29.324897: Epoch 661\n",
      "2024-01-11 11:03:29.329907: Current learning rate: 0.00378\n",
      "2024-01-11 11:04:10.372315: train_loss -0.95\n",
      "2024-01-11 11:04:10.382916: val_loss -0.8275\n",
      "2024-01-11 11:04:10.396163: Pseudo dice [0.8914, 0.9482, 0.9367]\n",
      "2024-01-11 11:04:10.430284: Epoch time: 41.05 s\n",
      "2024-01-11 11:04:11.914188: \n",
      "2024-01-11 11:04:11.920266: Epoch 662\n",
      "2024-01-11 11:04:11.924340: Current learning rate: 0.00377\n",
      "2024-01-11 11:04:53.617893: train_loss -0.9495\n",
      "2024-01-11 11:04:53.627896: val_loss -0.8268\n",
      "2024-01-11 11:04:53.634893: Pseudo dice [0.8894, 0.948, 0.9354]\n",
      "2024-01-11 11:04:53.641906: Epoch time: 41.7 s\n",
      "2024-01-11 11:04:55.228420: \n",
      "2024-01-11 11:04:55.236715: Epoch 663\n",
      "2024-01-11 11:04:55.244771: Current learning rate: 0.00376\n",
      "2024-01-11 11:05:36.898520: train_loss -0.9493\n",
      "2024-01-11 11:05:36.907046: val_loss -0.8322\n",
      "2024-01-11 11:05:36.914046: Pseudo dice [0.8885, 0.9491, 0.9363]\n",
      "2024-01-11 11:05:36.922048: Epoch time: 41.67 s\n",
      "2024-01-11 11:05:38.633981: \n",
      "2024-01-11 11:05:38.639910: Epoch 664\n",
      "2024-01-11 11:05:38.645521: Current learning rate: 0.00375\n",
      "2024-01-11 11:06:19.944183: train_loss -0.9498\n",
      "2024-01-11 11:06:19.953193: val_loss -0.8334\n",
      "2024-01-11 11:06:19.961452: Pseudo dice [0.8912, 0.9485, 0.9373]\n",
      "2024-01-11 11:06:19.968453: Epoch time: 41.31 s\n",
      "2024-01-11 11:06:21.420048: \n",
      "2024-01-11 11:06:21.426048: Epoch 665\n",
      "2024-01-11 11:06:21.431050: Current learning rate: 0.00374\n",
      "2024-01-11 11:07:02.993750: train_loss -0.9502\n",
      "2024-01-11 11:07:03.003203: val_loss -0.8339\n",
      "2024-01-11 11:07:03.011220: Pseudo dice [0.8899, 0.9494, 0.9381]\n",
      "2024-01-11 11:07:03.022230: Epoch time: 41.57 s\n",
      "2024-01-11 11:07:04.608322: \n",
      "2024-01-11 11:07:04.614841: Epoch 666\n",
      "2024-01-11 11:07:04.620591: Current learning rate: 0.00373\n",
      "2024-01-11 11:07:46.063161: train_loss -0.95\n",
      "2024-01-11 11:07:46.074164: val_loss -0.8302\n",
      "2024-01-11 11:07:46.081165: Pseudo dice [0.8854, 0.9486, 0.937]\n",
      "2024-01-11 11:07:46.089674: Epoch time: 41.46 s\n",
      "2024-01-11 11:07:47.586430: \n",
      "2024-01-11 11:07:47.592480: Epoch 667\n",
      "2024-01-11 11:07:47.598095: Current learning rate: 0.00372\n",
      "2024-01-11 11:08:29.117989: train_loss -0.9499\n",
      "2024-01-11 11:08:29.129988: val_loss -0.835\n",
      "2024-01-11 11:08:29.139992: Pseudo dice [0.8923, 0.9492, 0.9379]\n",
      "2024-01-11 11:08:29.145991: Epoch time: 41.53 s\n",
      "2024-01-11 11:08:30.747044: \n",
      "2024-01-11 11:08:30.756476: Epoch 668\n",
      "2024-01-11 11:08:30.761536: Current learning rate: 0.00371\n",
      "2024-01-11 11:09:12.156017: train_loss -0.9503\n",
      "2024-01-11 11:09:12.169018: val_loss -0.8344\n",
      "2024-01-11 11:09:12.177019: Pseudo dice [0.8911, 0.9496, 0.9376]\n",
      "2024-01-11 11:09:12.184016: Epoch time: 41.41 s\n",
      "2024-01-11 11:09:13.790185: \n",
      "2024-01-11 11:09:13.795826: Epoch 669\n",
      "2024-01-11 11:09:13.800892: Current learning rate: 0.0037\n",
      "2024-01-11 11:09:55.600279: train_loss -0.9504\n",
      "2024-01-11 11:09:55.638806: val_loss -0.8292\n",
      "2024-01-11 11:09:55.647806: Pseudo dice [0.8873, 0.9488, 0.9376]\n",
      "2024-01-11 11:09:55.653806: Epoch time: 41.81 s\n",
      "2024-01-11 11:09:57.519096: \n",
      "2024-01-11 11:09:57.526114: Epoch 670\n",
      "2024-01-11 11:09:57.530607: Current learning rate: 0.00369\n",
      "2024-01-11 11:10:39.785520: train_loss -0.9493\n",
      "2024-01-11 11:10:39.794520: val_loss -0.8327\n",
      "2024-01-11 11:10:39.801519: Pseudo dice [0.8898, 0.9485, 0.9362]\n",
      "2024-01-11 11:10:39.810521: Epoch time: 42.27 s\n",
      "2024-01-11 11:10:41.280699: \n",
      "2024-01-11 11:10:41.287709: Epoch 671\n",
      "2024-01-11 11:10:41.291726: Current learning rate: 0.00368\n",
      "2024-01-11 11:11:23.088763: train_loss -0.9499\n",
      "2024-01-11 11:11:23.100062: val_loss -0.8311\n",
      "2024-01-11 11:11:23.133060: Pseudo dice [0.8905, 0.9493, 0.9373]\n",
      "2024-01-11 11:11:23.142568: Epoch time: 41.81 s\n",
      "2024-01-11 11:11:24.734734: \n",
      "2024-01-11 11:11:24.740739: Epoch 672\n",
      "2024-01-11 11:11:24.745734: Current learning rate: 0.00367\n",
      "2024-01-11 11:12:06.483115: train_loss -0.95\n",
      "2024-01-11 11:12:06.494116: val_loss -0.8336\n",
      "2024-01-11 11:12:06.502115: Pseudo dice [0.8915, 0.9496, 0.9377]\n",
      "2024-01-11 11:12:06.531630: Epoch time: 41.75 s\n",
      "2024-01-11 11:12:08.122753: \n",
      "2024-01-11 11:12:08.128296: Epoch 673\n",
      "2024-01-11 11:12:08.133284: Current learning rate: 0.00366\n",
      "2024-01-11 11:12:50.866113: train_loss -0.95\n",
      "2024-01-11 11:12:50.880832: val_loss -0.8294\n",
      "2024-01-11 11:12:50.892123: Pseudo dice [0.8852, 0.9483, 0.9364]\n",
      "2024-01-11 11:12:50.905524: Epoch time: 42.75 s\n",
      "2024-01-11 11:12:52.610417: \n",
      "2024-01-11 11:12:52.616430: Epoch 674\n",
      "2024-01-11 11:12:52.622417: Current learning rate: 0.00365\n",
      "2024-01-11 11:13:34.598600: train_loss -0.9491\n",
      "2024-01-11 11:13:34.631086: val_loss -0.8318\n",
      "2024-01-11 11:13:34.640408: Pseudo dice [0.8925, 0.9485, 0.9378]\n",
      "2024-01-11 11:13:34.646409: Epoch time: 41.99 s\n",
      "2024-01-11 11:13:36.337045: \n",
      "2024-01-11 11:13:36.342044: Epoch 675\n",
      "2024-01-11 11:13:36.346045: Current learning rate: 0.00364\n",
      "2024-01-11 11:14:17.829870: train_loss -0.9497\n",
      "2024-01-11 11:14:17.846868: val_loss -0.8283\n",
      "2024-01-11 11:14:17.859996: Pseudo dice [0.8915, 0.9482, 0.9361]\n",
      "2024-01-11 11:14:17.902478: Epoch time: 41.49 s\n",
      "2024-01-11 11:14:19.748769: \n",
      "2024-01-11 11:14:19.755765: Epoch 676\n",
      "2024-01-11 11:14:19.760764: Current learning rate: 0.00363\n",
      "2024-01-11 11:15:02.750752: train_loss -0.9503\n",
      "2024-01-11 11:15:02.762717: val_loss -0.8309\n",
      "2024-01-11 11:15:02.774073: Pseudo dice [0.8892, 0.9487, 0.9371]\n",
      "2024-01-11 11:15:02.810980: Epoch time: 43.0 s\n",
      "2024-01-11 11:15:04.809478: \n",
      "2024-01-11 11:15:04.816486: Epoch 677\n",
      "2024-01-11 11:15:04.821485: Current learning rate: 0.00362\n",
      "2024-01-11 11:15:47.457652: train_loss -0.9497\n",
      "2024-01-11 11:15:47.468652: val_loss -0.8332\n",
      "2024-01-11 11:15:47.476172: Pseudo dice [0.8923, 0.9492, 0.9369]\n",
      "2024-01-11 11:15:47.484173: Epoch time: 42.65 s\n",
      "2024-01-11 11:15:48.845108: \n",
      "2024-01-11 11:15:48.850796: Epoch 678\n",
      "2024-01-11 11:15:48.856804: Current learning rate: 0.00361\n",
      "2024-01-11 11:16:31.716973: train_loss -0.9502\n",
      "2024-01-11 11:16:31.724985: val_loss -0.8321\n",
      "2024-01-11 11:16:31.733980: Pseudo dice [0.8912, 0.9474, 0.9363]\n",
      "2024-01-11 11:16:31.767980: Epoch time: 42.87 s\n",
      "2024-01-11 11:16:33.280535: \n",
      "2024-01-11 11:16:33.286546: Epoch 679\n",
      "2024-01-11 11:16:33.293542: Current learning rate: 0.0036\n",
      "2024-01-11 11:17:15.228238: train_loss -0.9498\n",
      "2024-01-11 11:17:15.240756: val_loss -0.8325\n",
      "2024-01-11 11:17:15.249756: Pseudo dice [0.8877, 0.9492, 0.9371]\n",
      "2024-01-11 11:17:15.258755: Epoch time: 41.95 s\n",
      "2024-01-11 11:17:17.037558: \n",
      "2024-01-11 11:17:17.043567: Epoch 680\n",
      "2024-01-11 11:17:17.048567: Current learning rate: 0.00359\n",
      "2024-01-11 11:18:02.282246: train_loss -0.9501\n",
      "2024-01-11 11:18:02.309432: val_loss -0.8346\n",
      "2024-01-11 11:18:02.385599: Pseudo dice [0.8935, 0.9497, 0.9381]\n",
      "2024-01-11 11:18:02.408209: Epoch time: 45.25 s\n",
      "2024-01-11 11:18:04.013782: \n",
      "2024-01-11 11:18:04.020327: Epoch 681\n",
      "2024-01-11 11:18:04.029409: Current learning rate: 0.00358\n",
      "2024-01-11 11:18:46.613399: train_loss -0.9503\n",
      "2024-01-11 11:18:46.621050: val_loss -0.8344\n",
      "2024-01-11 11:18:46.653141: Pseudo dice [0.8916, 0.9493, 0.9378]\n",
      "2024-01-11 11:18:46.662660: Epoch time: 42.6 s\n",
      "2024-01-11 11:18:48.102807: \n",
      "2024-01-11 11:18:48.108249: Epoch 682\n",
      "2024-01-11 11:18:48.113309: Current learning rate: 0.00357\n",
      "2024-01-11 11:19:30.280156: train_loss -0.95\n",
      "2024-01-11 11:19:30.289159: val_loss -0.8265\n",
      "2024-01-11 11:19:30.299157: Pseudo dice [0.8864, 0.9481, 0.9355]\n",
      "2024-01-11 11:19:30.308161: Epoch time: 42.18 s\n",
      "2024-01-11 11:19:31.922672: \n",
      "2024-01-11 11:19:31.929632: Epoch 683\n",
      "2024-01-11 11:19:31.935418: Current learning rate: 0.00356\n",
      "2024-01-11 11:20:14.233095: train_loss -0.9507\n",
      "2024-01-11 11:20:14.243094: val_loss -0.8363\n",
      "2024-01-11 11:20:14.253091: Pseudo dice [0.8932, 0.949, 0.9372]\n",
      "2024-01-11 11:20:14.266096: Epoch time: 42.31 s\n",
      "2024-01-11 11:20:15.911160: \n",
      "2024-01-11 11:20:15.918444: Epoch 684\n",
      "2024-01-11 11:20:15.923525: Current learning rate: 0.00355\n",
      "2024-01-11 11:20:58.051406: train_loss -0.9499\n",
      "2024-01-11 11:20:58.082404: val_loss -0.8312\n",
      "2024-01-11 11:20:58.093404: Pseudo dice [0.8955, 0.9485, 0.9368]\n",
      "2024-01-11 11:20:58.102410: Epoch time: 42.14 s\n",
      "2024-01-11 11:20:59.653273: \n",
      "2024-01-11 11:20:59.659270: Epoch 685\n",
      "2024-01-11 11:20:59.665283: Current learning rate: 0.00354\n",
      "2024-01-11 11:21:42.506426: train_loss -0.9502\n",
      "2024-01-11 11:21:42.514435: val_loss -0.828\n",
      "2024-01-11 11:21:42.526425: Pseudo dice [0.8914, 0.9481, 0.9367]\n",
      "2024-01-11 11:21:42.535761: Epoch time: 42.85 s\n",
      "2024-01-11 11:21:44.148610: \n",
      "2024-01-11 11:21:44.155091: Epoch 686\n",
      "2024-01-11 11:21:44.165174: Current learning rate: 0.00353\n",
      "2024-01-11 11:22:26.621930: train_loss -0.9504\n",
      "2024-01-11 11:22:26.631932: val_loss -0.8273\n",
      "2024-01-11 11:22:26.642284: Pseudo dice [0.8857, 0.9488, 0.937]\n",
      "2024-01-11 11:22:26.653399: Epoch time: 42.48 s\n",
      "2024-01-11 11:22:28.390438: \n",
      "2024-01-11 11:22:28.396431: Epoch 687\n",
      "2024-01-11 11:22:28.401435: Current learning rate: 0.00352\n",
      "2024-01-11 11:23:11.436045: train_loss -0.9506\n",
      "2024-01-11 11:23:11.445045: val_loss -0.8256\n",
      "2024-01-11 11:23:11.456018: Pseudo dice [0.8874, 0.9471, 0.9346]\n",
      "2024-01-11 11:23:11.487924: Epoch time: 43.05 s\n",
      "2024-01-11 11:23:13.144401: \n",
      "2024-01-11 11:23:13.150409: Epoch 688\n",
      "2024-01-11 11:23:13.156394: Current learning rate: 0.00351\n",
      "2024-01-11 11:23:54.438577: train_loss -0.9501\n",
      "2024-01-11 11:23:54.447584: val_loss -0.831\n",
      "2024-01-11 11:23:54.457397: Pseudo dice [0.8913, 0.9482, 0.9371]\n",
      "2024-01-11 11:23:54.469518: Epoch time: 41.3 s\n",
      "2024-01-11 11:23:56.116953: \n",
      "2024-01-11 11:23:56.122955: Epoch 689\n",
      "2024-01-11 11:23:56.127454: Current learning rate: 0.0035\n",
      "2024-01-11 11:24:37.208421: train_loss -0.9508\n",
      "2024-01-11 11:24:37.220413: val_loss -0.8314\n",
      "2024-01-11 11:24:37.227546: Pseudo dice [0.8904, 0.9484, 0.9361]\n",
      "2024-01-11 11:24:37.234546: Epoch time: 41.09 s\n",
      "2024-01-11 11:24:38.727954: \n",
      "2024-01-11 11:24:38.734634: Epoch 690\n",
      "2024-01-11 11:24:38.738740: Current learning rate: 0.00349\n",
      "2024-01-11 11:25:20.402257: train_loss -0.9505\n",
      "2024-01-11 11:25:20.433775: val_loss -0.8303\n",
      "2024-01-11 11:25:20.444775: Pseudo dice [0.8925, 0.9486, 0.9375]\n",
      "2024-01-11 11:25:20.453890: Epoch time: 41.68 s\n",
      "2024-01-11 11:25:22.282499: \n",
      "2024-01-11 11:25:22.288498: Epoch 691\n",
      "2024-01-11 11:25:22.293498: Current learning rate: 0.00348\n",
      "2024-01-11 11:26:04.363667: train_loss -0.9508\n",
      "2024-01-11 11:26:04.375665: val_loss -0.8347\n",
      "2024-01-11 11:26:04.385666: Pseudo dice [0.8963, 0.9495, 0.9377]\n",
      "2024-01-11 11:26:04.397177: Epoch time: 42.08 s\n",
      "2024-01-11 11:26:05.939276: \n",
      "2024-01-11 11:26:05.944285: Epoch 692\n",
      "2024-01-11 11:26:05.949288: Current learning rate: 0.00346\n",
      "2024-01-11 11:26:47.990124: train_loss -0.9511\n",
      "2024-01-11 11:26:48.035121: val_loss -0.8334\n",
      "2024-01-11 11:26:48.044127: Pseudo dice [0.8934, 0.949, 0.9369]\n",
      "2024-01-11 11:26:48.052126: Epoch time: 42.05 s\n",
      "2024-01-11 11:26:49.432382: \n",
      "2024-01-11 11:26:49.438381: Epoch 693\n",
      "2024-01-11 11:26:49.444381: Current learning rate: 0.00345\n",
      "2024-01-11 11:27:31.079316: train_loss -0.9499\n",
      "2024-01-11 11:27:31.090314: val_loss -0.831\n",
      "2024-01-11 11:27:31.098313: Pseudo dice [0.8872, 0.9483, 0.9371]\n",
      "2024-01-11 11:27:31.106830: Epoch time: 41.65 s\n",
      "2024-01-11 11:27:32.632489: \n",
      "2024-01-11 11:27:32.638285: Epoch 694\n",
      "2024-01-11 11:27:32.644229: Current learning rate: 0.00344\n",
      "2024-01-11 11:28:13.508450: train_loss -0.9514\n",
      "2024-01-11 11:28:13.517451: val_loss -0.8277\n",
      "2024-01-11 11:28:13.525450: Pseudo dice [0.8914, 0.9473, 0.934]\n",
      "2024-01-11 11:28:13.554964: Epoch time: 40.88 s\n",
      "2024-01-11 11:28:14.999008: \n",
      "2024-01-11 11:28:15.004500: Epoch 695\n",
      "2024-01-11 11:28:15.009491: Current learning rate: 0.00343\n",
      "2024-01-11 11:28:55.584066: train_loss -0.951\n",
      "2024-01-11 11:28:55.596067: val_loss -0.832\n",
      "2024-01-11 11:28:55.607070: Pseudo dice [0.8899, 0.9482, 0.937]\n",
      "2024-01-11 11:28:55.618068: Epoch time: 40.59 s\n",
      "2024-01-11 11:28:57.120842: \n",
      "2024-01-11 11:28:57.126899: Epoch 696\n",
      "2024-01-11 11:28:57.132842: Current learning rate: 0.00342\n",
      "2024-01-11 11:29:37.898972: train_loss -0.9514\n",
      "2024-01-11 11:29:37.909974: val_loss -0.8329\n",
      "2024-01-11 11:29:37.918495: Pseudo dice [0.8923, 0.9482, 0.9364]\n",
      "2024-01-11 11:29:37.946021: Epoch time: 40.78 s\n",
      "2024-01-11 11:29:39.377752: \n",
      "2024-01-11 11:29:39.382493: Epoch 697\n",
      "2024-01-11 11:29:39.394079: Current learning rate: 0.00341\n",
      "2024-01-11 11:30:21.273215: train_loss -0.9503\n",
      "2024-01-11 11:30:21.284439: val_loss -0.8329\n",
      "2024-01-11 11:30:21.292439: Pseudo dice [0.8903, 0.9489, 0.9372]\n",
      "2024-01-11 11:30:21.300470: Epoch time: 41.9 s\n",
      "2024-01-11 11:30:22.765340: \n",
      "2024-01-11 11:30:22.771246: Epoch 698\n",
      "2024-01-11 11:30:22.776193: Current learning rate: 0.0034\n",
      "2024-01-11 11:31:04.478217: train_loss -0.9506\n",
      "2024-01-11 11:31:04.490741: val_loss -0.8359\n",
      "2024-01-11 11:31:04.537738: Pseudo dice [0.8906, 0.9499, 0.9386]\n",
      "2024-01-11 11:31:04.545979: Epoch time: 41.71 s\n",
      "2024-01-11 11:31:06.039151: \n",
      "2024-01-11 11:31:06.051024: Epoch 699\n",
      "2024-01-11 11:31:06.061957: Current learning rate: 0.00339\n",
      "2024-01-11 11:31:48.466754: train_loss -0.9513\n",
      "2024-01-11 11:31:48.474329: val_loss -0.8284\n",
      "2024-01-11 11:31:48.482682: Pseudo dice [0.8919, 0.9482, 0.9364]\n",
      "2024-01-11 11:31:48.492193: Epoch time: 42.43 s\n",
      "2024-01-11 11:31:50.364983: \n",
      "2024-01-11 11:31:50.377400: Epoch 700\n",
      "2024-01-11 11:31:50.385467: Current learning rate: 0.00338\n",
      "2024-01-11 11:32:31.922633: train_loss -0.9502\n",
      "2024-01-11 11:32:31.935639: val_loss -0.8299\n",
      "2024-01-11 11:32:31.943639: Pseudo dice [0.8873, 0.9484, 0.9366]\n",
      "2024-01-11 11:32:31.953150: Epoch time: 41.56 s\n",
      "2024-01-11 11:32:33.418468: \n",
      "2024-01-11 11:32:33.425123: Epoch 701\n",
      "2024-01-11 11:32:33.429135: Current learning rate: 0.00337\n",
      "2024-01-11 11:33:17.875607: train_loss -0.9514\n",
      "2024-01-11 11:33:17.884118: val_loss -0.8276\n",
      "2024-01-11 11:33:17.890117: Pseudo dice [0.8912, 0.9486, 0.9365]\n",
      "2024-01-11 11:33:17.896117: Epoch time: 44.46 s\n",
      "2024-01-11 11:33:19.478862: \n",
      "2024-01-11 11:33:19.486431: Epoch 702\n",
      "2024-01-11 11:33:19.491419: Current learning rate: 0.00336\n",
      "2024-01-11 11:34:01.817597: train_loss -0.951\n",
      "2024-01-11 11:34:01.830597: val_loss -0.8324\n",
      "2024-01-11 11:34:01.837592: Pseudo dice [0.8904, 0.9479, 0.9364]\n",
      "2024-01-11 11:34:01.845591: Epoch time: 42.34 s\n",
      "2024-01-11 11:34:03.379586: \n",
      "2024-01-11 11:34:03.385779: Epoch 703\n",
      "2024-01-11 11:34:03.390811: Current learning rate: 0.00335\n",
      "2024-01-11 11:34:45.440126: train_loss -0.9511\n",
      "2024-01-11 11:34:45.474643: val_loss -0.8297\n",
      "2024-01-11 11:34:45.484646: Pseudo dice [0.8867, 0.9478, 0.9358]\n",
      "2024-01-11 11:34:45.492945: Epoch time: 42.06 s\n",
      "2024-01-11 11:34:46.894155: \n",
      "2024-01-11 11:34:46.899768: Epoch 704\n",
      "2024-01-11 11:34:46.906845: Current learning rate: 0.00334\n",
      "2024-01-11 11:35:30.572039: train_loss -0.9506\n",
      "2024-01-11 11:35:30.583556: val_loss -0.8263\n",
      "2024-01-11 11:35:30.591076: Pseudo dice [0.891, 0.9467, 0.935]\n",
      "2024-01-11 11:35:30.600595: Epoch time: 43.68 s\n",
      "2024-01-11 11:35:32.286488: \n",
      "2024-01-11 11:35:32.294023: Epoch 705\n",
      "2024-01-11 11:35:32.300466: Current learning rate: 0.00333\n",
      "2024-01-11 11:36:16.501092: train_loss -0.951\n",
      "2024-01-11 11:36:16.512298: val_loss -0.8308\n",
      "2024-01-11 11:36:16.528676: Pseudo dice [0.8948, 0.9479, 0.9355]\n",
      "2024-01-11 11:36:16.538190: Epoch time: 44.22 s\n",
      "2024-01-11 11:36:18.548213: \n",
      "2024-01-11 11:36:18.556232: Epoch 706\n",
      "2024-01-11 11:36:18.563745: Current learning rate: 0.00332\n",
      "2024-01-11 11:37:03.958220: train_loss -0.9514\n",
      "2024-01-11 11:37:04.001327: val_loss -0.8285\n",
      "2024-01-11 11:37:04.013865: Pseudo dice [0.8888, 0.9489, 0.9373]\n",
      "2024-01-11 11:37:04.025664: Epoch time: 45.41 s\n",
      "2024-01-11 11:37:06.393883: \n",
      "2024-01-11 11:37:06.400418: Epoch 707\n",
      "2024-01-11 11:37:06.406419: Current learning rate: 0.00331\n",
      "2024-01-11 11:37:50.892330: train_loss -0.9506\n",
      "2024-01-11 11:37:50.923555: val_loss -0.8335\n",
      "2024-01-11 11:37:50.938554: Pseudo dice [0.8909, 0.9482, 0.9364]\n",
      "2024-01-11 11:37:50.949060: Epoch time: 44.5 s\n",
      "2024-01-11 11:37:52.695067: \n",
      "2024-01-11 11:37:52.701056: Epoch 708\n",
      "2024-01-11 11:37:52.707048: Current learning rate: 0.0033\n",
      "2024-01-11 11:38:34.941596: train_loss -0.9511\n",
      "2024-01-11 11:38:34.953595: val_loss -0.8325\n",
      "2024-01-11 11:38:34.962598: Pseudo dice [0.8909, 0.9493, 0.9373]\n",
      "2024-01-11 11:38:34.972596: Epoch time: 42.25 s\n",
      "2024-01-11 11:38:36.535988: \n",
      "2024-01-11 11:38:36.543988: Epoch 709\n",
      "2024-01-11 11:38:36.550925: Current learning rate: 0.00329\n",
      "2024-01-11 11:39:18.955028: train_loss -0.9507\n",
      "2024-01-11 11:39:18.963029: val_loss -0.831\n",
      "2024-01-11 11:39:18.971023: Pseudo dice [0.8925, 0.9485, 0.9364]\n",
      "2024-01-11 11:39:19.005024: Epoch time: 42.42 s\n",
      "2024-01-11 11:39:20.619343: \n",
      "2024-01-11 11:39:20.625002: Epoch 710\n",
      "2024-01-11 11:39:20.629937: Current learning rate: 0.00328\n",
      "2024-01-11 11:40:02.778513: train_loss -0.9511\n",
      "2024-01-11 11:40:02.787514: val_loss -0.8343\n",
      "2024-01-11 11:40:02.794514: Pseudo dice [0.8939, 0.9481, 0.9359]\n",
      "2024-01-11 11:40:02.802514: Epoch time: 42.16 s\n",
      "2024-01-11 11:40:04.282449: \n",
      "2024-01-11 11:40:04.288458: Epoch 711\n",
      "2024-01-11 11:40:04.293571: Current learning rate: 0.00327\n",
      "2024-01-11 11:40:46.212342: train_loss -0.9497\n",
      "2024-01-11 11:40:46.223350: val_loss -0.8316\n",
      "2024-01-11 11:40:46.234579: Pseudo dice [0.8901, 0.9485, 0.9375]\n",
      "2024-01-11 11:40:46.244579: Epoch time: 41.93 s\n",
      "2024-01-11 11:40:48.047485: \n",
      "2024-01-11 11:40:48.053503: Epoch 712\n",
      "2024-01-11 11:40:48.058562: Current learning rate: 0.00326\n",
      "2024-01-11 11:41:30.102888: train_loss -0.9509\n",
      "2024-01-11 11:41:30.112220: val_loss -0.832\n",
      "2024-01-11 11:41:30.120645: Pseudo dice [0.8906, 0.9484, 0.9369]\n",
      "2024-01-11 11:41:30.129104: Epoch time: 42.06 s\n",
      "2024-01-11 11:41:31.708174: \n",
      "2024-01-11 11:41:31.713462: Epoch 713\n",
      "2024-01-11 11:41:31.718183: Current learning rate: 0.00325\n",
      "2024-01-11 11:42:15.313665: train_loss -0.9508\n",
      "2024-01-11 11:42:15.345674: val_loss -0.8345\n",
      "2024-01-11 11:42:15.356184: Pseudo dice [0.8969, 0.948, 0.9362]\n",
      "2024-01-11 11:42:15.365899: Epoch time: 43.61 s\n",
      "2024-01-11 11:42:16.964759: \n",
      "2024-01-11 11:42:16.971758: Epoch 714\n",
      "2024-01-11 11:42:16.977758: Current learning rate: 0.00324\n",
      "2024-01-11 11:43:00.656183: train_loss -0.9506\n",
      "2024-01-11 11:43:00.666181: val_loss -0.8374\n",
      "2024-01-11 11:43:00.677558: Pseudo dice [0.8951, 0.9493, 0.9376]\n",
      "2024-01-11 11:43:00.688621: Epoch time: 43.69 s\n",
      "2024-01-11 11:43:02.209215: \n",
      "2024-01-11 11:43:02.216213: Epoch 715\n",
      "2024-01-11 11:43:02.221213: Current learning rate: 0.00323\n",
      "2024-01-11 11:43:47.583577: train_loss -0.9515\n",
      "2024-01-11 11:43:47.666107: val_loss -0.8313\n",
      "2024-01-11 11:43:47.684105: Pseudo dice [0.8897, 0.9489, 0.9363]\n",
      "2024-01-11 11:43:47.702106: Epoch time: 45.38 s\n",
      "2024-01-11 11:43:49.511901: \n",
      "2024-01-11 11:43:49.520945: Epoch 716\n",
      "2024-01-11 11:43:49.528943: Current learning rate: 0.00322\n",
      "2024-01-11 11:44:34.208587: train_loss -0.9504\n",
      "2024-01-11 11:44:34.222587: val_loss -0.8313\n",
      "2024-01-11 11:44:34.232591: Pseudo dice [0.8942, 0.948, 0.9364]\n",
      "2024-01-11 11:44:34.245419: Epoch time: 44.7 s\n",
      "2024-01-11 11:44:35.965482: \n",
      "2024-01-11 11:44:35.971474: Epoch 717\n",
      "2024-01-11 11:44:35.976484: Current learning rate: 0.00321\n",
      "2024-01-11 11:45:17.646173: train_loss -0.9507\n",
      "2024-01-11 11:45:17.658178: val_loss -0.8351\n",
      "2024-01-11 11:45:17.667176: Pseudo dice [0.8938, 0.9486, 0.9379]\n",
      "2024-01-11 11:45:17.696817: Epoch time: 41.68 s\n",
      "2024-01-11 11:45:19.677804: \n",
      "2024-01-11 11:45:19.686742: Epoch 718\n",
      "2024-01-11 11:45:19.691743: Current learning rate: 0.0032\n",
      "2024-01-11 11:46:01.822096: train_loss -0.9512\n",
      "2024-01-11 11:46:01.834101: val_loss -0.8276\n",
      "2024-01-11 11:46:01.845157: Pseudo dice [0.8909, 0.9481, 0.9362]\n",
      "2024-01-11 11:46:01.878048: Epoch time: 42.15 s\n",
      "2024-01-11 11:46:03.340383: \n",
      "2024-01-11 11:46:03.345816: Epoch 719\n",
      "2024-01-11 11:46:03.350899: Current learning rate: 0.00319\n",
      "2024-01-11 11:46:45.227107: train_loss -0.9504\n",
      "2024-01-11 11:46:45.257624: val_loss -0.8269\n",
      "2024-01-11 11:46:45.266625: Pseudo dice [0.8885, 0.9476, 0.9353]\n",
      "2024-01-11 11:46:45.274624: Epoch time: 41.89 s\n",
      "2024-01-11 11:46:46.828448: \n",
      "2024-01-11 11:46:46.834186: Epoch 720\n",
      "2024-01-11 11:46:46.839627: Current learning rate: 0.00318\n",
      "2024-01-11 11:47:30.393883: train_loss -0.9508\n",
      "2024-01-11 11:47:30.410131: val_loss -0.8331\n",
      "2024-01-11 11:47:30.426606: Pseudo dice [0.8857, 0.9492, 0.9363]\n",
      "2024-01-11 11:47:30.440767: Epoch time: 43.57 s\n",
      "2024-01-11 11:47:32.877179: \n",
      "2024-01-11 11:47:32.888710: Epoch 721\n",
      "2024-01-11 11:47:32.900759: Current learning rate: 0.00317\n",
      "2024-01-11 11:48:19.274064: train_loss -0.9505\n",
      "2024-01-11 11:48:19.292591: val_loss -0.8342\n",
      "2024-01-11 11:48:19.312618: Pseudo dice [0.8946, 0.9491, 0.9383]\n",
      "2024-01-11 11:48:19.331723: Epoch time: 46.4 s\n",
      "2024-01-11 11:48:21.379618: \n",
      "2024-01-11 11:48:21.385726: Epoch 722\n",
      "2024-01-11 11:48:21.398710: Current learning rate: 0.00316\n",
      "2024-01-11 11:49:05.998804: train_loss -0.9508\n",
      "2024-01-11 11:49:06.036646: val_loss -0.8328\n",
      "2024-01-11 11:49:06.046007: Pseudo dice [0.8946, 0.9497, 0.937]\n",
      "2024-01-11 11:49:06.056670: Epoch time: 44.62 s\n",
      "2024-01-11 11:49:07.661403: \n",
      "2024-01-11 11:49:07.667569: Epoch 723\n",
      "2024-01-11 11:49:07.672483: Current learning rate: 0.00315\n",
      "2024-01-11 11:49:50.256836: train_loss -0.9498\n",
      "2024-01-11 11:49:50.288515: val_loss -0.8323\n",
      "2024-01-11 11:49:50.296795: Pseudo dice [0.8951, 0.9484, 0.9375]\n",
      "2024-01-11 11:49:50.305800: Epoch time: 42.6 s\n",
      "2024-01-11 11:49:51.931838: \n",
      "2024-01-11 11:49:51.937837: Epoch 724\n",
      "2024-01-11 11:49:51.943837: Current learning rate: 0.00314\n",
      "2024-01-11 11:50:35.638110: train_loss -0.9505\n",
      "2024-01-11 11:50:35.649626: val_loss -0.8308\n",
      "2024-01-11 11:50:35.659629: Pseudo dice [0.8931, 0.9474, 0.9366]\n",
      "2024-01-11 11:50:35.673652: Epoch time: 43.71 s\n",
      "2024-01-11 11:50:37.234223: \n",
      "2024-01-11 11:50:37.240225: Epoch 725\n",
      "2024-01-11 11:50:37.245740: Current learning rate: 0.00313\n",
      "2024-01-11 11:51:19.169292: train_loss -0.9504\n",
      "2024-01-11 11:51:19.198807: val_loss -0.8305\n",
      "2024-01-11 11:51:19.209806: Pseudo dice [0.8923, 0.9484, 0.9366]\n",
      "2024-01-11 11:51:19.218806: Epoch time: 41.94 s\n",
      "2024-01-11 11:51:20.882269: \n",
      "2024-01-11 11:51:20.889259: Epoch 726\n",
      "2024-01-11 11:51:20.894258: Current learning rate: 0.00312\n",
      "2024-01-11 11:52:03.597089: train_loss -0.9509\n",
      "2024-01-11 11:52:03.611603: val_loss -0.83\n",
      "2024-01-11 11:52:03.623603: Pseudo dice [0.8915, 0.9489, 0.937]\n",
      "2024-01-11 11:52:03.635604: Epoch time: 42.72 s\n",
      "2024-01-11 11:52:05.493949: \n",
      "2024-01-11 11:52:05.500949: Epoch 727\n",
      "2024-01-11 11:52:05.506953: Current learning rate: 0.00311\n",
      "2024-01-11 11:52:48.348844: train_loss -0.9507\n",
      "2024-01-11 11:52:48.361845: val_loss -0.8272\n",
      "2024-01-11 11:52:48.375844: Pseudo dice [0.8917, 0.9485, 0.9366]\n",
      "2024-01-11 11:52:48.412070: Epoch time: 42.86 s\n",
      "2024-01-11 11:52:50.001445: \n",
      "2024-01-11 11:52:50.009448: Epoch 728\n",
      "2024-01-11 11:52:50.015449: Current learning rate: 0.0031\n",
      "2024-01-11 11:53:32.072612: train_loss -0.9514\n",
      "2024-01-11 11:53:32.082901: val_loss -0.8278\n",
      "2024-01-11 11:53:32.093908: Pseudo dice [0.8868, 0.9487, 0.9367]\n",
      "2024-01-11 11:53:32.103907: Epoch time: 42.07 s\n",
      "2024-01-11 11:53:33.544599: \n",
      "2024-01-11 11:53:33.551603: Epoch 729\n",
      "2024-01-11 11:53:33.557597: Current learning rate: 0.00309\n",
      "2024-01-11 11:54:15.994693: train_loss -0.9515\n",
      "2024-01-11 11:54:16.035722: val_loss -0.8301\n",
      "2024-01-11 11:54:16.049007: Pseudo dice [0.8877, 0.9489, 0.9369]\n",
      "2024-01-11 11:54:16.059103: Epoch time: 42.45 s\n",
      "2024-01-11 11:54:17.625441: \n",
      "2024-01-11 11:54:17.635434: Epoch 730\n",
      "2024-01-11 11:54:17.643434: Current learning rate: 0.00308\n",
      "2024-01-11 11:54:59.754139: train_loss -0.9512\n",
      "2024-01-11 11:54:59.778139: val_loss -0.8322\n",
      "2024-01-11 11:54:59.790226: Pseudo dice [0.8964, 0.9482, 0.9369]\n",
      "2024-01-11 11:54:59.801226: Epoch time: 42.13 s\n",
      "2024-01-11 11:55:01.263333: \n",
      "2024-01-11 11:55:01.269421: Epoch 731\n",
      "2024-01-11 11:55:01.274344: Current learning rate: 0.00307\n",
      "2024-01-11 11:55:44.271191: train_loss -0.9506\n",
      "2024-01-11 11:55:44.282705: val_loss -0.8313\n",
      "2024-01-11 11:55:44.293706: Pseudo dice [0.8901, 0.9489, 0.9373]\n",
      "2024-01-11 11:55:44.305705: Epoch time: 43.01 s\n",
      "2024-01-11 11:55:45.967965: \n",
      "2024-01-11 11:55:45.973956: Epoch 732\n",
      "2024-01-11 11:55:45.978506: Current learning rate: 0.00306\n",
      "2024-01-11 11:56:28.092391: train_loss -0.9514\n",
      "2024-01-11 11:56:28.102501: val_loss -0.8332\n",
      "2024-01-11 11:56:28.112556: Pseudo dice [0.893, 0.9488, 0.9373]\n",
      "2024-01-11 11:56:28.123731: Epoch time: 42.13 s\n",
      "2024-01-11 11:56:29.794832: \n",
      "2024-01-11 11:56:29.800840: Epoch 733\n",
      "2024-01-11 11:56:29.806837: Current learning rate: 0.00305\n",
      "2024-01-11 11:57:12.012721: train_loss -0.9513\n",
      "2024-01-11 11:57:12.023872: val_loss -0.8309\n",
      "2024-01-11 11:57:12.044713: Pseudo dice [0.8921, 0.9483, 0.9365]\n",
      "2024-01-11 11:57:12.055964: Epoch time: 42.22 s\n",
      "2024-01-11 11:57:13.529596: \n",
      "2024-01-11 11:57:13.534597: Epoch 734\n",
      "2024-01-11 11:57:13.541115: Current learning rate: 0.00304\n",
      "2024-01-11 11:57:56.685605: train_loss -0.9514\n",
      "2024-01-11 11:57:56.729421: val_loss -0.8253\n",
      "2024-01-11 11:57:56.741422: Pseudo dice [0.8863, 0.9473, 0.9358]\n",
      "2024-01-11 11:57:56.754429: Epoch time: 43.16 s\n",
      "2024-01-11 11:57:58.215234: \n",
      "2024-01-11 11:57:58.221244: Epoch 735\n",
      "2024-01-11 11:57:58.232243: Current learning rate: 0.00303\n",
      "2024-01-11 11:58:40.398122: train_loss -0.9512\n",
      "2024-01-11 11:58:40.409646: val_loss -0.827\n",
      "2024-01-11 11:58:40.419639: Pseudo dice [0.886, 0.9481, 0.9365]\n",
      "2024-01-11 11:58:40.457638: Epoch time: 42.18 s\n",
      "2024-01-11 11:58:41.986914: \n",
      "2024-01-11 11:58:41.995920: Epoch 736\n",
      "2024-01-11 11:58:42.004013: Current learning rate: 0.00302\n",
      "2024-01-11 11:59:24.369290: train_loss -0.9512\n",
      "2024-01-11 11:59:24.382290: val_loss -0.8275\n",
      "2024-01-11 11:59:24.392289: Pseudo dice [0.8886, 0.9481, 0.9362]\n",
      "2024-01-11 11:59:24.402289: Epoch time: 42.38 s\n",
      "2024-01-11 11:59:25.877902: \n",
      "2024-01-11 11:59:25.885896: Epoch 737\n",
      "2024-01-11 11:59:25.892941: Current learning rate: 0.00301\n",
      "2024-01-11 12:00:08.844744: train_loss -0.9519\n",
      "2024-01-11 12:00:08.856752: val_loss -0.8285\n",
      "2024-01-11 12:00:08.869425: Pseudo dice [0.891, 0.9469, 0.9348]\n",
      "2024-01-11 12:00:08.883425: Epoch time: 42.97 s\n",
      "2024-01-11 12:00:10.537968: \n",
      "2024-01-11 12:00:10.545968: Epoch 738\n",
      "2024-01-11 12:00:10.551981: Current learning rate: 0.003\n",
      "2024-01-11 12:00:53.286359: train_loss -0.9517\n",
      "2024-01-11 12:00:53.297876: val_loss -0.8318\n",
      "2024-01-11 12:00:53.309878: Pseudo dice [0.8908, 0.948, 0.9361]\n",
      "2024-01-11 12:00:53.343876: Epoch time: 42.75 s\n",
      "2024-01-11 12:00:55.012474: \n",
      "2024-01-11 12:00:55.019483: Epoch 739\n",
      "2024-01-11 12:00:55.025478: Current learning rate: 0.00299\n",
      "2024-01-11 12:01:36.730186: train_loss -0.9515\n",
      "2024-01-11 12:01:36.762537: val_loss -0.829\n",
      "2024-01-11 12:01:36.773649: Pseudo dice [0.8905, 0.9475, 0.9353]\n",
      "2024-01-11 12:01:36.783655: Epoch time: 41.72 s\n",
      "2024-01-11 12:01:38.149741: \n",
      "2024-01-11 12:01:38.155732: Epoch 740\n",
      "2024-01-11 12:01:38.161740: Current learning rate: 0.00297\n",
      "2024-01-11 12:02:19.678847: train_loss -0.951\n",
      "2024-01-11 12:02:19.688847: val_loss -0.8306\n",
      "2024-01-11 12:02:19.700847: Pseudo dice [0.89, 0.9486, 0.9367]\n",
      "2024-01-11 12:02:19.728848: Epoch time: 41.53 s\n",
      "2024-01-11 12:02:21.209281: \n",
      "2024-01-11 12:02:21.218295: Epoch 741\n",
      "2024-01-11 12:02:21.224304: Current learning rate: 0.00296\n",
      "2024-01-11 12:03:03.559031: train_loss -0.9523\n",
      "2024-01-11 12:03:03.573035: val_loss -0.8281\n",
      "2024-01-11 12:03:03.585610: Pseudo dice [0.8915, 0.9474, 0.9364]\n",
      "2024-01-11 12:03:03.597685: Epoch time: 42.35 s\n",
      "2024-01-11 12:03:05.249125: \n",
      "2024-01-11 12:03:05.255109: Epoch 742\n",
      "2024-01-11 12:03:05.261115: Current learning rate: 0.00295\n",
      "2024-01-11 12:03:48.646822: train_loss -0.9516\n",
      "2024-01-11 12:03:48.658822: val_loss -0.8303\n",
      "2024-01-11 12:03:48.669879: Pseudo dice [0.8907, 0.9484, 0.9369]\n",
      "2024-01-11 12:03:48.682004: Epoch time: 43.4 s\n",
      "2024-01-11 12:03:50.132687: \n",
      "2024-01-11 12:03:50.138679: Epoch 743\n",
      "2024-01-11 12:03:50.143686: Current learning rate: 0.00294\n",
      "2024-01-11 12:04:33.037737: train_loss -0.9518\n",
      "2024-01-11 12:04:33.053551: val_loss -0.8307\n",
      "2024-01-11 12:04:33.065551: Pseudo dice [0.8903, 0.9485, 0.9367]\n",
      "2024-01-11 12:04:33.114552: Epoch time: 42.91 s\n",
      "2024-01-11 12:04:35.186090: \n",
      "2024-01-11 12:04:35.192750: Epoch 744\n",
      "2024-01-11 12:04:35.198729: Current learning rate: 0.00293\n",
      "2024-01-11 12:05:17.982731: train_loss -0.9516\n",
      "2024-01-11 12:05:18.023790: val_loss -0.8264\n",
      "2024-01-11 12:05:18.037031: Pseudo dice [0.8885, 0.9475, 0.935]\n",
      "2024-01-11 12:05:18.048031: Epoch time: 42.8 s\n",
      "2024-01-11 12:05:19.746334: \n",
      "2024-01-11 12:05:19.752334: Epoch 745\n",
      "2024-01-11 12:05:19.758334: Current learning rate: 0.00292\n",
      "2024-01-11 12:06:02.785404: train_loss -0.9517\n",
      "2024-01-11 12:06:02.800173: val_loss -0.8298\n",
      "2024-01-11 12:06:02.835912: Pseudo dice [0.8864, 0.9482, 0.9362]\n",
      "2024-01-11 12:06:02.849913: Epoch time: 43.04 s\n",
      "2024-01-11 12:06:04.932828: \n",
      "2024-01-11 12:06:04.940825: Epoch 746\n",
      "2024-01-11 12:06:04.947817: Current learning rate: 0.00291\n",
      "2024-01-11 12:06:48.757164: train_loss -0.952\n",
      "2024-01-11 12:06:48.770571: val_loss -0.829\n",
      "2024-01-11 12:06:48.781800: Pseudo dice [0.8909, 0.9481, 0.9362]\n",
      "2024-01-11 12:06:48.795228: Epoch time: 43.83 s\n",
      "2024-01-11 12:06:50.744780: \n",
      "2024-01-11 12:06:50.752779: Epoch 747\n",
      "2024-01-11 12:06:50.759285: Current learning rate: 0.0029\n",
      "2024-01-11 12:07:34.400357: train_loss -0.9522\n",
      "2024-01-11 12:07:34.412080: val_loss -0.8287\n",
      "2024-01-11 12:07:34.426511: Pseudo dice [0.8912, 0.9488, 0.9373]\n",
      "2024-01-11 12:07:34.468133: Epoch time: 43.66 s\n",
      "2024-01-11 12:07:36.006175: \n",
      "2024-01-11 12:07:36.014175: Epoch 748\n",
      "2024-01-11 12:07:36.020176: Current learning rate: 0.00289\n",
      "2024-01-11 12:08:21.213183: train_loss -0.9516\n",
      "2024-01-11 12:08:21.226375: val_loss -0.8323\n",
      "2024-01-11 12:08:21.240241: Pseudo dice [0.8921, 0.9488, 0.9369]\n",
      "2024-01-11 12:08:21.276498: Epoch time: 45.21 s\n",
      "2024-01-11 12:08:23.228105: \n",
      "2024-01-11 12:08:23.236623: Epoch 749\n",
      "2024-01-11 12:08:23.244143: Current learning rate: 0.00288\n",
      "2024-01-11 12:09:07.454450: train_loss -0.9522\n",
      "2024-01-11 12:09:07.507088: val_loss -0.8329\n",
      "2024-01-11 12:09:07.525771: Pseudo dice [0.8885, 0.9486, 0.9374]\n",
      "2024-01-11 12:09:07.541693: Epoch time: 44.23 s\n",
      "2024-01-11 12:09:09.586005: \n",
      "2024-01-11 12:09:09.592520: Epoch 750\n",
      "2024-01-11 12:09:09.598512: Current learning rate: 0.00287\n",
      "2024-01-11 12:09:53.984494: train_loss -0.9524\n",
      "2024-01-11 12:09:53.997495: val_loss -0.8312\n",
      "2024-01-11 12:09:54.012494: Pseudo dice [0.8889, 0.9485, 0.9376]\n",
      "2024-01-11 12:09:54.028071: Epoch time: 44.4 s\n",
      "2024-01-11 12:09:55.617443: \n",
      "2024-01-11 12:09:55.623446: Epoch 751\n",
      "2024-01-11 12:09:55.629443: Current learning rate: 0.00286\n",
      "2024-01-11 12:10:38.888584: train_loss -0.9523\n",
      "2024-01-11 12:10:38.901592: val_loss -0.8278\n",
      "2024-01-11 12:10:38.915267: Pseudo dice [0.8889, 0.948, 0.9351]\n",
      "2024-01-11 12:10:38.928266: Epoch time: 43.27 s\n",
      "2024-01-11 12:10:40.481531: \n",
      "2024-01-11 12:10:40.487547: Epoch 752\n",
      "2024-01-11 12:10:40.494539: Current learning rate: 0.00285\n",
      "2024-01-11 12:11:23.350943: train_loss -0.9519\n",
      "2024-01-11 12:11:23.362911: val_loss -0.8286\n",
      "2024-01-11 12:11:23.375418: Pseudo dice [0.8916, 0.9481, 0.9373]\n",
      "2024-01-11 12:11:23.385648: Epoch time: 42.87 s\n",
      "2024-01-11 12:11:24.887412: \n",
      "2024-01-11 12:11:24.893413: Epoch 753\n",
      "2024-01-11 12:11:24.898412: Current learning rate: 0.00284\n",
      "2024-01-11 12:12:08.134518: train_loss -0.952\n",
      "2024-01-11 12:12:08.147148: val_loss -0.8323\n",
      "2024-01-11 12:12:08.158185: Pseudo dice [0.8952, 0.9481, 0.9357]\n",
      "2024-01-11 12:12:08.191236: Epoch time: 43.25 s\n",
      "2024-01-11 12:12:09.872885: \n",
      "2024-01-11 12:12:09.879414: Epoch 754\n",
      "2024-01-11 12:12:09.884407: Current learning rate: 0.00283\n",
      "2024-01-11 12:12:53.451989: train_loss -0.9513\n",
      "2024-01-11 12:12:53.465987: val_loss -0.8301\n",
      "2024-01-11 12:12:53.508019: Pseudo dice [0.8914, 0.9477, 0.9362]\n",
      "2024-01-11 12:12:53.519021: Epoch time: 43.58 s\n",
      "2024-01-11 12:12:55.454048: \n",
      "2024-01-11 12:12:55.460048: Epoch 755\n",
      "2024-01-11 12:12:55.466036: Current learning rate: 0.00282\n",
      "2024-01-11 12:13:39.026064: train_loss -0.9513\n",
      "2024-01-11 12:13:39.043225: val_loss -0.8318\n",
      "2024-01-11 12:13:39.060467: Pseudo dice [0.8912, 0.9483, 0.9366]\n",
      "2024-01-11 12:13:39.074359: Epoch time: 43.57 s\n",
      "2024-01-11 12:13:41.131641: \n",
      "2024-01-11 12:13:41.139564: Epoch 756\n",
      "2024-01-11 12:13:41.146967: Current learning rate: 0.00281\n",
      "2024-01-11 12:14:24.801383: train_loss -0.9531\n",
      "2024-01-11 12:14:24.814969: val_loss -0.8236\n",
      "2024-01-11 12:14:24.827967: Pseudo dice [0.8862, 0.9469, 0.9342]\n",
      "2024-01-11 12:14:24.838967: Epoch time: 43.67 s\n",
      "2024-01-11 12:14:26.567209: \n",
      "2024-01-11 12:14:26.573233: Epoch 757\n",
      "2024-01-11 12:14:26.578649: Current learning rate: 0.0028\n",
      "2024-01-11 12:15:09.299352: train_loss -0.951\n",
      "2024-01-11 12:15:09.312366: val_loss -0.8301\n",
      "2024-01-11 12:15:09.346243: Pseudo dice [0.8879, 0.9483, 0.9378]\n",
      "2024-01-11 12:15:09.359935: Epoch time: 42.73 s\n",
      "2024-01-11 12:15:10.977707: \n",
      "2024-01-11 12:15:10.983717: Epoch 758\n",
      "2024-01-11 12:15:10.989717: Current learning rate: 0.00279\n",
      "2024-01-11 12:15:53.810692: train_loss -0.952\n",
      "2024-01-11 12:15:53.826258: val_loss -0.8321\n",
      "2024-01-11 12:15:53.868663: Pseudo dice [0.8948, 0.9492, 0.9374]\n",
      "2024-01-11 12:15:53.882671: Epoch time: 42.83 s\n",
      "2024-01-11 12:15:55.467295: \n",
      "2024-01-11 12:15:55.473884: Epoch 759\n",
      "2024-01-11 12:15:55.479564: Current learning rate: 0.00278\n",
      "2024-01-11 12:16:38.397046: train_loss -0.9523\n",
      "2024-01-11 12:16:38.407339: val_loss -0.8383\n",
      "2024-01-11 12:16:38.417133: Pseudo dice [0.8974, 0.95, 0.9392]\n",
      "2024-01-11 12:16:38.428684: Epoch time: 42.93 s\n",
      "2024-01-11 12:16:40.115595: \n",
      "2024-01-11 12:16:40.121589: Epoch 760\n",
      "2024-01-11 12:16:40.127813: Current learning rate: 0.00277\n",
      "2024-01-11 12:17:23.561296: train_loss -0.9527\n",
      "2024-01-11 12:17:23.572411: val_loss -0.8331\n",
      "2024-01-11 12:17:23.583932: Pseudo dice [0.8918, 0.9498, 0.9373]\n",
      "2024-01-11 12:17:23.623733: Epoch time: 43.45 s\n",
      "2024-01-11 12:17:25.648764: \n",
      "2024-01-11 12:17:25.655777: Epoch 761\n",
      "2024-01-11 12:17:25.660777: Current learning rate: 0.00276\n",
      "2024-01-11 12:18:08.331331: train_loss -0.9521\n",
      "2024-01-11 12:18:08.382853: val_loss -0.8344\n",
      "2024-01-11 12:18:08.397362: Pseudo dice [0.8931, 0.9494, 0.938]\n",
      "2024-01-11 12:18:08.408364: Epoch time: 42.68 s\n",
      "2024-01-11 12:18:09.934994: \n",
      "2024-01-11 12:18:09.941101: Epoch 762\n",
      "2024-01-11 12:18:09.945997: Current learning rate: 0.00275\n",
      "2024-01-11 12:18:52.348452: train_loss -0.9521\n",
      "2024-01-11 12:18:52.361455: val_loss -0.8371\n",
      "2024-01-11 12:18:52.373453: Pseudo dice [0.8952, 0.95, 0.9388]\n",
      "2024-01-11 12:18:52.385503: Epoch time: 42.41 s\n",
      "2024-01-11 12:18:53.981054: \n",
      "2024-01-11 12:18:53.987131: Epoch 763\n",
      "2024-01-11 12:18:53.993057: Current learning rate: 0.00274\n",
      "2024-01-11 12:19:36.292381: train_loss -0.9525\n",
      "2024-01-11 12:19:36.304351: val_loss -0.8352\n",
      "2024-01-11 12:19:36.318185: Pseudo dice [0.8915, 0.9498, 0.9388]\n",
      "2024-01-11 12:19:36.328547: Epoch time: 42.31 s\n",
      "2024-01-11 12:19:38.060533: \n",
      "2024-01-11 12:19:38.067071: Epoch 764\n",
      "2024-01-11 12:19:38.072063: Current learning rate: 0.00273\n",
      "2024-01-11 12:20:20.553247: train_loss -0.9518\n",
      "2024-01-11 12:20:20.566977: val_loss -0.8307\n",
      "2024-01-11 12:20:20.578888: Pseudo dice [0.8905, 0.9485, 0.9364]\n",
      "2024-01-11 12:20:20.591776: Epoch time: 42.49 s\n",
      "2024-01-11 12:20:22.388841: \n",
      "2024-01-11 12:20:22.395781: Epoch 765\n",
      "2024-01-11 12:20:22.401290: Current learning rate: 0.00272\n",
      "2024-01-11 12:21:05.130277: train_loss -0.9523\n",
      "2024-01-11 12:21:05.169276: val_loss -0.8295\n",
      "2024-01-11 12:21:05.182367: Pseudo dice [0.8878, 0.9492, 0.9369]\n",
      "2024-01-11 12:21:05.194380: Epoch time: 42.74 s\n",
      "2024-01-11 12:21:06.912880: \n",
      "2024-01-11 12:21:06.919886: Epoch 766\n",
      "2024-01-11 12:21:06.925909: Current learning rate: 0.00271\n",
      "2024-01-11 12:21:49.276991: train_loss -0.9527\n",
      "2024-01-11 12:21:49.288996: val_loss -0.834\n",
      "2024-01-11 12:21:49.302993: Pseudo dice [0.89, 0.9496, 0.9382]\n",
      "2024-01-11 12:21:49.353001: Epoch time: 42.37 s\n",
      "2024-01-11 12:21:51.172486: \n",
      "2024-01-11 12:21:51.179027: Epoch 767\n",
      "2024-01-11 12:21:51.184113: Current learning rate: 0.0027\n",
      "2024-01-11 12:22:33.827736: train_loss -0.9528\n",
      "2024-01-11 12:22:33.869910: val_loss -0.8332\n",
      "2024-01-11 12:22:33.884359: Pseudo dice [0.888, 0.9492, 0.938]\n",
      "2024-01-11 12:22:33.896651: Epoch time: 42.66 s\n",
      "2024-01-11 12:22:35.706573: \n",
      "2024-01-11 12:22:35.713573: Epoch 768\n",
      "2024-01-11 12:22:35.720572: Current learning rate: 0.00268\n",
      "2024-01-11 12:23:18.177350: train_loss -0.9527\n",
      "2024-01-11 12:23:18.192291: val_loss -0.8311\n",
      "2024-01-11 12:23:18.231431: Pseudo dice [0.8919, 0.9484, 0.9359]\n",
      "2024-01-11 12:23:18.244440: Epoch time: 42.47 s\n",
      "2024-01-11 12:23:20.167551: \n",
      "2024-01-11 12:23:20.173566: Epoch 769\n",
      "2024-01-11 12:23:20.180555: Current learning rate: 0.00267\n",
      "2024-01-11 12:24:03.834975: train_loss -0.9523\n",
      "2024-01-11 12:24:03.845980: val_loss -0.8252\n",
      "2024-01-11 12:24:03.855980: Pseudo dice [0.8858, 0.9474, 0.9369]\n",
      "2024-01-11 12:24:03.866498: Epoch time: 43.67 s\n",
      "2024-01-11 12:24:05.850086: \n",
      "2024-01-11 12:24:05.859461: Epoch 770\n",
      "2024-01-11 12:24:05.866477: Current learning rate: 0.00266\n",
      "2024-01-11 12:24:48.184407: train_loss -0.9524\n",
      "2024-01-11 12:24:48.196921: val_loss -0.834\n",
      "2024-01-11 12:24:48.208918: Pseudo dice [0.8912, 0.9482, 0.9363]\n",
      "2024-01-11 12:24:48.237919: Epoch time: 42.34 s\n",
      "2024-01-11 12:24:49.973289: \n",
      "2024-01-11 12:24:49.981309: Epoch 771\n",
      "2024-01-11 12:24:49.986305: Current learning rate: 0.00265\n",
      "2024-01-11 12:25:32.914472: train_loss -0.9527\n",
      "2024-01-11 12:25:32.924480: val_loss -0.8334\n",
      "2024-01-11 12:25:32.935994: Pseudo dice [0.8872, 0.9484, 0.9361]\n",
      "2024-01-11 12:25:32.983013: Epoch time: 42.94 s\n",
      "2024-01-11 12:25:34.741672: \n",
      "2024-01-11 12:25:34.747674: Epoch 772\n",
      "2024-01-11 12:25:34.753672: Current learning rate: 0.00264\n",
      "2024-01-11 12:26:17.358505: train_loss -0.9521\n",
      "2024-01-11 12:26:17.397302: val_loss -0.8274\n",
      "2024-01-11 12:26:17.410023: Pseudo dice [0.8893, 0.9484, 0.9368]\n",
      "2024-01-11 12:26:17.420022: Epoch time: 42.62 s\n",
      "2024-01-11 12:26:19.301568: \n",
      "2024-01-11 12:26:19.308318: Epoch 773\n",
      "2024-01-11 12:26:19.314317: Current learning rate: 0.00263\n",
      "2024-01-11 12:27:02.216391: train_loss -0.9523\n",
      "2024-01-11 12:27:02.231663: val_loss -0.8284\n",
      "2024-01-11 12:27:02.245659: Pseudo dice [0.8855, 0.9489, 0.9359]\n",
      "2024-01-11 12:27:02.259132: Epoch time: 42.92 s\n",
      "2024-01-11 12:27:04.060896: \n",
      "2024-01-11 12:27:04.069035: Epoch 774\n",
      "2024-01-11 12:27:04.075033: Current learning rate: 0.00262\n",
      "2024-01-11 12:27:47.426618: train_loss -0.9531\n",
      "2024-01-11 12:27:47.441674: val_loss -0.8278\n",
      "2024-01-11 12:27:47.454976: Pseudo dice [0.8905, 0.9473, 0.9362]\n",
      "2024-01-11 12:27:47.466311: Epoch time: 43.37 s\n",
      "2024-01-11 12:27:49.330681: \n",
      "2024-01-11 12:27:49.337794: Epoch 775\n",
      "2024-01-11 12:27:49.342755: Current learning rate: 0.00261\n",
      "2024-01-11 12:28:32.805343: train_loss -0.9532\n",
      "2024-01-11 12:28:32.818569: val_loss -0.8271\n",
      "2024-01-11 12:28:32.852401: Pseudo dice [0.8888, 0.9468, 0.9354]\n",
      "2024-01-11 12:28:32.862907: Epoch time: 43.47 s\n",
      "2024-01-11 12:28:34.630536: \n",
      "2024-01-11 12:28:34.636547: Epoch 776\n",
      "2024-01-11 12:28:34.642533: Current learning rate: 0.0026\n",
      "2024-01-11 12:29:18.044950: train_loss -0.9532\n",
      "2024-01-11 12:29:18.059023: val_loss -0.8324\n",
      "2024-01-11 12:29:18.071022: Pseudo dice [0.8906, 0.9487, 0.9367]\n",
      "2024-01-11 12:29:18.106000: Epoch time: 43.42 s\n",
      "2024-01-11 12:29:19.973697: \n",
      "2024-01-11 12:29:19.980696: Epoch 777\n",
      "2024-01-11 12:29:19.985696: Current learning rate: 0.00259\n",
      "2024-01-11 12:30:06.244412: train_loss -0.952\n",
      "2024-01-11 12:30:06.261487: val_loss -0.8318\n",
      "2024-01-11 12:30:06.277255: Pseudo dice [0.8922, 0.9487, 0.9361]\n",
      "2024-01-11 12:30:06.290060: Epoch time: 46.27 s\n",
      "2024-01-11 12:30:08.161930: \n",
      "2024-01-11 12:30:08.171928: Epoch 778\n",
      "2024-01-11 12:30:08.177939: Current learning rate: 0.00258\n",
      "2024-01-11 12:30:51.260410: train_loss -0.9531\n",
      "2024-01-11 12:30:51.274550: val_loss -0.8272\n",
      "2024-01-11 12:30:51.285797: Pseudo dice [0.8889, 0.9491, 0.9374]\n",
      "2024-01-11 12:30:51.298893: Epoch time: 43.1 s\n",
      "2024-01-11 12:30:53.293448: \n",
      "2024-01-11 12:30:53.299447: Epoch 779\n",
      "2024-01-11 12:30:53.305630: Current learning rate: 0.00257\n",
      "2024-01-11 12:31:38.644380: train_loss -0.9526\n",
      "2024-01-11 12:31:38.654437: val_loss -0.8312\n",
      "2024-01-11 12:31:38.665440: Pseudo dice [0.8892, 0.9492, 0.9378]\n",
      "2024-01-11 12:31:38.675802: Epoch time: 45.35 s\n",
      "2024-01-11 12:31:40.269516: \n",
      "2024-01-11 12:31:40.277530: Epoch 780\n",
      "2024-01-11 12:31:40.286517: Current learning rate: 0.00256\n",
      "2024-01-11 12:32:23.265652: train_loss -0.9532\n",
      "2024-01-11 12:32:23.276656: val_loss -0.8307\n",
      "2024-01-11 12:32:23.287653: Pseudo dice [0.8904, 0.9484, 0.937]\n",
      "2024-01-11 12:32:23.322870: Epoch time: 43.0 s\n",
      "2024-01-11 12:32:24.985624: \n",
      "2024-01-11 12:32:24.993443: Epoch 781\n",
      "2024-01-11 12:32:25.000033: Current learning rate: 0.00255\n",
      "2024-01-11 12:33:08.809105: train_loss -0.9532\n",
      "2024-01-11 12:33:08.842391: val_loss -0.8309\n",
      "2024-01-11 12:33:08.855372: Pseudo dice [0.8895, 0.9489, 0.9374]\n",
      "2024-01-11 12:33:08.866510: Epoch time: 43.83 s\n",
      "2024-01-11 12:33:10.790642: \n",
      "2024-01-11 12:33:10.800179: Epoch 782\n",
      "2024-01-11 12:33:10.806976: Current learning rate: 0.00254\n",
      "2024-01-11 12:33:54.341836: train_loss -0.9524\n",
      "2024-01-11 12:33:54.355480: val_loss -0.8279\n",
      "2024-01-11 12:33:54.369956: Pseudo dice [0.8851, 0.9483, 0.9361]\n",
      "2024-01-11 12:33:54.385969: Epoch time: 43.55 s\n",
      "2024-01-11 12:33:56.218612: \n",
      "2024-01-11 12:33:56.225601: Epoch 783\n",
      "2024-01-11 12:33:56.230592: Current learning rate: 0.00253\n",
      "2024-01-11 12:34:38.971929: train_loss -0.9524\n",
      "2024-01-11 12:34:38.984970: val_loss -0.8319\n",
      "2024-01-11 12:34:38.996502: Pseudo dice [0.8899, 0.9484, 0.9367]\n",
      "2024-01-11 12:34:39.046330: Epoch time: 42.75 s\n",
      "2024-01-11 12:34:41.001607: \n",
      "2024-01-11 12:34:41.008138: Epoch 784\n",
      "2024-01-11 12:34:41.013138: Current learning rate: 0.00252\n",
      "2024-01-11 12:35:22.912693: train_loss -0.9532\n",
      "2024-01-11 12:35:22.926847: val_loss -0.8319\n",
      "2024-01-11 12:35:22.939164: Pseudo dice [0.8916, 0.9487, 0.937]\n",
      "2024-01-11 12:35:22.949163: Epoch time: 41.91 s\n",
      "2024-01-11 12:35:24.657892: \n",
      "2024-01-11 12:35:24.666010: Epoch 785\n",
      "2024-01-11 12:35:24.671004: Current learning rate: 0.00251\n",
      "2024-01-11 12:36:06.363288: train_loss -0.9528\n",
      "2024-01-11 12:36:06.374286: val_loss -0.8332\n",
      "2024-01-11 12:36:06.385289: Pseudo dice [0.8946, 0.9485, 0.9366]\n",
      "2024-01-11 12:36:06.393288: Epoch time: 41.71 s\n",
      "2024-01-11 12:36:07.977919: \n",
      "2024-01-11 12:36:07.985958: Epoch 786\n",
      "2024-01-11 12:36:07.990964: Current learning rate: 0.0025\n",
      "2024-01-11 12:36:49.237025: train_loss -0.9527\n",
      "2024-01-11 12:36:49.247532: val_loss -0.8286\n",
      "2024-01-11 12:36:49.257016: Pseudo dice [0.8919, 0.9482, 0.9369]\n",
      "2024-01-11 12:36:49.289031: Epoch time: 41.26 s\n",
      "2024-01-11 12:36:50.890510: \n",
      "2024-01-11 12:36:50.902514: Epoch 787\n",
      "2024-01-11 12:36:50.908642: Current learning rate: 0.00249\n",
      "2024-01-11 12:37:32.306433: train_loss -0.9533\n",
      "2024-01-11 12:37:32.316943: val_loss -0.8318\n",
      "2024-01-11 12:37:32.326943: Pseudo dice [0.8913, 0.9481, 0.9367]\n",
      "2024-01-11 12:37:32.362952: Epoch time: 41.42 s\n",
      "2024-01-11 12:37:33.947767: \n",
      "2024-01-11 12:37:33.953764: Epoch 788\n",
      "2024-01-11 12:37:33.958763: Current learning rate: 0.00248\n",
      "2024-01-11 12:38:17.369775: train_loss -0.952\n",
      "2024-01-11 12:38:17.384849: val_loss -0.8332\n",
      "2024-01-11 12:38:17.395246: Pseudo dice [0.8896, 0.9493, 0.9388]\n",
      "2024-01-11 12:38:17.408168: Epoch time: 43.42 s\n",
      "2024-01-11 12:38:19.195689: \n",
      "2024-01-11 12:38:19.203690: Epoch 789\n",
      "2024-01-11 12:38:19.208807: Current learning rate: 0.00247\n",
      "2024-01-11 12:39:02.878101: train_loss -0.9528\n",
      "2024-01-11 12:39:02.893939: val_loss -0.8289\n",
      "2024-01-11 12:39:02.914264: Pseudo dice [0.8884, 0.9483, 0.9373]\n",
      "2024-01-11 12:39:02.968616: Epoch time: 43.68 s\n",
      "2024-01-11 12:39:04.746418: \n",
      "2024-01-11 12:39:04.752417: Epoch 790\n",
      "2024-01-11 12:39:04.757760: Current learning rate: 0.00245\n",
      "2024-01-11 12:39:47.636485: train_loss -0.9534\n",
      "2024-01-11 12:39:47.647997: val_loss -0.8312\n",
      "2024-01-11 12:39:47.657209: Pseudo dice [0.8917, 0.949, 0.937]\n",
      "2024-01-11 12:39:47.669218: Epoch time: 42.89 s\n",
      "2024-01-11 12:39:49.434163: \n",
      "2024-01-11 12:39:49.442164: Epoch 791\n",
      "2024-01-11 12:39:49.449166: Current learning rate: 0.00244\n",
      "2024-01-11 12:40:33.293727: train_loss -0.9529\n",
      "2024-01-11 12:40:33.309520: val_loss -0.8346\n",
      "2024-01-11 12:40:33.323529: Pseudo dice [0.8956, 0.9486, 0.9373]\n",
      "2024-01-11 12:40:33.337676: Epoch time: 43.86 s\n",
      "2024-01-11 12:40:35.521107: \n",
      "2024-01-11 12:40:35.528106: Epoch 792\n",
      "2024-01-11 12:40:35.534116: Current learning rate: 0.00243\n",
      "2024-01-11 12:41:19.876293: train_loss -0.9529\n",
      "2024-01-11 12:41:19.913305: val_loss -0.8362\n",
      "2024-01-11 12:41:19.925305: Pseudo dice [0.8929, 0.9492, 0.938]\n",
      "2024-01-11 12:41:19.937305: Epoch time: 44.36 s\n",
      "2024-01-11 12:41:22.146318: \n",
      "2024-01-11 12:41:22.153317: Epoch 793\n",
      "2024-01-11 12:41:22.159371: Current learning rate: 0.00242\n",
      "2024-01-11 12:42:05.475896: train_loss -0.9524\n",
      "2024-01-11 12:42:05.490895: val_loss -0.8261\n",
      "2024-01-11 12:42:05.502895: Pseudo dice [0.8877, 0.9482, 0.9361]\n",
      "2024-01-11 12:42:05.514903: Epoch time: 43.33 s\n",
      "2024-01-11 12:42:07.185848: \n",
      "2024-01-11 12:42:07.192862: Epoch 794\n",
      "2024-01-11 12:42:07.200849: Current learning rate: 0.00241\n",
      "2024-01-11 12:42:50.020813: train_loss -0.9535\n",
      "2024-01-11 12:42:50.033810: val_loss -0.8277\n",
      "2024-01-11 12:42:50.044813: Pseudo dice [0.893, 0.9476, 0.9364]\n",
      "2024-01-11 12:42:50.077329: Epoch time: 42.84 s\n",
      "2024-01-11 12:42:51.616131: \n",
      "2024-01-11 12:42:51.622133: Epoch 795\n",
      "2024-01-11 12:42:51.627137: Current learning rate: 0.0024\n",
      "2024-01-11 12:43:34.979278: train_loss -0.9533\n",
      "2024-01-11 12:43:34.993604: val_loss -0.8298\n",
      "2024-01-11 12:43:35.004622: Pseudo dice [0.8888, 0.9484, 0.9365]\n",
      "2024-01-11 12:43:35.042623: Epoch time: 43.36 s\n",
      "2024-01-11 12:43:36.733219: \n",
      "2024-01-11 12:43:36.740218: Epoch 796\n",
      "2024-01-11 12:43:36.746218: Current learning rate: 0.00239\n",
      "2024-01-11 12:44:17.363304: train_loss -0.9535\n",
      "2024-01-11 12:44:17.375302: val_loss -0.8316\n",
      "2024-01-11 12:44:17.385303: Pseudo dice [0.8937, 0.9483, 0.9368]\n",
      "2024-01-11 12:44:17.396304: Epoch time: 40.63 s\n",
      "2024-01-11 12:44:18.722191: \n",
      "2024-01-11 12:44:18.728706: Epoch 797\n",
      "2024-01-11 12:44:18.733718: Current learning rate: 0.00238\n",
      "2024-01-11 12:44:59.154747: train_loss -0.9526\n",
      "2024-01-11 12:44:59.167412: val_loss -0.8329\n",
      "2024-01-11 12:44:59.179410: Pseudo dice [0.8958, 0.9491, 0.9371]\n",
      "2024-01-11 12:44:59.190905: Epoch time: 40.43 s\n",
      "2024-01-11 12:45:00.676635: \n",
      "2024-01-11 12:45:00.682796: Epoch 798\n",
      "2024-01-11 12:45:00.687780: Current learning rate: 0.00237\n",
      "2024-01-11 12:45:41.139616: train_loss -0.9525\n",
      "2024-01-11 12:45:41.175617: val_loss -0.8313\n",
      "2024-01-11 12:45:41.186629: Pseudo dice [0.8909, 0.9487, 0.937]\n",
      "2024-01-11 12:45:41.197141: Epoch time: 40.46 s\n",
      "2024-01-11 12:45:42.533668: \n",
      "2024-01-11 12:45:42.539669: Epoch 799\n",
      "2024-01-11 12:45:42.544669: Current learning rate: 0.00236\n",
      "2024-01-11 12:46:23.080200: train_loss -0.9531\n",
      "2024-01-11 12:46:23.092615: val_loss -0.8267\n",
      "2024-01-11 12:46:23.103682: Pseudo dice [0.8889, 0.9474, 0.9358]\n",
      "2024-01-11 12:46:23.114691: Epoch time: 40.55 s\n",
      "2024-01-11 12:46:24.626838: \n",
      "2024-01-11 12:46:24.632838: Epoch 800\n",
      "2024-01-11 12:46:24.637839: Current learning rate: 0.00235\n",
      "2024-01-11 12:47:05.183113: train_loss -0.9528\n",
      "2024-01-11 12:47:05.195112: val_loss -0.8316\n",
      "2024-01-11 12:47:05.205131: Pseudo dice [0.895, 0.9485, 0.937]\n",
      "2024-01-11 12:47:05.240130: Epoch time: 40.56 s\n",
      "2024-01-11 12:47:06.522396: \n",
      "2024-01-11 12:47:06.530396: Epoch 801\n",
      "2024-01-11 12:47:06.535398: Current learning rate: 0.00234\n",
      "2024-01-11 12:47:47.053365: train_loss -0.9533\n",
      "2024-01-11 12:47:47.064366: val_loss -0.8308\n",
      "2024-01-11 12:47:47.076375: Pseudo dice [0.8885, 0.9478, 0.9365]\n",
      "2024-01-11 12:47:47.087335: Epoch time: 40.53 s\n",
      "2024-01-11 12:47:48.395916: \n",
      "2024-01-11 12:47:48.401908: Epoch 802\n",
      "2024-01-11 12:47:48.405909: Current learning rate: 0.00233\n",
      "2024-01-11 12:48:29.011011: train_loss -0.9535\n",
      "2024-01-11 12:48:29.022520: val_loss -0.8285\n",
      "2024-01-11 12:48:29.032520: Pseudo dice [0.8918, 0.948, 0.9368]\n",
      "2024-01-11 12:48:29.042522: Epoch time: 40.62 s\n",
      "2024-01-11 12:48:30.372416: \n",
      "2024-01-11 12:48:30.382496: Epoch 803\n",
      "2024-01-11 12:48:30.387425: Current learning rate: 0.00232\n",
      "2024-01-11 12:49:11.151597: train_loss -0.954\n",
      "2024-01-11 12:49:11.167598: val_loss -0.8292\n",
      "2024-01-11 12:49:11.200598: Pseudo dice [0.8896, 0.9485, 0.9366]\n",
      "2024-01-11 12:49:11.210596: Epoch time: 40.78 s\n",
      "2024-01-11 12:49:12.478394: \n",
      "2024-01-11 12:49:12.484400: Epoch 804\n",
      "2024-01-11 12:49:12.490385: Current learning rate: 0.00231\n",
      "2024-01-11 12:49:52.924073: train_loss -0.9531\n",
      "2024-01-11 12:49:52.970597: val_loss -0.8349\n",
      "2024-01-11 12:49:52.983112: Pseudo dice [0.893, 0.9489, 0.9376]\n",
      "2024-01-11 12:49:52.993114: Epoch time: 40.45 s\n",
      "2024-01-11 12:49:54.326412: \n",
      "2024-01-11 12:49:54.333401: Epoch 805\n",
      "2024-01-11 12:49:54.338402: Current learning rate: 0.0023\n",
      "2024-01-11 12:50:34.761866: train_loss -0.953\n",
      "2024-01-11 12:50:34.775865: val_loss -0.8308\n",
      "2024-01-11 12:50:34.811701: Pseudo dice [0.8904, 0.9486, 0.9374]\n",
      "2024-01-11 12:50:34.823695: Epoch time: 40.44 s\n",
      "2024-01-11 12:50:36.136193: \n",
      "2024-01-11 12:50:36.143185: Epoch 806\n",
      "2024-01-11 12:50:36.149186: Current learning rate: 0.00229\n",
      "2024-01-11 12:51:16.757406: train_loss -0.9527\n",
      "2024-01-11 12:51:16.767401: val_loss -0.8343\n",
      "2024-01-11 12:51:16.778401: Pseudo dice [0.8929, 0.9488, 0.9374]\n",
      "2024-01-11 12:51:16.812826: Epoch time: 40.62 s\n",
      "2024-01-11 12:51:18.080013: \n",
      "2024-01-11 12:51:18.086004: Epoch 807\n",
      "2024-01-11 12:51:18.091011: Current learning rate: 0.00228\n",
      "2024-01-11 12:51:58.609556: train_loss -0.9531\n",
      "2024-01-11 12:51:58.621739: val_loss -0.8322\n",
      "2024-01-11 12:51:58.631776: Pseudo dice [0.8931, 0.949, 0.9374]\n",
      "2024-01-11 12:51:58.671293: Epoch time: 40.53 s\n",
      "2024-01-11 12:51:59.978982: \n",
      "2024-01-11 12:51:59.984967: Epoch 808\n",
      "2024-01-11 12:51:59.989966: Current learning rate: 0.00226\n",
      "2024-01-11 12:52:40.330747: train_loss -0.9541\n",
      "2024-01-11 12:52:40.340749: val_loss -0.8356\n",
      "2024-01-11 12:52:40.353017: Pseudo dice [0.8917, 0.9497, 0.9385]\n",
      "2024-01-11 12:52:40.365017: Epoch time: 40.35 s\n",
      "2024-01-11 12:52:41.682404: \n",
      "2024-01-11 12:52:41.688404: Epoch 809\n",
      "2024-01-11 12:52:41.693426: Current learning rate: 0.00225\n",
      "2024-01-11 12:53:22.196701: train_loss -0.9532\n",
      "2024-01-11 12:53:22.209701: val_loss -0.8325\n",
      "2024-01-11 12:53:22.219133: Pseudo dice [0.8907, 0.9491, 0.9372]\n",
      "2024-01-11 12:53:22.231127: Epoch time: 40.52 s\n",
      "2024-01-11 12:53:23.534552: \n",
      "2024-01-11 12:53:23.541151: Epoch 810\n",
      "2024-01-11 12:53:23.545676: Current learning rate: 0.00224\n",
      "2024-01-11 12:54:03.789160: train_loss -0.9532\n",
      "2024-01-11 12:54:03.800159: val_loss -0.8346\n",
      "2024-01-11 12:54:03.811160: Pseudo dice [0.8967, 0.9492, 0.9377]\n",
      "2024-01-11 12:54:03.820160: Epoch time: 40.26 s\n",
      "2024-01-11 12:54:05.123917: \n",
      "2024-01-11 12:54:05.130095: Epoch 811\n",
      "2024-01-11 12:54:05.138026: Current learning rate: 0.00223\n",
      "2024-01-11 12:54:45.229533: train_loss -0.9541\n",
      "2024-01-11 12:54:45.238535: val_loss -0.8293\n",
      "2024-01-11 12:54:45.248024: Pseudo dice [0.8922, 0.9484, 0.936]\n",
      "2024-01-11 12:54:45.257024: Epoch time: 40.11 s\n",
      "2024-01-11 12:54:46.710756: \n",
      "2024-01-11 12:54:46.718295: Epoch 812\n",
      "2024-01-11 12:54:46.725214: Current learning rate: 0.00222\n",
      "2024-01-11 12:55:26.640835: train_loss -0.9539\n",
      "2024-01-11 12:55:26.653086: val_loss -0.8324\n",
      "2024-01-11 12:55:26.663474: Pseudo dice [0.8945, 0.9485, 0.9371]\n",
      "2024-01-11 12:55:26.673858: Epoch time: 39.93 s\n",
      "2024-01-11 12:55:27.962757: \n",
      "2024-01-11 12:55:27.968753: Epoch 813\n",
      "2024-01-11 12:55:27.973756: Current learning rate: 0.00221\n",
      "2024-01-11 12:56:07.954494: train_loss -0.9542\n",
      "2024-01-11 12:56:07.969135: val_loss -0.8291\n",
      "2024-01-11 12:56:07.999430: Pseudo dice [0.8905, 0.948, 0.9366]\n",
      "2024-01-11 12:56:08.009671: Epoch time: 39.99 s\n",
      "2024-01-11 12:56:09.307007: \n",
      "2024-01-11 12:56:09.313606: Epoch 814\n",
      "2024-01-11 12:56:09.318768: Current learning rate: 0.0022\n",
      "2024-01-11 12:56:49.381852: train_loss -0.9541\n",
      "2024-01-11 12:56:49.393375: val_loss -0.8295\n",
      "2024-01-11 12:56:49.406954: Pseudo dice [0.8948, 0.9479, 0.9356]\n",
      "2024-01-11 12:56:49.417485: Epoch time: 40.08 s\n",
      "2024-01-11 12:56:50.681563: \n",
      "2024-01-11 12:56:50.687562: Epoch 815\n",
      "2024-01-11 12:56:50.692561: Current learning rate: 0.00219\n",
      "2024-01-11 12:57:30.359257: train_loss -0.9542\n",
      "2024-01-11 12:57:30.370674: val_loss -0.8321\n",
      "2024-01-11 12:57:30.398674: Pseudo dice [0.891, 0.9486, 0.9367]\n",
      "2024-01-11 12:57:30.408674: Epoch time: 39.68 s\n",
      "2024-01-11 12:57:31.668043: \n",
      "2024-01-11 12:57:31.677033: Epoch 816\n",
      "2024-01-11 12:57:31.685992: Current learning rate: 0.00218\n",
      "2024-01-11 12:58:11.512475: train_loss -0.953\n",
      "2024-01-11 12:58:11.526469: val_loss -0.8262\n",
      "2024-01-11 12:58:11.536468: Pseudo dice [0.8868, 0.9482, 0.9363]\n",
      "2024-01-11 12:58:11.546467: Epoch time: 39.85 s\n",
      "2024-01-11 12:58:12.910659: \n",
      "2024-01-11 12:58:12.916650: Epoch 817\n",
      "2024-01-11 12:58:12.922533: Current learning rate: 0.00217\n",
      "2024-01-11 12:58:52.670080: train_loss -0.9538\n",
      "2024-01-11 12:58:52.682080: val_loss -0.834\n",
      "2024-01-11 12:58:52.691080: Pseudo dice [0.8962, 0.9491, 0.9387]\n",
      "2024-01-11 12:58:52.701180: Epoch time: 39.76 s\n",
      "2024-01-11 12:58:53.957966: \n",
      "2024-01-11 12:58:53.963805: Epoch 818\n",
      "2024-01-11 12:58:53.968373: Current learning rate: 0.00216\n",
      "2024-01-11 12:59:33.669758: train_loss -0.9536\n",
      "2024-01-11 12:59:33.680757: val_loss -0.828\n",
      "2024-01-11 12:59:33.690757: Pseudo dice [0.8922, 0.9475, 0.9366]\n",
      "2024-01-11 12:59:33.700758: Epoch time: 39.71 s\n",
      "2024-01-11 12:59:34.942576: \n",
      "2024-01-11 12:59:34.954491: Epoch 819\n",
      "2024-01-11 12:59:34.963483: Current learning rate: 0.00215\n",
      "2024-01-11 13:00:14.821620: train_loss -0.9538\n",
      "2024-01-11 13:00:14.852631: val_loss -0.8276\n",
      "2024-01-11 13:00:14.863631: Pseudo dice [0.8872, 0.9479, 0.9372]\n",
      "2024-01-11 13:00:14.873153: Epoch time: 39.88 s\n",
      "2024-01-11 13:00:16.009411: \n",
      "2024-01-11 13:00:16.015410: Epoch 820\n",
      "2024-01-11 13:00:16.020446: Current learning rate: 0.00214\n",
      "2024-01-11 13:00:55.860980: train_loss -0.9542\n",
      "2024-01-11 13:00:55.870986: val_loss -0.8308\n",
      "2024-01-11 13:00:55.880490: Pseudo dice [0.8897, 0.9481, 0.9369]\n",
      "2024-01-11 13:00:55.889496: Epoch time: 39.85 s\n",
      "2024-01-11 13:00:57.039882: \n",
      "2024-01-11 13:00:57.045885: Epoch 821\n",
      "2024-01-11 13:00:57.050882: Current learning rate: 0.00213\n",
      "2024-01-11 13:01:36.740399: train_loss -0.9537\n",
      "2024-01-11 13:01:36.750400: val_loss -0.8302\n",
      "2024-01-11 13:01:36.760412: Pseudo dice [0.8886, 0.9482, 0.9353]\n",
      "2024-01-11 13:01:36.770400: Epoch time: 39.7 s\n",
      "2024-01-11 13:01:38.107157: \n",
      "2024-01-11 13:01:38.118169: Epoch 822\n",
      "2024-01-11 13:01:38.125627: Current learning rate: 0.00212\n",
      "2024-01-11 13:02:17.850285: train_loss -0.9538\n",
      "2024-01-11 13:02:17.863286: val_loss -0.8322\n",
      "2024-01-11 13:02:17.872286: Pseudo dice [0.8911, 0.9487, 0.9371]\n",
      "2024-01-11 13:02:17.881289: Epoch time: 39.74 s\n",
      "2024-01-11 13:02:19.047543: \n",
      "2024-01-11 13:02:19.053419: Epoch 823\n",
      "2024-01-11 13:02:19.057999: Current learning rate: 0.0021\n",
      "2024-01-11 13:02:58.690608: train_loss -0.9534\n",
      "2024-01-11 13:02:58.702608: val_loss -0.8332\n",
      "2024-01-11 13:02:58.734612: Pseudo dice [0.896, 0.9487, 0.9375]\n",
      "2024-01-11 13:02:58.746127: Epoch time: 39.64 s\n",
      "2024-01-11 13:02:59.949209: \n",
      "2024-01-11 13:02:59.958880: Epoch 824\n",
      "2024-01-11 13:02:59.963878: Current learning rate: 0.00209\n",
      "2024-01-11 13:03:39.778653: train_loss -0.9545\n",
      "2024-01-11 13:03:39.811711: val_loss -0.8267\n",
      "2024-01-11 13:03:39.823046: Pseudo dice [0.8903, 0.9479, 0.9368]\n",
      "2024-01-11 13:03:39.836049: Epoch time: 39.83 s\n",
      "2024-01-11 13:03:40.976486: \n",
      "2024-01-11 13:03:40.986001: Epoch 825\n",
      "2024-01-11 13:03:40.990582: Current learning rate: 0.00208\n",
      "2024-01-11 13:04:21.207874: train_loss -0.9537\n",
      "2024-01-11 13:04:21.219870: val_loss -0.8315\n",
      "2024-01-11 13:04:21.229873: Pseudo dice [0.8934, 0.9487, 0.938]\n",
      "2024-01-11 13:04:21.257875: Epoch time: 40.23 s\n",
      "2024-01-11 13:04:22.401140: \n",
      "2024-01-11 13:04:22.408272: Epoch 826\n",
      "2024-01-11 13:04:22.412798: Current learning rate: 0.00207\n",
      "2024-01-11 13:05:02.480658: train_loss -0.9535\n",
      "2024-01-11 13:05:02.493658: val_loss -0.8307\n",
      "2024-01-11 13:05:02.503663: Pseudo dice [0.8885, 0.949, 0.9378]\n",
      "2024-01-11 13:05:02.512175: Epoch time: 40.08 s\n",
      "2024-01-11 13:05:03.887466: \n",
      "2024-01-11 13:05:03.896814: Epoch 827\n",
      "2024-01-11 13:05:03.901736: Current learning rate: 0.00206\n",
      "2024-01-11 13:05:43.972955: train_loss -0.9534\n",
      "2024-01-11 13:05:43.985953: val_loss -0.8364\n",
      "2024-01-11 13:05:43.994976: Pseudo dice [0.8959, 0.9491, 0.9381]\n",
      "2024-01-11 13:05:44.003098: Epoch time: 40.09 s\n",
      "2024-01-11 13:05:45.252510: \n",
      "2024-01-11 13:05:45.264585: Epoch 828\n",
      "2024-01-11 13:05:45.269586: Current learning rate: 0.00205\n",
      "2024-01-11 13:06:25.289693: train_loss -0.9539\n",
      "2024-01-11 13:06:25.319903: val_loss -0.8322\n",
      "2024-01-11 13:06:25.332899: Pseudo dice [0.8923, 0.9488, 0.9365]\n",
      "2024-01-11 13:06:25.340905: Epoch time: 40.04 s\n",
      "2024-01-11 13:06:26.559004: \n",
      "2024-01-11 13:06:26.568994: Epoch 829\n",
      "2024-01-11 13:06:26.576492: Current learning rate: 0.00204\n",
      "2024-01-11 13:07:06.641201: train_loss -0.9544\n",
      "2024-01-11 13:07:06.652208: val_loss -0.8298\n",
      "2024-01-11 13:07:06.662206: Pseudo dice [0.8887, 0.9488, 0.9361]\n",
      "2024-01-11 13:07:06.690729: Epoch time: 40.08 s\n",
      "2024-01-11 13:07:07.873479: \n",
      "2024-01-11 13:07:07.878480: Epoch 830\n",
      "2024-01-11 13:07:07.883997: Current learning rate: 0.00203\n",
      "2024-01-11 13:07:48.019737: train_loss -0.9539\n",
      "2024-01-11 13:07:48.032249: val_loss -0.8359\n",
      "2024-01-11 13:07:48.041472: Pseudo dice [0.8947, 0.9494, 0.9384]\n",
      "2024-01-11 13:07:48.052724: Epoch time: 40.15 s\n",
      "2024-01-11 13:07:49.249860: \n",
      "2024-01-11 13:07:49.258182: Epoch 831\n",
      "2024-01-11 13:07:49.263713: Current learning rate: 0.00202\n",
      "2024-01-11 13:08:29.671104: train_loss -0.9546\n",
      "2024-01-11 13:08:29.683103: val_loss -0.8337\n",
      "2024-01-11 13:08:29.694443: Pseudo dice [0.8936, 0.9481, 0.9363]\n",
      "2024-01-11 13:08:29.703451: Epoch time: 40.42 s\n",
      "2024-01-11 13:08:31.135139: \n",
      "2024-01-11 13:08:31.141119: Epoch 832\n",
      "2024-01-11 13:08:31.145666: Current learning rate: 0.00201\n",
      "2024-01-11 13:09:11.958740: train_loss -0.9539\n",
      "2024-01-11 13:09:11.973063: val_loss -0.8332\n",
      "2024-01-11 13:09:11.987581: Pseudo dice [0.8904, 0.949, 0.9382]\n",
      "2024-01-11 13:09:12.025755: Epoch time: 40.83 s\n",
      "2024-01-11 13:09:13.430233: \n",
      "2024-01-11 13:09:13.435235: Epoch 833\n",
      "2024-01-11 13:09:13.440235: Current learning rate: 0.002\n",
      "2024-01-11 13:09:53.264573: train_loss -0.9549\n",
      "2024-01-11 13:09:53.275572: val_loss -0.8347\n",
      "2024-01-11 13:09:53.305106: Pseudo dice [0.8937, 0.9487, 0.9377]\n",
      "2024-01-11 13:09:53.314919: Epoch time: 39.84 s\n",
      "2024-01-11 13:09:54.477510: \n",
      "2024-01-11 13:09:54.485511: Epoch 834\n",
      "2024-01-11 13:09:54.490510: Current learning rate: 0.00199\n",
      "2024-01-11 13:10:34.249720: train_loss -0.9547\n",
      "2024-01-11 13:10:34.263330: val_loss -0.8306\n",
      "2024-01-11 13:10:34.293328: Pseudo dice [0.8855, 0.9487, 0.9376]\n",
      "2024-01-11 13:10:34.303329: Epoch time: 39.77 s\n",
      "2024-01-11 13:10:35.472067: \n",
      "2024-01-11 13:10:35.478111: Epoch 835\n",
      "2024-01-11 13:10:35.482649: Current learning rate: 0.00198\n",
      "2024-01-11 13:11:15.314665: train_loss -0.9537\n",
      "2024-01-11 13:11:15.327667: val_loss -0.8275\n",
      "2024-01-11 13:11:15.338670: Pseudo dice [0.8891, 0.9479, 0.936]\n",
      "2024-01-11 13:11:15.370513: Epoch time: 39.84 s\n",
      "2024-01-11 13:11:16.526573: \n",
      "2024-01-11 13:11:16.532573: Epoch 836\n",
      "2024-01-11 13:11:16.537589: Current learning rate: 0.00196\n",
      "2024-01-11 13:11:56.599522: train_loss -0.9544\n",
      "2024-01-11 13:11:56.610527: val_loss -0.8328\n",
      "2024-01-11 13:11:56.622523: Pseudo dice [0.8914, 0.9485, 0.9372]\n",
      "2024-01-11 13:11:56.647525: Epoch time: 40.07 s\n",
      "2024-01-11 13:11:57.811302: \n",
      "2024-01-11 13:11:57.817375: Epoch 837\n",
      "2024-01-11 13:11:57.822391: Current learning rate: 0.00195\n",
      "2024-01-11 13:12:37.729215: train_loss -0.9546\n",
      "2024-01-11 13:12:37.740215: val_loss -0.8311\n",
      "2024-01-11 13:12:37.751215: Pseudo dice [0.894, 0.9476, 0.9359]\n",
      "2024-01-11 13:12:37.776217: Epoch time: 39.92 s\n",
      "2024-01-11 13:12:38.913040: \n",
      "2024-01-11 13:12:38.920189: Epoch 838\n",
      "2024-01-11 13:12:38.928351: Current learning rate: 0.00194\n",
      "2024-01-11 13:13:18.884809: train_loss -0.9539\n",
      "2024-01-11 13:13:18.895810: val_loss -0.8319\n",
      "2024-01-11 13:13:18.906809: Pseudo dice [0.8921, 0.949, 0.937]\n",
      "2024-01-11 13:13:18.944324: Epoch time: 39.97 s\n",
      "2024-01-11 13:13:20.117418: \n",
      "2024-01-11 13:13:20.126488: Epoch 839\n",
      "2024-01-11 13:13:20.131062: Current learning rate: 0.00193\n",
      "2024-01-11 13:13:59.879946: train_loss -0.9546\n",
      "2024-01-11 13:13:59.909947: val_loss -0.8342\n",
      "2024-01-11 13:13:59.919945: Pseudo dice [0.8905, 0.9483, 0.9366]\n",
      "2024-01-11 13:13:59.929946: Epoch time: 39.76 s\n",
      "2024-01-11 13:14:01.106090: \n",
      "2024-01-11 13:14:01.112087: Epoch 840\n",
      "2024-01-11 13:14:01.117082: Current learning rate: 0.00192\n",
      "2024-01-11 13:14:40.933546: train_loss -0.9541\n",
      "2024-01-11 13:14:40.943877: val_loss -0.8329\n",
      "2024-01-11 13:14:40.953014: Pseudo dice [0.8925, 0.9484, 0.9369]\n",
      "2024-01-11 13:14:40.979340: Epoch time: 39.83 s\n",
      "2024-01-11 13:14:42.147049: \n",
      "2024-01-11 13:14:42.153903: Epoch 841\n",
      "2024-01-11 13:14:42.159962: Current learning rate: 0.00191\n",
      "2024-01-11 13:15:21.992147: train_loss -0.9551\n",
      "2024-01-11 13:15:22.004148: val_loss -0.8348\n",
      "2024-01-11 13:15:22.014868: Pseudo dice [0.894, 0.9486, 0.9375]\n",
      "2024-01-11 13:15:22.023880: Epoch time: 39.85 s\n",
      "2024-01-11 13:15:23.415957: \n",
      "2024-01-11 13:15:23.421957: Epoch 842\n",
      "2024-01-11 13:15:23.425957: Current learning rate: 0.0019\n",
      "2024-01-11 13:16:03.252153: train_loss -0.9544\n",
      "2024-01-11 13:16:03.264670: val_loss -0.8299\n",
      "2024-01-11 13:16:03.274668: Pseudo dice [0.8906, 0.9488, 0.9372]\n",
      "2024-01-11 13:16:03.282666: Epoch time: 39.84 s\n",
      "2024-01-11 13:16:04.472234: \n",
      "2024-01-11 13:16:04.482300: Epoch 843\n",
      "2024-01-11 13:16:04.487244: Current learning rate: 0.00189\n",
      "2024-01-11 13:16:44.186321: train_loss -0.9544\n",
      "2024-01-11 13:16:44.223318: val_loss -0.8253\n",
      "2024-01-11 13:16:44.234319: Pseudo dice [0.8894, 0.9476, 0.9364]\n",
      "2024-01-11 13:16:44.244319: Epoch time: 39.72 s\n",
      "2024-01-11 13:16:45.427980: \n",
      "2024-01-11 13:16:45.433982: Epoch 844\n",
      "2024-01-11 13:16:45.442042: Current learning rate: 0.00188\n",
      "2024-01-11 13:17:25.327613: train_loss -0.9546\n",
      "2024-01-11 13:17:25.340131: val_loss -0.8296\n",
      "2024-01-11 13:17:25.370136: Pseudo dice [0.8874, 0.9483, 0.9362]\n",
      "2024-01-11 13:17:25.381133: Epoch time: 39.9 s\n",
      "2024-01-11 13:17:26.543407: \n",
      "2024-01-11 13:17:26.554422: Epoch 845\n",
      "2024-01-11 13:17:26.560416: Current learning rate: 0.00187\n",
      "2024-01-11 13:18:06.286120: train_loss -0.9545\n",
      "2024-01-11 13:18:06.298120: val_loss -0.8318\n",
      "2024-01-11 13:18:06.336128: Pseudo dice [0.8918, 0.9484, 0.9363]\n",
      "2024-01-11 13:18:06.347656: Epoch time: 39.74 s\n",
      "2024-01-11 13:18:07.545141: \n",
      "2024-01-11 13:18:07.550262: Epoch 846\n",
      "2024-01-11 13:18:07.555262: Current learning rate: 0.00186\n",
      "2024-01-11 13:18:47.456454: train_loss -0.9552\n",
      "2024-01-11 13:18:47.468455: val_loss -0.8309\n",
      "2024-01-11 13:18:47.479980: Pseudo dice [0.8895, 0.9484, 0.9372]\n",
      "2024-01-11 13:18:47.507146: Epoch time: 39.91 s\n",
      "2024-01-11 13:18:48.662292: \n",
      "2024-01-11 13:18:48.668322: Epoch 847\n",
      "2024-01-11 13:18:48.672995: Current learning rate: 0.00185\n",
      "2024-01-11 13:19:28.616085: train_loss -0.9551\n",
      "2024-01-11 13:19:28.630083: val_loss -0.831\n",
      "2024-01-11 13:19:28.659082: Pseudo dice [0.8902, 0.9487, 0.9366]\n",
      "2024-01-11 13:19:28.669084: Epoch time: 39.95 s\n",
      "2024-01-11 13:19:29.853806: \n",
      "2024-01-11 13:19:29.863067: Epoch 848\n",
      "2024-01-11 13:19:29.867650: Current learning rate: 0.00184\n",
      "2024-01-11 13:20:09.802355: train_loss -0.9546\n",
      "2024-01-11 13:20:09.811495: val_loss -0.8335\n",
      "2024-01-11 13:20:09.821505: Pseudo dice [0.8941, 0.9493, 0.937]\n",
      "2024-01-11 13:20:09.844038: Epoch time: 39.95 s\n",
      "2024-01-11 13:20:11.010904: \n",
      "2024-01-11 13:20:11.016904: Epoch 849\n",
      "2024-01-11 13:20:11.021669: Current learning rate: 0.00182\n",
      "2024-01-11 13:20:50.856266: train_loss -0.9546\n",
      "2024-01-11 13:20:50.868267: val_loss -0.8322\n",
      "2024-01-11 13:20:50.877268: Pseudo dice [0.8935, 0.9487, 0.937]\n",
      "2024-01-11 13:20:50.916268: Epoch time: 39.85 s\n",
      "2024-01-11 13:20:52.291979: \n",
      "2024-01-11 13:20:52.297982: Epoch 850\n",
      "2024-01-11 13:20:52.302980: Current learning rate: 0.00181\n",
      "2024-01-11 13:21:32.210048: train_loss -0.9544\n",
      "2024-01-11 13:21:32.247049: val_loss -0.831\n",
      "2024-01-11 13:21:32.257049: Pseudo dice [0.89, 0.9495, 0.9382]\n",
      "2024-01-11 13:21:32.267053: Epoch time: 39.92 s\n",
      "2024-01-11 13:21:33.455450: \n",
      "2024-01-11 13:21:33.468655: Epoch 851\n",
      "2024-01-11 13:21:33.475651: Current learning rate: 0.0018\n",
      "2024-01-11 13:22:13.519028: train_loss -0.9549\n",
      "2024-01-11 13:22:13.530196: val_loss -0.8319\n",
      "2024-01-11 13:22:13.540202: Pseudo dice [0.8904, 0.949, 0.9367]\n",
      "2024-01-11 13:22:13.548724: Epoch time: 40.06 s\n",
      "2024-01-11 13:22:14.688560: \n",
      "2024-01-11 13:22:14.694726: Epoch 852\n",
      "2024-01-11 13:22:14.698872: Current learning rate: 0.00179\n",
      "2024-01-11 13:22:54.534564: train_loss -0.9542\n",
      "2024-01-11 13:22:54.546074: val_loss -0.833\n",
      "2024-01-11 13:22:54.556073: Pseudo dice [0.8982, 0.9487, 0.9378]\n",
      "2024-01-11 13:22:54.565074: Epoch time: 39.85 s\n",
      "2024-01-11 13:22:55.741188: \n",
      "2024-01-11 13:22:55.747205: Epoch 853\n",
      "2024-01-11 13:22:55.752074: Current learning rate: 0.00178\n",
      "2024-01-11 13:23:35.530342: train_loss -0.955\n",
      "2024-01-11 13:23:35.543339: val_loss -0.833\n",
      "2024-01-11 13:23:35.551339: Pseudo dice [0.8898, 0.9494, 0.938]\n",
      "2024-01-11 13:23:35.559339: Epoch time: 39.79 s\n",
      "2024-01-11 13:23:36.747001: \n",
      "2024-01-11 13:23:36.752998: Epoch 854\n",
      "2024-01-11 13:23:36.757514: Current learning rate: 0.00177\n",
      "2024-01-11 13:24:16.595790: train_loss -0.9547\n",
      "2024-01-11 13:24:16.606302: val_loss -0.8312\n",
      "2024-01-11 13:24:16.616302: Pseudo dice [0.892, 0.9482, 0.9369]\n",
      "2024-01-11 13:24:16.625302: Epoch time: 39.85 s\n",
      "2024-01-11 13:24:17.816525: \n",
      "2024-01-11 13:24:17.822454: Epoch 855\n",
      "2024-01-11 13:24:17.826458: Current learning rate: 0.00176\n",
      "2024-01-11 13:24:57.697177: train_loss -0.9537\n",
      "2024-01-11 13:24:57.708180: val_loss -0.8334\n",
      "2024-01-11 13:24:57.741707: Pseudo dice [0.8892, 0.949, 0.9379]\n",
      "2024-01-11 13:24:57.752703: Epoch time: 39.88 s\n",
      "2024-01-11 13:24:58.903481: \n",
      "2024-01-11 13:24:58.911118: Epoch 856\n",
      "2024-01-11 13:24:58.916192: Current learning rate: 0.00175\n",
      "2024-01-11 13:25:38.783216: train_loss -0.9557\n",
      "2024-01-11 13:25:38.806216: val_loss -0.8283\n",
      "2024-01-11 13:25:38.817216: Pseudo dice [0.8864, 0.9473, 0.9357]\n",
      "2024-01-11 13:25:38.827221: Epoch time: 39.88 s\n",
      "2024-01-11 13:25:40.044150: \n",
      "2024-01-11 13:25:40.050237: Epoch 857\n",
      "2024-01-11 13:25:40.054153: Current learning rate: 0.00174\n",
      "2024-01-11 13:26:19.861199: train_loss -0.9552\n",
      "2024-01-11 13:26:19.870199: val_loss -0.8341\n",
      "2024-01-11 13:26:19.880714: Pseudo dice [0.8921, 0.9493, 0.9385]\n",
      "2024-01-11 13:26:19.911174: Epoch time: 39.82 s\n",
      "2024-01-11 13:26:21.084956: \n",
      "2024-01-11 13:26:21.090947: Epoch 858\n",
      "2024-01-11 13:26:21.094956: Current learning rate: 0.00173\n",
      "2024-01-11 13:27:00.929921: train_loss -0.9555\n",
      "2024-01-11 13:27:00.941918: val_loss -0.8295\n",
      "2024-01-11 13:27:00.950922: Pseudo dice [0.8888, 0.9479, 0.9361]\n",
      "2024-01-11 13:27:00.987923: Epoch time: 39.85 s\n",
      "2024-01-11 13:27:02.135517: \n",
      "2024-01-11 13:27:02.141526: Epoch 859\n",
      "2024-01-11 13:27:02.146517: Current learning rate: 0.00172\n",
      "2024-01-11 13:27:41.877945: train_loss -0.9548\n",
      "2024-01-11 13:27:41.886946: val_loss -0.832\n",
      "2024-01-11 13:27:41.896946: Pseudo dice [0.893, 0.9482, 0.9376]\n",
      "2024-01-11 13:27:41.905952: Epoch time: 39.74 s\n",
      "2024-01-11 13:27:43.094605: \n",
      "2024-01-11 13:27:43.102733: Epoch 860\n",
      "2024-01-11 13:27:43.106832: Current learning rate: 0.0017\n",
      "2024-01-11 13:28:23.057451: train_loss -0.9551\n",
      "2024-01-11 13:28:23.067450: val_loss -0.8315\n",
      "2024-01-11 13:28:23.077451: Pseudo dice [0.8933, 0.9491, 0.938]\n",
      "2024-01-11 13:28:23.085450: Epoch time: 39.96 s\n",
      "2024-01-11 13:28:24.269905: \n",
      "2024-01-11 13:28:24.277970: Epoch 861\n",
      "2024-01-11 13:28:24.287101: Current learning rate: 0.00169\n",
      "2024-01-11 13:29:04.187849: train_loss -0.9553\n",
      "2024-01-11 13:29:04.216848: val_loss -0.8313\n",
      "2024-01-11 13:29:04.227222: Pseudo dice [0.8883, 0.9485, 0.9368]\n",
      "2024-01-11 13:29:04.237368: Epoch time: 39.92 s\n",
      "2024-01-11 13:29:05.428388: \n",
      "2024-01-11 13:29:05.434992: Epoch 862\n",
      "2024-01-11 13:29:05.438992: Current learning rate: 0.00168\n",
      "2024-01-11 13:29:45.125044: train_loss -0.9543\n",
      "2024-01-11 13:29:45.135043: val_loss -0.8301\n",
      "2024-01-11 13:29:45.144045: Pseudo dice [0.8922, 0.9479, 0.9363]\n",
      "2024-01-11 13:29:45.168041: Epoch time: 39.7 s\n",
      "2024-01-11 13:29:46.320556: \n",
      "2024-01-11 13:29:46.326586: Epoch 863\n",
      "2024-01-11 13:29:46.331600: Current learning rate: 0.00167\n",
      "2024-01-11 13:30:26.194427: train_loss -0.9555\n",
      "2024-01-11 13:30:26.224948: val_loss -0.8311\n",
      "2024-01-11 13:30:26.235947: Pseudo dice [0.8893, 0.9486, 0.9374]\n",
      "2024-01-11 13:30:26.248949: Epoch time: 39.87 s\n",
      "2024-01-11 13:30:27.451920: \n",
      "2024-01-11 13:30:27.465917: Epoch 864\n",
      "2024-01-11 13:30:27.470917: Current learning rate: 0.00166\n",
      "2024-01-11 13:31:07.319160: train_loss -0.9549\n",
      "2024-01-11 13:31:07.326159: val_loss -0.8282\n",
      "2024-01-11 13:31:07.360164: Pseudo dice [0.8908, 0.9481, 0.9372]\n",
      "2024-01-11 13:31:07.372165: Epoch time: 39.87 s\n",
      "2024-01-11 13:31:08.522135: \n",
      "2024-01-11 13:31:08.528695: Epoch 865\n",
      "2024-01-11 13:31:08.532779: Current learning rate: 0.00165\n",
      "2024-01-11 13:31:48.416810: train_loss -0.9543\n",
      "2024-01-11 13:31:48.426809: val_loss -0.8307\n",
      "2024-01-11 13:31:48.435808: Pseudo dice [0.8896, 0.948, 0.9361]\n",
      "2024-01-11 13:31:48.465808: Epoch time: 39.9 s\n",
      "2024-01-11 13:31:49.627217: \n",
      "2024-01-11 13:31:49.639217: Epoch 866\n",
      "2024-01-11 13:31:49.644492: Current learning rate: 0.00164\n",
      "2024-01-11 13:32:29.608091: train_loss -0.9545\n",
      "2024-01-11 13:32:29.617094: val_loss -0.8319\n",
      "2024-01-11 13:32:29.649207: Pseudo dice [0.8905, 0.9486, 0.9383]\n",
      "2024-01-11 13:32:29.659235: Epoch time: 39.98 s\n",
      "2024-01-11 13:32:30.794290: \n",
      "2024-01-11 13:32:30.804835: Epoch 867\n",
      "2024-01-11 13:32:30.809826: Current learning rate: 0.00163\n",
      "2024-01-11 13:33:10.592980: train_loss -0.9552\n",
      "2024-01-11 13:33:10.602978: val_loss -0.8286\n",
      "2024-01-11 13:33:10.611979: Pseudo dice [0.8906, 0.9479, 0.9358]\n",
      "2024-01-11 13:33:10.618979: Epoch time: 39.8 s\n",
      "2024-01-11 13:33:11.782783: \n",
      "2024-01-11 13:33:11.791771: Epoch 868\n",
      "2024-01-11 13:33:11.797045: Current learning rate: 0.00162\n",
      "2024-01-11 13:33:51.523038: train_loss -0.9559\n",
      "2024-01-11 13:33:51.536038: val_loss -0.8291\n",
      "2024-01-11 13:33:51.544038: Pseudo dice [0.8904, 0.948, 0.9364]\n",
      "2024-01-11 13:33:51.553087: Epoch time: 39.74 s\n",
      "2024-01-11 13:33:52.747953: \n",
      "2024-01-11 13:33:52.754955: Epoch 869\n",
      "2024-01-11 13:33:52.761944: Current learning rate: 0.00161\n",
      "2024-01-11 13:34:32.495183: train_loss -0.955\n",
      "2024-01-11 13:34:32.507186: val_loss -0.827\n",
      "2024-01-11 13:34:32.516191: Pseudo dice [0.8862, 0.948, 0.9357]\n",
      "2024-01-11 13:34:32.544703: Epoch time: 39.75 s\n",
      "2024-01-11 13:34:33.690928: \n",
      "2024-01-11 13:34:33.697899: Epoch 870\n",
      "2024-01-11 13:34:33.702903: Current learning rate: 0.00159\n",
      "2024-01-11 13:35:14.131055: train_loss -0.9556\n",
      "2024-01-11 13:35:14.142127: val_loss -0.8283\n",
      "2024-01-11 13:35:14.150127: Pseudo dice [0.8897, 0.9482, 0.9362]\n",
      "2024-01-11 13:35:14.162126: Epoch time: 40.44 s\n",
      "2024-01-11 13:35:15.468649: \n",
      "2024-01-11 13:35:15.474531: Epoch 871\n",
      "2024-01-11 13:35:15.479529: Current learning rate: 0.00158\n",
      "2024-01-11 13:35:55.885432: train_loss -0.9555\n",
      "2024-01-11 13:35:55.896437: val_loss -0.8311\n",
      "2024-01-11 13:35:55.933927: Pseudo dice [0.8902, 0.9485, 0.9373]\n",
      "2024-01-11 13:35:55.947925: Epoch time: 40.42 s\n",
      "2024-01-11 13:35:57.148204: \n",
      "2024-01-11 13:35:57.157643: Epoch 872\n",
      "2024-01-11 13:35:57.164420: Current learning rate: 0.00157\n",
      "2024-01-11 13:36:37.262148: train_loss -0.9556\n",
      "2024-01-11 13:36:37.291662: val_loss -0.8299\n",
      "2024-01-11 13:36:37.302662: Pseudo dice [0.8915, 0.9482, 0.9365]\n",
      "2024-01-11 13:36:37.311661: Epoch time: 40.11 s\n",
      "2024-01-11 13:36:38.496312: \n",
      "2024-01-11 13:36:38.508312: Epoch 873\n",
      "2024-01-11 13:36:38.513320: Current learning rate: 0.00156\n",
      "2024-01-11 13:37:18.534157: train_loss -0.9558\n",
      "2024-01-11 13:37:18.546158: val_loss -0.8329\n",
      "2024-01-11 13:37:18.578157: Pseudo dice [0.8926, 0.9487, 0.9374]\n",
      "2024-01-11 13:37:18.589157: Epoch time: 40.04 s\n",
      "2024-01-11 13:37:19.753491: \n",
      "2024-01-11 13:37:19.763564: Epoch 874\n",
      "2024-01-11 13:37:19.768554: Current learning rate: 0.00155\n",
      "2024-01-11 13:37:59.730278: train_loss -0.9558\n",
      "2024-01-11 13:37:59.740794: val_loss -0.8263\n",
      "2024-01-11 13:37:59.749792: Pseudo dice [0.8887, 0.9469, 0.9346]\n",
      "2024-01-11 13:37:59.757792: Epoch time: 39.98 s\n",
      "2024-01-11 13:38:00.952292: \n",
      "2024-01-11 13:38:00.961485: Epoch 875\n",
      "2024-01-11 13:38:00.966496: Current learning rate: 0.00154\n",
      "2024-01-11 13:38:41.031250: train_loss -0.9558\n",
      "2024-01-11 13:38:41.043261: val_loss -0.8302\n",
      "2024-01-11 13:38:41.072763: Pseudo dice [0.8891, 0.9483, 0.9367]\n",
      "2024-01-11 13:38:41.093763: Epoch time: 40.08 s\n",
      "2024-01-11 13:38:42.266110: \n",
      "2024-01-11 13:38:42.275155: Epoch 876\n",
      "2024-01-11 13:38:42.283158: Current learning rate: 0.00153\n",
      "2024-01-11 13:39:22.482081: train_loss -0.9558\n",
      "2024-01-11 13:39:22.494284: val_loss -0.828\n",
      "2024-01-11 13:39:22.519520: Pseudo dice [0.887, 0.9476, 0.935]\n",
      "2024-01-11 13:39:22.533520: Epoch time: 40.22 s\n",
      "2024-01-11 13:39:23.696718: \n",
      "2024-01-11 13:39:23.702892: Epoch 877\n",
      "2024-01-11 13:39:23.707259: Current learning rate: 0.00152\n",
      "2024-01-11 13:40:03.771336: train_loss -0.9549\n",
      "2024-01-11 13:40:03.813378: val_loss -0.8295\n",
      "2024-01-11 13:40:03.824888: Pseudo dice [0.8913, 0.9479, 0.9351]\n",
      "2024-01-11 13:40:03.834885: Epoch time: 40.08 s\n",
      "2024-01-11 13:40:05.036206: \n",
      "2024-01-11 13:40:05.049194: Epoch 878\n",
      "2024-01-11 13:40:05.060670: Current learning rate: 0.00151\n",
      "2024-01-11 13:40:45.147131: train_loss -0.9561\n",
      "2024-01-11 13:40:45.158132: val_loss -0.8319\n",
      "2024-01-11 13:40:45.191994: Pseudo dice [0.8934, 0.9488, 0.9374]\n",
      "2024-01-11 13:40:45.199992: Epoch time: 40.11 s\n",
      "2024-01-11 13:40:46.335346: \n",
      "2024-01-11 13:40:46.348083: Epoch 879\n",
      "2024-01-11 13:40:46.356153: Current learning rate: 0.00149\n",
      "2024-01-11 13:41:26.059819: train_loss -0.9554\n",
      "2024-01-11 13:41:26.067826: val_loss -0.8252\n",
      "2024-01-11 13:41:26.076825: Pseudo dice [0.8881, 0.9485, 0.9366]\n",
      "2024-01-11 13:41:26.104827: Epoch time: 39.73 s\n",
      "2024-01-11 13:41:27.242882: \n",
      "2024-01-11 13:41:27.248885: Epoch 880\n",
      "2024-01-11 13:41:27.253397: Current learning rate: 0.00148\n",
      "2024-01-11 13:42:07.008538: train_loss -0.9561\n",
      "2024-01-11 13:42:07.019566: val_loss -0.8296\n",
      "2024-01-11 13:42:07.042565: Pseudo dice [0.8904, 0.9479, 0.9365]\n",
      "2024-01-11 13:42:07.053564: Epoch time: 39.77 s\n",
      "2024-01-11 13:42:08.186935: \n",
      "2024-01-11 13:42:08.196322: Epoch 881\n",
      "2024-01-11 13:42:08.200303: Current learning rate: 0.00147\n",
      "2024-01-11 13:42:48.164103: train_loss -0.955\n",
      "2024-01-11 13:42:48.174431: val_loss -0.8292\n",
      "2024-01-11 13:42:48.185438: Pseudo dice [0.8871, 0.9477, 0.936]\n",
      "2024-01-11 13:42:48.195436: Epoch time: 39.98 s\n",
      "2024-01-11 13:42:49.354700: \n",
      "2024-01-11 13:42:49.360701: Epoch 882\n",
      "2024-01-11 13:42:49.364700: Current learning rate: 0.00146\n",
      "2024-01-11 13:43:29.038118: train_loss -0.9558\n",
      "2024-01-11 13:43:29.073119: val_loss -0.828\n",
      "2024-01-11 13:43:29.082119: Pseudo dice [0.8875, 0.9482, 0.9365]\n",
      "2024-01-11 13:43:29.091119: Epoch time: 39.68 s\n",
      "2024-01-11 13:43:30.256665: \n",
      "2024-01-11 13:43:30.263469: Epoch 883\n",
      "2024-01-11 13:43:30.267546: Current learning rate: 0.00145\n",
      "2024-01-11 13:44:09.969887: train_loss -0.9555\n",
      "2024-01-11 13:44:09.980886: val_loss -0.8275\n",
      "2024-01-11 13:44:09.990886: Pseudo dice [0.8898, 0.9484, 0.9368]\n",
      "2024-01-11 13:44:10.001414: Epoch time: 39.71 s\n",
      "2024-01-11 13:44:11.202406: \n",
      "2024-01-11 13:44:11.211637: Epoch 884\n",
      "2024-01-11 13:44:11.216724: Current learning rate: 0.00144\n",
      "2024-01-11 13:44:50.804487: train_loss -0.9556\n",
      "2024-01-11 13:44:50.815648: val_loss -0.8312\n",
      "2024-01-11 13:44:50.826951: Pseudo dice [0.8931, 0.9484, 0.9368]\n",
      "2024-01-11 13:44:50.858486: Epoch time: 39.6 s\n",
      "2024-01-11 13:44:52.037067: \n",
      "2024-01-11 13:44:52.043197: Epoch 885\n",
      "2024-01-11 13:44:52.047200: Current learning rate: 0.00143\n",
      "2024-01-11 13:45:32.134674: train_loss -0.9555\n",
      "2024-01-11 13:45:32.144677: val_loss -0.8326\n",
      "2024-01-11 13:45:32.153674: Pseudo dice [0.8904, 0.9492, 0.9372]\n",
      "2024-01-11 13:45:32.162192: Epoch time: 40.1 s\n",
      "2024-01-11 13:45:33.355159: \n",
      "2024-01-11 13:45:33.364878: Epoch 886\n",
      "2024-01-11 13:45:33.369811: Current learning rate: 0.00142\n",
      "2024-01-11 13:46:13.270757: train_loss -0.9554\n",
      "2024-01-11 13:46:13.279763: val_loss -0.8309\n",
      "2024-01-11 13:46:13.313278: Pseudo dice [0.889, 0.9489, 0.9367]\n",
      "2024-01-11 13:46:13.326277: Epoch time: 39.92 s\n",
      "2024-01-11 13:46:14.462921: \n",
      "2024-01-11 13:46:14.468603: Epoch 887\n",
      "2024-01-11 13:46:14.473191: Current learning rate: 0.00141\n",
      "2024-01-11 13:46:54.238250: train_loss -0.9557\n",
      "2024-01-11 13:46:54.250247: val_loss -0.8299\n",
      "2024-01-11 13:46:54.259247: Pseudo dice [0.8894, 0.9481, 0.9369]\n",
      "2024-01-11 13:46:54.288248: Epoch time: 39.78 s\n",
      "2024-01-11 13:46:55.419298: \n",
      "2024-01-11 13:46:55.425291: Epoch 888\n",
      "2024-01-11 13:46:55.430291: Current learning rate: 0.00139\n",
      "2024-01-11 13:47:35.114442: train_loss -0.9548\n",
      "2024-01-11 13:47:35.124448: val_loss -0.8276\n",
      "2024-01-11 13:47:35.134960: Pseudo dice [0.8875, 0.9476, 0.9361]\n",
      "2024-01-11 13:47:35.169960: Epoch time: 39.7 s\n",
      "2024-01-11 13:47:36.303425: \n",
      "2024-01-11 13:47:36.311484: Epoch 889\n",
      "2024-01-11 13:47:36.316586: Current learning rate: 0.00138\n",
      "2024-01-11 13:48:16.097793: train_loss -0.9559\n",
      "2024-01-11 13:48:16.106792: val_loss -0.8302\n",
      "2024-01-11 13:48:16.115794: Pseudo dice [0.8943, 0.948, 0.9359]\n",
      "2024-01-11 13:48:16.123795: Epoch time: 39.8 s\n",
      "2024-01-11 13:48:17.280105: \n",
      "2024-01-11 13:48:17.289475: Epoch 890\n",
      "2024-01-11 13:48:17.294417: Current learning rate: 0.00137\n",
      "2024-01-11 13:48:57.067477: train_loss -0.955\n",
      "2024-01-11 13:48:57.076993: val_loss -0.8286\n",
      "2024-01-11 13:48:57.086992: Pseudo dice [0.8884, 0.9483, 0.9354]\n",
      "2024-01-11 13:48:57.112992: Epoch time: 39.79 s\n",
      "2024-01-11 13:48:58.277634: \n",
      "2024-01-11 13:48:58.287812: Epoch 891\n",
      "2024-01-11 13:48:58.292480: Current learning rate: 0.00136\n",
      "2024-01-11 13:49:38.291285: train_loss -0.9554\n",
      "2024-01-11 13:49:38.303285: val_loss -0.8279\n",
      "2024-01-11 13:49:38.313286: Pseudo dice [0.8871, 0.9475, 0.9358]\n",
      "2024-01-11 13:49:38.322286: Epoch time: 40.01 s\n",
      "2024-01-11 13:49:39.443206: \n",
      "2024-01-11 13:49:39.448671: Epoch 892\n",
      "2024-01-11 13:49:39.456636: Current learning rate: 0.00135\n",
      "2024-01-11 13:50:19.248628: train_loss -0.9559\n",
      "2024-01-11 13:50:19.259627: val_loss -0.8289\n",
      "2024-01-11 13:50:19.267628: Pseudo dice [0.889, 0.9478, 0.9362]\n",
      "2024-01-11 13:50:19.278630: Epoch time: 39.81 s\n",
      "2024-01-11 13:50:20.467095: \n",
      "2024-01-11 13:50:20.472095: Epoch 893\n",
      "2024-01-11 13:50:20.477097: Current learning rate: 0.00134\n",
      "2024-01-11 13:51:00.526433: train_loss -0.9552\n",
      "2024-01-11 13:51:00.553943: val_loss -0.8299\n",
      "2024-01-11 13:51:00.562943: Pseudo dice [0.892, 0.9483, 0.9369]\n",
      "2024-01-11 13:51:00.571944: Epoch time: 40.06 s\n",
      "2024-01-11 13:51:01.830469: \n",
      "2024-01-11 13:51:01.836469: Epoch 894\n",
      "2024-01-11 13:51:01.841469: Current learning rate: 0.00133\n",
      "2024-01-11 13:51:41.999693: train_loss -0.9565\n",
      "2024-01-11 13:51:42.034694: val_loss -0.8283\n",
      "2024-01-11 13:51:42.045694: Pseudo dice [0.8918, 0.9481, 0.9357]\n",
      "2024-01-11 13:51:42.057207: Epoch time: 40.17 s\n",
      "2024-01-11 13:51:43.228132: \n",
      "2024-01-11 13:51:43.237496: Epoch 895\n",
      "2024-01-11 13:51:43.242075: Current learning rate: 0.00132\n",
      "2024-01-11 13:52:22.958120: train_loss -0.9561\n",
      "2024-01-11 13:52:22.984868: val_loss -0.8251\n",
      "2024-01-11 13:52:22.994865: Pseudo dice [0.8892, 0.9484, 0.9365]\n",
      "2024-01-11 13:52:23.003863: Epoch time: 39.73 s\n",
      "2024-01-11 13:52:24.155894: \n",
      "2024-01-11 13:52:24.161894: Epoch 896\n",
      "2024-01-11 13:52:24.166719: Current learning rate: 0.0013\n",
      "2024-01-11 13:53:04.042075: train_loss -0.9557\n",
      "2024-01-11 13:53:04.053075: val_loss -0.8282\n",
      "2024-01-11 13:53:04.062076: Pseudo dice [0.888, 0.9481, 0.9368]\n",
      "2024-01-11 13:53:04.088080: Epoch time: 39.89 s\n",
      "2024-01-11 13:53:05.192596: \n",
      "2024-01-11 13:53:05.201722: Epoch 897\n",
      "2024-01-11 13:53:05.206729: Current learning rate: 0.00129\n",
      "2024-01-11 13:53:44.929363: train_loss -0.9548\n",
      "2024-01-11 13:53:44.942364: val_loss -0.8296\n",
      "2024-01-11 13:53:44.952362: Pseudo dice [0.8911, 0.948, 0.9359]\n",
      "2024-01-11 13:53:44.959372: Epoch time: 39.74 s\n",
      "2024-01-11 13:53:46.120353: \n",
      "2024-01-11 13:53:46.134352: Epoch 898\n",
      "2024-01-11 13:53:46.139357: Current learning rate: 0.00128\n",
      "2024-01-11 13:54:25.854403: train_loss -0.9558\n",
      "2024-01-11 13:54:25.862404: val_loss -0.8274\n",
      "2024-01-11 13:54:25.901412: Pseudo dice [0.8903, 0.9475, 0.9358]\n",
      "2024-01-11 13:54:25.911403: Epoch time: 39.74 s\n",
      "2024-01-11 13:54:27.058441: \n",
      "2024-01-11 13:54:27.067818: Epoch 899\n",
      "2024-01-11 13:54:27.071837: Current learning rate: 0.00127\n",
      "2024-01-11 13:55:06.763410: train_loss -0.9556\n",
      "2024-01-11 13:55:06.773411: val_loss -0.8267\n",
      "2024-01-11 13:55:06.783411: Pseudo dice [0.8908, 0.9476, 0.9359]\n",
      "2024-01-11 13:55:06.792415: Epoch time: 39.71 s\n",
      "2024-01-11 13:55:08.161049: \n",
      "2024-01-11 13:55:08.172317: Epoch 900\n",
      "2024-01-11 13:55:08.177317: Current learning rate: 0.00126\n",
      "2024-01-11 13:55:47.935231: train_loss -0.9569\n",
      "2024-01-11 13:55:47.947232: val_loss -0.8257\n",
      "2024-01-11 13:55:47.971748: Pseudo dice [0.8875, 0.9468, 0.9349]\n",
      "2024-01-11 13:55:47.983258: Epoch time: 39.78 s\n",
      "2024-01-11 13:55:49.120233: \n",
      "2024-01-11 13:55:49.126230: Epoch 901\n",
      "2024-01-11 13:55:49.130236: Current learning rate: 0.00125\n",
      "2024-01-11 13:56:29.017250: train_loss -0.956\n",
      "2024-01-11 13:56:29.027251: val_loss -0.8329\n",
      "2024-01-11 13:56:29.037252: Pseudo dice [0.8906, 0.9483, 0.9367]\n",
      "2024-01-11 13:56:29.046252: Epoch time: 39.9 s\n",
      "2024-01-11 13:56:30.229617: \n",
      "2024-01-11 13:56:30.237423: Epoch 902\n",
      "2024-01-11 13:56:30.241482: Current learning rate: 0.00124\n",
      "2024-01-11 13:57:10.069302: train_loss -0.9563\n",
      "2024-01-11 13:57:10.079301: val_loss -0.8247\n",
      "2024-01-11 13:57:10.108305: Pseudo dice [0.8857, 0.9484, 0.9354]\n",
      "2024-01-11 13:57:10.121813: Epoch time: 39.84 s\n",
      "2024-01-11 13:57:11.282627: \n",
      "2024-01-11 13:57:11.288630: Epoch 903\n",
      "2024-01-11 13:57:11.292710: Current learning rate: 0.00122\n",
      "2024-01-11 13:57:50.920292: train_loss -0.9558\n",
      "2024-01-11 13:57:50.932805: val_loss -0.8279\n",
      "2024-01-11 13:57:50.971805: Pseudo dice [0.8893, 0.9481, 0.9357]\n",
      "2024-01-11 13:57:50.981804: Epoch time: 39.64 s\n",
      "2024-01-11 13:57:52.163256: \n",
      "2024-01-11 13:57:52.168260: Epoch 904\n",
      "2024-01-11 13:57:52.173256: Current learning rate: 0.00121\n",
      "2024-01-11 13:58:31.819100: train_loss -0.9566\n",
      "2024-01-11 13:58:31.829099: val_loss -0.832\n",
      "2024-01-11 13:58:31.839104: Pseudo dice [0.893, 0.9477, 0.9363]\n",
      "2024-01-11 13:58:31.848941: Epoch time: 39.66 s\n",
      "2024-01-11 13:58:33.036631: \n",
      "2024-01-11 13:58:33.041631: Epoch 905\n",
      "2024-01-11 13:58:33.046634: Current learning rate: 0.0012\n",
      "2024-01-11 13:59:12.731336: train_loss -0.9561\n",
      "2024-01-11 13:59:12.743336: val_loss -0.829\n",
      "2024-01-11 13:59:12.751336: Pseudo dice [0.8895, 0.9481, 0.9362]\n",
      "2024-01-11 13:59:12.760341: Epoch time: 39.7 s\n",
      "2024-01-11 13:59:13.935080: \n",
      "2024-01-11 13:59:13.944190: Epoch 906\n",
      "2024-01-11 13:59:13.951200: Current learning rate: 0.00119\n",
      "2024-01-11 13:59:53.780497: train_loss -0.9561\n",
      "2024-01-11 13:59:53.792497: val_loss -0.8286\n",
      "2024-01-11 13:59:53.802497: Pseudo dice [0.89, 0.9479, 0.9361]\n",
      "2024-01-11 13:59:53.811497: Epoch time: 39.85 s\n",
      "2024-01-11 13:59:54.958661: \n",
      "2024-01-11 13:59:54.968707: Epoch 907\n",
      "2024-01-11 13:59:54.973619: Current learning rate: 0.00118\n",
      "2024-01-11 14:00:34.599028: train_loss -0.9565\n",
      "2024-01-11 14:00:34.609320: val_loss -0.8294\n",
      "2024-01-11 14:00:34.617399: Pseudo dice [0.8952, 0.9477, 0.9359]\n",
      "2024-01-11 14:00:34.626400: Epoch time: 39.64 s\n",
      "2024-01-11 14:00:35.840764: \n",
      "2024-01-11 14:00:35.847768: Epoch 908\n",
      "2024-01-11 14:00:35.855768: Current learning rate: 0.00117\n",
      "2024-01-11 14:01:15.492462: train_loss -0.9558\n",
      "2024-01-11 14:01:15.504468: val_loss -0.8339\n",
      "2024-01-11 14:01:15.542599: Pseudo dice [0.8935, 0.9488, 0.9375]\n",
      "2024-01-11 14:01:15.552605: Epoch time: 39.65 s\n",
      "2024-01-11 14:01:16.682233: \n",
      "2024-01-11 14:01:16.693235: Epoch 909\n",
      "2024-01-11 14:01:16.702283: Current learning rate: 0.00116\n",
      "2024-01-11 14:01:56.489849: train_loss -0.9558\n",
      "2024-01-11 14:01:56.521855: val_loss -0.8296\n",
      "2024-01-11 14:01:56.532365: Pseudo dice [0.8878, 0.9482, 0.9357]\n",
      "2024-01-11 14:01:56.542884: Epoch time: 39.81 s\n",
      "2024-01-11 14:01:57.726333: \n",
      "2024-01-11 14:01:57.735400: Epoch 910\n",
      "2024-01-11 14:01:57.741018: Current learning rate: 0.00115\n",
      "2024-01-11 14:02:37.439278: train_loss -0.9573\n",
      "2024-01-11 14:02:37.449280: val_loss -0.8315\n",
      "2024-01-11 14:02:37.458796: Pseudo dice [0.8912, 0.948, 0.9368]\n",
      "2024-01-11 14:02:37.467797: Epoch time: 39.71 s\n",
      "2024-01-11 14:02:38.634923: \n",
      "2024-01-11 14:02:38.640932: Epoch 911\n",
      "2024-01-11 14:02:38.648061: Current learning rate: 0.00113\n",
      "2024-01-11 14:03:18.563305: train_loss -0.9562\n",
      "2024-01-11 14:03:18.576307: val_loss -0.828\n",
      "2024-01-11 14:03:18.585307: Pseudo dice [0.8888, 0.9482, 0.936]\n",
      "2024-01-11 14:03:18.594821: Epoch time: 39.93 s\n",
      "2024-01-11 14:03:19.796460: \n",
      "2024-01-11 14:03:19.804853: Epoch 912\n",
      "2024-01-11 14:03:19.808865: Current learning rate: 0.00112\n",
      "2024-01-11 14:03:59.403892: train_loss -0.9561\n",
      "2024-01-11 14:03:59.412409: val_loss -0.8293\n",
      "2024-01-11 14:03:59.422406: Pseudo dice [0.8867, 0.9485, 0.9364]\n",
      "2024-01-11 14:03:59.431406: Epoch time: 39.61 s\n",
      "2024-01-11 14:04:00.641749: \n",
      "2024-01-11 14:04:00.648749: Epoch 913\n",
      "2024-01-11 14:04:00.656749: Current learning rate: 0.00111\n",
      "2024-01-11 14:04:40.313006: train_loss -0.956\n",
      "2024-01-11 14:04:40.325517: val_loss -0.8293\n",
      "2024-01-11 14:04:40.332524: Pseudo dice [0.8905, 0.9486, 0.9361]\n",
      "2024-01-11 14:04:40.341527: Epoch time: 39.67 s\n",
      "2024-01-11 14:04:41.498556: \n",
      "2024-01-11 14:04:41.504559: Epoch 914\n",
      "2024-01-11 14:04:41.509556: Current learning rate: 0.0011\n",
      "2024-01-11 14:05:21.180111: train_loss -0.9573\n",
      "2024-01-11 14:05:21.191110: val_loss -0.8282\n",
      "2024-01-11 14:05:21.201111: Pseudo dice [0.8913, 0.947, 0.9349]\n",
      "2024-01-11 14:05:21.233121: Epoch time: 39.68 s\n",
      "2024-01-11 14:05:22.381004: \n",
      "2024-01-11 14:05:22.388011: Epoch 915\n",
      "2024-01-11 14:05:22.393004: Current learning rate: 0.00109\n",
      "2024-01-11 14:06:02.176353: train_loss -0.9572\n",
      "2024-01-11 14:06:02.187349: val_loss -0.8286\n",
      "2024-01-11 14:06:02.199351: Pseudo dice [0.8912, 0.9488, 0.937]\n",
      "2024-01-11 14:06:02.228246: Epoch time: 39.8 s\n",
      "2024-01-11 14:06:03.378587: \n",
      "2024-01-11 14:06:03.383854: Epoch 916\n",
      "2024-01-11 14:06:03.388854: Current learning rate: 0.00108\n",
      "2024-01-11 14:06:43.267550: train_loss -0.9564\n",
      "2024-01-11 14:06:43.277554: val_loss -0.8286\n",
      "2024-01-11 14:06:43.287277: Pseudo dice [0.8905, 0.9485, 0.9362]\n",
      "2024-01-11 14:06:43.296277: Epoch time: 39.89 s\n",
      "2024-01-11 14:06:44.415117: \n",
      "2024-01-11 14:06:44.425384: Epoch 917\n",
      "2024-01-11 14:06:44.433324: Current learning rate: 0.00106\n",
      "2024-01-11 14:07:24.271575: train_loss -0.9568\n",
      "2024-01-11 14:07:24.302581: val_loss -0.8298\n",
      "2024-01-11 14:07:24.310090: Pseudo dice [0.8899, 0.9481, 0.937]\n",
      "2024-01-11 14:07:24.320091: Epoch time: 39.86 s\n",
      "2024-01-11 14:07:25.458095: \n",
      "2024-01-11 14:07:25.465523: Epoch 918\n",
      "2024-01-11 14:07:25.470452: Current learning rate: 0.00105\n",
      "2024-01-11 14:08:05.072031: train_loss -0.9563\n",
      "2024-01-11 14:08:05.084078: val_loss -0.8307\n",
      "2024-01-11 14:08:05.094079: Pseudo dice [0.8902, 0.9478, 0.9359]\n",
      "2024-01-11 14:08:05.119081: Epoch time: 39.61 s\n",
      "2024-01-11 14:08:06.263254: \n",
      "2024-01-11 14:08:06.269259: Epoch 919\n",
      "2024-01-11 14:08:06.274258: Current learning rate: 0.00104\n",
      "2024-01-11 14:08:46.094474: train_loss -0.9569\n",
      "2024-01-11 14:08:46.125473: val_loss -0.8298\n",
      "2024-01-11 14:08:46.135471: Pseudo dice [0.8883, 0.9484, 0.9367]\n",
      "2024-01-11 14:08:46.145472: Epoch time: 39.83 s\n",
      "2024-01-11 14:08:47.291638: \n",
      "2024-01-11 14:08:47.299638: Epoch 920\n",
      "2024-01-11 14:08:47.304641: Current learning rate: 0.00103\n",
      "2024-01-11 14:09:26.984542: train_loss -0.9564\n",
      "2024-01-11 14:09:26.995547: val_loss -0.829\n",
      "2024-01-11 14:09:27.025054: Pseudo dice [0.8909, 0.9479, 0.9358]\n",
      "2024-01-11 14:09:27.035285: Epoch time: 39.69 s\n",
      "2024-01-11 14:09:28.182104: \n",
      "2024-01-11 14:09:28.187611: Epoch 921\n",
      "2024-01-11 14:09:28.191674: Current learning rate: 0.00102\n",
      "2024-01-11 14:10:07.978758: train_loss -0.9569\n",
      "2024-01-11 14:10:07.991759: val_loss -0.8299\n",
      "2024-01-11 14:10:08.001825: Pseudo dice [0.8932, 0.9484, 0.9367]\n",
      "2024-01-11 14:10:08.010830: Epoch time: 39.8 s\n",
      "2024-01-11 14:10:09.188207: \n",
      "2024-01-11 14:10:09.196216: Epoch 922\n",
      "2024-01-11 14:10:09.205909: Current learning rate: 0.00101\n",
      "2024-01-11 14:10:48.892084: train_loss -0.9568\n",
      "2024-01-11 14:10:48.902084: val_loss -0.8261\n",
      "2024-01-11 14:10:48.939088: Pseudo dice [0.8901, 0.947, 0.9347]\n",
      "2024-01-11 14:10:48.950600: Epoch time: 39.7 s\n",
      "2024-01-11 14:10:50.083906: \n",
      "2024-01-11 14:10:50.089661: Epoch 923\n",
      "2024-01-11 14:10:50.093642: Current learning rate: 0.001\n",
      "2024-01-11 14:11:29.784859: train_loss -0.9562\n",
      "2024-01-11 14:11:29.794859: val_loss -0.8329\n",
      "2024-01-11 14:11:29.803860: Pseudo dice [0.8938, 0.9488, 0.9376]\n",
      "2024-01-11 14:11:29.813863: Epoch time: 39.7 s\n",
      "2024-01-11 14:11:30.974748: \n",
      "2024-01-11 14:11:30.981597: Epoch 924\n",
      "2024-01-11 14:11:30.988561: Current learning rate: 0.00098\n",
      "2024-01-11 14:12:10.756882: train_loss -0.9573\n",
      "2024-01-11 14:12:10.768892: val_loss -0.8302\n",
      "2024-01-11 14:12:10.801399: Pseudo dice [0.8923, 0.9478, 0.9367]\n",
      "2024-01-11 14:12:10.811399: Epoch time: 39.78 s\n",
      "2024-01-11 14:12:11.947455: \n",
      "2024-01-11 14:12:11.955518: Epoch 925\n",
      "2024-01-11 14:12:11.963477: Current learning rate: 0.00097\n",
      "2024-01-11 14:12:51.684980: train_loss -0.9567\n",
      "2024-01-11 14:12:51.695491: val_loss -0.8258\n",
      "2024-01-11 14:12:51.703492: Pseudo dice [0.8898, 0.9475, 0.9352]\n",
      "2024-01-11 14:12:51.714492: Epoch time: 39.74 s\n",
      "2024-01-11 14:12:52.908368: \n",
      "2024-01-11 14:12:52.914590: Epoch 926\n",
      "2024-01-11 14:12:52.918588: Current learning rate: 0.00096\n",
      "2024-01-11 14:13:32.961380: train_loss -0.9565\n",
      "2024-01-11 14:13:32.971379: val_loss -0.83\n",
      "2024-01-11 14:13:32.980379: Pseudo dice [0.8904, 0.948, 0.9369]\n",
      "2024-01-11 14:13:32.988380: Epoch time: 40.05 s\n",
      "2024-01-11 14:13:34.121234: \n",
      "2024-01-11 14:13:34.129233: Epoch 927\n",
      "2024-01-11 14:13:34.135768: Current learning rate: 0.00095\n",
      "2024-01-11 14:14:13.812458: train_loss -0.9569\n",
      "2024-01-11 14:14:13.823454: val_loss -0.8308\n",
      "2024-01-11 14:14:13.832452: Pseudo dice [0.8925, 0.9477, 0.9367]\n",
      "2024-01-11 14:14:13.859981: Epoch time: 39.69 s\n",
      "2024-01-11 14:14:15.022755: \n",
      "2024-01-11 14:14:15.031359: Epoch 928\n",
      "2024-01-11 14:14:15.039511: Current learning rate: 0.00094\n",
      "2024-01-11 14:14:54.627479: train_loss -0.9569\n",
      "2024-01-11 14:14:54.639481: val_loss -0.8316\n",
      "2024-01-11 14:14:54.651480: Pseudo dice [0.8945, 0.9482, 0.9366]\n",
      "2024-01-11 14:14:54.685309: Epoch time: 39.61 s\n",
      "2024-01-11 14:14:55.885005: \n",
      "2024-01-11 14:14:55.897780: Epoch 929\n",
      "2024-01-11 14:14:55.901858: Current learning rate: 0.00092\n",
      "2024-01-11 14:15:35.646612: train_loss -0.9572\n",
      "2024-01-11 14:15:35.656615: val_loss -0.8264\n",
      "2024-01-11 14:15:35.665613: Pseudo dice [0.8888, 0.9476, 0.9359]\n",
      "2024-01-11 14:15:35.674618: Epoch time: 39.76 s\n",
      "2024-01-11 14:15:36.831722: \n",
      "2024-01-11 14:15:36.836785: Epoch 930\n",
      "2024-01-11 14:15:36.841732: Current learning rate: 0.00091\n",
      "2024-01-11 14:16:16.600689: train_loss -0.9573\n",
      "2024-01-11 14:16:16.610695: val_loss -0.831\n",
      "2024-01-11 14:16:16.621207: Pseudo dice [0.894, 0.9477, 0.9361]\n",
      "2024-01-11 14:16:16.630206: Epoch time: 39.77 s\n",
      "2024-01-11 14:16:17.823074: \n",
      "2024-01-11 14:16:17.829593: Epoch 931\n",
      "2024-01-11 14:16:17.834180: Current learning rate: 0.0009\n",
      "2024-01-11 14:16:57.654786: train_loss -0.9581\n",
      "2024-01-11 14:16:57.664787: val_loss -0.8336\n",
      "2024-01-11 14:16:57.674788: Pseudo dice [0.8943, 0.9492, 0.9374]\n",
      "2024-01-11 14:16:57.701788: Epoch time: 39.83 s\n",
      "2024-01-11 14:16:59.006927: \n",
      "2024-01-11 14:16:59.016190: Epoch 932\n",
      "2024-01-11 14:16:59.021186: Current learning rate: 0.00089\n",
      "2024-01-11 14:17:38.881475: train_loss -0.9568\n",
      "2024-01-11 14:17:38.893468: val_loss -0.8291\n",
      "2024-01-11 14:17:38.903466: Pseudo dice [0.8886, 0.9478, 0.9355]\n",
      "2024-01-11 14:17:38.915466: Epoch time: 39.87 s\n",
      "2024-01-11 14:17:40.076066: \n",
      "2024-01-11 14:17:40.082168: Epoch 933\n",
      "2024-01-11 14:17:40.086818: Current learning rate: 0.00088\n",
      "2024-01-11 14:18:19.815303: train_loss -0.9573\n",
      "2024-01-11 14:18:19.825305: val_loss -0.8285\n",
      "2024-01-11 14:18:19.834304: Pseudo dice [0.892, 0.9487, 0.9361]\n",
      "2024-01-11 14:18:19.842613: Epoch time: 39.74 s\n",
      "2024-01-11 14:18:21.016743: \n",
      "2024-01-11 14:18:21.022751: Epoch 934\n",
      "2024-01-11 14:18:21.027575: Current learning rate: 0.00087\n",
      "2024-01-11 14:19:01.034196: train_loss -0.9568\n",
      "2024-01-11 14:19:01.045190: val_loss -0.8343\n",
      "2024-01-11 14:19:01.056191: Pseudo dice [0.8952, 0.9495, 0.9379]\n",
      "2024-01-11 14:19:01.082201: Epoch time: 40.02 s\n",
      "2024-01-11 14:19:02.235875: \n",
      "2024-01-11 14:19:02.243876: Epoch 935\n",
      "2024-01-11 14:19:02.249878: Current learning rate: 0.00085\n",
      "2024-01-11 14:19:42.042662: train_loss -0.9572\n",
      "2024-01-11 14:19:42.053664: val_loss -0.83\n",
      "2024-01-11 14:19:42.062662: Pseudo dice [0.8903, 0.9483, 0.9355]\n",
      "2024-01-11 14:19:42.069664: Epoch time: 39.81 s\n",
      "2024-01-11 14:19:43.203679: \n",
      "2024-01-11 14:19:43.209687: Epoch 936\n",
      "2024-01-11 14:19:43.214697: Current learning rate: 0.00084\n",
      "2024-01-11 14:20:22.811990: train_loss -0.957\n",
      "2024-01-11 14:20:22.822990: val_loss -0.8302\n",
      "2024-01-11 14:20:22.831990: Pseudo dice [0.8867, 0.9483, 0.9375]\n",
      "2024-01-11 14:20:22.839994: Epoch time: 39.61 s\n",
      "2024-01-11 14:20:24.155168: \n",
      "2024-01-11 14:20:24.160702: Epoch 937\n",
      "2024-01-11 14:20:24.165702: Current learning rate: 0.00083\n",
      "2024-01-11 14:21:03.836367: train_loss -0.9568\n",
      "2024-01-11 14:21:03.848372: val_loss -0.8266\n",
      "2024-01-11 14:21:03.857360: Pseudo dice [0.8927, 0.9476, 0.9355]\n",
      "2024-01-11 14:21:03.867365: Epoch time: 39.68 s\n",
      "2024-01-11 14:21:05.034098: \n",
      "2024-01-11 14:21:05.040094: Epoch 938\n",
      "2024-01-11 14:21:05.045093: Current learning rate: 0.00082\n",
      "2024-01-11 14:21:44.726368: train_loss -0.9576\n",
      "2024-01-11 14:21:44.736370: val_loss -0.8287\n",
      "2024-01-11 14:21:44.745558: Pseudo dice [0.8889, 0.9477, 0.9353]\n",
      "2024-01-11 14:21:44.775555: Epoch time: 39.69 s\n",
      "2024-01-11 14:21:45.904529: \n",
      "2024-01-11 14:21:45.912529: Epoch 939\n",
      "2024-01-11 14:21:45.919529: Current learning rate: 0.00081\n",
      "2024-01-11 14:22:25.883902: train_loss -0.9574\n",
      "2024-01-11 14:22:25.894902: val_loss -0.8273\n",
      "2024-01-11 14:22:25.904900: Pseudo dice [0.8884, 0.9477, 0.9355]\n",
      "2024-01-11 14:22:25.913925: Epoch time: 39.98 s\n",
      "2024-01-11 14:22:27.167114: \n",
      "2024-01-11 14:22:27.176116: Epoch 940\n",
      "2024-01-11 14:22:27.183117: Current learning rate: 0.00079\n",
      "2024-01-11 14:23:06.866146: train_loss -0.9573\n",
      "2024-01-11 14:23:06.896142: val_loss -0.8313\n",
      "2024-01-11 14:23:06.907140: Pseudo dice [0.8951, 0.9483, 0.9365]\n",
      "2024-01-11 14:23:06.916140: Epoch time: 39.7 s\n",
      "2024-01-11 14:23:08.062958: \n",
      "2024-01-11 14:23:08.069183: Epoch 941\n",
      "2024-01-11 14:23:08.073201: Current learning rate: 0.00078\n",
      "2024-01-11 14:23:47.936035: train_loss -0.9571\n",
      "2024-01-11 14:23:47.946499: val_loss -0.8287\n",
      "2024-01-11 14:23:47.955501: Pseudo dice [0.8909, 0.9474, 0.9362]\n",
      "2024-01-11 14:23:47.966043: Epoch time: 39.87 s\n",
      "2024-01-11 14:23:49.250278: \n",
      "2024-01-11 14:23:49.258798: Epoch 942\n",
      "2024-01-11 14:23:49.265333: Current learning rate: 0.00077\n",
      "2024-01-11 14:24:28.998664: train_loss -0.9578\n",
      "2024-01-11 14:24:29.009665: val_loss -0.8291\n",
      "2024-01-11 14:24:29.044666: Pseudo dice [0.8941, 0.9477, 0.9353]\n",
      "2024-01-11 14:24:29.054665: Epoch time: 39.75 s\n",
      "2024-01-11 14:24:30.197813: \n",
      "2024-01-11 14:24:30.203811: Epoch 943\n",
      "2024-01-11 14:24:30.208648: Current learning rate: 0.00076\n",
      "2024-01-11 14:25:09.901682: train_loss -0.9572\n",
      "2024-01-11 14:25:09.937199: val_loss -0.8296\n",
      "2024-01-11 14:25:09.948197: Pseudo dice [0.8926, 0.9474, 0.9357]\n",
      "2024-01-11 14:25:09.957197: Epoch time: 39.7 s\n",
      "2024-01-11 14:25:11.094918: \n",
      "2024-01-11 14:25:11.102833: Epoch 944\n",
      "2024-01-11 14:25:11.108491: Current learning rate: 0.00075\n",
      "2024-01-11 14:25:50.882404: train_loss -0.9574\n",
      "2024-01-11 14:25:50.913402: val_loss -0.8305\n",
      "2024-01-11 14:25:50.925409: Pseudo dice [0.8946, 0.948, 0.9366]\n",
      "2024-01-11 14:25:50.934422: Epoch time: 39.79 s\n",
      "2024-01-11 14:25:52.075349: \n",
      "2024-01-11 14:25:52.086349: Epoch 945\n",
      "2024-01-11 14:25:52.094349: Current learning rate: 0.00074\n",
      "2024-01-11 14:26:33.047757: train_loss -0.9568\n",
      "2024-01-11 14:26:33.058264: val_loss -0.8295\n",
      "2024-01-11 14:26:33.093446: Pseudo dice [0.8913, 0.9477, 0.936]\n",
      "2024-01-11 14:26:33.102452: Epoch time: 40.97 s\n",
      "2024-01-11 14:26:34.475959: \n",
      "2024-01-11 14:26:34.482950: Epoch 946\n",
      "2024-01-11 14:26:34.487889: Current learning rate: 0.00072\n",
      "2024-01-11 14:27:16.885584: train_loss -0.9575\n",
      "2024-01-11 14:27:16.894099: val_loss -0.8278\n",
      "2024-01-11 14:27:16.902475: Pseudo dice [0.8911, 0.9479, 0.9367]\n",
      "2024-01-11 14:27:16.910475: Epoch time: 42.41 s\n",
      "2024-01-11 14:27:18.375679: \n",
      "2024-01-11 14:27:18.387726: Epoch 947\n",
      "2024-01-11 14:27:18.400334: Current learning rate: 0.00071\n",
      "2024-01-11 14:28:01.092195: train_loss -0.9575\n",
      "2024-01-11 14:28:01.103709: val_loss -0.8276\n",
      "2024-01-11 14:28:01.114712: Pseudo dice [0.8885, 0.948, 0.9363]\n",
      "2024-01-11 14:28:01.126823: Epoch time: 42.72 s\n",
      "2024-01-11 14:28:02.719871: \n",
      "2024-01-11 14:28:02.729403: Epoch 948\n",
      "2024-01-11 14:28:02.734933: Current learning rate: 0.0007\n",
      "2024-01-11 14:28:45.395338: train_loss -0.9578\n",
      "2024-01-11 14:28:45.406288: val_loss -0.8284\n",
      "2024-01-11 14:28:45.416203: Pseudo dice [0.8902, 0.9475, 0.9354]\n",
      "2024-01-11 14:28:45.430694: Epoch time: 42.68 s\n",
      "2024-01-11 14:28:47.001892: \n",
      "2024-01-11 14:28:47.008048: Epoch 949\n",
      "2024-01-11 14:28:47.014802: Current learning rate: 0.00069\n",
      "2024-01-11 14:29:29.007069: train_loss -0.9581\n",
      "2024-01-11 14:29:29.016761: val_loss -0.8266\n",
      "2024-01-11 14:29:29.024274: Pseudo dice [0.8858, 0.9468, 0.935]\n",
      "2024-01-11 14:29:29.032811: Epoch time: 42.01 s\n",
      "2024-01-11 14:29:31.126681: \n",
      "2024-01-11 14:29:31.133095: Epoch 950\n",
      "2024-01-11 14:29:31.140098: Current learning rate: 0.00067\n",
      "2024-01-11 14:30:12.842173: train_loss -0.9575\n",
      "2024-01-11 14:30:12.856705: val_loss -0.8288\n",
      "2024-01-11 14:30:12.866627: Pseudo dice [0.8889, 0.9479, 0.9358]\n",
      "2024-01-11 14:30:12.874161: Epoch time: 41.72 s\n",
      "2024-01-11 14:30:14.350144: \n",
      "2024-01-11 14:30:14.356654: Epoch 951\n",
      "2024-01-11 14:30:14.361653: Current learning rate: 0.00066\n",
      "2024-01-11 14:30:56.101730: train_loss -0.9573\n",
      "2024-01-11 14:30:56.115809: val_loss -0.8285\n",
      "2024-01-11 14:30:56.155541: Pseudo dice [0.8906, 0.9473, 0.935]\n",
      "2024-01-11 14:30:56.166496: Epoch time: 41.75 s\n",
      "2024-01-11 14:30:57.889824: \n",
      "2024-01-11 14:30:57.895520: Epoch 952\n",
      "2024-01-11 14:30:57.900526: Current learning rate: 0.00065\n",
      "2024-01-11 14:31:39.210644: train_loss -0.9578\n",
      "2024-01-11 14:31:39.221644: val_loss -0.827\n",
      "2024-01-11 14:31:39.232644: Pseudo dice [0.8893, 0.9481, 0.9358]\n",
      "2024-01-11 14:31:39.268207: Epoch time: 41.32 s\n",
      "2024-01-11 14:31:40.983906: \n",
      "2024-01-11 14:31:40.990908: Epoch 953\n",
      "2024-01-11 14:31:40.996932: Current learning rate: 0.00064\n",
      "2024-01-11 14:32:23.834996: train_loss -0.9578\n",
      "2024-01-11 14:32:23.848529: val_loss -0.8275\n",
      "2024-01-11 14:32:23.861055: Pseudo dice [0.8902, 0.9472, 0.9356]\n",
      "2024-01-11 14:32:23.873577: Epoch time: 42.85 s\n",
      "2024-01-11 14:32:25.716367: \n",
      "2024-01-11 14:32:25.723366: Epoch 954\n",
      "2024-01-11 14:32:25.730891: Current learning rate: 0.00063\n",
      "2024-01-11 14:33:08.295834: train_loss -0.9576\n",
      "2024-01-11 14:33:08.307663: val_loss -0.8283\n",
      "2024-01-11 14:33:08.316172: Pseudo dice [0.8912, 0.9481, 0.9358]\n",
      "2024-01-11 14:33:08.325682: Epoch time: 42.58 s\n",
      "2024-01-11 14:33:09.819875: \n",
      "2024-01-11 14:33:09.828404: Epoch 955\n",
      "2024-01-11 14:33:09.833942: Current learning rate: 0.00061\n",
      "2024-01-11 14:33:52.450380: train_loss -0.9574\n",
      "2024-01-11 14:33:52.463893: val_loss -0.8268\n",
      "2024-01-11 14:33:52.474920: Pseudo dice [0.8913, 0.9472, 0.9353]\n",
      "2024-01-11 14:33:52.486441: Epoch time: 42.63 s\n",
      "2024-01-11 14:33:53.908933: \n",
      "2024-01-11 14:33:53.915608: Epoch 956\n",
      "2024-01-11 14:33:53.921626: Current learning rate: 0.0006\n",
      "2024-01-11 14:34:36.427917: train_loss -0.9573\n",
      "2024-01-11 14:34:36.442768: val_loss -0.8293\n",
      "2024-01-11 14:34:36.450788: Pseudo dice [0.8916, 0.9474, 0.9357]\n",
      "2024-01-11 14:34:36.459812: Epoch time: 42.52 s\n",
      "2024-01-11 14:34:37.933439: \n",
      "2024-01-11 14:34:37.939956: Epoch 957\n",
      "2024-01-11 14:34:37.944959: Current learning rate: 0.00059\n",
      "2024-01-11 14:35:20.110718: train_loss -0.9576\n",
      "2024-01-11 14:35:20.120677: val_loss -0.8276\n",
      "2024-01-11 14:35:20.130204: Pseudo dice [0.8917, 0.9467, 0.9346]\n",
      "2024-01-11 14:35:20.138243: Epoch time: 42.18 s\n",
      "2024-01-11 14:35:21.632921: \n",
      "2024-01-11 14:35:21.638980: Epoch 958\n",
      "2024-01-11 14:35:21.643985: Current learning rate: 0.00058\n",
      "2024-01-11 14:36:03.230479: train_loss -0.9583\n",
      "2024-01-11 14:36:03.240475: val_loss -0.8289\n",
      "2024-01-11 14:36:03.248477: Pseudo dice [0.8902, 0.9477, 0.9353]\n",
      "2024-01-11 14:36:03.255481: Epoch time: 41.6 s\n",
      "2024-01-11 14:36:04.654353: \n",
      "2024-01-11 14:36:04.660408: Epoch 959\n",
      "2024-01-11 14:36:04.664421: Current learning rate: 0.00056\n",
      "2024-01-11 14:36:47.132287: train_loss -0.958\n",
      "2024-01-11 14:36:47.168352: val_loss -0.826\n",
      "2024-01-11 14:36:47.180775: Pseudo dice [0.8854, 0.9476, 0.9359]\n",
      "2024-01-11 14:36:47.192298: Epoch time: 42.48 s\n",
      "2024-01-11 14:36:48.899777: \n",
      "2024-01-11 14:36:48.907031: Epoch 960\n",
      "2024-01-11 14:36:48.913551: Current learning rate: 0.00055\n",
      "2024-01-11 14:37:31.868333: train_loss -0.9576\n",
      "2024-01-11 14:37:31.881368: val_loss -0.8239\n",
      "2024-01-11 14:37:31.892897: Pseudo dice [0.8886, 0.9481, 0.9355]\n",
      "2024-01-11 14:37:31.906423: Epoch time: 42.97 s\n",
      "2024-01-11 14:37:33.707980: \n",
      "2024-01-11 14:37:33.716024: Epoch 961\n",
      "2024-01-11 14:37:33.722550: Current learning rate: 0.00054\n",
      "2024-01-11 14:38:16.420973: train_loss -0.9577\n",
      "2024-01-11 14:38:16.433499: val_loss -0.8279\n",
      "2024-01-11 14:38:16.445023: Pseudo dice [0.8929, 0.9479, 0.9361]\n",
      "2024-01-11 14:38:16.453549: Epoch time: 42.71 s\n",
      "2024-01-11 14:38:17.940844: \n",
      "2024-01-11 14:38:17.947907: Epoch 962\n",
      "2024-01-11 14:38:17.952496: Current learning rate: 0.00053\n",
      "2024-01-11 14:39:00.309977: train_loss -0.9578\n",
      "2024-01-11 14:39:00.324506: val_loss -0.8289\n",
      "2024-01-11 14:39:00.337034: Pseudo dice [0.8905, 0.948, 0.9363]\n",
      "2024-01-11 14:39:00.350079: Epoch time: 42.37 s\n",
      "2024-01-11 14:39:02.112881: \n",
      "2024-01-11 14:39:02.119390: Epoch 963\n",
      "2024-01-11 14:39:02.126441: Current learning rate: 0.00051\n",
      "2024-01-11 14:39:44.807171: train_loss -0.9584\n",
      "2024-01-11 14:39:44.819682: val_loss -0.8302\n",
      "2024-01-11 14:39:44.834252: Pseudo dice [0.8921, 0.948, 0.9365]\n",
      "2024-01-11 14:39:44.876713: Epoch time: 42.7 s\n",
      "2024-01-11 14:39:46.600626: \n",
      "2024-01-11 14:39:46.606618: Epoch 964\n",
      "2024-01-11 14:39:46.612166: Current learning rate: 0.0005\n",
      "2024-01-11 14:40:29.433332: train_loss -0.9586\n",
      "2024-01-11 14:40:29.481530: val_loss -0.8276\n",
      "2024-01-11 14:40:29.494050: Pseudo dice [0.8849, 0.948, 0.9354]\n",
      "2024-01-11 14:40:29.502563: Epoch time: 42.84 s\n",
      "2024-01-11 14:40:31.152544: \n",
      "2024-01-11 14:40:31.162708: Epoch 965\n",
      "2024-01-11 14:40:31.168705: Current learning rate: 0.00049\n",
      "2024-01-11 14:41:13.611574: train_loss -0.9581\n",
      "2024-01-11 14:41:13.623103: val_loss -0.8295\n",
      "2024-01-11 14:41:13.635646: Pseudo dice [0.8905, 0.9482, 0.9368]\n",
      "2024-01-11 14:41:13.673743: Epoch time: 42.46 s\n",
      "2024-01-11 14:41:15.144788: \n",
      "2024-01-11 14:41:15.153313: Epoch 966\n",
      "2024-01-11 14:41:15.158314: Current learning rate: 0.00048\n",
      "2024-01-11 14:41:57.466580: train_loss -0.9589\n",
      "2024-01-11 14:41:57.503014: val_loss -0.8283\n",
      "2024-01-11 14:41:57.515708: Pseudo dice [0.8902, 0.9481, 0.9365]\n",
      "2024-01-11 14:41:57.524746: Epoch time: 42.32 s\n",
      "2024-01-11 14:41:58.998806: \n",
      "2024-01-11 14:41:59.005314: Epoch 967\n",
      "2024-01-11 14:41:59.011865: Current learning rate: 0.00046\n",
      "2024-01-11 14:42:41.886167: train_loss -0.958\n",
      "2024-01-11 14:42:41.899529: val_loss -0.8307\n",
      "2024-01-11 14:42:41.910094: Pseudo dice [0.892, 0.9484, 0.9364]\n",
      "2024-01-11 14:42:41.923308: Epoch time: 42.89 s\n",
      "2024-01-11 14:42:43.559986: \n",
      "2024-01-11 14:42:43.566005: Epoch 968\n",
      "2024-01-11 14:42:43.571523: Current learning rate: 0.00045\n",
      "2024-01-11 14:43:25.217696: train_loss -0.9588\n",
      "2024-01-11 14:43:25.227694: val_loss -0.8315\n",
      "2024-01-11 14:43:25.236694: Pseudo dice [0.89, 0.948, 0.937]\n",
      "2024-01-11 14:43:25.245694: Epoch time: 41.66 s\n",
      "2024-01-11 14:43:26.646223: \n",
      "2024-01-11 14:43:26.660077: Epoch 969\n",
      "2024-01-11 14:43:26.665100: Current learning rate: 0.00044\n",
      "2024-01-11 14:44:08.384901: train_loss -0.9585\n",
      "2024-01-11 14:44:08.397272: val_loss -0.8279\n",
      "2024-01-11 14:44:08.406279: Pseudo dice [0.8893, 0.9479, 0.9359]\n",
      "2024-01-11 14:44:08.416800: Epoch time: 41.74 s\n",
      "2024-01-11 14:44:09.735086: \n",
      "2024-01-11 14:44:09.741086: Epoch 970\n",
      "2024-01-11 14:44:09.746093: Current learning rate: 0.00043\n",
      "2024-01-11 14:44:50.967091: train_loss -0.958\n",
      "2024-01-11 14:44:50.980091: val_loss -0.8282\n",
      "2024-01-11 14:44:50.993091: Pseudo dice [0.8892, 0.9477, 0.936]\n",
      "2024-01-11 14:44:51.039616: Epoch time: 41.23 s\n",
      "2024-01-11 14:44:52.360496: \n",
      "2024-01-11 14:44:52.366498: Epoch 971\n",
      "2024-01-11 14:44:52.371497: Current learning rate: 0.00041\n",
      "2024-01-11 14:45:35.123013: train_loss -0.9583\n",
      "2024-01-11 14:45:35.158526: val_loss -0.8311\n",
      "2024-01-11 14:45:35.171190: Pseudo dice [0.8927, 0.9483, 0.9365]\n",
      "2024-01-11 14:45:35.181192: Epoch time: 42.76 s\n",
      "2024-01-11 14:45:36.712559: \n",
      "2024-01-11 14:45:36.718560: Epoch 972\n",
      "2024-01-11 14:45:36.723632: Current learning rate: 0.0004\n",
      "2024-01-11 14:46:18.595576: train_loss -0.9582\n",
      "2024-01-11 14:46:18.629574: val_loss -0.8302\n",
      "2024-01-11 14:46:18.639573: Pseudo dice [0.8938, 0.9478, 0.9353]\n",
      "2024-01-11 14:46:18.648581: Epoch time: 41.88 s\n",
      "2024-01-11 14:46:19.924577: \n",
      "2024-01-11 14:46:19.934579: Epoch 973\n",
      "2024-01-11 14:46:19.943581: Current learning rate: 0.00039\n",
      "2024-01-11 14:47:01.075792: train_loss -0.958\n",
      "2024-01-11 14:47:01.130800: val_loss -0.8296\n",
      "2024-01-11 14:47:01.147802: Pseudo dice [0.8892, 0.947, 0.9357]\n",
      "2024-01-11 14:47:01.165816: Epoch time: 41.15 s\n",
      "2024-01-11 14:47:02.608641: \n",
      "2024-01-11 14:47:02.614646: Epoch 974\n",
      "2024-01-11 14:47:02.619647: Current learning rate: 0.00037\n",
      "2024-01-11 14:47:45.532490: train_loss -0.9581\n",
      "2024-01-11 14:47:45.545530: val_loss -0.8264\n",
      "2024-01-11 14:47:45.559427: Pseudo dice [0.8912, 0.9474, 0.9365]\n",
      "2024-01-11 14:47:45.570535: Epoch time: 42.92 s\n",
      "2024-01-11 14:47:47.024471: \n",
      "2024-01-11 14:47:47.031472: Epoch 975\n",
      "2024-01-11 14:47:47.036478: Current learning rate: 0.00036\n",
      "2024-01-11 14:48:28.259550: train_loss -0.9576\n",
      "2024-01-11 14:48:28.272494: val_loss -0.8273\n",
      "2024-01-11 14:48:28.283495: Pseudo dice [0.8882, 0.9481, 0.9356]\n",
      "2024-01-11 14:48:28.320576: Epoch time: 41.24 s\n",
      "2024-01-11 14:48:29.739504: \n",
      "2024-01-11 14:48:29.747516: Epoch 976\n",
      "2024-01-11 14:48:29.756519: Current learning rate: 0.00035\n",
      "2024-01-11 14:49:11.059440: train_loss -0.959\n",
      "2024-01-11 14:49:11.070440: val_loss -0.8253\n",
      "2024-01-11 14:49:11.079669: Pseudo dice [0.8891, 0.9475, 0.936]\n",
      "2024-01-11 14:49:11.088672: Epoch time: 41.32 s\n",
      "2024-01-11 14:49:12.454158: \n",
      "2024-01-11 14:49:12.460155: Epoch 977\n",
      "2024-01-11 14:49:12.465159: Current learning rate: 0.00034\n",
      "2024-01-11 14:49:54.140069: train_loss -0.9588\n",
      "2024-01-11 14:49:54.152370: val_loss -0.8282\n",
      "2024-01-11 14:49:54.183792: Pseudo dice [0.8873, 0.9482, 0.9369]\n",
      "2024-01-11 14:49:54.197795: Epoch time: 41.69 s\n",
      "2024-01-11 14:49:55.580688: \n",
      "2024-01-11 14:49:55.587678: Epoch 978\n",
      "2024-01-11 14:49:55.592688: Current learning rate: 0.00032\n",
      "2024-01-11 14:50:36.636277: train_loss -0.9584\n",
      "2024-01-11 14:50:36.648284: val_loss -0.8277\n",
      "2024-01-11 14:50:36.660583: Pseudo dice [0.8865, 0.948, 0.9357]\n",
      "2024-01-11 14:50:36.704136: Epoch time: 41.06 s\n",
      "2024-01-11 14:50:38.124463: \n",
      "2024-01-11 14:50:38.131455: Epoch 979\n",
      "2024-01-11 14:50:38.136464: Current learning rate: 0.00031\n",
      "2024-01-11 14:51:21.758806: train_loss -0.9586\n",
      "2024-01-11 14:51:21.773930: val_loss -0.8269\n",
      "2024-01-11 14:51:21.784930: Pseudo dice [0.8874, 0.948, 0.936]\n",
      "2024-01-11 14:51:21.826504: Epoch time: 43.64 s\n",
      "2024-01-11 14:51:23.089922: \n",
      "2024-01-11 14:51:23.095922: Epoch 980\n",
      "2024-01-11 14:51:23.104929: Current learning rate: 0.0003\n",
      "2024-01-11 14:52:09.088450: train_loss -0.9581\n",
      "2024-01-11 14:52:09.104453: val_loss -0.8306\n",
      "2024-01-11 14:52:09.119836: Pseudo dice [0.8927, 0.948, 0.9369]\n",
      "2024-01-11 14:52:09.132308: Epoch time: 46.0 s\n",
      "2024-01-11 14:52:10.722289: \n",
      "2024-01-11 14:52:10.730279: Epoch 981\n",
      "2024-01-11 14:52:10.736280: Current learning rate: 0.00028\n",
      "2024-01-11 14:52:57.455036: train_loss -0.9586\n",
      "2024-01-11 14:52:57.470894: val_loss -0.8269\n",
      "2024-01-11 14:52:57.485180: Pseudo dice [0.8877, 0.9479, 0.936]\n",
      "2024-01-11 14:52:57.498220: Epoch time: 46.73 s\n",
      "2024-01-11 14:52:59.579144: \n",
      "2024-01-11 14:52:59.585680: Epoch 982\n",
      "2024-01-11 14:52:59.591219: Current learning rate: 0.00027\n",
      "2024-01-11 14:53:47.695819: train_loss -0.958\n",
      "2024-01-11 14:53:47.739657: val_loss -0.831\n",
      "2024-01-11 14:53:47.753656: Pseudo dice [0.8904, 0.9478, 0.9366]\n",
      "2024-01-11 14:53:47.765655: Epoch time: 48.12 s\n",
      "2024-01-11 14:53:49.207825: \n",
      "2024-01-11 14:53:49.215001: Epoch 983\n",
      "2024-01-11 14:53:49.220011: Current learning rate: 0.00026\n",
      "2024-01-11 14:54:33.423459: train_loss -0.9584\n",
      "2024-01-11 14:54:33.441454: val_loss -0.8278\n",
      "2024-01-11 14:54:33.487987: Pseudo dice [0.8917, 0.9476, 0.9356]\n",
      "2024-01-11 14:54:33.507294: Epoch time: 44.22 s\n",
      "2024-01-11 14:54:34.934486: \n",
      "2024-01-11 14:54:34.941486: Epoch 984\n",
      "2024-01-11 14:54:34.947502: Current learning rate: 0.00024\n",
      "2024-01-11 14:55:16.557235: train_loss -0.9582\n",
      "2024-01-11 14:55:16.570539: val_loss -0.8259\n",
      "2024-01-11 14:55:16.582547: Pseudo dice [0.8889, 0.9473, 0.9355]\n",
      "2024-01-11 14:55:16.617675: Epoch time: 41.62 s\n",
      "2024-01-11 14:55:18.126349: \n",
      "2024-01-11 14:55:18.131862: Epoch 985\n",
      "2024-01-11 14:55:18.137103: Current learning rate: 0.00023\n",
      "2024-01-11 14:56:00.770021: train_loss -0.9588\n",
      "2024-01-11 14:56:00.781022: val_loss -0.829\n",
      "2024-01-11 14:56:00.789025: Pseudo dice [0.8901, 0.948, 0.9363]\n",
      "2024-01-11 14:56:00.798023: Epoch time: 42.65 s\n",
      "2024-01-11 14:56:02.411187: \n",
      "2024-01-11 14:56:02.419188: Epoch 986\n",
      "2024-01-11 14:56:02.427199: Current learning rate: 0.00021\n",
      "2024-01-11 14:56:47.149158: train_loss -0.959\n",
      "2024-01-11 14:56:47.170842: val_loss -0.8275\n",
      "2024-01-11 14:56:47.186853: Pseudo dice [0.8907, 0.9481, 0.937]\n",
      "2024-01-11 14:56:47.204892: Epoch time: 44.74 s\n",
      "2024-01-11 14:56:49.268016: \n",
      "2024-01-11 14:56:49.274028: Epoch 987\n",
      "2024-01-11 14:56:49.279088: Current learning rate: 0.0002\n",
      "2024-01-11 14:57:34.665865: train_loss -0.9589\n",
      "2024-01-11 14:57:34.680083: val_loss -0.8269\n",
      "2024-01-11 14:57:34.691084: Pseudo dice [0.8912, 0.948, 0.9359]\n",
      "2024-01-11 14:57:34.703084: Epoch time: 45.4 s\n",
      "2024-01-11 14:57:36.270602: \n",
      "2024-01-11 14:57:36.278603: Epoch 988\n",
      "2024-01-11 14:57:36.288602: Current learning rate: 0.00019\n",
      "2024-01-11 14:58:19.799113: train_loss -0.9583\n",
      "2024-01-11 14:58:19.807255: val_loss -0.828\n",
      "2024-01-11 14:58:19.818242: Pseudo dice [0.891, 0.9481, 0.9365]\n",
      "2024-01-11 14:58:19.827758: Epoch time: 43.53 s\n",
      "2024-01-11 14:58:21.369421: \n",
      "2024-01-11 14:58:21.376421: Epoch 989\n",
      "2024-01-11 14:58:21.381959: Current learning rate: 0.00017\n",
      "2024-01-11 14:59:05.280900: train_loss -0.9586\n",
      "2024-01-11 14:59:05.293906: val_loss -0.8271\n",
      "2024-01-11 14:59:05.306422: Pseudo dice [0.8881, 0.9474, 0.9357]\n",
      "2024-01-11 14:59:05.341419: Epoch time: 43.91 s\n",
      "2024-01-11 14:59:06.639486: \n",
      "2024-01-11 14:59:06.645479: Epoch 990\n",
      "2024-01-11 14:59:06.650486: Current learning rate: 0.00016\n",
      "2024-01-11 14:59:49.251346: train_loss -0.9599\n",
      "2024-01-11 14:59:49.264344: val_loss -0.828\n",
      "2024-01-11 14:59:49.274344: Pseudo dice [0.8897, 0.9477, 0.9366]\n",
      "2024-01-11 14:59:49.284344: Epoch time: 42.61 s\n",
      "2024-01-11 14:59:50.573787: \n",
      "2024-01-11 14:59:50.580714: Epoch 991\n",
      "2024-01-11 14:59:50.585716: Current learning rate: 0.00014\n",
      "2024-01-11 15:00:31.900859: train_loss -0.9591\n",
      "2024-01-11 15:00:31.921859: val_loss -0.828\n",
      "2024-01-11 15:00:31.944646: Pseudo dice [0.8873, 0.9481, 0.9357]\n",
      "2024-01-11 15:00:31.977292: Epoch time: 41.33 s\n",
      "2024-01-11 15:00:33.335663: \n",
      "2024-01-11 15:00:33.341664: Epoch 992\n",
      "2024-01-11 15:00:33.345654: Current learning rate: 0.00013\n",
      "2024-01-11 15:01:14.600220: train_loss -0.9586\n",
      "2024-01-11 15:01:14.612220: val_loss -0.8307\n",
      "2024-01-11 15:01:14.623296: Pseudo dice [0.8938, 0.9483, 0.9364]\n",
      "2024-01-11 15:01:14.651298: Epoch time: 41.27 s\n",
      "2024-01-11 15:01:16.129297: \n",
      "2024-01-11 15:01:16.139496: Epoch 993\n",
      "2024-01-11 15:01:16.147496: Current learning rate: 0.00011\n",
      "2024-01-11 15:01:58.115056: train_loss -0.9587\n",
      "2024-01-11 15:01:58.128062: val_loss -0.8269\n",
      "2024-01-11 15:01:58.138066: Pseudo dice [0.891, 0.9476, 0.9362]\n",
      "2024-01-11 15:01:58.149063: Epoch time: 41.99 s\n",
      "2024-01-11 15:01:59.512058: \n",
      "2024-01-11 15:01:59.520577: Epoch 994\n",
      "2024-01-11 15:01:59.531587: Current learning rate: 0.0001\n",
      "2024-01-11 15:02:41.102667: train_loss -0.9595\n",
      "2024-01-11 15:02:41.114191: val_loss -0.8265\n",
      "2024-01-11 15:02:41.124713: Pseudo dice [0.8901, 0.9477, 0.9358]\n",
      "2024-01-11 15:02:41.134233: Epoch time: 41.59 s\n",
      "2024-01-11 15:02:42.867043: \n",
      "2024-01-11 15:02:42.873585: Epoch 995\n",
      "2024-01-11 15:02:42.878098: Current learning rate: 8e-05\n",
      "2024-01-11 15:03:25.013693: train_loss -0.9591\n",
      "2024-01-11 15:03:25.022695: val_loss -0.83\n",
      "2024-01-11 15:03:25.031694: Pseudo dice [0.8923, 0.9477, 0.9363]\n",
      "2024-01-11 15:03:25.041697: Epoch time: 42.15 s\n",
      "2024-01-11 15:03:26.490526: \n",
      "2024-01-11 15:03:26.496524: Epoch 996\n",
      "2024-01-11 15:03:26.502525: Current learning rate: 7e-05\n",
      "2024-01-11 15:04:08.224663: train_loss -0.9588\n",
      "2024-01-11 15:04:08.239738: val_loss -0.8276\n",
      "2024-01-11 15:04:08.254736: Pseudo dice [0.8923, 0.9482, 0.936]\n",
      "2024-01-11 15:04:08.270832: Epoch time: 41.74 s\n",
      "2024-01-11 15:04:09.966618: \n",
      "2024-01-11 15:04:09.974678: Epoch 997\n",
      "2024-01-11 15:04:09.985627: Current learning rate: 5e-05\n",
      "2024-01-11 15:04:51.307780: train_loss -0.9593\n",
      "2024-01-11 15:04:51.316781: val_loss -0.8264\n",
      "2024-01-11 15:04:51.329780: Pseudo dice [0.8881, 0.9475, 0.935]\n",
      "2024-01-11 15:04:51.338875: Epoch time: 41.34 s\n",
      "2024-01-11 15:04:52.927827: \n",
      "2024-01-11 15:04:52.933403: Epoch 998\n",
      "2024-01-11 15:04:52.938446: Current learning rate: 4e-05\n",
      "2024-01-11 15:05:34.384318: train_loss -0.9589\n",
      "2024-01-11 15:05:34.397933: val_loss -0.8291\n",
      "2024-01-11 15:05:34.408206: Pseudo dice [0.8906, 0.948, 0.9366]\n",
      "2024-01-11 15:05:34.416268: Epoch time: 41.46 s\n",
      "2024-01-11 15:05:36.074073: \n",
      "2024-01-11 15:05:36.080709: Epoch 999\n",
      "2024-01-11 15:05:36.091630: Current learning rate: 2e-05\n",
      "2024-01-11 15:06:17.742854: train_loss -0.9588\n",
      "2024-01-11 15:06:17.751406: val_loss -0.8292\n",
      "2024-01-11 15:06:17.758404: Pseudo dice [0.8906, 0.9477, 0.9362]\n",
      "2024-01-11 15:06:17.776067: Epoch time: 41.67 s\n",
      "2024-01-11 15:06:19.655820: Training done.\n",
      "2024-01-11 15:06:19.695536: Using splits from existing split file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\splits_final.json\n",
      "2024-01-11 15:06:19.710774: The split file contains 5 splits.\n",
      "2024-01-11 15:06:19.718774: Desired fold for training: 4\n",
      "2024-01-11 15:06:19.725774: This split has 8 training and 2 validation cases.\n",
      "2024-01-11 15:06:19.733775: predicting case_2\n",
      "2024-01-11 15:06:22.662133: predicting case_5\n",
      "2024-01-11 15:06:34.142012: Validation complete\n",
      "2024-01-11 15:06:34.149090: Mean Validation Dice:  0.9276752963556131\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Requested configuration 3d_lowres not found in plans. Available configurations: ['2d', '3d_fullres']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\apps\\nnunet\\nnunetv2_runner.py:589\u001b[0m, in \u001b[0;36mnnUNetV2Runner.train\u001b[1;34m(self, configs, gpu_id_for_all, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m ensure_tuple(configs):\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_folds):\n\u001b[1;32m--> 589\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_single_model(config\u001b[38;5;241m=\u001b[39mcfg, fold\u001b[38;5;241m=\u001b[39m_fold, gpu_id\u001b[38;5;241m=\u001b[39mgpu_id_for_all, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\monai\\apps\\nnunet\\nnunetv2_runner.py:538\u001b[0m, in \u001b[0;36mnnUNetV2Runner.train_single_model\u001b[1;34m(self, config, fold, gpu_id, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnnunetv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_training\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpu_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gpu_id) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 538\u001b[0m     run_training(\n\u001b[0;32m    539\u001b[0m         dataset_name_or_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name_or_id,\n\u001b[0;32m    540\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    541\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m    542\u001b[0m         trainer_class_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_class_name,\n\u001b[0;32m    543\u001b[0m         export_validation_probabilities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_validation_probabilities,\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    545\u001b[0m     )\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     run_training(\n\u001b[0;32m    548\u001b[0m         dataset_name_or_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name_or_id,\n\u001b[0;32m    549\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\run\\run_training.py:189\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(dataset_name_or_id, configuration, fold, trainer_class_name, plans_identifier, pretrained_weights, num_gpus, use_compressed_data, export_validation_probabilities, continue_training, only_run_validation, disable_checkpointing, val_with_best, device)\u001b[0m\n\u001b[0;32m    171\u001b[0m     mp\u001b[38;5;241m.\u001b[39mspawn(run_ddp,\n\u001b[0;32m    172\u001b[0m              args\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    173\u001b[0m                  dataset_name_or_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m              nprocs\u001b[38;5;241m=\u001b[39mnum_gpus,\n\u001b[0;32m    187\u001b[0m              join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     nnunet_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mget_trainer_from_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name_or_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mplans_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_compressed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m disable_checkpointing:\n\u001b[0;32m    193\u001b[0m         nnunet_trainer\u001b[38;5;241m.\u001b[39mdisable_checkpointing \u001b[38;5;241m=\u001b[39m disable_checkpointing\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\run\\run_training.py:65\u001b[0m, in \u001b[0;36mget_trainer_from_args\u001b[1;34m(dataset_name_or_id, configuration, fold, trainer_name, plans_identifier, use_compressed, device)\u001b[0m\n\u001b[0;32m     63\u001b[0m plans \u001b[38;5;241m=\u001b[39m load_json(plans_file)\n\u001b[0;32m     64\u001b[0m dataset_json \u001b[38;5;241m=\u001b[39m load_json(join(preprocessed_dataset_folder_base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m nnunet_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mnnunet_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdataset_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_compressed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nnunet_trainer\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\training\\nnUNetTrainer\\nnUNetTrainer.py:113\u001b[0m, in \u001b[0;36mnnUNetTrainer.__init__\u001b[1;34m(self, plans, configuration, fold, dataset_json, unpack_dataset, device)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m###  Saving all the init args into class variables for later access\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplans_manager \u001b[38;5;241m=\u001b[39m PlansManager(plans)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplans_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration_name \u001b[38;5;241m=\u001b[39m configuration\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_json \u001b[38;5;241m=\u001b[39m dataset_json\n",
      "File \u001b[1;32mc:\\Users\\legor\\miniforge3\\envs\\p310-torch\\lib\\site-packages\\nnunetv2\\utilities\\plans_handling\\plans_handler.py:224\u001b[0m, in \u001b[0;36mPlansManager.get_configuration\u001b[1;34m(self, configuration_name)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_configuration\u001b[39m(\u001b[38;5;28mself\u001b[39m, configuration_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m configuration_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplans[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested configuration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in plans. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable configurations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplans[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m     configuration_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_resolve_configuration_inheritance(configuration_name)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ConfigurationManager(configuration_dict)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested configuration 3d_lowres not found in plans. Available configurations: ['2d', '3d_fullres']"
     ]
    }
   ],
   "source": [
    "runner.train(continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 3d_lowres not found in plans nnUNetPlans.\n",
      "Inferred plans file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\nnUNetPlans.json.\n",
      "Configuration 3d_cascade_fullres not found in plans nnUNetPlans.\n",
      "Inferred plans file: D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_preprocessed\\Dataset001_dataset\\nnUNetPlans.json.\n",
      "\n",
      "***All results:***\n",
      "nnUNetTrainer__nnUNetPlans__2d: 0.9330357486433947\n",
      "nnUNetTrainer__nnUNetPlans__3d_fullres: 0.932586311810331\n",
      "ensemble___nnUNetTrainer__nnUNetPlans__2d___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4: 0.9362938720075822\n",
      "\n",
      "*Best*: ensemble___nnUNetTrainer__nnUNetPlans__2d___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4: 0.9362938720075822\n",
      "\n",
      "***Determining postprocessing for best model/ensemble***\n",
      "Results were improved by removing all but the largest foreground region. Mean dice before: 0.93629 after: 0.93629\n",
      "Removing all but the largest component for 1 did not improve results! Dice before: 0.91118 after: 0.86727\n",
      "Results were improved by removing all but the largest component for 2. Dice before: 0.95189 after: 0.95191\n",
      "Removing all but the largest component for 3 did not improve results! Dice before: 0.94581 after: 0.89414\n",
      "\n",
      "***Run inference like this:***\n",
      "\n",
      "An ensemble won! What a surprise! Run the following commands to run predictions with the ensemble members:\n",
      "\n",
      "nnUNetv2_predict -d 1 -i INPUT_FOLDER -o OUTPUT_FOLDER_MODEL_1 -f  0 1 2 3 4 -tr nnUNetTrainer -c 2d -p nnUNetPlans --save_probabilities\n",
      "nnUNetv2_predict -d 1 -i INPUT_FOLDER -o OUTPUT_FOLDER_MODEL_2 -f  0 1 2 3 4 -tr nnUNetTrainer -c 3d_fullres -p nnUNetPlans --save_probabilities\n",
      "\n",
      "The run ensembling with:\n",
      "\n",
      "nnUNetv2_ensemble -i OUTPUT_FOLDER_MODEL_1 OUTPUT_FOLDER_MODEL_2 -o OUTPUT_FOLDER -np 8\n",
      "\n",
      "***Once inference is completed, run postprocessing like this:***\n",
      "\n",
      "nnUNetv2_apply_postprocessing -i OUTPUT_FOLDER -o OUTPUT_FOLDER_PP -pp_pkl_file D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_trained_models\\Dataset001_dataset\\ensembles\\ensemble___nnUNetTrainer__nnUNetPlans__2d___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4\\postprocessing.pkl -np 8 -plans_json D:\\Files\\Projects\\MAIA\\UdG\\MISA\\Project\\nnunetv2\\nnUNet_trained_models\\Dataset001_dataset\\ensembles\\ensemble___nnUNetTrainer__nnUNetPlans__2d___nnUNetTrainer__nnUNetPlans__3d_fullres___0_1_2_3_4\\plans.json\n"
     ]
    }
   ],
   "source": [
    "runner.find_best_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 8 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 8 cases that I would like to predict\n",
      "using pin_memory on device 0\n",
      "\n",
      "Predicting case_10:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_10\n",
      "\n",
      "Predicting case_11:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_11\n",
      "\n",
      "Predicting case_12:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_12\n",
      "\n",
      "Predicting case_13:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_13\n",
      "\n",
      "Predicting case_14:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_14\n",
      "\n",
      "Predicting case_15:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_15\n",
      "\n",
      "Predicting case_16:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_16\n",
      "\n",
      "Predicting case_17:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_17\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 8 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 8 cases that I would like to predict\n",
      "using pin_memory on device 0\n",
      "\n",
      "Predicting case_10:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_10\n",
      "\n",
      "Predicting case_11:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_11\n",
      "\n",
      "Predicting case_12:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_12\n",
      "\n",
      "Predicting case_13:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_13\n",
      "\n",
      "Predicting case_14:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_14\n",
      "\n",
      "Predicting case_15:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_15\n",
      "\n",
      "Predicting case_16:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_16\n",
      "\n",
      "Predicting case_17:\n",
      "perform_everything_on_gpu: True\n",
      "Prediction done, transferring to CPU if needed\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with case_17\n"
     ]
    }
   ],
   "source": [
    "runner.predict_ensemble_postprocessing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get validation results for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_score(labels_seg, labels_gt, print_score=True):\n",
    "    dice_score = {}\n",
    "    \n",
    "    # please check what are the actual label values for the dataset you are working with\n",
    "\n",
    "    label_seg_csf = labels_seg == 1\n",
    "    label_seg_gm = labels_seg == 2\n",
    "    label_seg_wm = labels_seg == 3\n",
    "\n",
    "    label_gt_csf = labels_gt == 1\n",
    "    label_gt_gm = labels_gt == 2\n",
    "    label_gt_wm = labels_gt == 3\n",
    "\n",
    "    dice_score['csf'] = np.sum(label_gt_csf[label_seg_csf]) * 2.0 / (np.sum(label_gt_csf) + np.sum(label_seg_csf))\n",
    "    dice_score['gm'] = np.sum(label_gt_gm[label_seg_gm]) * 2.0 / (np.sum(label_gt_gm) + np.sum(label_seg_gm))\n",
    "    dice_score['wm'] = np.sum(label_gt_wm[label_seg_wm]) * 2.0 / (np.sum(label_gt_wm) + np.sum(label_seg_wm))\n",
    "\n",
    "    if print_score:\n",
    "        print('Dice scores:')\n",
    "        for key, value in dice_score.items():\n",
    "            print(f'{key}: {value:.4f}')\n",
    "            \n",
    "        print(f'Average: {np.mean(list(dice_score.values())):.4f}')\n",
    "\n",
    "    return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hausdorff_distance(labels_pred, labels_gt, print_score=True):\n",
    "    \"\"\"\n",
    "    Calculate the Hausdorff distance for each label in the predicted and ground truth segmentation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_pred : numpy array\n",
    "        Predicted segmentation labels.\n",
    "    labels_gt : numpy array\n",
    "        Ground truth segmentation labels.\n",
    "    print_score : bool, optional\n",
    "        Whether to print the Hausdorff distance for each label, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hd_score : dict\n",
    "        Dictionary containing the Hausdorff distance for each label.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to SimpleITK images\n",
    "    pred_image = sitk.GetImageFromArray(labels_pred)\n",
    "    gt_image = sitk.GetImageFromArray(labels_gt)\n",
    "\n",
    "    # Create the Hausdorff distance filter\n",
    "    hd_filter = sitk.HausdorffDistanceImageFilter()\n",
    "\n",
    "    # Calculate the Hausdorff distance for each label\n",
    "    hd_scores = {}\n",
    "    for label, label_name in zip([1, 2, 3], ['csf', 'gm', 'wm']):\n",
    "        pred_label = sitk.BinaryThreshold(pred_image, label, label)\n",
    "        gt_label = sitk.BinaryThreshold(gt_image, label, label)\n",
    "\n",
    "        hd_filter.Execute(pred_label, gt_label)\n",
    "        hd_scores[label_name] = hd_filter.GetHausdorffDistance()\n",
    "    \n",
    "    if print_score:\n",
    "        print('Hausdorff distance:')\n",
    "        for key, value in hd_scores.items():\n",
    "            print(f'{key}: {value:.4f}')\n",
    "        \n",
    "        print(f'Average: {np.mean(list(hd_scores.values())):.4f}')\n",
    "\n",
    "    return hd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avd(pred_labels, gt_labels, print_scores=True):\n",
    "    \"\"\"\n",
    "    Calculate the average volumetric difference (AVD) per label between predicted and ground truth labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_labels : numpy.ndarray\n",
    "        Predicted segmentation labels.\n",
    "    gt_labels : numpy.ndarray\n",
    "        Ground truth segmentation labels.\n",
    "    print_scores : bool, optional\n",
    "        Whether to print the AVD scores per label, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avd_scores : dict\n",
    "        Dictionary containing the average volumetric difference (AVD) score per label.\n",
    "    \"\"\"\n",
    "    avd_scores = {}\n",
    "    \n",
    "    for label, label_name in zip([1, 2, 3], ['csf', 'gm', 'wm']):\n",
    "        pred_label = pred_labels == label\n",
    "        gt_label = gt_labels == label\n",
    "\n",
    "        avd_scores[label_name] = np.abs(np.sum(pred_label) - np.sum(gt_label)) / np.sum(gt_label)\n",
    "\n",
    "    if print_scores:\n",
    "        print(\"Average Volumetric Difference (AVD) per label:\")\n",
    "        for label, score in avd_scores.items():\n",
    "            print(f\"Label {label}: {score:.4f}\")\n",
    "            \n",
    "        print(f\"Average: {np.mean(list(avd_scores.values())):.4f}\")\n",
    "\n",
    "    return avd_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice scores:\n",
      "csf: 0.9125\n",
      "gm: 0.9545\n",
      "wm: 0.9622\n",
      "Average: 0.9431\n",
      "\n",
      "Hausdorff distance:\n",
      "csf: 10.0499\n",
      "gm: 7.8740\n",
      "wm: 5.7446\n",
      "Average: 7.8895\n",
      "\n",
      "Average Volumetric Difference (AVD) per label:\n",
      "Label csf: 0.0325\n",
      "Label gm: 0.0142\n",
      "Label wm: 0.0054\n",
      "Average: 0.0174\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice scores:\n",
      "csf: 0.8998\n",
      "gm: 0.9470\n",
      "wm: 0.9511\n",
      "Average: 0.9326\n",
      "\n",
      "Hausdorff distance:\n",
      "csf: 10.2470\n",
      "gm: 6.7082\n",
      "wm: 5.3852\n",
      "Average: 7.4468\n",
      "\n",
      "Average Volumetric Difference (AVD) per label:\n",
      "Label csf: 0.1413\n",
      "Label gm: 0.0082\n",
      "Label wm: 0.0465\n",
      "Average: 0.0653\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice scores:\n",
      "csf: 0.9069\n",
      "gm: 0.9463\n",
      "wm: 0.9312\n",
      "Average: 0.9281\n",
      "\n",
      "Hausdorff distance:\n",
      "csf: 10.4403\n",
      "gm: 11.0454\n",
      "wm: 8.6023\n",
      "Average: 10.0293\n",
      "\n",
      "Average Volumetric Difference (AVD) per label:\n",
      "Label csf: 0.0209\n",
      "Label gm: 0.0573\n",
      "Label wm: 0.0547\n",
      "Average: 0.0443\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice scores:\n",
      "csf: 0.9376\n",
      "gm: 0.9652\n",
      "wm: 0.9601\n",
      "Average: 0.9543\n",
      "\n",
      "Hausdorff distance:\n",
      "csf: 4.2426\n",
      "gm: 5.0990\n",
      "wm: 6.0000\n",
      "Average: 5.1139\n",
      "\n",
      "Average Volumetric Difference (AVD) per label:\n",
      "Label csf: 0.0777\n",
      "Label gm: 0.0128\n",
      "Label wm: 0.0378\n",
      "Average: 0.0428\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAHmCAYAAAD5mB0yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6b0lEQVR4nOydd3gc1dm375nZviutVr1Zci+4F9xoNqb3XgMkIQl5QwohQBJ4E0oSSIeEhPDxJoHQQxJa6L03Y2Nw77Itq3dt352Z74+1ZK21klbSNknnvi5d9s5OObs7vzPPOecpkq7rOgKBQCAQCASHIKe7AQKBQCAQCDITYSQIBAKBQCCIiTASBAKBQCAQxEQYCQKBQCAQCGIijASBQCAQCAQxEUaCQCAQCASCmAgjQSAQCAQCQUyEkSAQCAQCgSAmwkgQCAQCgUAQE2EkCIZFVVUVkiTxwAMPdG+75ZZbkCRp0Od69NFHueuuu2K+J0kSt9xyy9AaKRAI0sIDDzyAJElUVVX1u98LL7yQVH33d35Jkvj2t7+dtGuPdISRIEg4X/va1/jwww8HfVx/RsKHH37I1772tWG2TCAQZCIvvPACt95664g9/2jGkO4GCNKHz+fDarUm/Lzl5eWUl5cn9JxLly5N6PkEgtGCruv4/f6kaDkTGWufN92ImYQRTtfU/meffcY555xDdnY2TqeTL33pSzQ2NnbvN378eE477TSefPJJ5s+fj8Vi6bas6+rquOqqqygvL8dkMjFhwgRuvfVWwuFw1LVqamq44IILyMrKwul0cuGFF1JXV9dnmw7l0UcfZdmyZTgcDhwOB/PmzeNvf/sbACtWrOD5559nz549SJLU/ddFrOWGDRs2cOaZZ+JyubBYLMybN49//OMfUfu89dZbSJLEY489xk033URpaSnZ2dkcd9xxbN26dXBftkCQRJ555hnmzJmD2Wxm4sSJ/OEPf4ippa7p8XvvvZcZM2ZgNpu77/v33nuPVatWkZWVhc1mY/ny5Tz//PNRx/elz1hLA139xksvvcSCBQuwWq1Mnz6dv//9772O/+ijjzjiiCOwWCyUlpby4x//mFAoNODn/vKXv8yf//zn7s/W9dfVjr4+b5e233rrrajzHboEOtD5u3jooYeYMWMGNpuNuXPn8txzzw3Y9rGAmEkYJZx99tlccMEFfPOb32Tjxo385Cc/YdOmTXz88ccYjUYA1q5dy+bNm/nf//1fJkyYgN1up66ujsWLFyPLMj/96U+ZNGkSH374IT//+c+pqqri/vvvByKzDscddxw1NTXccccdTJ06leeff54LL7wwrvb99Kc/5Wc/+xnnnHMOP/jBD3A6nWzYsIE9e/YAcM899/CNb3yDnTt38tRTTw14vq1bt7J8+XIKCwv54x//SF5eHg8//DBf/vKXqa+v54Ybboja/8Ybb+SII47gr3/9Kx0dHfzwhz/k9NNPZ/PmzSiKMpivWiBIOC+99BLnnHMORx99NP/85z8Jh8P89re/pb6+Pub+Tz/9NO+++y4//elPKS4uprCwkLfffpvjjz+eOXPm8Le//Q2z2cw999zD6aefzmOPPRa3Vg/l888/5wc/+AE/+tGPKCoq4q9//StXXnklkydP5uijjwZg06ZNrFq1ivHjx/PAAw9gs9m45557ePTRRwc8/09+8hM8Hg///ve/o5YpS0pK+v28PQdBwz3/888/z+rVq7nttttwOBz8+te/5uyzz2br1q1MnDgxruuMWnTBiObmm2/WAf373/9+1PZHHnlEB/SHH35Y13Vdr6ys1BVF0bdu3Rq131VXXaU7HA59z549Udt/+9vf6oC+ceNGXdd1/S9/+YsO6M8880zUfl//+td1QL///vt7tamLXbt26Yqi6Jdeemm/n+XUU0/VKysrY74H6DfffHP364suukg3m8363r17o/Y7+eSTdZvNpre1tem6rutvvvmmDuinnHJK1H5PPPGEDugffvhhv20SCFLB4Ycfro8bN04PBALd2zo7O/W8vDz90G4a0J1Op97S0hK1fenSpXphYaHe2dnZvS0cDuuzZs3Sy8vLdU3TdF3vrc8u7r//fh3Qd+/e3b2tsrJSt1gsUf2Dz+fTc3Nz9auuuqp724UXXqhbrVa9rq4u6trTp0/vdc5YXH311THb1N/n7dL2m2++GbV99+7dvfqkgc5fVFSkd3R0dG+rq6vTZVnW77jjjn7bPRYQyw2jhEsvvTTq9QUXXIDBYODNN9/s3jZnzhymTp0atd9zzz3HypUrKS0tJRwOd/+dfPLJALz99tsAvPnmm2RlZXHGGWdEHX/JJZcM2LZXX30VVVW5+uqrh/TZYvHGG2+watUqxo0bF7X9y1/+Ml6vt5fj5KHtnjNnDkD3TIZAkC48Hg+ffvopZ511FiaTqXu7w+Hg9NNPj3nMsccei8vlijrHxx9/zHnnnYfD4ejerigKl112GdXV1UNeXps3bx4VFRXdry0WC1OnTo3SzptvvsmqVasoKiqKuvZQZy8O5dDPm2hWrlxJVlZW9+uioiIKCwtF/4BYbhg1FBcXR702GAzk5eXR3Nzcva3n9FoX9fX1/Pe//+1ekjiUpqYmAJqbm6M6gL6uG4uuacFEOjM2NzfH/DylpaXd7/ckLy8v6rXZbAYiyygCQTppbW1F1/WY+oq1DXpruescg9FEvByqHYjop6d2mpubY/YF8fQP8RDrcyWSeD7jWEUYCaOEuro6ysrKul+Hw2Gam5ujbv5Yzkr5+fnMmTOHX/ziFzHP29XB5OXl8cknn8S87kAUFBQAUF1d3WvkP1Ty8vKora3ttb2mpgaIfC6BYCTgcrmQJCmm/0Ff+jpUyy6XC1mW49KExWIBIBAIdBvLcHBAMBTy8vJitjWe/iEeYvVdPT9HT4bzOQS9EcsNo4RHHnkk6vUTTzxBOBxmxYoV/R532mmnsWHDBiZNmsSiRYt6/XUZCStXrqSzs5Nnn3026vh4HJNOOOEEFEXhL3/5S7/7DcZyX7VqFW+88UZ3B9jFgw8+iM1mEyGTghGD3W5n0aJFPP300wSDwe7tbrc7bg97u93OkiVLePLJJ6M0pGkaDz/8MOXl5d1LjePHjwfgiy++iDrHf//73yF/hpUrV/L6669HGTqqqvLPf/4zruOHMrPX1+c4tI8a6vkFEcRMwijhySefxGAwcPzxx3dHN8ydO5cLLrig3+Nuu+02Xn31VZYvX853v/tdpk2bht/vp6qqihdeeIF7772X8vJyLr/8cu68804uv/xyfvGLXzBlyhReeOEFXn755QHbNn78eG688UZ+9rOf4fP5uPjii3E6nWzatImmpqbuUMzZs2fz5JNP8pe//IWFCxciyzKLFi2Kec6bb76525/ipz/9Kbm5uTzyyCM8//zz/PrXv8bpdA7+SxQI0sRtt93Gqaeeyoknnsj3vvc9VFXlN7/5DQ6Hg5aWlrjOcccdd3D88cezcuVKrrvuOkwmE/fccw8bNmzgscce6x6Nn3LKKeTm5nLllVdy2223YTAYeOCBB9i3b9+Q2/+///u/PPvssxx77LH89Kc/xWaz8ec//xmPxxPX8bNnzwbgV7/6FSeffDKKojBnzpwoH41DKS4u5rjjjuOOO+7A5XJRWVnJ66+/zpNPPpmQ8wsOkG7PScHw6PJUXrNmjX766afrDodDz8rK0i+++GK9vr6+e7/Kykr91FNPjXmOxsZG/bvf/a4+YcIE3Wg06rm5ufrChQv1m266SXe73d37VVdX6+eee273Nc4991z9gw8+GDC6oYsHH3xQP/zww3WLxaI7HA59/vz5Uce1tLTo5513np6Tk6NLkhR1Dg6JbtB1XV+/fr1++umn606nUzeZTPrcuXOjzqfrBz2g//Wvf0Vtj+UBLRCkk6eeekqfPXu2bjKZ9IqKCv2Xv/yl/t3vfld3uVxR+wH61VdfHfMc7777rn7sscfqdrtdt1qt+tKlS/X//ve/vfb75JNP9OXLl+t2u10vKyvTb775Zv2vf/1rzOiGWP3GMcccox9zzDFR295//3196dKlutls1ouLi/Xrr79ev+++++KKbggEAvrXvvY1vaCgoFv7Xcf093lra2v18847T8/NzdWdTqf+pS99Sf/00097aXso56+srNSvuOKKfts9FpB0XdfTYJsIEsQtt9zCrbfeSmNjo1iHFwhGEaFQiHnz5lFWVsYrr7yS7uYIxihiuUEgEAgygCuvvJLjjz+ekpIS6urquPfee9m8eTN/+MMf0t00wRhGGAkCgUCQAXR2dnLdddfR2NiI0WhkwYIFvPDCCxx33HHpbppgDCOWGwQCgUAgEMREhEAKBAKBQCCISVqNhHvuuYcJEyZgsVhYuHAh7777bjqbIxAIkozQvEAwskibkfDPf/6Ta665hptuuonPPvuMo446ipNPPpm9e/emq0kCgSCJCM0LBCOPtPkkLFmyhAULFkRl4ZsxYwZnnXUWd9xxR9S+gUAgKvWmpmm0tLSQl5cXM12nQCCIH13X6ezspLS0FFlO3rhhMJoHoXuBIFkMSvPpSM4QCAR0RVH0J598Mmr7d7/7Xf3oo4/utX9Xch7xJ/7EX/L+9u3blzGaF7oXf+Iv+X/xaD4tIZBNTU2oqtqrwllRUVHMgiA//vGPufbaa7tft7e3U1FRwZGcgoHY1QsFAkF8hAnxHi9ElcpNNIPVPAjdCwTJYjCaT2uehEOnDHVdjzmNaDabo6qVdWHAiEESnYVAMCz0yD+pmMKPV/MgdC8QJI1BaD4tjov5+fkoitJrBNHQ0NBn/XSBQDByEZoXCEYmaTESTCYTCxcu5NVXX43a3lWNUCAQjC6E5gWCkUnalhuuvfZaLrvsMhYtWsSyZcu477772Lt3L9/85jfT1SSBQJBEhOYFgpFH2oyECy+8kObmZm677TZqa2uZNWsWL7zwApWVlelqkkAgSCJC8wLByGNE1m7o6OjA6XSygjOFA5NAMEzCeoi3eIb29nays7PT3Zw+EboXCBLDYDQvajcIBAKBQCCIiTASBAKBQCAQxEQYCQKBQCAQCGIijASBQCAQCAQxEUaCQCAQCASCmAgjQSAQCAQCQUyEkSAQCAQCgSAmwkgQCAQCgUAQE2EkCAQCgUAgiIkwEgQCgUAgEMREGAkCgUAgEAhiIowEgUAgEAgEMRFGgkAgEAgEgpgII0EgEAgEAkFMhJEgEAgEAoEgJsJIEAgEAoFAEBNhJAgEAoFAIIiJMBIEAoFAIBDERBgJAoFAIBAIYiKMBIFAIBAIBDERRoJAIBAIBIKYCCNBIBAIBAJBTISRIBAIBAKBICbCSBAIBAKBQBATYSQIBAKBQCCIiTASBAKBQCAQxEQYCQKBQCAQCGIijASBQCAQCAQxEUaCQCAQCASCmAgjQSAQCAQCQUyEkSAQCAQCgSAmwkgQCAQCgUAQE2EkCAQCgUAgiIkwEgQCgUAgEMREGAkCgUAgEAhiIowEgUAgEAgEMRFGgkAgEAgEgpgII0EgEAgEAkFMhJEgEAgEAoEgJsJIEAgEAoFAEBNhJAgEAoFAIIiJMBIEAoFAIBDERBgJAoFAIBAIYiKMBIFAIBAIBDERRoJAIBAIBIKYCCNBIBAIBAJBTISRIBAIBAKBICbCSBAIBAKBQBATYSQIBAKBQCCIiTASBAKBQCAQxEQYCQKBQCAQCGIijASBQCAQCAQxEUaCQCAQCASCmAgjQSAQCAQCQUyEkSAQCAQCgSAmwkgQCAQCgUAQE2EkCAQCgUAgiIkwEgQCgUAgEMREGAkCgUAgEAhiIowEgUAgEAgEMRFGgkAgEAgEgpgII0EgEAgEAkFMhJEgEAgEAoEgJsJIEAgEAoFAEBNhJAgEAoFAIIiJMBIEAoFAIBDERBgJAoFAIBAIYiKMBIFAIBAIBDExpLsBgsSjHzGPgMvU5/v27S2oW3eksEUCgSAVKNnZeI+chi5Lvd4TuhcMBWEkjDZkhZ3nWyiY2tTnLp6nCsgTnYVAMPooK6L1G26splCvt4TuBUNBGAmjAENJMbu+PhHNpANgLHX3u3/z0hCdlcsAsNVKFN7zIeh60tspEAgSx6G6B1AtOi5DS8z9e+q+i7K3ghhfW5PUdgpGNgn3SbjllluQJCnqr7i4uPt9Xde55ZZbKC0txWq1smLFCjZu3JjoZowZlPw8QuOLMC5oxbWgEdeCRnIcvn6PKSpp6963bXYYQ0U5clZWilosGI0I3aeWWLp3LWgk/7AmFFmLeUxP3Xf9tU41YygrRTIkcLwoSRhKijGUl2EoL0O2WBJ3bkHKSYrj4syZM6mtre3+W79+ffd7v/71r/n973/Pn/70J1avXk1xcTHHH388nZ2dyWjK6EaS2Pm9qey/LowtxvRiPOSWtbHrtzm0nT4zwY0TjDWE7lNEAnTfhXZCK1t/U4g8flyCGgdKfj6bflHOrjtz2XVnLqFlhyXs3ILUk5TlBoPBEDWK6ELXde666y5uuukmzjnnHAD+8Y9/UFRUxKOPPspVV12VjOaMalQLuKyBIR9vVDScdh8tMxwYzllC1htbUNvah90ufflcvKW9RxAGn4bltS/QA0NvsyAzEbpPHYfqXpJ05ufvxyCprG6sQNd7Oy7GwmoKocga9ccW4xrnQnlz7bDapS+fS/NkG/acDhyWSPvqF2XhzFsCgH2vFz5Z398pBBlGUmYStm/fTmlpKRMmTOCiiy5i165dAOzevZu6ujpOOOGE7n3NZjPHHHMMH3zwQZ/nCwQCdHR0RP0JEotrYSMdl3dAUUFCzrfveDvhrzb3+qu+JIzssCfkGoLMQug+vVyZ/w7fLngLRRqcf5HJoGI8q5HdZ5hAVobVhn3H25Euauw2EADsKxsO6n+VWNYcaSR8JmHJkiU8+OCDTJ06lfr6en7+85+zfPlyNm7cSF1dHQBFRUVRxxQVFbFnz54+z3nHHXdw6623JrqpIxp15QL2nGzGUNm/k+JgMBlUtnwrH8UfMRTGvZp4p6Ysh49tN05FCkdGOpMfbUP7fHNCryFIPUL3qaEv3eu6xG9qTsIgaahxziIciqncw65fLgYd5BBM/tMuwnX1cR0rHT6bnec5YJy33/0Cs73s+tUyofsRRMKNhJNPPrn7/7Nnz2bZsmVMmjSJf/zjHyxduhQASYq+iXVd77WtJz/+8Y+59tpru193dHQwblzi1tBGEpLBgFJeStMUM3mzGhN6bkXWKJxyMHSyraqAkp2VkRf+AOHaugHPIdvtyIX5qJbYoxmzMYx5xsFrhFzZDG/sIsgEhO6TS3+6txpDZJkCeMMmgurQ1eS0+2BWxOnZGzQSnFyCSZL6172sYBhXSssUO3mzB+6P8nPckOOmc0oezpZywtX7RWRVhpP0EEi73c7s2bPZvn07Z511FgB1dXWUlJR079PQ0NBrlNETs9mM2WxOdlNHBEpJMVtvzyXL3pr0a+kntbL3hMjSQHBTKeNvGthICCybTuPVPrIMzcluniCDEbpPLP3pfkXhdq7PWwfAjrDGt7ZcQniIswld2Ewh6q+DwMYJ/epecTnZdHMR2bntWAdx/s4vdbD/hBKm/6AdTTivZjRJT8scCATYvHkzJSUlTJgwgeLiYl599dXu94PBIG+//TbLly9PdlNGPKHjFlJ76jiy7H7MxnCv9yuyWzmjfH3U3+ScvpMqDYTZGMZuDmI3BwmWhGj/0lKUGVNi7itbLHjOXUL9YjN2cxCjEjsM61Dqlljwnr0Eydh3hkjByEPoPnEMpHujpGKTTdhkE6WKyullEe2fVr6BLPPQHYRtptCAugdQzGrM5E39YTWFMOf4aTpvFvqyuUNuoyD5JHwm4brrruP000+noqKChoYGfv7zn9PR0cEVV1yBJElcc8013H777UyZMoUpU6Zw++23Y7PZuOSSSxLdlNGFrLDvOBN5c/qe0puXXc2N+Vujtt1rcLOzPS9ub+e+KCpug0ugPZiPY/P2Xu9Ldht15wTJdw1uVOA4poH9M7OZ/qYVtS04rDYK0ofQfYI4dPlFkgfUfU/yFTs/zIvoU9U1NnWW4A6ahqz/Lt23hfLJ2rIjoUsDOQ4fnO+j9o1CSj9M2GkFCSbhRkJ1dTUXX3wxTU1NFBQUsHTpUj766CMqKyNr2zfccAM+n49vfetbtLa2smTJEl555RWyRDKfPlFXLGD3mSZM5YN3UjzTsZV50/byy32nUOfJzO/Y6fSy5dYZFK4G58Mfpbs5giEgdJ8YGq9aSvu06AfxUHQPoEgyPyl7ns/zy7h790pUbegTx7UnhWiat5TJd+1ErW8Y8nkEI4+EGwmPP/54v+9LksQtt9zCLbfckuhLjwqU7GwoKYza1jzZTMG0vkcSiqxRYPWQb+w9ii8xOChUNCY5GgnrMs0+27BmFXy5Es5pkwGQPL6I49EwMRvDFE5pwrOvEOewzyZIB0L3w6NL9x2TdQqmxbdE2J/uu5hhsqFRM+z2FRW143aaCU0tw5iT3b1ddVqRFXVY5w47QDnQp3TT1iGMkQxB1G7IMLxHTaP169EjB0WOnYu9izyrl39MehqHZCaWm4kiyfyyeDUbc8NcvfXiIYdIAUinNrP/JCMAgS8KqPzp8I0EgWCs06X7vuouxGIg3ScahyVA03Uqmm7s3iZLIXJj+EkM6ryLmtg/zxi1TX5tMoV/EkZCJiCMhAxBttloO2sOrdNkcgbhBHRE0S5m26uxSSYUqe+OwigplBr8nD9uLZ+2j2dHW/6Q2mlUtG6nxI6yEC1fjRSMUc1gtgwv2Y1nvNp9vv7I3eSBj74Y1rUEgkwg2boHKFC0Yeu+i1iOk8NFkTWspmhH54YpGoZD+gKh+/QgjIRMQFaQXTk0n+ElN7v/ZCRdSJKOLOmc71rNUosCcWQbKFTsXOOq4i4YdmcBkYIxnBf5vwL0FZ/Q1VZNl/pd6iic2AwTB75uwwuFFH1qQA8nvsMSCFLGCNV9PPQsMjWQ7mNROKUJDgmo6NI9gK7poA1vmUMQH8JIyAAav7GY1nkqufa2uI85vGAvV+W/wwRDfB1FOpmVW8d3il7nzrrj2dzSd1x8vLiXe2mfupDpf25G3bojAS0UCFLPaNW93RTk5onPkiP7ARKr+xkLAbDUKFT86hMxUEgBwkjIAHxFEkUV8a9FAuQZPcw0DSZ9SfrIMXqZY7JgVxIT4piX4yaU5aXzsDyydB11286EnFcgSAVKjhO9spTOiYxa3Sto5Mphyg2OhOqenIi/Vr2cCwMsswgSg/iWBSMSo6Lh/WorW76bLxIxCUYUviVTqP+Zhmvm0BOdZTKeoIkbtp3H3c1HprspggQgjIQ0okybTOP/LMNfPrya8COFM1xrOXvc5xjk+LIxDoRR0TDk+an/xiKRtU0wcpAkTAYVeZDVGkcSqiajHfBDSLTuAYw5AaH7FCGMhDTim+DCdHojRWWDr8MQ0hVCutr9NxI4wRbiKznrMBsSt46Y6/RgPrWBprk2JLO5d8Y6gWAUkQrdK7KGQdaQEmTEdOnebgxGOTQOh0N1L5nNYkYxSQifhBHK+40TubCjGIBiayd3lr6LWTIOcNToxb3CQ+vcucz4XSPqjt3pbo5AkBRSofuzyj/nRMdGbth1Lm3+xPg/5MlW7pn8OC+4Z/LInsMTck44oPsFcwCw7jMy7hcfi6iHBCOMhBGKL2TEF4rkJ/SGTXwaUKg0uCk3ONLcsv4xIjHN2UC1N4cGT+LampvtJWT30zG3EHvuwfNKmo60aReaN74QM4Egk0mF7gsMnUw2yJiGmUmxJ4okM8nooNSY2Oq1udleOBA+Wi+50JfMQlIPzlYo7gDqpm0JveZYQxgJo4A2v5Xrtp7PWeVfdBd3yVRcio17yj7iCbeT3+84PqHnNioawa+2EOgRk+0PGRj3wxIQERCCUcZI0n0qKCprpfWm6BX01q25TLpOSmhhqrGGMBJShDJ1EvtPjo4V9hXr5Cbo/Lousaa9gl8Blzo/S8jIYlHBPmbZq/lXzUI8waGv923vLOR2wzTOc65lqtEOwELzfr4+4T3+2zC338JTU10NHOvaAkB9yMmz+2f3W6hGlnTosZZqMoSpOr8Io7vvOO2S99rR12wc7McSCPpEtlhovHQ+IUdvH5lE617VE+9aZpYMXFHyAV/4xvHy/hkJO+88c01SdN9FL5+H4gB131sGfdgISlCn+PEtqK2JneEYTQgjIYlIRhOSKbJe6J2Ui/XE6Fzk/a32GWQNWdIJqvEnTNnb4WJfZw7HOTZSnoBfdo5jH+c4tvOcYc6wjIQ6TxbPemazyLaLqcZIJMcko4NJzjo+6ZzQb2cx3V7Pl7Mj39u20G7+u3/2oK5tVDSMR/SfA76tKR/XFnvUNj0QEIlaBENCMpqQXTm0rvBTkNe7+FKidR/SFbxaEJucOMc9RZI519FBsWE9r0jT486Y2NX+kCZ3t8ssGbpTR6dK913kuzrhhL4LYHX6zPBGHpLXix4IDOkaox1hJCSRxq8spGVp5KGoWEIMJiHqN8a/w3RzLT/ccR6+UHocEv+5fxHPGubSmiDnpUyl5XQv9auiR0sTHgfjK5+mqUWCkUzjVxbSujyIK6fvh1NfDEX3bzVM4bO2cfyi8um0J1r6SuUHLLTs4Qc7z2ddczkXuc/gh+Ne4AhLWpvVJ3ZLkC03ZWPfsJDSX3+Q7uZkJMJISCJBp0RRcduQji02tDPeEExoLLVXC/JFUGF/wBXX/p6gaVgzCIeyKVBGnrKNeSbDgEVpTIrKFGcjE82R0cTGoI/1gfK+Zg2HRU/npy5aphdS2DkXee0WMcIQxIXichGaWUnnRCgsbB/SOYaie1/IiD9swK/Hnn0YrO53Bwr4yF/LfLPGYFM/FxvbKDeEkSUdT9iAP+xgtW8iFila9zPstfhUIzvb86OWEVKpe4gsTxbmd9BQYUQ7aj6m7TWE6+qTdLWRiTASxhC7wyrXbbuIcBxre8ng8T2LeMkyk39O/TdOqf8RT47Fx93lb2CTTai6xq9qT2Rba2GKWgqWkxrYc4SNyde4CNfWpey6gpFLaGYl7T/2kCu5B945hQxW9y/vn8Hbhik8OP1hwDbs68fS/TWuKqqzNvClzi9FGQnp0n3hxGbafyRh/ut4bE8JI6EnIplSElCmTqLmhuW4pw0vZ7lTNvGdyjc4pmRwRYz+1nQ0f24bh6r3TlySaT6+l+Z/yGXjP47KxnZa+QauKnsLsyRsWEHmI5nNNH5zGXtOsyZk5i8TdK8NsmpjF082LeJXjUcOa4lUSVNCNFnSqTlKov67y1FynGlpQyYieuEEIVssSBYzEMmkaD2mEfswOwyzZOQsuxtV38XbTI7rGF2XWNtYTp0vi685d6FkuB14tAWmGDbzvHk2vnCkYzkhaz2LzUbSbcNKko6e7UBut4k8C4KYyBYLcl4ubcsCFBZ0JOScmaL7Tk3Bo5kHdczGluK491UAmzEYldnRZkhMMaihUjC9ibZyK7xcgBwMCd0jjISE0XjZfFqPjpRGVYxBckdxXvZEU6jYeGDyv1APxDLnK5nhKJllC7DtZgfWz+ZR+hvh1CToTeNl82lb4ceVnVlLDMMlpMlcu/N8NKS4IxsGy6G6BzBKEmbJ3s9RyUfoPhphJAwVWUFfOouwLfIVdkwkZrjTUPncX4Fd3s7SwRnyIxJFkilUYncM1WE3m4IuOoKpNxwUWSM/x01bVmYYLYIMIIbu812jT/e6LtEeSG5IQqbrvmGcmdBxC7Fu3D+m/ZKEkTBEZJORLV8xdRdnSlRylC7+s3c+r1um88+p/07wmUcWr3kncu+uo9PdDIEAELpPFZmg+8LJzXivBe3P4zA/P3aNhMxesBYAsMRSw41TXqA8qy3dTRmThCb62feT5UgLZ6a7KQJBxmMxhPnOpDc5sWxz2tqgyBpfnfABN055gRunvMDsvNq0tWWkI2YShoBstyPnukBJjd9BucFBucHL82YP1Z05KblmIpAkHZsxFOXxnW3yozDwGqeqa3RofjrV4U832k29487dQVPca60FuZ2wrJPOrfk49+ShNreIXPBjkFTr3iIHyTIH8IaMcaUkzhSMisoptn0UKB18YJ4IpF73ErDKto1Jxkh6+l3B/VS5cwel+y5CdgVb/tjVvTAShkDjRXPoON5Drj0x3syjFZsxxF+mPEZuj/5NQcIhDxx7Xat6+eau83EHh7c4a5A17pj0Hyb28Jr26zrf2HkBLb7BxYC3XdhJzaqJzLgJ1KbmYbVLMPJIte6Ps3ayZOrj/HD/iYOKGsgUVlr9LJj6OJB63R/KN5zbOC9rg9D9EBBGwhBQrVIkS18G4w2beM6Tx2xzLVONdt73a6zzT01pG2RJp0CWcCmDT8gS0qEjYBkwh73VGGJB7j52e/KocceObc6TA+QrBwtehXSVY/K30xDsnTu+MejoM3lLljUA+dC2agrZO4pFUagxgpKXi3fpJDomkxLdB1QDL3pKOcxcyxyTHYMcf8nmdOk+x+JjprOWzR3FhLSIZo2SQn4fjol9kUjd98QmmzBKitD9EBBGwiilzW/lNztO4IKKtXzPtYM/165iV3teupuVcAqsbn5ZvJo/t03icfeiuI4xSkqfpXXf8sn8qPWcPo/NsgbQrgiw98NCxq0ZUpMFIwytooTOb7aTq/ROUpQMfCEjd+1cxbEl25hT9MWgjk2X7qdmN/C7krX8gAV80VaW9OsJ3aeOkbPQlQEYJo5nz63LaVuQmoQf7qCZH9eu5JHO0fdwH+kEpvmo+tky5HmHpbspAkHa2dpexHdrDmdjewm+kJEf1a7iwY7BlLSDu1srub3uREIJ8L9QdYnba0/irtbxwz5XT8ai7sVMwiBQnXbsC5rISdGIIqzJrG0sJ8fghayRtxam6RJ1KsiSD6c8unINFOR2Qm4nnZ/n4awrIlzfMCadmgQCgPaAhU8aKrtff95URrbBD9lNcZ9jXee4hPpe7PW4yDL6gaqEnXMs6l7MJAiShjdk5FvbL+b3zfFNB45E2i/tZNNtFSjO7HQ3RSAQHECRdH4z8d/cXpScjIljSfdiJmEEsNeXyxNuJ41+x8A7H8BqDLE4fw+zLPuS2LL+0XUJX8iIV01cuekuJElnfv5+Ztr3I8cRWjUQIV3ldZ+Njz2TBnWc3Rwk7FJoOXUGBn9kROH8aB/h/TXDbpMgzUgSoVULaJ1qxiDFPyJOFCNV90PlqJxt2A0B1jSN6zPkczC6z5JD2OT4v7vB0FP3zu0e+GR9Uq6TCQgjYTCkad5lR1s+v287flDHuMxebitcjVkyoo7C2TBZ0vlmwVvMMVlIxA8T0EP8cd+qQYdHATjtPrjUR7jrXC3FKMJIGPFIBiO7z1MoqmhIy/XHmu6/nN3AMdZdXNF8GX3FcyRa98OhS/fV7xZS/klam5JUhJEQB5LBQM33FuMer5Ivt6StHZdWrqbU2Mofdq8aMERoNLO0qIrzXZ8wMUPv3p0XGDCcuAyAgnU6WY9/lOYWCUYyQvcRMlX3gVk+dv0yondTu0TFH9aNquqRGfZ1ZyiSTOeUMEUVQzcQTIpKttmPO2jGHx7a1z7XuoeJBveANeudZj8FFjfyCHY5MUpQaOukNWDDE4wsVyiyhsviY7atmiMsMpDcAjRDpajy4H3S5inENaESrbYeze9PY6sE6SCVuh8NdOner0bKxrf5reiQ0brPd3XCgSJfTa1ZSBPGodQ3jZrESyP3KTLCmOGq44nJz3B4/p6kXkeSdG6a8Bx/LHsHozRyRx3lBgcPTniB80sPBiUX2Tp5aNJ/uDx7fxpbNjgshzdT9VsH6vxp6W6KIA2kSvejhS7dPzH5GR6f/BTjs1tGlO5zc9zs+7nC/ktHj97FTMIASPNn0jorGyXbN6zzKJKOTTYNezSQJSucWrqeTe5SNrcUxdzHLgUxS4l3Fhwq+3wuHuzI51hbFeWGiCPRzpCb933jOd5WRYkhtnORTTaxwFpFU3kkQ1qhsYNs2YIiJd62NUoKJxduZLO3hLWN5Qk7r8mgYjKo1B7hxFW4GNvza9HD4YEPFKSVvnSvyBrLC3fjNES2r2sv77eeSqJ0PxCSpHN4wV5m2fdjIDMGB8PRfRcn5G7Ar5uSpvtEI0s6dnOQhokatkuWkvv23hHvxCyMhAFoXJyN8cxGYqUFkQ4R/mALhwwFp2zl+tydPGJs69NIyDSq2nO5p30FxVOfodwQAmBNoIx7dq2gYtqTlBj6zjux2GxksXlLjy3J6SjMkpHvuPbwlmVfQo2ELuzHNrDvsBymv2oSRsIIoC/dS8A3897tLhx0O3rKiq5Jko4k6X32M1fkvc9Cs4lMmSAeju67uLw7z0JmfKZ4KZzShDpZIrivCLmmdkTnUhhZ33wGYZA1rpv0Kn+Y/jh/mP44hxfsTXeTRgStqpdv7V/Kw7URR58/7l/FD2oXENBDaW6ZQJC5FClW7pz8BKeXbUh3U4bEWNS9LOns+JKBfTctQzIntmBVKhEzCcNgnrmme0Txmim+ynAlpjaK7Z00+uxxl3+1GkO4zF7sUhDoexnBbgriMnuxSPEXhEklNWEXW2U/m1qLu524atxOvGETW/NUSpXgoAvCCASJRDKbkSvLCToljAk871B03xOjpDDPrLAx2Dtfg9B95lI0rpV6Yw6SojBS5xKEkZBivpOzi0uyN3LZtgtpD8Tnpbs4f8+B2Of+/QxWFm7j+rz1mKXMTIH8f1VHIhFJN92T9oCFb225hJNKN/G/+VtiHywQpABpygT23KpgMSU2edJQdB8vQveCZCKWG/pAKSqk9cvLaJ8an/233L6ds8d9jtUYPX1mkDVOLNvMCTmRaUJFkjFLci9/hn7bgoZZih7XzDbv59yKz8i1HozHldF77ZdJqJrcq6OAiC9HWJPZ1FnCvW1l1IbdaWhd8jHZgzR8aS76EfPS3RRBX8hgNoYxxlmfJZm6L89q44KKtYxTDupB6H7kMdJ1L2YS+kAvyiV8Tgv5hvim8FZYNRabN/Jhy8SohCdmQ5hvuj7u05N3qMwxWZiTu5NtnuLukYkixe7YwqhoCUhdnGx2teexu2Mps6fto2SQd6aCjiJrg57KVXUtZV7TriwvnOWl3lpI8fspuaRgMMgKyDL0me+vN8nU/VRHA9fm7gIOnkPoPhqh++QjjIQEYpNN/Hr8k/j1g52Fgk6+krxpwBuLX6azKPIzFshhenYoAE97HDzesJgatzNpbcgELszexPJp27l17xk0eOLrmL8I+vnl/pP5ctF7nGAbvQ5UgjiQFap/uATvxBAFStugDhW6Tx9C98lHGAkxUKZMxD0hG0lqG/SxU40DO+AoSFQ6WjDIOTR5+95fkTVK7B2UmVv73GeCsX9h1IWcVLXnDtimTKIqVECOXN1ru0VSux1FD6VQseOSVSzKQdE7zX4KrG7MfQymOjUTuztyacl3AH1/x4km6AR5znTYuQ/N40nZdQV9I8kSvjKVorKh3QeJ1H08JEP3noAJd50Do8tPbnbq0wr3pfsuChSNwkMcHIXuk48wEg5BMhjY8u1CXBNb4l6XHCwO2cI9Ze/zks/Gz7ad1ud+WaYA945/BpdsZay4j+i6xD27j4n5XpmjnUcmvBJ3JsmzSz7j6859KFJyKsENFcfiJmoWKJT8YiKsHr3V4wTRxKv7dOHdk820H35GzdUL4bjUGgn96b6L88et5RpX1YDnErpPLMJI6Mni2dQvykIu8PZrIMzOq2VhVhW58tAf3IokE09QjFGSR0SmsUTS1/pia8DGXa1TOcq2jaWW3oaCjMR5hZ/SmBup8b7UugtFyrz4ZEXWMBt19p7gImfSUrL//alIsJROunSf2/+DMZW6Tws66MEgfbg4JJ2B/Ao0Pfb7QvfJRRgJPWiZ5cB8WgMD3V6HZ+/mG84aYPBlhQdLSNdS6mSTyXQGzPxn73yUCp2lll293lckmQsc7UD7gS39/5IKOoqkIx/SKxpkDVWXkppBU5Z0so9ooHa8i5yX7KgdbtAyM859tJOJuk8LEshmM3pmZHWOG6H75CKePBlMZ9DMN3afxT1tE9LdlFHJHJPK32c8xIm2uu5ti81+/j7jIRbm9702mkhyCjvZ/OupdFx0eEquJxD0ha2ygy1/mE1g8egMRexC6H5wCCNhEFgMYSbnNFFsaEvI+XIVN5NzmrCbgjHfVzWZGreT2uDo9lBOFzbZxFSjHads7bVtlr2aic5mFDm5c69mY5ii8lbaJ8pIi2Yh20dn5rlMRLbZkBbNwlfQ/8gx1bpPF3ZzkKJxrZGQvQN0eC00bM+npX303Jf96d5uCKSkDV269+dk/iM481uYQZTa27m/8nXOsCfGqWex2cg/Kt9gnis11qsgfr6Zs58/VzyPw5iajtyxrJH6n4RhSmVKricAqbKM/Tdp2I5u7He/saz74O4spnx/NebPRo+RIBgcwieBSHbF2vMn0z5Fo3CAfeVDkpNUh9080j4ftYdTzURzAxdlxRdao0gyZ7rWMt7SzL+qF0QlZEkEy207MU5Q+VftQtr8mZm2NVORJWlQGfKGdy0dWdbQ44zcECQGWdJjlnGenVfLEc7tABQYOnpF1IwV3Us6kTXzDPW1TDRnONdSYW7mX9ULu+tMJJP2mSr6d5ZT9tQewtX7k369oSCMBIBcJ9pxrRSa+k+soSHh04PIPTqGXWEHT1fPiXJ2mZlbxxn2NzBLhrgcDo+2wGHGTTxrmJPwzmKOycJMYw3vtU/BHTTHTI8q6BuTomJSIo5Fqi4NqTjPYNAsBpQDSw56MIQeyqwp6dGOJOkYZY2FWVV8Obuhe7tXi/4dxpru5TB4g0YsxnBMoyoVhHSl+3dQJCkpqaiPsMjMMm3jZfNMND1iXCVT94UTmglVyIQ+zUdpakbz+5NyneEgjIRBUN2ZwyU7z4raFlSVXt6w29sLuChwBleVvcWptvT/6Iokc3vpi6zPd3HrjtOT/qAbLTgkM/dM+DddXgm/qDuez5vKknY9o6Kx82oZLTgDgJJXDGQ9/lHSrifoTYm9g99UPkW+ogCRh8TmoJcf7zk7KsXxWNN92cPb0d/IZ/sNDvJdnQlsZfy8VHcY77dMAmC8vZk7Sz5OStRXunRv2D2fCbeuzriwSGEkDIKwJseV+jOoKjR4HHzmHU+Bsp65Jga0ei2SwsycWvZ6XElJpVpicBCilVm5tWi6jKZL7OzIT8mUWqLZH8jhfX9EwllykDmmxFbV60KRZMp75N53KMl3airIPdgBt00uxHHEPJQvdqJ1pqdjHmsYZI0KgzVqeSGEHFeJ59Go+3CWinbUfDRdJ2w3IMvpS2PsCxnxhQ5+n+/6DUw2dkRpNBGkS/f1IZnwUXMw724iXLU36deMl5H3hBhBPFc9i9eN03h02iOUGPrvLByyhbtKPuVpj4Nfbj8pKe2pMDj4f+UfAhDSVS7dfQJ7O1xJuVYyeb9+Ih80RMJCK7NbeWj8q3FnYRxJZC9voPFwAyU3lsLGrelujiBORpPuiypbaP/RwRmT3DQtNRxKg8fBj7edzZcqP+HqnH3pbk5CKCpqp+M6Ce2pMvL+KoyEMUNQVfht49EscOzh0qzmAfdfYK7jmkmvR22baGoAMrcUbDromupt9Dm4tXEex2VtZIU1TanikkiqHCcFEZr99u776ShLmHvbK9nsKUUbZIKd0aT7dPkgDISuS31mYRypyJJO4wKVwHXLqXisivD+mnQ3SRgJyUbVZN6vn4hPM8XVWVQYHFT08pBOf0eRqXiCJl6rmUZRZQcrrHvS3ZykoNpNyHb7iCoKM1Lpup9yKzzMNW3gzeZpVHfmDPo8XbrvCFs50/46ZsnY72yX0P3QCOkKbs2PVTKNmqy0ReNbCJYrhN/JQ25rT7vuR8e3KhCMUkwGlV3XKOz68RyQR9+SSqby39rZXLrjPGo92cM6z+bWIi7Yfg6v+0ZpKuc089/a2Vyy8yyqwqmvWplMMkn3wkhIES1BG6/7FBrUzBgNykjMy65mqqthVExpV/nzeMsn49bS71WeaPJy3ISco28pJZ1Ii2bROj+vz4yavpCRFp9t2JFAYU2mxWfDo2VGwaHRpntfyEizz0ZwlC07QObofvR9sxlKVXsuN209m3d9JeluChDx4L0xfyvXFr+asWuOg+H9+on87/az2BdOv6gEGY6ssP1LDvQvNWEyZHZxnUQz2nQvSD5j2ydBkmj5ylLap4DTMLC/wGhkglHjh5NeQkMmqCv8bd9RtAeSE1KYbESXJxDEx2jSvSC5jHEjQaZljk7B1KakX0qRNWzGEBY5szLoOWVrd076gB7icWNgWJ2FQdawGiOx1Jou4Q0Zk1p6NZU4DAHspuCo+kxjDdliQXJmoyupMSlHi+4thjBGJb5Zl3Tovl0z49b8OOTEGzrp1L1u0DEU5qO1tactG+PYNhJSyPScBn5V9jJZsonR7LU8J6+G24ojoVwtGvzP9ovxBE1pblViuD5vDZfmfDKqPtNYo+P0uTSe4yfH3paS640W3V8y7hPOcWyPa99U617VZG7adTbTnA3cXfpBwqMc0ql7V1k72+8sJv+ZCWnLvjpmjQRl8gQ80wvQslKTAtOshMlXRm4ltSxzgDk5AxcgmW2v7v6cFsnPkQU7cYfjc9ra78+hqj13WO182zuFFm0fR1iSkK5VtpCre4a9lltodzPFEakL4FbNfNFcOuAIRXeE8Z+6EPuWJtTtu4Z1/bGMapbIdSbfeViRNebm1bDQUTUqdD/HvC/uz5EO3XuCJtqCySlglwzdd7HHm9tviK3JoJLr9BC2pC86ZswaCS1Li+CSJorS3ZARwnhHC78rWTuoYxyyhVsLNsa9/yOdedzdvnKwTetG1WQeqFrGROdUlo5/PWPjppe6dnNjfiSD4raQh6+2XEZ4ACOhqKidwP+A74kiXMJIyHhMisqNRa8lPGVwqjmo+/jD8FKt+5FCT913cW9bGQ90LktTi+JjzBoJqcKkqFw1/h1mmmqAxIdBLfv8XOr29W2FnzBvQ3dK1oEwoPCD8pdZ65vAo3sXoesSBlnjG+PfYaa5BkjuVNsx1j0UTH0GgLqwk/uqjs64qpVO2cJN458nqEc6zQfqj2RXe17UPhdUrGWWpe9UseONbcDIHV2ORAzFRez++iR85aFRMTAQuk8tY1n3Y85IkAwGlIJ8QnYp6SuEVmMIl9nL8ba9FCZhylHVNeq355O7oW9BvZ0zmXUFbwFgk8NMNfbdDkWSOcICWfJWXrTMRNMlLEqYY21VKRkRlRsclBsiTo/V4Sr+bV1EZ9AcVdRlILLMAXJNyUusYpSUA+mfI6GW79ibaAlETwUut21nqaW/kVfkN2hVvTSqyZkiFUSjO2woC9ooMiffgbBL9yYp8U5uTaqHPWEjjZsKyN3a9/nHmu6TTaJ1HzoQi9WpZX5EyZgzEpRxZWz5WS52R1vSjYQvlX/MhVlVOOT0WY/Wd7K4/OPvA+AZp7Hp/LsHrEw302jisSn/6X7tlFM/ZVpucPDQpKd4uGMqD1UtiesYRdb4xcQnmWXUUaTUOBjdmL+GUP4nUdscUnwzRjfVHcum1uKMGzUJhkcydX/NvlP54t+Hke3rf318LOk+HQxX91vbCwEIjIAqvJnfwgSjKzI2ewCbKfklT+1yICkhOQCveI38vf4oTK0K/WUIkIM6XdFXik9C1XUYYICjSDJOKf0jXKdsZZ5lD/tL4qtYZ5RVxikBbCns3Gzy0IwRVddwh00jslS3oH+SqXtv2IjRPbAD3VjR/Q5PAfs6c1iYX80se3XK/JCGq/vBRkl0TATz2UvIenMLalv7kK49VEQPNUK5t2YFex6ejH2UpxA6wiJzhOWLQRwxsh3FBAJB/Lq/q3U8/3Yv4OqCN5lhGr31MXIWNdIxx0jWxnwQRsLIpyK7lW+VvMk0Ywf9PbTWBILc13hMn+/nGH3cXPDJkK3WQ7HWS8x6+juctORz7ilLT8ytAF73KTzRvIx97vhmSHrSeFSI9snLmHzvXsLVA4ekCqDxf5bRPlUn19iS1OukQvc3jXueZ6+ez+PPHY0jzqKnmaT79/0aDzYdwXcL32CmafizFudkbWDB1CoqDZn7KGtQPfy8fgUBLdLGoeg+nWTuN5sEDMVFBIudSFIgqddxGv0HnFz67ihqw242BsbzeVNZ3+cx+9nt+pBSxYtLibaSCyxuthRKmNsiU4vxYPDquL6Qeat4Mh/lvc8ck5owA0TQP02qB78e+Z0+983p93fvj6KSNnx5RnRb5js8ZQSSRMdEKJgWnVXVafZ3ZxD0hEwJcZJLhe4Xmk3My1/PQ1lHEm/pnUzSfV04h8+bymjOT8yyRoXBQYUBkh2BMVSaVA/bQ1bWNZcTVEdmFdexYyRIEju+MxHTYe3YU+CP0B/tmo//2X0uTb7+HZs6gmau3nYxK4u2cXPBpqj3/lj2Di3feJUj/3MdOZsH50VtfjObr3/0HX5x1QPdqVkFyeWWumPZ0Bop7iUcFdOLJOncNOE5Fpkii/b3tU/l8T2Lkn7dROh+OAjdp55b6o7li5bSEWsgwBCqQL7zzjucfvrplJaWIkkSTz/9dNT7uq5zyy23UFpaitVqZcWKFWzcGJ1YIxAI8J3vfIf8/HzsdjtnnHEG1dXVw/og8aCawN4jBEqSdBYX7uGIol0JKZuqyBpHFe9kRc6WAff1q4YBHxa6LuEPG7qnqXpilowUKrYh5aCXQzpGj85PNp7JNbWLUHVROTHZ+FQj/rABf3jg3z3TGMma7wu7FMQmm7DJJizS8AYNydL9VncRD3bkUxt2D6t9XWSq7reFPDzYkU+DmvxMmKnGpxpHtIEAQzASPB4Pc+fO5U9/+lPM93/961/z+9//nj/96U+sXr2a4uJijj/+eDo7O7v3ueaaa3jqqad4/PHHee+993C73Zx22mmoaurLtl6R9z5X5b+LkggjQdL5Tv67XJrVf0VJTdfTXiBIUsHwQg7Pv3Y4AT01qakFI5PRpvlEkyzd72rP455dK9geTpwzbibo/tAB2Wp/BX/ZfQy7QmIJbUAUGZKQf6M/Br3ccPLJJ3PyySfHfE/Xde666y5uuukmzjnnHAD+8Y9/UFRUxKOPPspVV11Fe3s7f/vb33jooYc47rjjAHj44YcZN24cr732GieeeOIwPk7mc197Ke+0TsUTin8NbXVLJV8JOPl+8avMMSVWSLYaidlPfJcTj1qXdqcmQWYiND901gUC/KE+8plDmjIo3SeTdOn+GGst46Y/zjRjGIj4JZxo28uU6Y9zmFFlMOmfxxpmY5gt384la/syiv/wIeipiWxL6Lzn7t27qaur44QTTujeZjabOeaYY/jggw8AWLNmDaFQKGqf0tJSZs2a1b3PoQQCATo6OqL+EsW+cC5VoZyEna8vvFqQnSE3X7jHsaMtf1BTzp0BM5tbimg7JDuXIsnI+QF8RdKAMdB9YfDp5GyReGffJF7xGvFqmVXSdrRQYmkn3+ZJyLKWLOkExuVgGFeegJYNj2RpHpKre4CQrrI75KYpnJXQ8/akTbOyuaWIzS1Fg9Z9X4xk3ecrdhabjThla69tycotkU4SrfvCCc24K1O7TJRQI6Gurg6AoqLo7OhFRUXd79XV1WEymXC5XH3ucyh33HEHTqez+2/cuHEJaa+uS/x25wnctuP0pK8Tfxo08ZUtl7GmKbEd+xdH38ftlz+IahreFJTp1Wyu+8vXec8/8nKLjwRuLljH7yf8G6M8fIGbjWFavuth23fGpXzq8VCSpXlInu672Bv28dVtl/JizWEJPW8qELofGSRS9+kiKU9G6ZCOS9f1XtsOpb99fvzjH9Pe3t79t29f30U0BktYk5NqIIR0lcc7XTzfPo+gqiTcF8Emm8iSfUMeUXQhqZFQqe9/fgFXVWd2VbKRiFFSMCYw8ZXZGEYzZk4irURrHpKr+6faF/JY+yKCqoI6DP3PzK3j/PK1OOXoafIu3b/YMWe4TY2J0P3IwCgpFMgS55R/xozc+oScU88L0vLlpchzZyTkfAOR0KdjcXExQK/RQUNDQ/dIo7i4mGAwSGtra5/7HIrZbCY7Ozvqb1BIEpLBCAmY8un7EjpKDGsxoId4sGYZb9dOHvY1NF1OqkeypILl5Wxef3cuXi2YMd7Pgj6QOHBfp282IVmah+TpXtclXquZxnPVs4ZlIAAclbON77j2RE2fQ+p0ryugD7MX70v3qq6JPiABuBQb17iqOCZn68A7x0FhfgfK+Y20znIm5HwDkVAjYcKECRQXF/Pqq692bwsGg7z99tssX74cgIULF2I0GqP2qa2tZcOGDd37JJrgCQvZdud8zBM6B955iJxetoE/TXmcEiV5uc9/X30C369dQkBPbp4H+16ZuY98j+/ViJFFJmOt6GTbnfMJnJz8GP++yFTNQ2p0nwr60v0Ss4cbvvlP/Mcl5vP11H1AD/H92iXc0TzylmIEiWXQ0Q1ut5sdO3Z0v969ezfr1q0jNzeXiooKrrnmGm6//XamTJnClClTuP3227HZbFxyySUAOJ1OrrzySn7wgx+Ql5dHbm4u1113HbNnz+72fE40gRwDhRObBt5xEEiSTom9A8OB2YO5tj0JSTPaHw0eB5ouEdLVqIpuWbIfd6WGuUnG0jz82RKDTyd7J+xanDfwzoK42Bt2sz3kQkvgcpPDEsAxMYDflU989eeGxkjUPCRe906zH6fZF7WtwNDbmbI27Gafah72LEUXfeneIVu4NKuZh3JbaWT4zpddun9rymQey9rN5vZifI7MiMYYqewNu+nUIktRNcHEpmP25cvkzpyGtqMKPZC8LMKDNhI+/fRTVq5c2f362muvBeCKK67ggQce4IYbbsDn8/Gtb32L1tZWlixZwiuvvEJW1sGb+M4778RgMHDBBRfg8/lYtWoVDzzwAIoycsJfFEnnFxXPMNVoOfA6fQlyFpuNbLzobk7ZfC5tTwwt3a8gufyi7ng2tJQk7MGRSoTmI5xd8hlfd0b7RcTS/X2ti3mldvqI/K0BTK9kc9db5xE4qpPxjuTWuxjtdOkeSOgAAcB0fBO7jzQz8YZiwrvjLOQxBAZtJKxYsQK9n/hMSZK45ZZbuOWWW/rcx2KxcPfdd3P33XcP9vIZwey8WhZmVVGs9G8cvOWT+dg7G194+HnhB8IsGTHKiU1Ms2XDOJb6zuPEss24DJFsaMfbNyd9xmQ08UXQzxue6ezzuJL20GieKxG2LqPwP1tQD1n3TwRjXfM5Fh+nFm1gqXUXijTwnI2GNGINBABJA+lAnqVaXzZ3tY4Xuh8kqdC9ImsYDMlPRjZ2ajf0gyTpKJKODv3+oF37Lc3eyZXOOqD/0qRvumfw8v7UeKACGGQNzSghh3US4UjvWi8T2FzIY0fbcDr8KLJG7kQ3U431GKWRMwJMJ+sDZUmvC5A3qxHPFBPS69mQBCNhJCKZzegD3KLx6j7X7OVbObvjMhCSRWS5IeJIeOjAJNG670JVJerdWTzuXUTxpHZmmsS9FS+J1r0ia0iAqkspz9Y7cs3dBFJg83DP9Ec5ubT/Yipljnb+3/RHOMexK0UtGxx/mPAvfvy9R+gcn7hzyqqOZbWD9g0R/4RHa5Zw5d6VNI3CPOuC0YGhpJhtv55P46n9r9PGq/tMoD1g4crdZ/J/7b1zRaRC94L0cnLpJu6Z/ih51tQX5hpzMwkGWaMiq5W2oJUWn41SRztTshqZYTSyy1rNlpxi9na6YhblMMkq043mtPof9Mcko4MKQws3mnSGHUDdhQ7GTp2wNXK+Nr8Vb8jEp4FcphmbmWBMXF75gQjoITYHY4dklRrCFCqjNyGMxRCm3NEGREaV1e6cEV84JmmYjFjLO8myHjQShqP7TEDVZGrcTuqdTmB/1Hup0P2eYD7rApEwV6ccSqnuu2hQPdSEYz+yZpjkKKfO0UKX7udY9zLNqGCSo+ttyJKOZ0YhNqMBddvOpLRhzBkJuVYvf618mcc6K/hr1RH8aNwLLDYbAYWz7G5Otb3OZVXHU9Wem+6mZixBVeHW7aexML+aP5V9nLLrVocDfHfbZYRiTA1/qfITrs5JXLKdTKPU3s79la8jIxHQw1yy83TqPMlLJzzaELofHk9Xz+FpIomhUq37Lp7snMo/9izttd0oa/y/aY8wwzT6jIQu3RslhUCMpSSrKYTnW+3s31jIxB8KI2HQKC4XdRdNp2OSTn7P7ZLEUutuGA/jDUHg4M0lIyEPc2Fvb9jNY+3z2dhRMqzzDAWjpHDS0Z/x9sRJGF9xIiXIr8XohuYNBehlfvJdnei6RJUnl181TwHALIf4mnNL0vKvP9KZxxfeeX2uyX3YNgmvZuIK5xejakZBknROL9vAXNuebj8QBQlZip5RMSoqVReV4dpejP3fqe/ARwL96d4oKVxS+BEbssbx3/2zhrTum0zdW40hzitby0JLFbGKIKVK90C37k/L+oLJRgN/bZ+IW43o/oysz5lh6t9XKx7WBIK85p4ZtW19Z1nM3yWkyfy95QjyjZGS2isdmw4YgCOT2Xm1LM2OPPCLjW0D+n8pspawCaRYjGojQcp2EDiug3xr77XJmSYrM011QO8HilkJY5C1IaVrDughtoecPF09J23loP9U9jHP5qzn5te/jKQmxpPJ4NXJ3gEtDiMcCPdt8tp5yjsXALspyDlZGzBKkYQvBpRhL8t0JY9RdZ0Xm2ezoy2/z313tOWzuyOX4+2bcMqhjJl6NEphTMrQe2xF1jjPuZapxuj71KSoUfeoyaBiOqqBelcBE/89rCaPavrT/Rl2L1OMn/Fq/fRBp2tOtu7txiBXZG/v1whPpu5bbSZC2TJGRevW/eTJ9ZQb6vlv3Rza/JHIh1lT9zHD5B/WNQN6iNW+CTy1b25c++u6xLt1k7pfuyZ4WGyuGVYbhktP3Wu6FPUsMcgacj/ZfxdmVR1wjM8MRrWRMBQUSeb20hdZn+/i1h2nD7qj+Hb1CvZ5ctJmIKQLb8jIt3ad333zX1v+CiusQ0/pWh12c+3eM/GrkYd9i2/g0YmmS9xYdTZTsxv4XfEnGeE7coqtnsOnPzTk4xWJXlk8zZKR3457lk8Dxfxqx4lj7l5LJpONZv4x7RHub13Mi/vjyzY4FnSfvU0hUJ1PYHE7DkvyEvd06b41MPzZiHTSU/f7VAc3bjuHsCYjSTo3THqJ2aaGPo/NVRS6ymhnAsJIiEGJwUGIVmbl1rLfm0OTN77pa1XXqfdndVvV6aRQ6aTtsDDWWgPW+sTFRRk6FRqbs8jJ8WBUDhoBui5FfU9rfeMxSjt6HW+RwswzGWI+wBtUD9tDke+uLlxKjds5qNkcXZdo8dloMGXOWr1DtuBIgq1SbnBQo4qQtERjlBQqDA7m2vayNzfinzDB1n/WxrGge8WvI6kSAe2gEbQjUIRdDhDSDk6Hb/WXkiNHdF+seJnUh4Pj5qCXFq33rMhQdJ+J9NS9XfYwJ6+GgGpAlnSmmxqH5PgpI3NYdh0uUyTr536vMyX3nDAS+qDC4OD/lX/IXa3jkx7nngyWWhR2n3UfR68/G89/ihN23uydoO2z4lsRwmjre1rxib0L+Jc0v9d2p9nPP6f+G6fU++Z+1VvBn3at6H49WkdlgsznLLubs+zv9dgyMh5aydJ9LJ7aN5enpejllZ66P7JoF78p/izmsXc1rOKL5tKY74023Rcqdu4p+6jHlqH5TBklhZ8Xru9+/fOm6TxXPWuYrRsYYSQMwPH2zRRPagcgV3GndQr75LJNzLdVDWq9/dsT3uDRS5ey7ZVJWBsTM7KQwjrq5izq82wUje87bWsssXtDJm5vXIZF7l2karc3LyEdxNaWAmbs+zJXzvyQH+ZtH/b5euLVgvy5bQYTTY2c6+idtz+VTDQE+d7EN3i5dRbr6stQnnNRuSN5U8GCkUNSdK/21n0svR667fFOFzUhF1e7tkb1XYk2Bjq8FkI7srhj72ncmefj34ffl7AskV2673LQvMi5OiEOmsNBRicQMiRd92PKSDApKjZDMGqbVwuiEZk2N0vGXp6kEUen+Kd1LUoIk6ImJd56iX0nR1s6MUrxF125wNHOmZOfY8bn38boVjD4ht9hSBo49oE3bIDxgzs2qCoJKZ/bH+0ddpzvWHjaOYfLnZ9RqNiGZdypuoZPj9w3jWqYl+pmMttVw4m2yEhTRsYmJ7cQTs82QGQ6M1+xc1FWK7sCDXyqjaP0tZqk5nAfTQyk+8GSTN0PhUzS/XsdU9neUcAF2V9QoJA0p+JgwIhzN6AraKYs9s3PYaZpaA/PnvcHHNS9JxjR+XzbHioNLWnRfU/Cmpx03Y8ZI0GSdK6b+DJLLY2YpYPTPTfWL2dbRyEAZxWv4xvOoXvF2mQTf6p8lg/8RUlxKPvdnhN41NbOveNeG9TNaZaMPH/GnTzStoTn7zsqkr51DOB9pYhjP76Bv1x277CcKN8PyPxyzzlAxDnSGzKytmkcF3REtuWYfNxX+WLSwj8BtoX8XF91LpouYZJV/jjhP1QYUp/QZrQwknQ/VDJJ9x1BM1/bcSGrCrdwfW5y4vkTRbvm43/2nEJb8OAsRJfuu7h7z7E8Ye0cE7ofM0YCQJ7ijoqhV3WNlqCt23O+ayppOBQqdgqU5ExDdwbMmBV7lIUbLzNMNk7O/pxH5i3Fst+IrXb0GQqaLtHc4kBvjRhQBo+OHJLw60Zg8COKkK7ySUDiY++UXtEVQVXp3hZQDbzhy2WKsSlpU5AhZFr9VlRNRpE13vKOp8wYmeGqDiS2BO1oZ6TpfjikU/f1gSxe9ym0BG3oukSb30praGTkMGkJ2Pp1CvQETdSTNSZ0P6aMhLHOERaZ3af9H4s/Ox/1mb5zDoxUdF3Ctt6KqS0xHaFXD/KzqvMG9CD2BE3cvv0Uji3Zxu1FXyTk2v2hajL39HDwFAj6I12639ZayE2tZ6fseqlmrOh+ZLjsJgBdl/h7w9H8pmUSIV3lHT/8qH4htV5nupuWcm6c+iIzvrwZf/7wpkVNbdC6toDGlvSHHNbX5tDxWR6GHvVPWpaEWHz5Z8w1NaevYSnCZAiz42ultF6xLN1NEWQoidL9cPiivYzr6+ZT1Zm4wlGaLtG4JR/TFivoQveJZkzNJGxsKaben8XF2Z/zuW9mVJauRGKUVBymIL6QMaHxvhZDGIcxgDxM2+4su5sTrK8wzzEDS/8h4P2iBHSs9RLtRb1vo0DIgBpjbVYikm88UWi6hD9kwNBsxLEPetbKHV/RyP8r/xAY/BqeVwvSoqqDWl8OaAZaVS8O2dzLEc6t+Qnp0ctE2bIlYdEyRkXDNb+RZmMBYvGhbwJ6iE4tSFhTUGQNiyGMTU6MZ3iX7mOR6L5gKHTpflbRdAw+GYMn9UuOTV4773oT1+8GwwrBsAFbrYypPfJ5hO4Ty5gyEgBa/Va+sv1iQkn0Qp5vknls6uP8qvHIhBoi10x4jWMsDdjkzMhG5s+XkBe0k2vs/dD3bHZhre0tNNUCvmWtCTMU2jqtWD9y4AgCw6y50ZO7WmbzZuNU3MH4HURXN1VyUVspt054hqWHLHPfXL+cDW0H48IlSefXE//DHFPynJ4EvXnKXcj91UfgDpkosHr404T/kCubgOF7qXfpPhY/bziGj+rHD/saw8Umm3j1rN9yf+syXrj3SKSh+/NmBG1VOWTvUJCDidG+0H1vxpyRoGoynQFzUq9hlBTyFTuHO3YRKooYIy1BG9taC+M+x+ScJrKNfj5vLsVp9jMju45pxgZcSmIMBEWSsM9spdWeg2uTNKTnqxwCT7MNT4z3bO1SzLArWZVoq8uiQ+n9nsEeIi/HHff1Gxqzoc0UuU6CB0XtqnXQ90lQVSJ/MQrwtIVstAcOdgySpPO2Zxpt2h6OMGsDjixy5DBLC6rY6S4Q1R+HQCBs4EWvizWe8d2/gyxpFCjmhIXkdek+FuZDSvwOhqmuBqbaG4YdptnFJKODyZb6YRcFampzoHoi351iC3cXgOqi02fG2zJwfzUU3evByHdhblVQ/IkT/3B0/653Kn59N0dZwt16TpTuVWQ0XeKLljL8fZTLThZjzkhIJRc42rnA8SkAb/lkftR6TtzHXl74PvPMbVzcfjHzcqoPOMYkbgbBLBn57PDHeWBaIX/ccu6QqsaZW3XMq/u6hWILVw7q5K6N3dl1TjRAnJ2FqsnYNlswt4zMKA1dl3hkz+G86ZjKkkkvoAywhFRhcPC7krX8pmUS//H0zmQp6J/2gIVfbj8p3c0YEl8reoejLdCzamUmoOy24jwQnu8pM8Lh0UaCt8lG7pqBHzHD033m6P8/e+fztm0qiyY/hUOKPVMwVN1DZKnsYu8p1LhT60cnjIQMximbuHHC8xQrbpJV8ONY2y7Wf+1jnly9qM+Hd6qwNEi0rY5/tsXaR7/iLZZYdfoazs79NEEtGxz31a3gTXsDP8r/fMBRaovfzg9qjuRk1+ec2k+a6y7OzPqCaVNquWffyoyoFTCSGex3PxQ2Bn38X/NR3TkZBuLscZ8z17o3atthRg9DTeXbF8PRvbkV2lYXYmmDroe0uYVe2rX7Dr7fHz11r1p0cmY2R8ofH6DTZya8KRvpQN2IvnQ/kkjFvZcoRreRoGp4W63oukR2Gn+IJtVDszr4GvNmycgqq0oyK4J1WaqrJ1TSUlWCqV1PWC36wWJ06xgT0AGEHTp3ln6QsOnZwbKjLZ+2oJVQ3toBjQR/2MCnjeOYaG3kVNuuAc891Win0tDOA4YgbRlUKW4kMtjvfrA0qR62BEsG5Ysw31rFCbZof512TaZBjV7Uc8mWYd3fw9G94tex74/eZvDpGPbH3n8geuo+5JDonGDGYDhoJPjcZnJqGLD8tWaQCGWDy+ztd79MINn3XiIZ1UZCeH8NM6730nTGdLgwPUaCW/PzrT1n0ODN7HXkZw57hD1TFS5+4PujMtGSQJBKEqn72+qPYF1redS2Oyb9h8Xm4RvBmaZ7o0dHeSf6O3PqAxsIAJ4Knecu/i1FikwmlVoe6YxqIwFdR21txeDX+81RaDWGWJy/h1mWfUlphjtkHpazyd6wm9X+2BXTALJkH6usgWGF1bgUGzY5hOPwJpr3uHB9PvJSaOgytM5TGTehEXm4XllppF3z8aavgGnGhrQXkRnNjBTd7/bkddcMgIjz25vuw2hTq0af7nWGHKmgyzDBMLwZlnTxlk+mWY2EbB5mqsso3Y9uIyFOXGYvtxWuTlrhkeGy2l/Kb3ac0Of7hXY3R056FtsgCj/FwiwZWb3gCe6eUMn9608ZceFRuizx22MfP1CdcXidnTKE1NeJojoMv9l5AheNW8MMkyjYlCxGqu51XeKpfXN53z5J6D7BpEP3qq5xX91KdrVHEkxdWrk6o3QvjIQk8khnHm+0zqAjEH9M7Izcer5S+C6zjV7aNYnb6o9gt6f/7GRtfivf27+S410buMDRPtxmc2bWRlr+x84DHx6ZdmfGdPHl3A85Imsbv686IWoUFy9tfivf37+KFTlbuDRr4MxvbzRNY6u3CABP2IzWTzIXs2TkpnHPs8Y/nvurlmVMQaFMQatvpPTOPOqWOrEf25Dy6w9F9z1p13xp1T1AR9jCy/9aOiKih3QFSi+s4ltFnw17FmG4uh8sXbqv9WYPuG+X7j16pF1PNC/h08ZxyW6iMBKSyTZ/MRtbigd1TL7JfSDcyUaD6mFda/mAN2tQVVjfXMIEaxMkoLOoMDi4uWATr46fTsu+Yiwt6XNmjJeQI+K0ZE9Q9rwCWSJkaEWWhtZJhjWZ9c0llFtaIQ4joclrp8kb7cHerlqpDrspUqy9Or+FZhMqewCRhvlQNL8f+b11ZJUt7XNc6DT7KbC4h529NBZD0b1JUck2+7HLAQK6llbdA7SqXp4qWQy6jLk1cw2FLt1/u/wNTrINX/tTjXaKlFbulofW4YU1mX1hjVKDD6c8sF/EUHTfxXvmNiD5RsLIW3gWpIyXZj3Kv77+O/x5mT9SNaxo5u0rf8PxVl9CzndXy+Fcve3iQWVeSzQv1RzGFdsuYXc4s0OkRhqSpHPThOf4Y9k7GbN+vSB/H09Mfoalyc3zFjcuxcYn5/yOUy/6YNhJl5JJonU/XNoDFv5n2yX8uWXekM+Rabof0zMJkqRzeMFeysxt/LMzEqKoSDqn2PbFndmwQfXwkqeSpdY9TDUmLpb5HT+s908fVPro7d5CHuls4kTb3j4zvw0Gh2xhgiGI68g6Wjoj59O2OA7USMgszMZwVBnw/lB1jf96s+k4UCL4CGsVFQYrz3ud3du2uwtTntnsUMKajB428EznHIoMB0eKiqRzmr2aMS7fYWGXgpgPrOVvDPpYFyhPq+7r/Nn82x1xTu7UrGnVfRf5ip0Tstfz7HGzx4zuAbyamYA6NG3puoQ/bCCkD9347NK9qks0qR5e9lag6lK37uOZoUgkY76XuSLvfbLkEF/dfBnhAzW7Z09/FFecv3FV2MRfdh+DNuFdphoTt/45lPWmzS1FbGktZMr0x8lP0ADJJpt4f86T3a+nqpdDdQ9RZu5MZJ/49CB/rzmSBk/Em1iZ9Cb59mruqz6aFl9ivYo1JFR96M5Qqibzn73RGRYNssbSGQ9yqHw1fWjptcc673on89DeJWnVfVV7Lve0rxjSscnQfRerrCqbj3hI6H6QDFf3XVSHDdyz+xhUTe7WvVM+eI1UMOaNhKES0lVubpjPNndhQh3H1rWW85Wgg73uodX00nWJ3+4/CbvhYDU6GZ3/LXmJCcbBV0U7lPsOf5h1sysAeLJ6Pp7/DG7tNd080pnHyy0zozqFf9YdzovG2UN2NOuPj5om8DVfLlWduQk7p6pL3FR9RmTEceDe8wWN5Pwti/ydzWS4+0jGUB12c2vtidT6BnYay3SE7vtnpOle0yVu3X8ami71cmLeFvJwR+1JcTk7JgJhJAwRDY0tnUUJz6PdGTCzOVA0rHPs7Yg2MBRZY1N+PkapiXLD8DqMFVaNFdaq7tf3lp0SfS0/WJpTN8zQTBL+fKi0d8S1/55Afq9CWw0eBw1DKCsbD+0BS1SBl0Sg6xJV7dGdj6ZLODbUE96dOaFTmY5Xl9jcWtQ9gxgv1WE3NeG+H5L5BjeFdjfNPhtqCstDjyXdx0tAD1ETDrDOM39E6T6WxgGqwk46NStbWiODU02XaGrMxtaSvFkFYSSMAVRN5o4dpzA+u4X7K19PmLPW1Tk7+cqVv4vadt3+41j7tzkJOX88eEp13vjSbyhQzGRaARzB6COgh7h275nUuJ19ziB+M2cXF2dv4vId5yd8GnswjGbdx8uGoM73t12GOgrChMOazM3bzwSImkGc/qsOtF17k7YCJIyEQ9B0iafaF7DLtpez7G7WBIKs8Y/vtV9IV+gMjZya4GFNJqgldsHSKCk4pWgnmtNzP+OTUyqitnXUZJG7Lkle5BLkyqYBE+I0qB6edk9hk3vwNTQynabN+Ti3Segddeluyqiiy9FNRuMMu5dPAiFW+ybSGrAR7meGwCgpWDIkamK06j6YI8GyNi4at67f/VQkQj2W5UY6se47yR9EDyQm9DsWY8JIkHQIqTKKrPeKe1eRoqxMXZd4cf9hbMwu4eQJL/GudyqP71k04DVUXe7lqKLqmRdhqqEByevAzrB7OWPxY1Hbrqpexur18yIvdBKW0U2XI3/xUBU2jdrEQ/lrwfnIh8IXoQ+6pvwHs6QAEEbl4bqlmGSVk8e/xpvuw3hq39zoc8fQPYCaYZ596dS9pJFwR8eAS2fd4Q9gk9MXojxWGBNGgvPNnWTtLGTbVx0UTjiY2EbXJX657xRkSe81HVXnyeYrVSfQEohvuvA/dQt4vWVG1LZGf3LWu4ZK12c6t2DNgdTFqeFHRa/y9vd2ArDNV8zz/zgSg3d4vYYuw/TLtnBR4ceioxD0iaZLGB7Nxdym0voNN1ZTaOCDgCfcTp5uXNBdoOmKquNo9vcOtYule4h4nifDIW4opFv3P3vhHHK2jD7jfKwwJowEtbERmpqQL1ra672+HA+DqtKdSzseWny2tK4/xkPXZ6rJcQGp6ywmGB1MOBAmts26m3+OX46lQcbSNAxDQYIT8jZyhn3gsrC7Q26qQmX97qNqMk21TiSTSmFB6r6boaDpEo21TghHRsgV7WM42X4/mNtVave6mL6pA8nto1k/OEW+M1TQKzvnrlA+RiL36Rfe2VH676svELrvmy7d/25CB52Bvh28jR1S/E6PEniLJcIlARSpf8MjHt2PJA7VPYAUkCE0/Gyb/TEmjARB5jDVaGfT+Xdz2e6T2Png1KRfL6CHuH7vWdR6svtdavD4TRx2RyO+yfl4r0l6s4ZFIGRgxp2daNt2A6CH4xsdjzVML3/KtNeNaOEQyqTx3dt1XeIPu1YBB5ciVE3mNzsPFlPqr3aGYHCsXfwQocP7Xgy7YMeZ1D46Pq5z6TJ878Jn+Iqzql8/pHh1P5I4VPddhEPBPo5IDGPHSNB1yt5Raa0txLCiGZNBrOCmC7NkxDTE3OhDQdOl+ELRVBXrjiY6niqheaFKUUVL8hs3VEJh9CR3DiMeXe/zO4p1P6QyXHEsYZSUXpEVX993BPu9Tp6Y8iQmORzXefx5Er4SlYmmhn4NhPf9Gh97p9EaGDgENRAyoL2ZSzAbnEtTXwwMIved77387tmU1sN0CqY1xd45DbofO0YCYPnvJ4ybOJ5dy7KQpABGRUzTpgOvFky4x3Ui0I0GtD37yX+gBm/hIqgY+Ji0YTQgGSO+GLqqgiaM3nhQNZmwKmM0qEMu3gURwzMUVjAo2qAdIscyqq7h04O8vnoWllqF+olhglp8j6FArk7+pBYKFTfQd5GLj72TebJ6Xlxhj4GQgfHP1eKblIev92p0Qunr3tN0KH+5Hf2zjQDoVy2DaX2cxGhAMhjQw/EZVolgTBkJAOq+Gib8tJKa4wswnpQey3Essy3k4eT//ABLo4wlgzzA7ZYgm3+UB+ECACx5nWluUd+YjWE2/8AJwXkAVDwHluc+SW+jRgiej/KpfLqFLVe7KKoc+kxR414X0//cwZ6z88leJvqReLmxYQFPv7CMrAYJOQSnPHg9Bq8UV1/g2CvhaSlg25RC5pn7Xoe/3Lme4xybuLHq7F4VFg+lS/eSSaOw3z2Hz3DvvS7d27YvpvyXH4Kemv5zzBkJeiiIunk7ORNy2Ls9P7LNrFEwbuhlgQXx8bpP4aX2I3HslYcd3RAvMjKTHJGpu/6yYyqyRlFpW0raNFxkSaeouK37dcCZT2b40Wcukj+Ib2cRBTs1tI1bcexcRr2UO2Tdy34ZbeNWnHOWUF+UR1Z5R9yRE2OFnSE3r3uj/Y5e2Tsdxx7oiomMFI2K7/tXAjpyOFKAqT/yFTtZciiuZYxU6l43gGYz9qqqKUvQOdlBljILAH+uFDMtXJfu64O5SItmoexrINzQhHzYFKRgCHXbzqS0e8wZCV2YX/yUKS8diJ+eMZn9vzAIkSeZr73xVVxrDBi01BljRknhN8Wf8ZFf5ftbLxg1TkyCwRGu3s+kG+pA10DXKf3dhyiHTR227rMf+xjXc1ls+eMUrIXJ9TIfafy6/nhW/31e1DZ5DI/Dsg5vpGWRRJEcXdZakTWCV7TSFZxvlxr7PU9RRQvNN8sYHp1AzjNutnzfgaHJyMQfCiMhseg6oNF5wRLaJstYlT4cRUYRORYf55R8xmLLbvpb00s0T7id/GzjqVj3GpESZCBIGtz64ek8Mb6WZ6b+d8CUs3KiMjgJRhzu85cQssvkPrIGff5h1C/JouzZfeiaBgzNWVEqDFD33WVIOmgGsKQw/8BgSZfuNV1OWOK0LuLVvQGFy0s+5DNvJa/UTM+IwYEs6ZHMfjEYrF+LImt0fyRZ7zU7kUjGhJEgGQxIphgJdxSFuiN0CiePjTXFbJOfr2Tvwyglv6Nwa35CBzLRPd24EtOLThKadk2H3A9N7KirJDRFHdBIUNAxKyohTR51XuyaUUK2RWL1dVVNaorWEYckUb9YRs0LkfeEgbapdownNhFcl4+hM8BQ78l8VyecmLl+Kz1Jpe67aFW9eMJJSHIWp+4VSeYsu5txhs94u37KqNW95LAzQLqIYTMmjAT/ifPZc27sziA7N3NHACOZZau/SnhdDhCpEGdOs5PiLJPE/dMe5t6WI3m7dnJa25Jo2k7y0Hj0YQDYtpsov+ODNLcos1FkjV1Xga5ZyDeOjAf9SGJ3yM3xT1yPpUkSuk8ibSd5aDxmHK5cN50NiStFfyhjwkgIOhSKijN7OUGRNSY5m3CHzMMuP12R3UqWITKarPdn0eyzUZndyjRHPXKS5qUCeoj728fTqUVc6Lx7ssmpTX4Hofgkft8yh+McG1lq6Xs2wSwZmWA04lR8fe4zUsnN9kJ2JPNkc1NBmluTmShmldCS6XhKZOxAQe5B4yDX6qXEGhkseFQTezpcGTE9PVjSqftSYytn2L0877Xwctvh2GskDJ7k6V/ovkv3kf+Hs1T0I+Zh3FVHuDaxhd7GhJEwErAZQ/y+/EXe8xfxy+0nDfk8kqRzXdlLLDZH/GPvbSvjob1LuLHsBeaYLAx1DXYgasIB7n7oTCwtkY4hJ0UDCFudzlN/XsmLZx/G+3OeTM1FBSOOfFcnHddJ2KXeM4crC7ZyfW7E6Wtj0Mc3Oi8dcaWF061776QgZ5zyV7797qXkfmTCkOTwPKH7aIoqW2i7UcL4j/E4nhBGQtKYnVfLipwtANSEcnhy3zx0XUKRNc4tX0dIV3i6ek5SRhmBsIHfNB1BkbGD70x6M+q9dZ4K3q2bNOA5ZuTWc6JrAxMNQYgZRDN8vrV/Ka/v6p1OORxSyPKQ8GpvcaGLNLqCgekr1FGJcdPOzd/P0c5tvbaHdIXH9i+mPUbxpmJ7J+cXf8rLLTPZ1jq4qPvxzhZOz/+cpxrm0+y3c3H5arb7ikaM7q1VJqa9ezmW3eaUxe8L3UeTrBB+YST0YIa9lkuzIoEom4P7eMk4E1WTMSoqp2WtJ6TLvGKc0e0AE1CVmAaDImsYe3ir9rVfT4Kqwtu1kzmqeCfX5u6Kes8ihXiXgTuLKbYGLspqBfpPIDIUQrpKk+rj5a0zyHmnr6j8zI9vssghLIZw1G9ikDUMshbX7yQYHZiUg1nvzPLBEEgZHYshzMKsPd19QU8CeohnjYGYRkKBxc2lWc3sCdSztzN3UPfTOGsrl2Y1s87TjKZLXJK1i5cVd0brXldAlyOfz9wKvONAUlPbBwRCBmrDbvIV64DOy4cy0nWfqvYLI6EPJhvNPDz1UVQiVdjzlUgFua5tIR2+V3UeDZ7e5aAnOZu4vewFALy6xHd3XkCb39prv5HE39or+OOjZ+LohJFgDPTFlTkbOTPrC76z64Lu6n0XV6zmePuWUfE7CQZGknR+OOlF5poifko5sgEOpKPq0n1Wj22D5ercdZznXDuk++nGgvcJFeg45MQ/8IdCf7pvm6FhKfVEbdPXZ2NLgS9SF+qbeRy75gbuvOxvnGQbXFTPSNf9eePWcrpjQ1RflgyEkdAHRkmhxNDbACgxONgdcrM1lEdQjW25mmWV8gPHBvQQi3L3ssNTQFV7cjxQTYrKDFcdky2916LGm5qYn7efnDiLqBxKSFe5u3UK/9o3H2tDbPEHcyTCh2jM1AYGX+o6i7q6HG4uncnVuZ9QqPTdwTplK0aCGHrM9OQpbkoVZcRm3Cx1tFNgcbO5tTjdTRkxFCsd3RrtSV+67w9J0pnuamCmYz8Quccs0uB03xR08JZPZpZJ7/f+7Um6da+bdLJt/qhtDXl2JFXB0qQnPEdCLAweHTkg4dHMQN9GwmRLHXPzc9ncWtzdb4903ecoXkoUU1RflgyEkTAE/t05l//snR/XvmbJyM8L1/OWT+ZH7eckpT05Fh9/LHsnZmW0k2wBTrJ9DAyu4+uiXfPzf0+chK2ubyG5J4Z7VUxsXVNwIOVqasj90MSznx7DnO/t49wMTmyTDM4o/JwLHNVc5DuD2nQ3ZgxilDVuLX2Rih7GxWB1v7mliB+1nMNtU5/hBFt8GSDTrftYFE5uxjPOhPZmDoo/cx6+FzjaOc32Nhf5zog5+yvom1FtJCj5eez7yjQ8lWpcxTvea5lEU8jBt/I+iDnKGAiDrHF55UfMNFczVG/izR3F/FCax+WuD5lpSu8U2FXVy3hl42FkDyHbbHi8n1aXCedmBTmUOkemeDBLBr5d9jpePZJcZoG5DrNk5Zpxr/KJdxLPVc9KYiMFqUJfPpfqlXakMm+v9+5tWEmu6eBUuYI2oO5f8pp5vWM+rYHkTe2mk9d9Ct/44HJ0XUJXpT51H8yRcE8MY8kbWWGFo0X3hXY3V5R8wAJzDZCEhFWHMKqNBMluQ1vaTqE1vrWqGreTem8WJzm/wC5FOhabbOy3dnlPZEnnJPu2qBFFF0YpjN0UJBA2EO4n81eT187b3smsytrIzAM55buO9YcNvbKGWQxhHIbEZtgL6CGqwwHe2DGN3A9NDMUHoSCvE4/DhL4tBzKsJIYiyQdGa10Ni/xeJ9hCmKRtPEdmdRZWY6h7SjSkKt3TpYqsYTGEsUgZ9gVnCO5xVrKP6J1NVdcl1jeXRG1TZI2TnF+QI/txyLF9Edb7y+OKNuhJvLrv79hYuk8Gm/zl5Lxn6SH32LoPW+k1c9gfmlECCeRgemcWRoruTYqKUYku/d5T9zkm74HZUgdeLZj09oxqI2EoqJrMrbvOwHBgQe3isk/4cvbw0zYvNuv8c9pj/LzhGD6qHz+oY0+0tbN82mP8cP+JbGw5uO4sSTrXT3yJ5eYWzFLiRjcPd4zj9w+eg90NI9lJcTSgyBo/n/QU04yRUdufWw7nxf2R7IqVWa3cVfEcTtlEip3KRx1dup+Y1cy95e+iSIl5KCdD9yONttlhDNlBHB/Y0m4ojAROLd3AlTnrorb11H2qEUZCDDzBg1M46zwVvGRo50hLe58jjHgwSgr5ip2ljp2ENYV1zWVxjyzMkhGzYmSJcxdZxoOOQgoaU4zNuOJ0dBqIkK7yq+aZPLtvdiQpktBzWrAaQ8zNjTjBGSWViQYv+Upk1DPfVkVrUcQgnGRppFCxsznoZVOwGF84OTHyYwVP0EStL5vnvQ4USUNBzwjdG+SDo8p8m4fJWZEqgYXGTuQEJEnqqft4UPxQv9+F1eXr5bgIkWVXT4lOV2Spkh3EZBazXX0hSTqHueq7+/b51iryD+nTD9U9kDLdCyNhAD6qH8/qxgr+Ov1hZiRg+eeirFZOtr/FhZ3nDTrs5hvOGnDWHLI1caFSXj3Iw0+vxF6dsFMKhkCB1c1vij/uEfd9cPnqDLuXM+yfRu3/VMd8nq2Or4MX9E+T187t208BIrM4maD7nhzu2sPNBZt6bBlcboBYDFb35lYd82oDLQtsZFf0NhLMxjDmudHljjt9qSsuNdKQJZ3vFb92IDNmbNKpe2EkxIGmS/y27gRmOmp6JToaLXx93xG8tu4wspslEjWFYDKEaZkXhAMROvYdJswt8Z07bJXoOCwESv/7Sx4Drk0SnjIIjAswzthMsrLOJZNFBfs4zbUOgBzZO+jEMILEkym6/0bB2zTmZgFQaWgFEre0mAzdx8JiCtGy4GBf4NhmwtQWfb32aTqqc3Ahm+b9Jqz1iWpl6llUsI8zXGupNGTutO3oNhJUDW+rFV2XYk6LxYuuS2xsKcanGlFdOxLYwL5p02w0qTW9pp2SxSe1FeSuMZDIjsKoaBQVt3W/bmksQPHFlxksbIfc0naMSv8xwC0dNsJVDoK5KqUlrWRJIUaSkaDIGg5jkDmOfT2SwQgDIdF0fc8B1YA/HF+3F4/uLYYw2WZ/wiojxNL9QrOJgzkAEmMgeLUgO8Iab++elHDdx+LQvqC1rgA50KMvkEAv8lOUO7iqnA2+PEwdCm2qjVa1DpcyMiJPeuo+4kwZ/+xSSFdp1fx4teRHNsAoNxLC+2uYcb2XpjOmw4VDNxLSwT1VK3nC6uaBCc8Na000k7Af1oo6Iz4jwQADGggAOVk+/MeEcB3iDTxSKLB6uG/ikzhlEyPJuBlpuCw+/m/iv3jSPYOHqpYk7Lznl6/hS9nbcCTIcThVun/WU8Qv/nYxdi+kw/nINru1VxGtXMPgNewa30p4nMI/9i7jE9cE7il7P2FOp8lkOLrfGAxz/a6L8IVS01+MaiMBXUdtbcXg10lkTqrZlmoairP5tLmi+4ca72xhmqOerATdoP6wAXfIjJpkAe8Mufl57cl07s/GldQrRdYqE40s6dhMI8spqtTRzpQDzmclpjbyZOuQOrYm1cN7/iJ2ekV56IEIqAbe9o0D4JiSHaxtGUdnILJOXp7VxiRHE582V0Q5LR9KLN3b5QBOOXH5TFKle79uxNipI6XJtk5UX2BUNIyKRlBV6Axl9mAqUboPIuMOmlJWb2J0GwlJ4iRbgJXWT7jYnY8v5Ixsy9twIFRy4BGFlqoqaXHwlncyn/9jFq6RORAfkSzK2cuN+Vt7bBmaYbkrbOJXO04ckcVpUk1nwMxdO1dxRvl6bi9cy1f82XQGIinWlrt28T3XDi7zutgVzOvzHLF0PxgySfeqnvmj7dFGonSfaoSRkGIe6Cjk3bapuIMDe/u6g2au238cRzm3cnl2U0LbEdBDzP/wKwR3Z+FMQY71VKDpEnfUnsRMRy0/zNuesuvaTUGuHf8Kn3nH98raNjd/PxfnfRy1bZwhkghlqIR0ldubZrPVXSQMhEHyXvMk9vhz2eeOnjdTJJkbSl9iXd44/q/qyIQnLxqK7o0HQh+vLngzYdlXE6H7QK6EZ1oAu3NkLeEmghm59VxR8H7M9+6tW9FdpyPVuq/fm8vEJ1Qs2/aR6PlaYST0wGoMYTdGZ7DyhEwx135kZPLMXoJa5CvMkuMTzGZfadyJUcKazPrmEuyGAMfaqmLuU6CY484I2UV12M3WkBNtYzbO/UMf3egKqBYJDJlhZei6xLbWQsKagpq7dUhTeRYpRL7NQ0fA0mcBL4hkRcs2R37zHJOPYyxtKOzmI9uEqP1m2GtZYT30+xle7ngNjc/by6hxD340O9Zp8dn6rJg3z2wmS97Fk9YFdAbNadd9F1ucRTjl6NDndOpeNUNR0RBytWcw8eo+3+SOoecIr9iacIciRmCqdW/oUDC88UnCDQQQRkIUxxdt4bu5n0dt+0vbzJjFnIySwh/K3kQ74O0QEWxyvNJXN1bwpeYv9douSTq/mvxvFg8yBPmSzZfR/mIJtmEWYPHnS5gXtZAfh4PhSOFws8Tjk5/i1oZl/abgXZC/j58XvQdEHhw22cLxVh9HTn4qar9k3heCxDPeYOPxSc/y57YZadd9F3/YvSqqUmG6dT8aiVf3/XFz4RpCBZ8Ao0v3Y8JIyN7lYf8bhfjm+8h39R1is9uXzzOeMk6x7esOpTncuht3mYX3GnvfODY5NSEoqib3Of35Ysdc6mxVnGpzxz1y9ocNGLzD7yh0efAOSCZF5cjCndQGnGxuKRp2GxKNIsk4JAtHZW3FoUTCzqr9ru6RXVf7Fzt29fI+7zpWMHJRJBmbZOJw6268ZQccG00tUdpKle67iDWyTYfudRk85RDMCzPYOawu3ZgPlK7+uGV8nzM66aCn7o2SynsNk6K+95667wuzFH+dn8GyLhBgXWAcnlDqk1KNCSNBX72e0k8ldv56KfRjJKxvLmFTaxGzpz+K68D9scKqcYTlCy7tLM7IuuMv7j+MNfYKTpz8X5Q4HGFUXUvbOrYk6diMQX5Y8DGveYvY0np8zP0yYZ09kuEsktnuJa+ZDS2nAWA1hLg+/6O0x2NrMRzPNF0SqbSHgErv+22FVWOFdVOMvTODdOheVySU6R0UxZFzRurRV+q61K37rkiQbwWzaT0k82Sm6P4Y68esbR1HSDto8Kdb9696Dos5swXJ1/2YMBKGi1FSuLn8OQAUKb3lm4fDWz6Zr75wFZY6Besw7irNINE2L4Qxe3DVJy+p+JSjbNtwSGaOsdZSNu2JmPv9vuYE9nYkOyAzfpaYW7nzQFstUhiHnL4Us0+4nTzXPJcWf3Rn5Qsacd6fxZQdLYhAlcHxZuM0tnmKubH4ZSYYh7dunIkkSvfxYjWG+PGEF8g+4K/xp7pVNPmjk8JdU/g6LXnRs26ZonuHZOaXE57Erx98PKZb932RCt0LIyFOhutd7NWC1KtBPOH03GifBEI83baMnE0y0nBLBkpgzY1d3CUWFkOYHLOPw627mGeOfP58xU5+H0t2k+2NeMMmmn22jBhduBQbS7vbmt51xj3BfLa1FvbarukSWV/UE969Jw2tGtm0+Gy0Byx0FiW+OxxVuu+HLo23BawYZI3Dze3do+7XHHXslAtQeszYTDXae+UQyhTdK5Lc3U/12JqWtgxEKnQvjIQU8WnQxP9uv5BQCurCH0pAD3HxU98he6eMnIaawgvz9nJ78ccY4hTarYWfscP1EVdtvbRfT2OBINMZK7qf5arld6Xv8cO6ZWxsK4l674d5G9HyNMwD+OsI3WcmwkhIEZoup/XGl0MScmj4HYWvUCKQq+HoI4Vqod3NUYfkKDjMsn9QDj1GSaFYgXPKP+PzznEZ6eCYappUD8+4J7G+syzdTRkxZG/rpO7FQjoX+yjIG1xNgEQx2nVvkDVWlWxloW03ZsnI8c4NTLHWY5YOPloixcoG/g6E7g+i6hr/9WbTEM4GSKvuB23evvPOO5x++umUlpYiSRJPP/101Ptf/vKXkSQp6m/p0qVR+wQCAb7zne+Qn5+P3W7njDPOoLp69NcnVmQtyqEnEUiSjqGf3KqqruHXwwlzbPGVhimY3oS1RypkSdJRZA1F1pjoaOL63J1Rf6cOobiWS7FxjauKY3K2Dup7kyQ9Ix1Mh4Oqa+wJG/n7nuV9LjWoavJGqiNV8/pnGyn604dIdX2PYLvu22RObidD9wORCt0DmA1hrs79hDMiRSA41ebn6px9Q44A6an7vpAkvdf3Odp0r6HzaP0S/r57OX/fHVv3qWLQMwkej4e5c+fyla98hXPPPTfmPieddBL3339/92uTKfqGueaaa/jvf//L448/Tl5eHj/4wQ847bTTWLNmDYoyOqeZFpm9/GX6o/y5YSWfNyXGKpQknf+Z8DaHW/Zi7qPAzNf2HcN7b8/CXpO8UrDnj/uM4xwRT/AcOcxwk4b05DT7buZN28fPq0+lujOn330VWeP6Sa8w21SHIo0OBzRV17ixYQHbOwv6DIMNvFZA5ZttqDV1SWnDaNV8vs3DTyufxXLAyJ5sSLyhlQzdx0MqdJ8OJEnn+5NeQ0Hn97uOQ9XkUan7TGLQRsLJJ5/MySef3O8+ZrOZ4uLY2cXa29v529/+xkMPPcRxxx0HwMMPP8y4ceN47bXXOPHEEwfbpPjQdawNEvWuHApK2mNanVnmAHkWD3Yp8XmrHLKFOSaYZa+hOWCn1pOdkNSvk0wNzDD1HZazsz2f7J0w3I5CM0iEsgDTwcRJFkOYQlsnC6xVzDElJz9Al4Pj9Kz6AUcKBkljrqluVHmoa+js9uT1m13R2qChrUteuN6I1fwAmOQwc00MuFY+HJKl+3ybh/H25j7fT4XuAbIMAYxJnIcptLuxKAdnL2RJZ6F5PzJQmdVKWJdHpe7joak1C63JDKHkZr9Mik/CW2+9RWFhITk5ORxzzDH84he/oLAwMl2yZs0aQqEQJ5xwQvf+paWlzJo1iw8++CBmhxEIBAgEDobbdXR0DKldZXd+gjxlAtV3GGJWDjyhaBPXuLZhTKI1+q2c3VyYvYnLtl1Ie2DkJN4J5IJ1SRM2+WCnMy2nnj+VvXdgzTG53FrwOVrBwB1eMn87Qd8kWvOQON1nAonUvSTp3FDxIkeYNZQkJe/poj/dd2FMUJnsQ5EknRvHPc/CQwINujT+j/Gv9do2lsh72YLrsTWEQ8GBdx4GCTcSTj75ZM4//3wqKyvZvXs3P/nJTzj22GNZs2YNZrOZuro6TCYTLld0PGxRURF1dbGnS++44w5uvfXWYbdND4chFEbXYwtLQU/6A0+RZMySPOw1yvHOFo7I2UmlwUus6f1tIQ9f3XwZNVsLh1UCWjNI+Fd2MqmgiRMLokerE831KTEQIPK9jbaFqP+4s/HrRi5yNA6pzkSmkAzNQ4J0r+uUvqfSVleI4ZhmTD0c7zpDFv7cOo3ltu0stSRf907ZxMVln7DOU8FH9eOHdz60pN4zsXTv1w28UDsLRUp+P7nAspdLKj6l0uDr0wBIVd+TKL4I+nnbMy1q2+HWXUO+9yRVR0+ygQBJMBIuvPDC7v/PmjWLRYsWUVlZyfPPP88555zT53G6riNJsaetfvzjH3Pttdd2v+7o6GDcuHGJa/QBVCRCet9OgDJSxnTmMxx1fMe1h77W/7cEC/A8V4wrODxjRDfA/1v4EEePnEmPjKbr/tLQeKppAd6wkTPt/8WKadD3lqZLhFUZc5pLZyRD85A43Vuf+QTH5AnsO9IabSQEzDyxdwFqhcRC87buh07PPiCRDyKzZOTSrFoKDB1DNhIkSUeRdJR+lhG8WhB1uNkVY+jerfl5t3lKv47SiWKOycIcU9/920gjpKt85JvII3sOj9rurzBE3XvxkGrdJz0EsqSkhMrKSrZvj4TFFRcXEwwGaW1tjRpZNDQ0sHz58pjnMJvNmHslt0g8r9Qfxuq28X2+P8HezO2FazPGUBCMLGrDbq6vPo2gpqDpEg3eLDRd4su7T+OMgnWDLgfeuNfF9L90INVsz6gsi4nQPKRW92vaK/lF+bPIwI+rz0DTJWRJ547yZxO21u3W/Hx//yr2e4devXNJ4R6uynuHSoOBWGGF7/jhyn9/D3OzhDnBDotWycSd4/+NWYLR8vBOBV26b/T1/s563nuT4rzPUq37pBsJzc3N7Nu3j5KSSIKNhQsXYjQaefXVV7ngggsAqK2tZcOGDfz6179OdnOQ/EF8O4vwFgV6xU53Bsx0BvrulMKazLqcMJWGEPmKvc/93JqfHaG+LfkO3UpIy/ypMl+BhL9QI1sKAJmXknSk4ddhb6erV9x8dWcOGx3lrDPv73VMEBm/Gnt5TPbLaOu3gp5Z3uuZpvmB6AyYcQdNrAmUoaCzrzOn22t+baCUdq0RgHJDeJi6t7CrMw9PcOgFolwGb7+Oym2qDcdeCXkYM4h96V6R5LgfZIKD9KV7iL73OrWG7u2ZpPtBGwlut5sdO3Z0v969ezfr1q0jNzeX3NxcbrnlFs4991xKSkqoqqrixhtvJD8/n7PPPhsAp9PJlVdeyQ9+8APy8vLIzc3luuuuY/bs2d2ez8kkXL2fSTfU0falxXDR4BKs1LidfHvLxXxt/Ptc6ex7LfXzoIkbtp0XKbzRB5mQbnggxh27l+emP4NREgZCsnmnbhLv9FGiNt33ykjXfDzousSdO4/r/j9Eqq/+dufBImRC94Jk0PPeO3R7JjBoI+HTTz9l5cqV3a+71gyvuOIK/vKXv7B+/XoefPBB2traKCkpYeXKlfzzn/8kKyur+5g777wTg8HABRdcgM/nY9WqVTzwwAOpi5fWVCRtaFaYrku83TaVdtXGV5wb+qwKpulSWn5kVdc4f+eJfL6nHOcQU7H6CiVKVlTzlfL3R5xz0EglUzqEWIwKzTe3Yf93ES2zJHLnNsbcJdZv0HOb2kfuuZCucn/HODZ4yhIS3jgUhO5HNpms/0EbCStWrEDvZ5rj5ZdfHvAcFouFu+++m7vvvnuwl08YkgqeoBGzQUWRB+cBsq21kF3t+ZyZ9UV3SelMQUPni9WTyNk69JsukKvxyoynhe+FABgdmldbW3E+8hF8aSnMTey5Q7rKK02HUefJTuyJB4HQvSBZjNm7wfXSVsp/GKJpb066myIQCEYwNtnEXZVP892Jr6e7KQJBwhmzRoLa2oq6bSfZ2ww07Mrrdx0xkzDIGlNdDUwwNwy88yDRZeiYBNbx6SmGM9qxyxKzcmsptLuHdR5Vk2nYlYejaszKN+MoNzgoM7SmvEZDIkiF7kO6yppAkN2h4d37I5FE6T5djO1eRtcpvusDpt/TSiA0Mgpi5lh83DPulUGHy8WDbpC469z72bD0ETHlmAQKFTv3lH3EmYXrhnWeYFhh+t3NFP/hg4yLbBCMPFKh+1bNzw07z+P/WvoOeR2tJEr36WJkPBmTTYI72ilGH9+e+BYAfs3Io/sXDyvsKV5+3zKRezcciaVBZrA521sWqBw+aydzTU2IGOjh0675+HPLPEJ6xGnl8pxPuuPtj7LtxDIpxGO1i2ny9h1S1y/COBgSuWtbaAvnU3dSiMLCxOa876n7Lrb5i3l5/4yEXieRCN3Hh1cL8ue2GUwx13NWHDMCawJBnuuYl1DdB8MKvJBL5Y5gSvU/5o0E2WZDs5shgWkpChU7l2ZFiq+4NT8vmmd1ez2HNDlpHtCvNMwg6w07gzEQdBlUi8SMadU8MfF1REfRNwE91Csjp1WKnSmxU1N5pW4G/rABSdI50rGVAsUDwASDhUmORl42e4ZuJAiGhLppG1lbFOqXHg4Jrr7bU/ddvGNs5h3jZIKqMizdS5KOWVGxyL1rzni1IE1aELT4l0zToXuzEsYsJ754XrIJ6CGatCAv1c1kv8vFcdb3+tQ9RH6Pdf4KXqqZkVDdq5pM2RsNqNt2DuvzDJaxbSRIElU3zCM01UuuMTDw/kPAIVu4Z/wzBA9Yfr9uWMknDZVJudZQcFfAXy+6hykGN8JA6J/fNM/mg+aJUdtuqnyOIwZIWa3rEr+sOgWjHDEwTi1azzede5LVTEEGscQc4qFpjwxb906znz9OeoIiRQasUe99bc+JrHthBlltEO8AIdW6z5Ot/G3SE9hkhUPbn+n8pnk27zVNwhsysrZpHBd0nNOn7htUD9+qOpPWgG3U6H6MGwkywRyNgpzBOZQU2zupsLdgi9NwL+yRpW2RYzdhve+4yaaAnb0dwynJFCcStE3TsU9sP1BNThgIA9EastHii86L8bF3MgrbOdx8sK7HF0E/W4JlUc6wPTN5bvaU8pa5Bndo8Mlq6mtyMNcYkbx7h/gpBKnELBkpNxijdN+l8YrsVvLNnqhtfWGQNUoVBYfc+8nUErBhbYxz9jBNulckmRLDyOpjmlQPG4JZbPMU0uaPGDZBVaHFZ+tH96U0eLO6syvG0n2FtRWA7W0FGZ0foYuxbSQMkVMK1/MNZw1DscAvz27q1+nwWY+N2ztOGUbr4kNTJO447VEucLQz1v1Xh8MTexfwimUG/5z6b5xSpCP5c8NKPm8q6/OYTxvH8Wnj0AqUFbxnxPXAh4y8SduxTU/dv+Q1c0vH6VxR/D6n2vyA0H0m8mkgl1u2nR7zvaHqfk1TOX+a/hhZrhBfbb+MsDASMhxNZeJTAVqnFKCd0RJVIS5dLDA3cP3kV4BIHvZ/7F3WbZWeUb6eRbZdWKXhOUG2LAxz7NzNLLXsRywxDB9vyMRt9UewPGsH5zo6knKNhsZsSp81kr2hMaOKOQkGz1xTM9dPfiXKWbBL9/9sOLzXjILQfWYyVN3f13gMRkkbVKXOttWFFK4NQ92WoTR1WIxtIwGQ3/6MwoYp7D/NQCKdF4dKucFB+YEbrkmt5RnLPDyhSOdwrGPTgdrjsUcADmOAmiwJg1cnVjVXXYGwTWLy5Dr+VvEe8XYUbs2Peshap1MeWeuKySSoKrxfH/FVONb6HuEEF+/q8FpQ6szYn/oEVUv/PTri0TUUr0SH10L2gZF8fyiyhsUQxiIFE3L5EoPjwEPloP66dP+Jp5lmf7RTm9B9ejBKYeymIIGwgXAMp9Oh6F7XJT5vKkOSdGzGECFViVn4CSK6V9XIdXO3aFif+SQtT6gxbyRkMi7Zyt8n/get+3X/HnL3jn+G6m8bOP+Ja8iO4QDrKYOHLv0DEw1BID7v2oAe4pt7T4oqc2o1hLhv4pNRvhYCWN1UyUVtpd1GXSIIhhXK/mDEuHmbMBASha4z+XdbCU+voPEGA2Zj/4s3ZY52/lDxDE7ZBCQ3lPnGgg8J5L8ftU3oPj0cbQkye9pj3FZ3bL/Lg0PRfZHNzZ8m/IfHOubyn73ze73fpXvTjloAtI5OBlc8IHEIIyGDUSS539K0h5Kv2HHJGnmzG6nPy+n1fk6Bm/kmGaMU/zlVXacjZKE9cLCj8oWNvOSpZKa5hoXm5Od/GCkE+xkVDAdjixe1qXngHQVxoza3YGjJQ9MH7gINkkahYktJgrGhjNTTqfuQrvK+34hfj5Q1nmtqHnEOin1hlBQKFTvmGGGnPRmK7v2qgbWBQmxykMWFe1jfWoovFPkO6/e7sOwzYqraS7i276qjqUIYCaMMRZL5aN6/YV6fewz7GkFV4Z5dK1hQUM3Cso+GfT6BQDA80qV7rx7kl3vO6Pb+/8nU5zjVMPASzlinzW/l9u2ncEHFWu4s+ZjLAqvY1Z4HQN4nBvL+74OMcU4WRsIQCekqv2k+jIbQwXK4ZjnM9fnvD2r0n8k87XHwWtvM7g7gUHZ25HONvGjA8yywV3F5dhN/ay9mvbe8z/2WZ20/4HWdmVyR9wGHO3Zz755j8IeFdMYK9b4svl+7hDNda1lljSz57Ay5ubtpBTD6dD8QPXUf1hTcwYNhfo/UL+NVy+BqQIxl3b/TPIUqfx613vRVEB0I0dMBhFU627NQs2QcloGTKrlVC/Wqj49aJ9DgOTi1ZlJU9uR8goYnrsuaJTmjHYE2+cr6XYtrD1j4qH78gOfRCiVOsu/hk84J/YYImeUwx1rfxylbkJFo1XxR63BGJFyKrc/jk80ck4VSpZq/KWpKjAS334zXbaY4nJyICUF8eIImPmmoZLylmdmm9QDsDLn4uKESXZdGne7dmp8WLRzTWQ/61/2u9rzuEXG8jGXd13myqPNkDbxjGhFGAqDu3MP0a9uou2AanN444P5P75/L83Wz8B5YQ+oipMlct+N85DgrwS3Jq+L2oi+G1OaRxNqmcVzcdvGAAnu/fiJrW8bxi4lPUqwE+ObOCwioB48ps7fxt4o3MUqJX/fPRIxvOJn+2BbU1swdZY0lunQPkRS5XYlwRpvu72s7LGb/liyE7jMbYSQAaCpqcwtKnJmZ+3JU0XWp2/kkHtzq4DPuJYuNQR9bgkVR23Z6CxJy7rAmE46jwFVYk3EHTbzpPgyn4qUjYIkazbhNZjQ0ErG+OlQsksIxBdvZ5ilkW2uCk/8fghLQUZtbknqNsYzU1on64UTqx4cpqhz4ex4rut/oLk1JQbouBtJ9g5zFf9z5zDXvZ4YpPTMKqdR9piGMBAEAL7hnxQzFSTW6LvHUvrnpbkafOGQL/5u/hWete7m9NfkZ8gTJI1xbR9mv6mj56jLInHIqKWUk6L7Nb+Wunau4oGItM3J3pbhlEcay7kVezjSytb2Ib+1fyppAYpK0DIXdITff3r+Et5umpq0N8dLit/Pd/UfztGd0hFgJxiZC94KRhJhJSCO+sJG9HheduRZIU6qMTt3AZ81lSStfnUj8YQOfN5UxzVYPcdR0HwoBPUSjGr3uZJfklDpOBcMKbW12ijypqxkvSB3p1n2D6mF7yMWapvIRUWAoFcTSfRcFihmzlHz/jEzVvTAS0sic3Bp+WfL2gZzsmf+QHgt8HDDy0x3nR21bUlDFr4rWpawNrTVOZvzvTrT2zjgL/wpGEunUfUhXua76ZKo68oSB0INYuu/iRxNf5CRbnA5rwyBTdS+MhDQiS1rM0q+p4iWvmc99U6JKGo91NF3uFYWx25PHI515HGXdQ8WBbHLjjS2cXLaJj1vG9yofPWx0Ih1FKH3T0YLkkW7d+1VjUjKDjmRi6b4LlYP9Y6J1X783F8eOyHULmnTUljbIsPTrwkhIE5Kkxx0ylQxCusrf647st4b9WKRnh9DF3g4Xd3esJGvKC1QYvEAkdnpOwSa+HcxKmJHQbaxl0jBCkFDSrXtBbGLpvvs9/eBsT6J1n/OFgcJ7Phj2eZKJMBLSgMUQ5vqJLzHF2Ey8BVcEycWrBbm5YQm7PX0ngnmg9gjesLfy86L3Ej4SbP+okHGvRwyQPHcnWrj/fPGCkcdwdN+kevhp3aru/AHXFb+StnDA0US6dT8SEEZCD0ydGjW1OWTnebCaEttJmxSVXEvkIeAwBlhubsE1yDSuDaoHj9Z7FGKUIqVmBUNHQ2Nje0m/o4Mat5POkIVQYeKdzSxNIL2/7kBbBKOFweq+p8azZIl8xX7A0dDKhpYSgqqCJOlsyi0mS64Ruh8m8eq+PWhla65MpcEzZtJvdyGMhB5kPbWGGS/b2PzL6VjHJzaJzQxXHX8se6f7tVka/Cjg5/UrWNfcu/bB/2/v3uOjKu/8gX/OXDOTTIbcJ4EQwh25qaggq4g3rCu6ardqu92u+3O7+quwUnW3a9td7U2tVm1/WnXbtXil0FVQtyoCoghSFAG5CSEQCAnkfpnMTOZ2znl+fwwZGHNymfsM+bxfr7xeZOZk5plDPiff85znPI8jtwevVK9NyQhcIhq+aHP/y9ZLsb09NGnD5Y5D+EnJfvy8ZSF2tleGJxgSQsJTdVehIs/J3KdIb9CIe2puw4Kywxk9W2YysEg4g5BlqL29kNTYBvLpdSouKzsMu8Hb77mJ5pZBw+wXQbzudsCjDjwb23FPoeaAI5/Cg0SqBBU9Xuo5B+dbjmFBAnoeu1xW5PwlD6X7+//OUGoUHOxFy/ul6J3bi6JR0d1aO62wBVNzB17ON9rcH/UUhTNe4y7D70xdOO4p7LeOgqzqUp57f9CA4NZCnBqWA+esIMpGd6W0DekihARZSAiKkTfgk0VCgkiSgEmv4K6ireER8NFwqgG82DgfLn/mTNk6mDPnVUjlYCwpAe+jiFCHvl6K/vYzn2zAyvoLcMJRgAWOXdBJKiRJxHw7WcBpxvjnd0L1cXnddJG27kb5ZwYcmjQHiLJIuGxUDW7Pb435vQfL/TFnIf7gnB/zayeSKiR4/UZMWHEcckMjACD4wHwo5Troddlxgawv931iyX+feHMPhI6hksj8QawsEhLkutH7cUP+FyjXR7+629NdVfike0LKFlSJl8dvQvHzVpjbQ2e/dd/IR+GsoRfGilex1YP/rHoblXo/gNiuxb7cU4x1naFFes7Ja8IPi2viatP3Sz5EXYEdPztyPZePpqhkU+4Da0tQvbkbSuvpnI9beQL+vxSh4f/KKLD1prF1Qzsz933+sWwz5sZ4ThZv7lvb8jHxeQXGhuOQY2tCyvCoFieTXkGp1YVZluOYZYqu/9mt+tAgq9jjHoNjzsIktTDxFEUH68EWyPUNAADjNak52zHpZMw2AWYp9sFajYEiHO4uBhA6OzqQvwsVBgn6QW6BGky1MQ+5up6sOZuixOjLfaE++pk/syn3bp8ZntZcjKsNQv3iy4jn5KP1MPW4oMiZu/BFrxpAvSxjt2dsOPd92oryAfPwlvf+qnhzL/x66Hd9CTkLehBZJMRpXH4Hfj/2AxigR7Szp2312fDTw9dD4WRGaXG0pxD/5Po27hn/Af7a2pLu5lAWGSm59x3Jx5Qf74QIZOfEXgeCwD01386KfZ2pWCTESK9Tsaj8IGZYGqMeXewXQfzRNRpfeMb2G5BEydEku/G6azr2uSrCjwkhQRESVBHf/0GeZMStoz/HF+6x2NnW/+6Tr2o9UoSCvaH3LHcKCDnTOxwJAKYXNuNcW6j3rMLYNSJyLwlA+JM/JXGyKAhlPBlTUA83991uCywf2eAeJ1AwvR2eT0pQfkTNmtyzSNAgyRICsh4mw8DTY0oAvj3qM0wwRtf1rQgVTjWAP564CE5//MPj9ToVBimzpvHMRA2KGSuOX6B5sAgIPYIi9ssFVp0J/2w/iTf07kEPFqqQICs65B/So/h3p2dZy/yhSyPDULmfa6/DP9tPxvTaic59qggJkMxm4NTcDRFThev0kEymUCWRRH0Do3VSZl3SG07ug4oO/u4cVL/4BbpvnIXeyQZUrndBbN+bNblnkfAVQpYx9f81w31OKdx3dsOoT+wv5rPd1fiwYzLcQVPcryVJAt+r3oT5OfVxXacf6VacnIu3jecm/eDd0ZmHSb8JQn+iLuMHK4002ZT7VMqZ0IOa38wGAOi8Okz95VHIzaFLc+3/dBE6L5BRaHMmtQ1Fll48PG4NKvQKsmmGWlVIML9aiGk7W6F4vShYV4uC3cUQR49kTYEAsEjQJNcdgzUvB64kdFE1Bgpw0m1P2OuNM7ahOsrejHjp9Sp6p5bBXJwPAAjmZ9OvfH/dPgu6fdHflRItNaiH9GUN5N7MHgk+Ug2V+6bAKOwJ1GGKUR/1pYZE596R68KEvDbokryKZF6OH3ljQ5cbPH4TemdXwjw6NADQNR4oG5P8eRJMOvnUPk9sEV8fKMZuY0fE7dyJphoAmEK/K0p7B9DekbT3ShYWCRS1XHMA7qUy+sZ1j5La09oeolRYe3IaPmyZjBemvIoJxvTdtihJAv9a+R7mmQG9lLrJfc623L/ZOAtvYlbSlszWSQLKNztx4PJ8TP0XC9QsPTlgkTAAqakDxtcnon2WQPE5/cMgALzQOR+zrA24zTZ0NX006MbrrtmodZUkobXR00HCt8o+xW7bWKw9OS3qoKTqlr/JBa24ZNRhAECR3n1qNPnwKULFiz0V+LK3YuiNY+RWfVjunII97qEHLVJmGyz3QkgIqrqMyX0skwFlS+7jNZzcx1scDCf3ep0a7c0vGYdFwgCUtjaMerkNyh0Xo3eiETlGOWJWQUXVYWPTZDQWjMJtto8HeaWQejkfbxw/L2Ht0+tU6CUBfYxXt/SSDjfmujHeuBPrmqZm3C1CkiSglwRm2k58ZbBYdImToeCdtpkJ7er9ql6h4M2m2fAEsut6M/XXl3v5zouBczSeZ+4ziiJUyAgNNNVBB+OpnhXmPnFYJAyh9K1DwF+KcOA+O8rKu9PdnLB5JcewpPhjVBjMQJRn19mgIMeLx6vfQIVeAOCSuETA2Z/7aL3mKsXqljkAgAl5bXjM8Xlc0y1TfywShqC0d0ByuoDguZrP98ombPOFKlmjpGCGSYJPyKgJGjDOEEBpkpYVzTf4Uj5gMVXG2LoxIa8dk42m8JnBUBplNxrl0OBDmy6A6abkDkTsDOSG/9871KKkDn6i1LO2qjh5sBjW6h7kmvtPJNSX+/FGXzjjTtWb9Nx7VRNaFAtaFMCq82G60YQO1Yu6YGhQn1UXjHrm10wxxtYNm7H/nAyjc7oHHKDZqeSh2WMDAFgNyZ/wKdrctxwvhPW4AULJ3tvUWSTEqdE1Ct+vuQUAYNYreHnqq2iQrbi/5hu4Z/zGYV23pNMkSeD7o9fhr3J0iOZMaY1rOlYcvwAAMMHegZeqNib1jGJ/pwPf77ol/H2yBj9ReljXfIpJ75pR85vZyB3bf9n4vtzfWb05vMDTvoA56bnf2TYGu9pDv3dlVjdWTfwz3vVU47+OXgoAGJ3nxGvj38u65aMjc68lM3pNosm9KiRMfDUA3SfbIbJgIaeBsEgYBiEHUfWWQPekUpiuaeu34mHfL0pQ1eHp9kvhUcxJ/6Ox2zkaPxnk+TKjE9+1N2Rs19t4eweuKw6ty94p52FVwxyMt7fjmsL9mGT0Itr7oRWcnlWtxWvDT9tn4lrbHlxo1uOOis3Y1TsO75yYntD/l2heq/eDUoytkbN2etuRSAQCQ+Z+Q+c5qPeHbglsCeSnpFjsew9nIAc/azsfx3qLwo91+a14uP1cXJ53AAstmTfIcLy9A18r2os3W8+La7xAq+LB77rmRMyg2ubLC+f+vCQupjuc/+O2Q8Uo2waYjh6HnMUFAsAiYXiEgPm97Sg/OQ1HL8mBNSegOSubouqwuXlC+HufMKJXDcCqS/zAlpNu+6Ahc+S6cKutFnk687C77FNpnLUDf2cL3TPcJNfjHdMMTM1tOfVYfF21Lr8Z75+YhtHVXZiXcxLXWX0o0e/FO5iegJZHJyDr4QsYMWarB9JfdmfVJCoj3jByf7i7uN/CQcnM/Zm8QSM2nJwS8ZgnYML7J6bBPtaLhZa6pL5/LMZZO/Cd/Hbs9HSh03c655IkYJQUDHdgcpuiw3snz4no7j8z9+eZ65Gjl2HSKwgoqTv+qUJCr9+EvKM62FZuPSsmTWOREAVxsA4T/rUU9bdVwnTZ0GvIv9I4D3829+LZcW8BsCW/gWdo8+bi7w7/Lb7u2Ik77M0pfe9oleqtWD5xFaw6PYDkT2qUSr7tRah+uQFqSxsy77yOhiObcp8tfly6Cb6SyJK5RJ+403+zZMSvx/4Z2/1F+HntdSm7HNjRlYcpD3uAloPI3lEIkVgkREH4/ZDrG2D0VA5re0/AhKCix0feCpwMFiS5dZEUVYdOrxVOZfA7A2ySjPOKTuCYpxDtvemZ8lQv6VBuiG0Qplv1YWcgBw2+zFpytzdghLdmFEoOqeEltSk7xZJ7VUgIZHA3c7pzX5ykgZ1nKtXnokTvSvr79Gk9WgRrgx7iWE3WTpykhUVCkgUUPZ48fHW6mzGgamMenhn9KR7vnJDQ+7lTpV4WeODQzRl3d4HbacHUX+yH0tOT7qYQ9ZPtuc9E418PQv/hp2ddjyGLhBg4Pu5ET0sRWm7yo3iUe+gfoBHFu64UVQcCZ9XZBEWX+4Cix2Otl8MjJ3EEHWWWzO04iktmnX5lCXXPQeT/724onuy6zWgwOZKMXFMAUpKXfU00o6QizxiASZ85VwBLvvDB9P7nWbNePA1PNLlXVB0+a63C/k5HCloWu2zNfSbxBw3oclmhC55tfQghLBIIAHCHvRYvTV6BEqsn3U2JygSDBa9NXoWvlX+Z7qYQZZ1szX0m8e0qxMRlrdB9fiDdTUkKFgkxEoqK/H1GtB0sHnrjLGDVmVCiN8MgJfeM3KBTcUFJA2ZZEzOYTy/pUKzPhVXff6a2Wm8Z1vWGbkdLhdb2fDg/K4WxnZcZzlbMfWziyb1b9WFdrxFre81Y22uGU/XCrlMwr+QYHLn9ByamOvc6PyA3NUP4+x+DzgYsEmIkggE4frMVE//UC5Wz7Q1bnsmPX5R/lJKZKDc3T8BPD1+PJiU1BwvLgRyMfWgr1H0HU/J+lHrMfWziyf1JRcFPD1+Phw5dj58dvg71soQxhjw8Ub4TC4tr+m2f6tyf7ThwMU76wydg/e14nFhgQNGstnQ3Jy4G6LFszHrs9I7DquNzMm6q4VbFg8fbLkFQnJ4cZay5E/cUHB7wZxQh4bGWqxFQDUn7PN1uC4pXWTH2SPdZN7KZzn7pyn2X0ovH2+ehVzVBDxV3Fm/GZOPgt0aqQsJvmq/C9LyTzH2KsEiIk9LeAfO7HcivvBidVbmw27wZteZ6r2pCu+JBgc4y5BTNekmHBTlAjlSLVZiTsDYYdCpyjaGqfpTZCz0iQ6sIFV2qN/y9XZfTb5ZI56lFbP7SWg35jNsdT+bbsbSgdsD3FkLC7vbRifgYmpweC/ztFuR/cBBKtzNp70OZRQoq6OzOgzXXB6spmO7m9BNL7vOlGqw1T0dv0JSQWQqHyr1HqNjaNh4+2QBJErjC/iVKdJ0o0J+e28WpetGpnF6wSggJ+zsdcMlm5j5FWCQkSOmre+BYW4gDPy1BaUnm3Bu/rmkaNrdNxK8n/AnTTOlZcvn84gb8pGwTAEAnScjTRc6q2Kr04rt134BXNkInCfxi/GpcZI48SD3QdDkOdJdFFAiZoHiFFbaPDkFxZs7/OaXA7hpMvseO43dMAYYxC2OqxZL7mSYj/jjpf/BEx9x+0z3HYqjcn0kICY/XfQ0VuU4sH7cuvEAVc59+LBISRPV4IGQZ9u2VaK8yo3h6Zlx6CCh6KEJCMI3DT8w6OeLs4KsUAO6AOXxG8aH7HLjUOizMCUIv6aAIFW7ZBG+w/61n7qAZazyFONxblsRPcJo3YETgiwJIp04eq450QuniSp8jjZBlKO0dKNovo81QCgAIjBJZnfse1YdNvlK0+PMT0gat3B+X3djpD90W2iE7Iv74+2QD3GfMK5FJue/T2paPnEM5Iyr3LBISSPj9KH1mK4ovORfOc6R+q8bR0ISQsKZhNj7JnYBLJv4v9EMc5Dq91pTOaOn1mjDxt7VQ2kJ/DDLnwhKlQ86fP0Pln0P/VrM897WyEb88fE1SxyRs91Xg8cOL4n6dVOe+T05tDip/sXVE5Z5FwlnAbvbh+1Xr8bFrCjY2TR5y+7c9VqztngUAqMrpwA+KBr62ly7dPgv+5cQCXDnqS3w9NzMqdt/aUlTt7IV6ll+DpOyQzbm/repzVBi78MzRKyLGP2Rq7ov2hW5vLGjtHFEFAsAiISn0fgXt7TZYbX7k5ST/3lmzQcblFjd61Hp8YRmDHn/OoNfw6gKl2Nk2BgDQbbdAKayJGNyUIykosvSGLwGkQ0DRY3f7aFTldAIZcrAYdSQI3ZYvztbZVylO6cp9s9yMjRi6SEhn7hWholXphU8UodDSi/nWWlTo/Xj2K70umZp7w8YdAEZmz2FmjQY5S4gdX2Lq0sMQ2+0pfd+bcjvxx0lvYFx+Z1yvM91owsqJa3BZWeb1MBBlqnTlPlGSmfsO1Ys7jtyCA94K/GnSaswxxX/3BKUGi4RkUBUoPT0o2ROEa3Mp3L7kLfJyfkkjri4+AB10MEp65Elm6KTh17vdAQtec5XiQOD0LIF6SYc8XQ7MUvrXHqjtLcVKdwk6/elZxhoIDVbybCyFpSF1y85SFkpT7mORytx/7AP+xzUV7oAZqpCQp8sZ8rZM5j5z8HJDEpnf2Y6qT+w4OG1CUrofJUngO0WfYF6OHkBslXmn14pn6xbi/1RvxTTTycQ2MAoDLTBzoLMMBzpTO4L5TKqQYDqWg4pfjazBShQ75j4yz3/qmIvP2yqH3O5MzH3mYJGQpc4vacR3ij7BNFMAwMD3H/9VWR2+UbAdEw3p7zR6uqsKX7hCB4v59sO4w94MACjRm/HLia9jnWsm/tw4I51NjODymlHyghXFR9uQOWtM0kiWDbm3GIP4QfVaTDJ2AMiLeG5H51jc4c/HPY4NmGJk7rMBi4QkE4oKXYsZ7XobigsS121VZuo5dSZx+kDhVL1oVxQE1NP/raPN3XGdcSRSTa8jvHRumbkHOFUkmCUjLjIDtYHMuMccADq686C0m2HdXhe+3ZFouFKZey3pzL1Bp2KuuQsF+rx+z/kVAzr8uQgKHXOfJVgkJJnqcmHSf+yCb+FMeP8lue/1lrsS/11/KYIZNjtZNip9Mwf5b38BxedLd1MoC6Uy99lkbskx/LR0OwwZ+qeHue+Pf01SQPX5YKnrhO+9UvjeK0XvB6Xw+E1xvWaNuwwvOB1okt2n3wc6BBR9xi3MVN9biN85K3DSm5iZ3BKtfX8JejeUwuU1o7UtH773SpFf44TKAwXF4czct5woSMhrauU+0ebn1mLxmH2wGONbk+JI0N0v93qoMEvGIQcuphpzP7DMLOfOQsqhIyg7dAQAoC8qRM25VbCYgjHPzlbnLMLRnr/C1CknUZ7E/0VJEnEXHQ2uUVjuunjQbRShQklScaMM0bNS9pkK+/sHcHDWJBhPmFD2NAcrUWL05T74wHwo5aHfQ0kSSc+9AgmKUGP6Y7zQouIi8x581jVOc0rk4VAgsDfgwPJjodwPdBxJZ+7PZGw0M/cDYJGQBqqzB5N/5UPbhcXQ3Rz7dS8hJDzReA1W53bgUcf2BLYw5DujduCyvAP42bHr0e0b/BroYGYUNuPu0o14svlqHOoq7fd8u+LBAycXoSkJPQ3+oAH5/50PS1PvgNvoGo5CcXsw5UkPJG8XBytRwo1beQLKehsAoP1cW9Jz/0HrVOx1jcaDFe9igrH/2IBk6g0aseT49ZiW14znpq4AALQqeXi47rqI7dKd+zOVOFuY+wGwSEgDIcsQuw+gMO9c1B4v1NzGWuKBzTL07VMn3Xb4FCP2F8poCfafxKU9mIdDQQ+qDKbwympaWoP5OBSsRbXh9DLNYwx5KNQFYNLHFh+9TkWZ1YVpuU0412zG1NwW9AQsaOnNQ49sCd+j3aLYUNNdmpDlac/U0Z0HpdOMir1NkI8dH3C7vk+n7jmY0Pcn6iMfrQeOhv5daE5+7rt9FrgCZnhi7GbUSxLGWLuhCgntvbkYleNFnjGyXR2+3HBPQ45BRrHFjQ5fLgKKHgFFD5veh3PNobkiWpVu6HWnz9MbZTcOB/PTmvszsUAYGIuENJK27saUzzT+cOskHP3P84Hzhne20eWzYEnNNzWnC/6weRK2tE7Ac1NWYLpp4CLhvZPn4IOWKfjD5NdQnaAzD6sxiN+OW4NSvRWADj8oOoCmUTvw9zV/hx3tY/Ddjm8DAASi6xocrlEbLCh6ZQfkYCDhr00Uq1TkPl5myYjfVHyCT3xGPHDoJnyr4lN809YSsc2PW+dgU9NEAMCswhN4onwb/rV5LvZ3l0fkXsujLVdiV8do5j4LsEhIJyEgBvpFjiL5QkiQB7iuJ4QERUhQIKFd8WCVayp29oztt52i6hBE/7nJjZIef1v2Ofb2VmJz84ThN+qUHEkXvi6ql3QwSaFV8gZrc7zau2zI35yD0r3ugfcvUbqkIPdnGiz3nzrHQxUSbrXV9lvW2SjpMdHYg2+N/RxzchpglCIvOS7K34tCgwfvNU0Pb68/dQQ5M/cAkCcZcevoz+FXjXi6qwrHewsSViAoqg7ezcUwd4d2Xglzn1AsEjKUpEgIyHqYDInrCGtRdFhx/MLoBvRIenwnvx3bTC34S2t1v2PYUK8VhOg3gMogqQkZEAmEZkaTlcg2iDYzSn73GYTKTkTKLqnO/f5OB2q6S3H1tBoUaPT6jzHkYWlBPbTmZbjSouAC8x582NZ/camv5t6qM+Gf7Sfxhjs/IUtFn5n7gGzA2D93QNlfAyCqOouGgUVChprwhwb4q0vQtNQ/rGuUyTbLpOD3U1+NeOxT3zg8X7dgwJ/pDRpx19EbsaCoFssKjgEAinQWPD1xFd51T8fK+gvibldbkx1TfuuFFDx9UHX4WqCwQKAslGm5j4VW7hPtzNxLQoY4Up+U96Eo50l45JFHcOGFF8Jms6G0tBQ33ngjampqIrYRQuChhx5CRUUFLBYLFi5ciP3790ds4/f7sXTpUhQXFyM3Nxc33HADGhsb4/80ZxG5vgHm2mYoSvxdcqqQcDBQhoOB2OdCt+pMmGayRnzNNDdivL0DuSbtrj1F1eGk244T/tP3iOslHSYbc1FpjG+lSlVIaGksgPWICdhbA3XfwfCXcvhoXK9NkZj71Mm03MdCK/cJF9QB+2pDed9fw7kNkiiq38RNmzbh7rvvxrZt27B+/XrIsoxFixbB4/GEt3nsscfw5JNP4plnnsH27dvhcDhw9dVXw+U6PTXpsmXLsGbNGqxcuRJbtmyB2+3G4sWLoSg8+0sGISQ8deQq/OrI1QkdKDTHbMIr4z7ABUXDG0GcSIoqYcrvvBjzy08h5PSvVnk2Y+6zU7JyTyNLVJcb1q5dG/H98uXLUVpaih07dmDBggUQQuDXv/41fvSjH+Hmm28GALz00ksoKyvDihUrcOedd8LpdOKFF17AK6+8gquuugoA8Oqrr6KyshIbNmzANddck6CPlv3UbicK3hwLxRS626Dt0iDKyrtjeq1orv83ym78sWc2AMAoKfhH+wHYdf2vSR4NuvG6azZqXSWDvl6tqxSPmybgZtvumO7ZVoUE76YSWFpPX200qIC+sQ4yLyskHXOfWn2575qah1EXxLd+QDJyr+WYpwiPd07AEffpY0G8ue/T6cxF/gYrdGdEvbJbgZDjmxGShieuMQlOpxMAUFgYuuf36NGjaG5uxqJFpwemmM1mXHbZZdi6dSvuvPNO7NixA8FgMGKbiooKzJgxA1u3btU8WPj9fvj9p6/P9fT0xNPsrKF6PMhfsS38vXPCxfAWGWExJS8cPqFHs2LG6oZzIYQEk17BX+fth1VSwvMn9KmX8/HG8fOGfM1mjw1veM7DOZNPYIwhdGYZFIPfG60KCf6gAQKAquowZmMPxOf7IrZh/0F6MPfJ1Zd7499chO5ZoVslJSCm3BtOzU0gD6Mn4eQwc6+l1ZOHNzyRx4JYct8nqOgQPDV/gtyZg+JXd0L4s3OMRraLuUgQQuDee+/FJZdcghkzQst8NjeHVvUrK4u8BlZWVob6+vrwNiaTCQUFBf226fv5r3rkkUfwk5/8JNamnjUmPl+PwPgyNN8rIdec+Ft8FCHhB8duhiqk8BlIUNXh3qN/i3NHNeLnpXvjev2nj1+J3+tDf9q98uDTvbZ32DDlV72QfMHQLWMNJzlqOQMw96mT9+FB5B0InZkHy/Kjzr1ep+KBie9CD4GfHb4uqksO6cp9H++2Yoz7n9C8DFKgCXKAtzSmS8xFwpIlS7Bnzx5s2bKl33OSFNnFJYTo99hXDbbNAw88gHvvvTf8fU9PDyorK2NodXaTT5yEUZbhPzgB7rIgyhzdCX19cWp2Na3HDhtLsM2nYLzRhyKdBXsDQez3T4zq9Yea2rm9Ow9oygEAWDp0EDX7ofLsIaMw96mj9PQAp3pPjM7SmHJfaeiGfhjl9Rf+CrjUnPD3Wrkv1YeODU7ViwOB0AJ1PWo+gurgvQPR5L5P6WEVyqm1bii9YioSli5dirfffhsff/wxxowZE37c4XAACJ01lJeXhx9vbW0Nn2U4HA4EAgF0dXVFnFW0trZi/vz5mu9nNpthPjW950intLSi+odtcN06F+p3Uve+x5yF+H7PLbhn/EYszm3Cj+tvQofXOvQPRsGyy4qKX/0l/L0Q7DvIJMx9+iQz97Kqw6+OXA2g/xiGM3N/m60LALDLn4sHDt0U3ibe+U6+mvtTLxrXa1LiRDXkVQiBJUuWYPXq1di4cSOqq6sjnq+urobD4cD69evDjwUCAWzatCl8IJgzZw6MRmPENk1NTdi3b9+ABwv6CiEwancHpFeL0dI8KoVvK+H9rhl4tP1CuAPmhC1J3e22QF1VgorN7tDBoe+LMgJznyGizL0qJPyh4xL8vuNSqENkVZxxiVHruYG2j+cYMGDumf2MElVPwt13340VK1bgrbfegs1mC19LtNvtsFgskCQJy5Ytw8MPP4xJkyZh0qRJePjhh2G1WvGtb30rvO0dd9yB++67D0VFRSgsLMT999+PmTNnhkc909CUA7XIP3gYndPnwVNgSsoYBS0HOstwAIm777o3YIS/KwdFq/dBPeN2OcoczH3miCb3QkjY1jIuIe/rE0b0qqH38oj4V21k7rNHVEXCc889BwBYuHBhxOPLly/H7bffDgD4t3/7N3i9Xnzve99DV1cX5s6di3Xr1sFms4W3f+qpp2AwGHDLLbfA6/XiyiuvxIsvvgi9PrGrgZ31hMDEp+sQmFiOlvsFrEm86yFZ8l/JR+WnjZDd7nQ3hQbA3GeYNOT+lcZ5eN0Qeh+/Ev9Evcx99pBEFl747enpgd1ux0L8DQyDLH88UhjKHTj8vWoEKmKfRyFROp25QH1orIJqEMif3KU5D317lw26hhxM+GM31N0HUt1MOoMsgvgIb8HpdCI/P/6zxGRh7iNlYu7FWC+KRvX/w+8PGuA6VBCe64C5T69oMs+1G84CclMzxv1HM7r//mLgm+lti67Wiqr/DA1C0peV4vBvHCjM7+23nXmfBWMe2dpv1UkiGp5MzP3xB+cDc/sXCS63BZMfrYHSEZqOnbnPHiwSziJFn7Whp7cIJxbLKCtzpvS9PX4TrGvsGFt7uiAQzh5UvFIFxdz/FqjKBhfnPSBKgEzK/dj33ejdW9RvuzE+FWoPLy1kIxYJZxGl5jByDx2B8YJ56M4N/WE2meSkX7P0+E1wd1kxdl0d5OaW8OOqzwfzO9s1f4YFAlFiaOW+T67FD6M+OeftmrnftgcD3RjNzGcnFglnGyEw8YlDkIyha7Ynbx4PXNea1Le0rrZj7Po6yK3tSX0fIhrAV3IPADAY8OWPKlA2Nr4VVwfC3I8MLBLOQkp7R/jfBbVjcPLz/gswCQnQT3Mh3zr4EqtBRYeeL4ugH2Tiw7GHeyN6EIgo9c7MPQBAp8eoPWPR3do//3KuQOHUDugk7fN75p76sEg4yxnXfY6qdf0fl4wm1Dx9LvLHDl4kBGQDJv9XE+S6Y8lpIBElh6qg9Nmtmk9J501H208l6PTaRQJzT31YJIxQQg5i4mtBBG0Fg25nUQTUlvoUtYqIUkF3vAm2ZyeElpfUwNxTHxYJI5UQ0G3eheHMjM/blYjOLkpHJ8zvDT5WgbknIMq1G4iIiGjkYJFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkaaoioRHHnkEF154IWw2G0pLS3HjjTeipqYmYpvbb78dkiRFfM2bNy9iG7/fj6VLl6K4uBi5ubm44YYb0NjYGP+nIaKEY+6JRq6oioRNmzbh7rvvxrZt27B+/XrIsoxFixbB4/FEbPe1r30NTU1N4a9333034vlly5ZhzZo1WLlyJbZs2QK3243FixdDUZT4PxERJRRzTzRyGaLZeO3atRHfL1++HKWlpdixYwcWLFgQftxsNsPhcGi+htPpxAsvvIBXXnkFV111FQDg1VdfRWVlJTZs2IBrrrkm2s9AREnE3BONXHGNSXA6nQCAwsLCiMc/+ugjlJaWYvLkyfjud7+L1tbW8HM7duxAMBjEokWLwo9VVFRgxowZ2Lp1q+b7+P1+9PT0RHwRUXow90QjR8xFghAC9957Ly655BLMmDEj/Pi1116L1157DRs3bsQTTzyB7du344orroDf7wcANDc3w2QyoaCgIOL1ysrK0NzcrPlejzzyCOx2e/irsrIy1mYTURyYe6KRJarLDWdasmQJ9uzZgy1btkQ8fuutt4b/PWPGDFxwwQWoqqrCO++8g5tvvnnA1xNCQJIkzeceeOAB3HvvveHve3p6eMAgSgPmnmhkiaknYenSpXj77bfx4YcfYsyYMYNuW15ejqqqKtTW1gIAHA4HAoEAurq6IrZrbW1FWVmZ5muYzWbk5+dHfBFRajH3RCNPVEWCEAJLlizB6tWrsXHjRlRXVw/5Mx0dHWhoaEB5eTkAYM6cOTAajVi/fn14m6amJuzbtw/z58+PsvlElGzMPdHIFdXlhrvvvhsrVqzAW2+9BZvNFr6WaLfbYbFY4Ha78dBDD+HrX/86ysvLcezYMfzwhz9EcXExbrrppvC2d9xxB+677z4UFRWhsLAQ999/P2bOnBke9UxEmYO5Jxq5oioSnnvuOQDAwoULIx5fvnw5br/9duj1euzduxcvv/wyuru7UV5ejssvvxyrVq2CzWYLb//UU0/BYDDglltugdfrxZVXXokXX3wRer0+/k9ERAnF3BONXJIQQqS7EdHq6emB3W7HQvwNDJIx3c0hymqyCOIjvAWn05nR1/2Ze6LEiCbzXLuBiIiINMV8C2Q69XV+yAgCWdcPQpRZZAQBnM5VpmLuiRIjmsxnZZHgcrkAAFvw7hBbEtFwuVwu2O32dDdjQMw9UWINJ/NZOSZBVVXU1NTgnHPOQUNDQ0ZfR80GfZPUcF/GJ1v3oxACLpcLFRUV0Oky9wokc5842fq7momycV9Gk/ms7EnQ6XQYPXo0AHCSlQTivkyMbNyPmdyD0Ie5Tzzux8TJtn053Mxn7mkDERERpRWLBCIiItKUtUWC2WzGgw8+CLPZnO6mZD3uy8Tgfkw+7uPE4H5MnLN9X2blwEUiIiJKvqztSSAiIqLkYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpytoi4dlnn0V1dTVycnIwZ84cbN68Od1NymgPPfQQJEmK+HI4HOHnhRB46KGHUFFRAYvFgoULF2L//v1pbHHm+Pjjj3H99dejoqICkiThzTffjHh+OPvO7/dj6dKlKC4uRm5uLm644QY0Njam8FNkP2Y+esx9bJj507KySFi1ahWWLVuGH/3oR9i1axcuvfRSXHvttTh+/Hi6m5bRpk+fjqampvDX3r17w8899thjePLJJ/HMM89g+/btcDgcuPrqq8OL6oxkHo8Hs2fPxjPPPKP5/HD23bJly7BmzRqsXLkSW7ZsgdvtxuLFi6EoSqo+RlZj5mPH3EePmT+DyEIXXXSRuOuuuyIemzp1qvj3f//3NLUo8z344INi9uzZms+pqiocDod49NFHw4/5fD5ht9vF888/n6IWZgcAYs2aNeHvh7Pvuru7hdFoFCtXrgxvc+LECaHT6cTatWtT1vZsxszHhrmP30jPfNb1JAQCAezYsQOLFi2KeHzRokXYunVrmlqVHWpra1FRUYHq6mrcdtttqKurAwAcPXoUzc3NEfvUbDbjsssu4z4dwnD23Y4dOxAMBiO2qaiowIwZM7h/h4GZjw9zn1gjLfNZVyS0t7dDURSUlZVFPF5WVobm5uY0tSrzzZ07Fy+//DLef/99/P73v0dzczPmz5+Pjo6O8H7jPo3ecPZdc3MzTCYTCgoKBtyGBsbMx465T7yRlvmsXCoaACRJivheCNHvMTrt2muvDf975syZuPjiizFhwgS89NJLmDdvHgDu03jEsu+4f6PD38/oMffJM1Iyn3U9CcXFxdDr9f2qsdbW1n6VHQ0sNzcXM2fORG1tbXi0M/dp9Iaz7xwOBwKBALq6ugbchgbGzCcOcx+/kZb5rCsSTCYT5syZg/Xr10c8vn79esyfPz9Nrco+fr8fBw4cQHl5Oaqrq+FwOCL2aSAQwKZNm7hPhzCcfTdnzhwYjcaIbZqamrBv3z7u32Fg5hOHuY/fiMt8+sZMxm7lypXCaDSKF154QXz55Zdi2bJlIjc3Vxw7dizdTctY9913n/joo49EXV2d2LZtm1i8eLGw2Wzhffboo48Ku90uVq9eLfbu3Su++c1vivLyctHT05Pmlqefy+USu3btErt27RIAxJNPPil27dol6uvrhRDD23d33XWXGDNmjNiwYYPYuXOnuOKKK8Ts2bOFLMvp+lhZhZmPDXMfG2b+tKwsEoQQ4re//a2oqqoSJpNJnH/++WLTpk3pblJGu/XWW0V5ebkwGo2ioqJC3HzzzWL//v3h51VVFQ8++KBwOBzCbDaLBQsWiL1796axxZnjww8/FAD6ff3DP/yDEGJ4+87r9YolS5aIwsJCYbFYxOLFi8Xx48fT8GmyFzMfPeY+Nsz8aZIQQqSnD4OIiIgyWdaNSSAiIqLUYJFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkSYWCURERKSJRQIRERFpYpFAREREmlgkEBERkab/D4cP66qs0DTQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice scores:\n",
      "csf: 0.9530\n",
      "gm: 0.9565\n",
      "wm: 0.9464\n",
      "Average: 0.9520\n",
      "\n",
      "Hausdorff distance:\n",
      "csf: 11.9164\n",
      "gm: 8.7750\n",
      "wm: 7.8740\n",
      "Average: 9.5218\n",
      "\n",
      "Average Volumetric Difference (AVD) per label:\n",
      "Label csf: 0.0083\n",
      "Label gm: 0.0001\n",
      "Label wm: 0.0402\n",
      "Average: 0.0162\n",
      "\n",
      "##################################################\n",
      "AVERAGE SCORES FOR ALL CASES\n",
      "##################################################\n",
      "\n",
      "Average dice scores for all cases:\n",
      "csf: 0.9220\n",
      "gm: 0.9539\n",
      "wm: 0.9502\n",
      "Total average: 0.9420\n",
      "\n",
      "Average Hausdorff scores for all cases:\n",
      "csf: 9.3792\n",
      "gm: 7.9003\n",
      "wm: 6.7212\n",
      "Total average: 8.0003\n",
      "\n",
      "Average AVD scores for all cases:\n",
      "csf: 0.0561\n",
      "gm: 0.0185\n",
      "wm: 0.0369\n",
      "Total average: 0.0372\n"
     ]
    }
   ],
   "source": [
    "PREDICTIONS_DIR = \"D:\\\\Files\\\\Projects\\\\MAIA\\\\UdG\\\\MISA\\\\Project\\\\nnunetv2\\\\nnUNet_trained_models\\\\Dataset001_dataset\\\\ensemble_predictions_postprocessed\"\n",
    "ORIG_VAL_DIR = \"D:\\\\Files\\\\Projects\\\\MAIA\\\\UdG\\\\MISA\\\\Project\\\\nnunetv2\\\\dataset\\\\Validation_Set\"\n",
    "\n",
    "val_cases = [13, 14, 15, 16, 17]\n",
    "val_cases_orig = [11, 12, 13, 14, 17]\n",
    "\n",
    "dice_scores = []\n",
    "hd_scores = []\n",
    "avd_scores = []\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for case_pred, case_orig in zip(val_cases, val_cases_orig):\n",
    "    pred_labels = sitk.ReadImage(\n",
    "        os.path.join(PREDICTIONS_DIR, f\"case_{case_pred}.nii.gz\")\n",
    "    )\n",
    "    \n",
    "    gt_labels = sitk.ReadImage(\n",
    "        os.path.join(ORIG_VAL_DIR, f\"IBSR_{case_orig}\", f\"IBSR_{case_orig}_seg.nii.gz\")\n",
    "    )\n",
    "    \n",
    "    y_pred.append(sitk.GetArrayFromImage(pred_labels).transpose(2, 1, 0))\n",
    "    y_true.append(sitk.GetArrayFromImage(gt_labels).transpose(2, 1, 0))\n",
    "    \n",
    "    # plot the data\n",
    "    plt.figure(\"check\", (6, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"prediction\")\n",
    "    plt.imshow(sitk.GetArrayFromImage(pred_labels).transpose(0, 2, 1)[150, :, :])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"ground truth\")\n",
    "    plt.imshow(sitk.GetArrayFromImage(gt_labels).transpose(0, 2, 1)[150, :, :])\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate the dice score\n",
    "    dice_score = calculate_dice_score(\n",
    "        sitk.GetArrayFromImage(pred_labels),\n",
    "        sitk.GetArrayFromImage(gt_labels)\n",
    "    )\n",
    "    dice_scores.append(dice_score)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # calculate the Hausdorff distance\n",
    "    hd_score = calculate_hausdorff_distance(\n",
    "        sitk.GetArrayFromImage(pred_labels),\n",
    "        sitk.GetArrayFromImage(gt_labels)\n",
    "    )\n",
    "    hd_scores.append(hd_score)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # calculate the volumetric difference    \n",
    "    avd_score = calculate_avd(\n",
    "        sitk.GetArrayFromImage(pred_labels),\n",
    "        sitk.GetArrayFromImage(gt_labels)\n",
    "    )\n",
    "    avd_scores.append(avd_score)\n",
    "\n",
    "print()\n",
    "print(\"#\" * 50)\n",
    "print(\"AVERAGE SCORES FOR ALL CASES\")\n",
    "print(\"#\" * 50)\n",
    "\n",
    "# calculate the average for csf, gm, and wm for dice scores\n",
    "print(\"\\nAverage dice scores for all cases:\")\n",
    "average_dice_score = {}\n",
    "for key in dice_scores[0].keys():\n",
    "    average_dice_score[key] = np.mean([dice_score[key] for dice_score in dice_scores])\n",
    "for key, value in average_dice_score.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(f\"Total average: {np.mean(list(average_dice_score.values())):.4f}\")\n",
    "\n",
    "# calculate the average for csf, gm, and wm for Hausdorff scores\n",
    "print(\"\\nAverage Hausdorff scores for all cases:\")\n",
    "average_hd_score = {}\n",
    "for key in hd_scores[0].keys():\n",
    "    average_hd_score[key] = np.mean([hd_score[key] for hd_score in hd_scores])\n",
    "for key, value in average_hd_score.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(f\"Total average: {np.mean(list(average_hd_score.values())):.4f}\")\n",
    "\n",
    "# calculate the average for csf, gm, and wm for AVD scores\n",
    "print(\"\\nAverage AVD scores for all cases:\")\n",
    "average_avd_score = {}\n",
    "for key in avd_scores[0].keys():\n",
    "    average_avd_score[key] = np.mean([avd_score[key] for avd_score in avd_scores])\n",
    "for key, value in average_avd_score.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(f\"Total average: {np.mean(list(average_avd_score.values())):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dice_orig = pd.DataFrame(dice_scores)\n",
    "df_dice = df_dice_orig.copy()\n",
    "df_dice['average'] = df_dice_orig.mean(axis=1)\n",
    "df_dice['std'] = df_dice_orig.std(axis=1)\n",
    "\n",
    "df_hausdorff_orig = pd.DataFrame(hd_scores)\n",
    "df_hausdorff = df_hausdorff_orig.copy()\n",
    "df_hausdorff['average'] = df_hausdorff_orig.mean(axis=1)\n",
    "df_hausdorff['std'] = df_hausdorff_orig.std(axis=1)\n",
    "\n",
    "df_avd_orig = pd.DataFrame(avd_scores)\n",
    "df_avd = df_avd_orig.copy()\n",
    "df_avd['average'] = df_avd_orig.mean(axis=1)\n",
    "df_avd['std'] = df_avd_orig.std(axis=1)\n",
    "\n",
    "df_scores = pd.concat([df_dice, df_hausdorff, df_avd], axis=1)\n",
    "df_scores.index = val_cases_orig\n",
    "\n",
    "# rename the columns\n",
    "df_scores.columns = [\n",
    "    'dice_csf', 'dice_gm', 'dice_wm', 'dice_average', 'dice_std',\n",
    "    'hd', 'hd_gm', 'hd_wm', 'hd_average', 'hd_std',\n",
    "    'avd_csf', 'avd_gm', 'avd_wm', 'avd_average', 'avd_std'\n",
    "]\n",
    "\n",
    "# export to excel\n",
    "df_scores.to_excel('results/exp04_scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_slice = 150\n",
    "\n",
    "# Create subplots with 3 rows and the number of columns based on the number of images\n",
    "fig, axes = plt.subplots(2, len(y_true), figsize=(12, 8))\n",
    "\n",
    "# Add vertical text to indicate original, prediction, and ground truth\n",
    "axes[0, 0].text(-20, 128, \"Prediction\", rotation=\"vertical\", va=\"center\", ha=\"center\", fontsize=12)\n",
    "axes[1, 0].text(-20, 128, \"Ground Truth\", rotation=\"vertical\", va=\"center\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# Plot predictions\n",
    "for i in range(len(y_pred)):\n",
    "    axes[0, i].imshow(y_pred[i][:, :, n_slice])\n",
    "    axes[0, i].set_title(f\"Case {val_cases_orig[i]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Plot ground truth\n",
    "for i in range(len(y_true)):\n",
    "    axes[1, i].imshow(y_true[i][:, :, n_slice])\n",
    "    axes[1, i].set_title(f\"Case {val_cases_orig[i]}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
